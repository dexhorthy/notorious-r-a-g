[
    {
        "author": "hellovai",
        "content": "BAML editor UI component",
        "timestamp": "2024-10-12 15:03:17.382000+00:00",
        "id": 1294676776219578413,
        "parent_id": null
    },
    {
        "author": ".barelyexisting",
        "content": "Hey, I am creating a website where i allow the user to input the baml to take the structure. can you guide me on how I can have an editor like the one on promptfiddle",
        "timestamp": "2024-10-12 09:34:49.813000+00:00",
        "id": 1294594116683956244,
        "parent_id": null
    },
    {
        "author": "danbecker",
        "content": "I'm tinkering with asking for responses as JSON arrays to save the response tokens the LLM otherwise spends repeating back object keys.  It would has a system prompt like\n```\nExtract from this content:\nRespond with a CSV string. The elements and types should be objects with the following fields:\n- company_name: string\n- is_remote: bool\n- city: string\n- job_title: string\n- salary: int\nLeave missing items blank (can have consecutive commas).\n  Don't use quotes around strings and don't give field names. Just values.\n```\n\nIt cuts response tokens in half on the examples I've tried, and the responses make sense just eyeballing them.\n\nI haven't thought much about how to handle arrays in responses... or whether we'd get more errors by relying on ordering instead of explicit keys/names.\n\nDoes baml have a mode for deserializing arrays rather than objects... have others here tinkered with something like this?",
        "timestamp": "2024-10-11 19:37:41.061000+00:00",
        "id": 1294383442028396574,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "vim syntax",
        "timestamp": "2024-10-10 15:00:45.754000+00:00",
        "id": 1293951364514320471,
        "parent_id": null
    },
    {
        "author": "kirilligum",
        "content": "is there a vim syntax support?",
        "timestamp": "2024-10-10 06:39:56.539000+00:00",
        "id": 1293825328971649036,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Auto generated baml functions",
        "timestamp": "2024-10-10 03:25:43.365000+00:00",
        "id": 1293776452017197152,
        "parent_id": null
    },
    {
        "author": "demontrius",
        "content": "<@99252724855496704>/ <@201399017161097216> is there something in the works for autogenerated baml functions... I have a use case for restructuring unstructured data like LangChain's LLMGraphTransformer and wondering if baml can be used for something like this",
        "timestamp": "2024-10-10 02:46:27.474000+00:00",
        "id": 1293766570694410311,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "Hi, I'm starting to see this issue and I'm not sure what I did wrong:",
        "timestamp": "2024-10-09 23:15:10.742000+00:00",
        "id": 1293713400626413639,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "bug",
        "timestamp": "2024-10-09 13:07:12.753000+00:00",
        "id": 1293560400851370004,
        "parent_id": null
    },
    {
        "author": "brandburner",
        "content": "what am I doing wrong here:\n\n`retry_policy Backoff {\nmax_retries 3\nstrategy {\n  type exponential_backoff\n  delay_ms 600\n  multiplier 1.5\n  max_delay_ms 6000\n}\n\nclient<llm> GPT4o {\n  provider openai\n  retry_policy Backoff\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}`\n\n~/JsonToDocStyler$ baml-cli generate\n[2024-10-09T10:12:33Z ERROR baml_runtime::cli::generate] Error generating clients: Failed to build BAML runtime\n    \n    Caused by:\n        error: Error validating RetryPolicy \"Backoff\": This field declaration is invalid. It is either missing a name or a type.\n          -->  ./baml_src/clients.baml:12\n           | \n        11 | \n        12 | client<llm> GPT4o {\n           | \n        error: Error validating: This line is not a valid field or attribute definition. A valid property may look like: 'myProperty \"some value\"' for example, with no colons.\n          -->  ./baml_src/clients.baml:12\n           | \n        11 | \n        12 | client<llm> GPT4o {\n        13 |   provider openai\n           | \n        error: Property not known: \"provider\". Did you mean one of these: \"options\", \"strategy\", \"max_retries\"?\n          -->  ./baml_src/clients.baml:13\n           | \n        12 | client<llm> GPT4o {\n        13 |   provider openai\n           | \n        error: Property not known: \"retry_policy\". Did you mean one of these: \"strategy\", \"options\", \"max_retries\"?\n          -->  ./baml_src/clients.baml:14\n           | \n        13 |   provider openai\n        14 |   retry_policy Backoff\n           | \n        \n        \n`Traceback (most recent call last):\n  File \"/home/runner/JsonToDocStyler/.pythonlibs/bin/baml-cli\", line 8, in <module>\n    sys.exit(invoke_runtime_cli())\n             ^^^^^^^^^^^^^^^^^^^^\nbaml_py.BamlError: Failed to build BAML runtime`",
        "timestamp": "2024-10-09 10:13:58.933000+00:00",
        "id": 1293516806010568705,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "class alias",
        "timestamp": "2024-10-08 22:33:19.259000+00:00",
        "id": 1293340478837620896,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "there's no support for aliasing classes right now right? just fields?",
        "timestamp": "2024-10-08 22:07:51.103000+00:00",
        "id": 1293334069287059486,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "getting raw json",
        "timestamp": "2024-10-08 13:33:26.282000+00:00",
        "id": 1293204612844617856,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "dynamic types",
        "timestamp": "2024-10-08 13:32:28.204000+00:00",
        "id": 1293204369247834257,
        "parent_id": null
    },
    {
        "author": "aethrvmn_32073",
        "content": "Hey, the baml output displays a JSON as the Parsed Response which is then converted into a python object, is there a way to save the JSON or do we have to manipulate the object afterwards?",
        "timestamp": "2024-10-08 13:22:23.318000+00:00",
        "id": 1293201832172060767,
        "parent_id": null
    },
    {
        "author": "goalpha22",
        "content": "Hi - I have a usecase where we want to extract specific fields from a document. Our user can define the schema they want, in a JSON format. Any pointers on how to do this with BAML?",
        "timestamp": "2024-10-08 12:56:52.600000+00:00",
        "id": 1293195411875696671,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "output from CURL:\n\n`{\"id\":\"chatcmpl-MmJJ9RuMoxE8nkA4Mn9wTF\",\"object\":\"chat.completion\",\"created\":1728346255,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The source of this article is CNN.\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":3167,\"total_tokens\":3175,\"completion_tokens\":8}}(base)`",
        "timestamp": "2024-10-08 00:11:43.862000+00:00",
        "id": 1293002856672006207,
        "parent_id": 1293002431847858246
    },
    {
        "author": "arindamkhaled4530",
        "content": "Hi! I'm tried running  unit test(s) from the prompt-shepards model using Llama3.1 70B-instruct but am getting the following error:\nFailed to coerce value: <root>: Expected Source enum value, got String(\"\").",
        "timestamp": "2024-10-08 00:10:02.576000+00:00",
        "id": 1293002431847858246,
        "parent_id": null
    },
    {
        "author": "nathan9086",
        "content": "I was gonna serve an mmlm w LMDeploy instead of vLLM, but  having trouble getting structured output and I stumbled on baml",
        "timestamp": "2024-10-07 22:41:27.591000+00:00",
        "id": 1292980139184885761,
        "parent_id": null
    },
    {
        "author": "nathan9086",
        "content": "Any plan to integrate with LMDeploy soon?",
        "timestamp": "2024-10-07 22:40:15.995000+00:00",
        "id": 1292979838889492481,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "vllm with images",
        "timestamp": "2024-10-07 17:24:54.288000+00:00",
        "id": 1292900475498139699,
        "parent_id": null
    },
    {
        "author": "xyan4330",
        "content": "Hi! I am trying to run some few shot using openbmb/MiniCPM-V-2_6 model served with vLLM. When I run the few-shot example using BAML playground it works well since the raw response includes the actual request to LLM and the few shot example, however the parsed LLM response just shows the first, thus the few shot example parsed, but not the response from the real request. Going back, my problem is that when I use the FastAPI endpoint to send requests using the url of the image or the loaded image in base64 using the BAML Image type as `Image.from_base64(media_type, image_base64)` in order to send the image part of the prompt I get the following error  `ERROR 10-07 14:37:42 serving_chat.py:161] Error in loading multi-modal data: Invalid 'image_url': A valid 'image_url' must start with either 'data:image' or 'http'` I had already set the vllm serve to accept more images, as the BAML playground works just fine. Not sure why this is happening and unfortunately cannot find any relevant information about this to keep trying stuff ðŸ˜¦\nThank you in advance!",
        "timestamp": "2024-10-07 16:26:58.568000+00:00",
        "id": 1292885897271967754,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Recursion",
        "timestamp": "2024-10-06 01:02:37.911000+00:00",
        "id": 1292290890550874177,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "does BAML support recursive types or mutually recursive types?",
        "timestamp": "2024-10-05 18:40:39.894000+00:00",
        "id": 1292194765420499025,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "is this a vscode error \"Function 'baml::Chat' expects 1 arguments, but got 2baml\" when i do {{ _.role(\"user\", cache_control={\"type\": \"ephemeral\"}) }}",
        "timestamp": "2024-10-04 16:18:20.748000+00:00",
        "id": 1291796561780805835,
        "parent_id": null
    },
    {
        "author": ".alex4o",
        "content": "Tokenize",
        "timestamp": "2024-10-03 20:05:24.803000+00:00",
        "id": 1291491317343457336,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does anyone know of a good way to programmatically determine which content should fit in a context window? For example, I know Cursor was doing some interesting stuff with re-ranking the prompt inputs dynamically. Basically trying to prevent the situation where the input exceeds the 128k context window for gpt-4o",
        "timestamp": "2024-10-03 18:19:27.705000+00:00",
        "id": 1291464653741883485,
        "parent_id": null
    },
    {
        "author": ".alex4o",
        "content": "Is it possible to call a BAML function directly from BAML instead of going through TypeScript I can't seem to find it in the docs",
        "timestamp": "2024-10-03 00:09:10.576000+00:00",
        "id": 1291190274416050258,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "that didn't seem to stop it -- i probably am not doing it right",
        "timestamp": "2024-10-02 22:35:32.954000+00:00",
        "id": 1291166712401756182,
        "parent_id": 1291162943513301105
    },
    {
        "author": "arindamkhaled4530",
        "content": "thanks!",
        "timestamp": "2024-10-02 22:20:41.623000+00:00",
        "id": 1291162973888450570,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "set BAML_LOG=warn",
        "timestamp": "2024-10-02 22:20:34.381000+00:00",
        "id": 1291162943513301105,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "is there a way to stop BAML logging (in the notebook). I upgraded to the version 0.57.1 and am noticing the logs/outputs are being printed:",
        "timestamp": "2024-10-02 22:20:17.642000+00:00",
        "id": 1291162873304846358,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "any recommendations if I want to track all my LLM calls from baml and store them somewhere (input / output for each LLM api call) and also tagging of calls",
        "timestamp": "2024-10-02 15:32:44.893000+00:00",
        "id": 1291060311041769614,
        "parent_id": null
    },
    {
        "author": "try_another",
        "content": "Any tips on multi-label classification with BAML?",
        "timestamp": "2024-10-02 15:25:03.052000+00:00",
        "id": 1291058373940346991,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "I asked about how to deal with",
        "timestamp": "2024-10-02 04:57:23.504000+00:00",
        "id": 1290900418347663386,
        "parent_id": null
    },
    {
        "author": "flyingaudio",
        "content": "Is BAML a LangChain replacement?",
        "timestamp": "2024-10-02 04:55:44.586000+00:00",
        "id": 1290900003455242292,
        "parent_id": null
    },
    {
        "author": "simontam0",
        "content": "I asked about how to deal with structured responses in history before but not sure i was clear. I'll try agan. Presently w/o BAML we are asking the LLM to process an array of chat messages, some are from user, AIMessage (from the LLM) and some may be a FunctionMessage (from a tool call).. How should we formulate our prompt function to include/translate these messages to make it usable in BAML and to the LLM, specifically how might I convert/clean the FunctionMessage's  additional_kwargs[\"function_call\"][\"arguments\"] (which is the json output from a tool call). to something usable for BAML? When I was trying it out in the playground just pasting the json into a string didn't work well. Here's an example json string:       'assistant {\"is_outbound_flight_choices\": true, \"indicies_of_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59]}'.\n\nI believe another way to think about it is if I wanted BAML to extract data from a json object how should I do that in the prompt function? Does the JSON have to be \"cleaned\" in some way for BAML? The test cases I was using that worked were very basic json, not the same kind of json output I'm getting from json.dumps.",
        "timestamp": "2024-10-02 04:35:10.795000+00:00",
        "id": 1290894828560842815,
        "parent_id": null
    },
    {
        "author": "joatmon.pockets",
        "content": "recursive types",
        "timestamp": "2024-10-01 20:34:25.543000+00:00",
        "id": 1290773842805133448,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "BAML still doesn't support infinite nesting of fields right? like if i had a \n\nclass Element {\n  children Element[]\n}",
        "timestamp": "2024-10-01 17:58:19.315000+00:00",
        "id": 1290734557997437018,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Parens highlighting",
        "timestamp": "2024-10-01 15:58:58.959000+00:00",
        "id": 1290704525287624839,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "I've noticed that within a #\"\"# block, if there are any sections surrounded by parenthesis (sample text), the text after the ) is not highlighted correctly in vscode",
        "timestamp": "2024-10-01 04:58:53.120000+00:00",
        "id": 1290538406358814843,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "client graph",
        "timestamp": "2024-09-30 18:58:06.405000+00:00",
        "id": 1290387215477903509,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "What is Client Graph on promptfiddle supposed to represent?",
        "timestamp": "2024-09-30 16:35:49.932000+00:00",
        "id": 1290351410914791506,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Guys, is there a way to generate models on the fly using JSON objects? My goal is to generate fields in different languages to improve translations. I've noticed that when you describe the object name very well, the translation ends up being more accurate, even with simpler models",
        "timestamp": "2024-09-30 14:49:03.150000+00:00",
        "id": 1290324538923548774,
        "parent_id": null
    },
    {
        "author": "kirilligum",
        "content": "have you tried to compare baml (json) to code4struct approach where the ouput is a python code?",
        "timestamp": "2024-09-30 13:03:33.385000+00:00",
        "id": 1290297989965021195,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Input output tokens",
        "timestamp": "2024-09-30 05:20:54.422000+00:00",
        "id": 1290181560435212385,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Inputs rendering with aliases",
        "timestamp": "2024-09-30 05:19:38.238000+00:00",
        "id": 1290181240896491536,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "I'm looking at the symbol tuning example on promptfiddle\n\nIt looks like for the MyClass output type, the parser will convert from the alias (k4) to to the non-alias category (AccountIssue)\n\nHowever, if i wanted to pass that output into a 2nd function, one that has input MyClass and output MyClass, for example a function that validates the output of ClassifyMessageWithSymbol and returns the correct MyClass, would I have to convert the non-alias category (AccountIssue) back into the alias (k4) form for the input of the 2nd function, given that the context only sees the aliases because of the below.\n\nAnswer with any of the categories:\nMyClass\n----\n- k1: Customer wants to refund a product\n- k2: Customer wants to cancel an order\n- k3: Customer needs help with a technical issue unrelated to account creation or login\n- k4: Specifically relates to account-login or account-creation\n- k5: Customer has a question",
        "timestamp": "2024-09-30 03:54:54.771000+00:00",
        "id": 1290159919290650674,
        "parent_id": null
    },
    {
        "author": "saurabhj80",
        "content": "How can I know input and output tokens after I am done calling the baml function?",
        "timestamp": "2024-09-30 03:38:31.562000+00:00",
        "id": 1290155795413205086,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Default params",
        "timestamp": "2024-09-29 02:13:10.659000+00:00",
        "id": 1289771928923406367,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "did support for default parameters get added?",
        "timestamp": "2024-09-28 22:16:55.709000+00:00",
        "id": 1289712474874052690,
        "parent_id": null
    },
    {
        "author": "roeybc",
        "content": "Might have missed it in the docs - any way to get a template string as an input? I 'm looking into loading strings that have working baml code (taking into account that all inputs are valid).\nI'd use jinja, but I'm using typescript.",
        "timestamp": "2024-09-28 00:54:33.973000+00:00",
        "id": 1289389757842849886,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "If Iâ€™m using chain of thought and asking LLM to explain its thoughts, it looks like I can just add a â€œCoT stringâ€ output to my output schema to make the parsing easier. More a general question but in this case i should have the CoT output first in the output schema so that it works correctly?",
        "timestamp": "2024-09-24 16:36:13.873000+00:00",
        "id": 1288177184137150598,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "retry policy",
        "timestamp": "2024-09-23 20:05:49.975000+00:00",
        "id": 1287867544266346552,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, i'm getting one issue with:\n\nretry_policy PolicyToRetry {\n  max_retries 8\n  strategy {\n    type exponential_backoff\n    delay 5000\n    multiplier 2\n  }\n}\n\nstrategy using azure, is there a quick fix?",
        "timestamp": "2024-09-23 19:44:47.811000+00:00",
        "id": 1287862250366963764,
        "parent_id": null
    },
    {
        "author": "gggooo",
        "content": "Is boundary ml better than openai's structured outputs?",
        "timestamp": "2024-09-23 19:28:17.874000+00:00",
        "id": 1287858098270371891,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Is there a way currently to have @@",
        "timestamp": "2024-09-23 17:16:58.484000+00:00",
        "id": 1287825049713119344,
        "parent_id": null
    },
    {
        "author": "davidyoung",
        "content": "Ignore, just saw you answered above ðŸ™‚",
        "timestamp": "2024-09-23 16:53:40.929000+00:00",
        "id": 1287819187942850682,
        "parent_id": null
    },
    {
        "author": "davidyoung",
        "content": "Is there a way currently to have @@dynamic in an enum, and fill that enum in a test case?",
        "timestamp": "2024-09-23 16:52:35.540000+00:00",
        "id": 1287818913681248256,
        "parent_id": null
    },
    {
        "author": ".alex4o",
        "content": "While you guys are answering questions let me ask something, here I pass an argument in the test but it shows up as null in the user prompt (line 2)",
        "timestamp": "2024-09-23 16:52:19.030000+00:00",
        "id": 1287818844433420391,
        "parent_id": null
    },
    {
        "author": ".alex4o",
        "content": "Quck question I see that my teemmate uses |tojson() in our baml config but I can't seem to find any docs for it? Also do you support other formats like yaml",
        "timestamp": "2024-09-23 16:40:57.226000+00:00",
        "id": 1287815984740175915,
        "parent_id": null
    },
    {
        "author": "tdn8",
        "content": "When doing function/tool calling with",
        "timestamp": "2024-09-23 15:15:30.731000+00:00",
        "id": 1287794482661687301,
        "parent_id": null
    },
    {
        "author": "simontam0",
        "content": "When doing function/tool calling with BAML if you happen to have more than 1 class that may look similar to another is there a way to better guide the LLM into which function/tool it should call? Would this be in the name of the function, prompt itself, the docstring for the class or maybe an added member in the class to give it a hint?",
        "timestamp": "2024-09-23 14:57:52.447000+00:00",
        "id": 1287790043896873010,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Schema robustness",
        "timestamp": "2024-09-23 13:28:02.503000+00:00",
        "id": 1287767436833067054,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I'm probably repeating myself but what is the best way to add some silly robustness to schema failures? Eg. Assume one output was an invalid enum value, can we return \"None\" or smth like that vs the request failing comlletely and getting no values?",
        "timestamp": "2024-09-23 07:20:53.310000+00:00",
        "id": 1287675039700881450,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Guys is there a way to create subfolders that have works like dependencies for a series of calls? for example, 1 -> 2 -> 3 (in an orderly manner?) Or it should be done programmatically?",
        "timestamp": "2024-09-22 13:54:59.902000+00:00",
        "id": 1287411832830689382,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "validations",
        "timestamp": "2024-09-22 13:25:41.727000+00:00",
        "id": 1287404458510127119,
        "parent_id": null
    },
    {
        "author": "seawatts",
        "content": "Has anyone setup baml with a Chrome Extension yet? I'm trying to get it going with Vite and CRXjs  https://crxjs.dev/vite-plugin <@201399017161097216> <@99252724855496704>",
        "timestamp": "2024-09-22 13:21:53.299000+00:00",
        "id": 1287403500413784065,
        "parent_id": null
    },
    {
        "author": "philosopherstone",
        "content": "Hi, I use BAML for vision tasks which mostly involves extracting product information along with their price - is there a way to ensure that the extracted price is valid? I'm assuming that LLM can hallucinate and come up with a number ðŸ˜“",
        "timestamp": "2024-09-22 09:33:15.353000+00:00",
        "id": 1287345963178131478,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Is it possible to run tests within",
        "timestamp": "2024-09-22 02:52:38.309000+00:00",
        "id": 1287245144508076087,
        "parent_id": null
    },
    {
        "author": "gitzalytics",
        "content": "Is it possible to run tests within playground on dynamically created enums?",
        "timestamp": "2024-09-21 22:08:55.561000+00:00",
        "id": 1287173745928175749,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Would it be possible to expose the underlying LLM Error upfront. I see LLM error followed by the input text which is thousands of tokens longâ€¦ makes it tricky to see what the actual model error was (I assumed rate limiting?)",
        "timestamp": "2024-09-21 19:50:15.301000+00:00",
        "id": 1287138848228507729,
        "parent_id": null
    },
    {
        "author": "charizard_98",
        "content": "How do I use gpt-4o-2024-08-06? \"gpt-4o\" right now defaults to gpt-4o-2024-05-13.",
        "timestamp": "2024-09-21 00:15:36.581000+00:00",
        "id": 1286843239051300937,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Error message",
        "timestamp": "2024-09-20 11:35:17.688000+00:00",
        "id": 1286651899545653280,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Guys is there a way when using azure openai and hitting the rate limits per minute to know what time should i wait from the errors? I'm using client error from the documentation",
        "timestamp": "2024-09-20 08:40:07.424000+00:00",
        "id": 1286607816303575102,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Reduce probability of each token",
        "timestamp": "2024-09-19 19:51:41.304000+00:00",
        "id": 1286414433220034685,
        "parent_id": null
    },
    {
        "author": "kirilligum",
        "content": "another example of this problem is one of the simple task where all llms fail \n```\nWrite me a sentence without any words that appear in The Bible.\n\n```\nhttps://arxiv.org/html/2405.19616v2#S9.SS2.SSS8 see 9.2.8",
        "timestamp": "2024-09-19 19:50:08.787000+00:00",
        "id": 1286414045175349368,
        "parent_id": null
    },
    {
        "author": "kirilligum",
        "content": "i assume that pydantic validation happens after json of the structured response was completed. i filter after the response now already using jq (or python). what i was wondering is since baml works with streaming api (my assumption), it checks every generated token (to make sure json syntax is correct). so it would makes sense to regenerate a wrong token instead of the whole response. if there are multiple wrong tokens in the response, the probability of regecting response goes up",
        "timestamp": "2024-09-19 19:47:56.795000+00:00",
        "id": 1286413491560775723,
        "parent_id": 1286411109456150599
    },
    {
        "author": "kirilligum",
        "content": "not 100% of the time. i end up removing answers with keywords using jq",
        "timestamp": "2024-09-19 19:39:24.031000+00:00",
        "id": 1286411340872945728,
        "parent_id": 1286410778072842323
    },
    {
        "author": "andrewcka",
        "content": "You can do an after validation with pydantic or a dataclass if the word that appears is on the list, send the error and then try again using the feedback of the error",
        "timestamp": "2024-09-19 19:38:28.857000+00:00",
        "id": 1286411109456150599,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Prompting doesnâ€™t work for you?",
        "timestamp": "2024-09-19 19:37:09.849000+00:00",
        "id": 1286410778072842323,
        "parent_id": 1286410413210079355
    },
    {
        "author": "kirilligum",
        "content": "a follow up question. is it possible to easily identify encoding for a token and reduce the probability of it appearing? for example, i don't wnat certain words to appear in the response",
        "timestamp": "2024-09-19 19:35:42.859000+00:00",
        "id": 1286410413210079355,
        "parent_id": null
    },
    {
        "author": "kirilligum",
        "content": "is there a way to easily define a python filtering function for each generated token? for example regex",
        "timestamp": "2024-09-19 19:34:48.153000+00:00",
        "id": 1286410183756746762,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Dashboard bug",
        "timestamp": "2024-09-19 19:00:36.119000+00:00",
        "id": 1286401576902201369,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Have you guys seen this error before? \n\n```\nNo events in the chain\n```",
        "timestamp": "2024-09-19 18:20:09.463000+00:00",
        "id": 1286391398769365025,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Retry on parse failure",
        "timestamp": "2024-09-19 15:55:33.948000+00:00",
        "id": 1286355011021963338,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "No idea if internally do also the same",
        "timestamp": "2024-09-19 14:15:49.445000+00:00",
        "id": 1286329910197227601,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "I was reading it, but it seems for network error",
        "timestamp": "2024-09-19 14:15:31.506000+00:00",
        "id": 1286329834955477094,
        "parent_id": 1286327153306239091
    },
    {
        "author": "cat_ethos",
        "content": "maybe this will help https://docs.boundaryml.com/docs/snippets/clients/retry",
        "timestamp": "2024-09-19 14:04:52.151000+00:00",
        "id": 1286327153306239091,
        "parent_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, is there a way to retry in case there is a fail to parse the output? The retry can work with this? or what is your recommendation?",
        "timestamp": "2024-09-19 13:58:08.693000+00:00",
        "id": 1286325461080739921,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Also, what was the way we could view/print that again?",
        "timestamp": "2024-09-19 06:33:49.166000+00:00",
        "id": 1286213642919874570,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Anyone had consistent way of imducing CoT before extraction?",
        "timestamp": "2024-09-19 06:33:14.869000+00:00",
        "id": 1286213499068088341,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'm trying to add a field dynamically which would describe another class field but am getting the following error:\n\nCell In[15], line 6, in get_graph(text)\n      4 tb = TypeBuilder()\n      5 tb.SimpleNode.add_property(\"descrpition\", tb.string()).description(\"if possible, provide a description of id\")\n----> 6 res = await b.ExtractGraph(text.content, { \"tb\": tb })\n      7 return res\n\nTypeError: object DynamicGraph can't be used in 'await' expression",
        "timestamp": "2024-09-19 00:10:39.499000+00:00",
        "id": 1286117217267617872,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "i tried using baml (in my own way) to parse a string (of list of strings):\n\nclass DeDupeResult {\n    duplicates string[]\n}\n\n^^returns nothing\n\nclass DeDupeResult {\n    merged_results string[]\n}\n\n^^ returns the intended list of string (merged_results)\n\ni am wondering how this is working?",
        "timestamp": "2024-09-18 22:19:11.602000+00:00",
        "id": 1286089166194610286,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Guys quick question, how can I use azure-openai? I'm havin troubles when i declare it into the function should i declare the client? When i declare it like:\n\n  client \"azure-openai/gpt4o\"\n\nGives me an error, an also when i call the name on the client that i defined into the clients.baml doesn't work, how should it be?",
        "timestamp": "2024-09-18 15:59:14.532000+00:00",
        "id": 1285993548352720971,
        "parent_id": null
    },
    {
        "author": "loohly",
        "content": "Hi all! First of all, am having a blast with baml!\nI have a question about serving baml as an API: **Is it possible to have headers from the request to the served baml API be passed through to the LLM provider?**\n\nI have two use cases for this:\n1. The LLM provider is azure-openai \"like\", but requires an OAuth token that has to be refreshed every 12h. Being able to forward a token header from the request to the baml API would mean that the requesting client could take care of having a non-expired token\n2. The client that calls the baml API also wants to add some headers to the LLM provider that let it know who is making the request (required by the provider for metering). \n\nHappy to give more details if needed.",
        "timestamp": "2024-09-18 14:04:19.120000+00:00",
        "id": 1285964626898456708,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, is there a way to use the new commet Opik with BAML?",
        "timestamp": "2024-09-18 13:31:27.349000+00:00",
        "id": 1285956356691460178,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Session history in BAML",
        "timestamp": "2024-09-17 14:03:58.775000+00:00",
        "id": 1285602153699872826,
        "parent_id": null
    },
    {
        "author": "brandburner",
        "content": "I am LOVING using BAML so far for generating structured metadata from drama transcripts. With Sonnet 3.5 and prompt caching I'm able to process a script scene by scene and extract all the entities, dramatic themes etc. So I'm curious if BAML natively maintains any kind of session history when executing on functions, or whether each call is independent of the previous? For my use case, it would help the consistency of responses if the model could see its previous few responses",
        "timestamp": "2024-09-17 10:30:20.265000+00:00",
        "id": 1285548388972236842,
        "parent_id": null
    },
    {
        "author": "sidd065",
        "content": "Hey guys, Is it possible to change the value of the description of a property inside a class via python?\nExample:\n```class Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n  extra_data string @description(\"Description for what extra data to extract, passed as a variable\")\n}```\nHere could I set the description of `extra_data` dynamically using a function call or some other way?",
        "timestamp": "2024-09-17 03:53:45+00:00",
        "id": 1285448584397066414,
        "parent_id": null
    },
    {
        "author": "nicarq",
        "content": "if we are using the wasm lib to generate the schemas, how difficult it would be to extend it so it also can run code using the runtime? it seems that it's already doing it for some test cases. I saw that's not quite there right now but wondering how much extra work it could be and if someone like me (external) could be able to do it",
        "timestamp": "2024-09-17 03:50:28.695000+00:00",
        "id": 1285447761034084402,
        "parent_id": null
    },
    {
        "author": "faizansattar",
        "content": "Hey team! Do you have any recommendations on how to convert a langgraph flow to BAML?",
        "timestamp": "2024-09-16 23:56:52.717000+00:00",
        "id": 1285388973761626122,
        "parent_id": null
    },
    {
        "author": "demontrius",
        "content": "Hi guys.... Been reading the docs and still not sure how to use BAML on my own. Is there some starter video? Tried following the doc but not sure I am getting the sequence",
        "timestamp": "2024-09-16 21:56:18.538000+00:00",
        "id": 1285358631415447595,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Hi guys, I love the project, it's really",
        "timestamp": "2024-09-16 13:36:32.192000+00:00",
        "id": 1285232859564740663,
        "parent_id": null
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, I love the project, it's really awesome. I havea question, is therea way to access the raw response from the model?",
        "timestamp": "2024-09-16 08:02:39.429000+00:00",
        "id": 1285148836066754634,
        "parent_id": null
    },
    {
        "author": "charizard_98",
        "content": "I'm at a loss here:\n\nIn the playground, I get the intended results under lookup_list.\n\nBut when I call the same function with the same user prompt from my application, I don't get both the items in the list\n\nDo you guys know if there is a way to keep it consistent?",
        "timestamp": "2024-09-14 15:19:42.176000+00:00",
        "id": 1284534046508191894,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "We discussed at some point about conditional logic inside the schemas, is that close?",
        "timestamp": "2024-09-12 23:53:01.550000+00:00",
        "id": 1283938452714553496,
        "parent_id": null
    },
    {
        "author": "joatmon.pockets",
        "content": "optional list",
        "timestamp": "2024-09-12 04:52:43.506000+00:00",
        "id": 1283651486638804996,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Any way to assign a list of types as optional?",
        "timestamp": "2024-09-12 04:25:03.665000+00:00",
        "id": 1283644524761055294,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "how does BAML manage LLM provider rate limits if at all?",
        "timestamp": "2024-09-11 13:50:24.304000+00:00",
        "id": 1283424410367299645,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Baml raw request",
        "timestamp": "2024-09-11 03:26:41.269000+00:00",
        "id": 1283267446781775918,
        "parent_id": null
    },
    {
        "author": "cat_ethos",
        "content": "is there a way to get the token usage info besides logging?",
        "timestamp": "2024-09-11 02:43:51.501000+00:00",
        "id": 1283256668393701419,
        "parent_id": null
    },
    {
        "author": "philosopherstone",
        "content": "hey guys, just a heads up - baml-cli is broken on 0.55.0 version release, but 0.54 works though!",
        "timestamp": "2024-09-10 15:57:42.121000+00:00",
        "id": 1283094057828089907,
        "parent_id": null
    },
    {
        "author": "joatmon.pockets",
        "content": "<@198831631602024450> re your question:\n\n> Heeey <@711679663746842796> I was playing-around/reading-code about serve function. I think it's pretty useful.\n> \n> As some feedback/questions, why serve needs the BAML files (throught the --from param) in place of being fully generic?\n> Let's say, on every network request to the BAML Rest service, besides:\n> - function (in the route param)\n> - args (in the body)\n> \n> The client (IE, typescript client) could send the BAML files to the endpoint so we could:\n> - Creates a BAML Runtime dynamically\n> - Execute the param parsing pipeline\n> - Execute the function\n> - Collect results\n> - Send response\n> \n> \n> I'm not sure about how inefficient could be stay instantiating a new BAML Runtime on every request but maybe it could be an improve over the first use case or even a completely different use case where we can share/scale the same instance of BAML Http Service throught different set of tools with no need to have them tied ðŸ¤”",
        "timestamp": "2024-09-09 19:00:58.370000+00:00",
        "id": 1282777791573786657,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Follow up questions:\n1. Is https://app.boundaryml.com still the correct dashboard URL? \n2. Running a test case in my BAML Playground is stalling, not sure how to see the what's going on",
        "timestamp": "2024-09-09 14:46:01.290000+00:00",
        "id": 1282713630969958441,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Hey guys, quick question:\nHow can I modify the azure-openai provider to use the government cloud? All that's needed is to change the endpoint from \n`<base>.openai.azure.com/` to `<base>.openai.azure.us/`",
        "timestamp": "2024-09-09 14:41:44.268000+00:00",
        "id": 1282712552941289543,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Also, the playground is now gone. Wonder if I did smth to my vscode setup smh",
        "timestamp": "2024-09-09 02:08:40.868000+00:00",
        "id": 1282523040026198089,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I literally added am empty line and saved",
        "timestamp": "2024-09-09 02:07:52.233000+00:00",
        "id": 1282522836036223037,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I somehow can't make a previous successful client compile",
        "timestamp": "2024-09-09 02:07:32.028000+00:00",
        "id": 1282522751290310812,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Any idea what sn empty error linting is?",
        "timestamp": "2024-09-09 02:07:19.474000+00:00",
        "id": 1282522698635022366,
        "parent_id": null
    },
    {
        "author": "nazimgirach",
        "content": "",
        "timestamp": "2024-09-07 21:39:38.622000+00:00",
        "id": 1282092946673631293,
        "parent_id": null
    },
    {
        "author": "nazimgirach",
        "content": "Does the BAML parser remove \\n from the response?",
        "timestamp": "2024-09-07 21:37:07.479000+00:00",
        "id": 1282092312733683734,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Pydantic description",
        "timestamp": "2024-09-07 19:45:06.915000+00:00",
        "id": 1282064124645343297,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Classes in test cases",
        "timestamp": "2024-09-07 19:05:55.058000+00:00",
        "id": 1282054260242251874,
        "parent_id": null
    },
    {
        "author": "nazimgirach",
        "content": "I also noticed that the description I add to the BAML code doesn't carry over to the generated Python types in Pydantic.",
        "timestamp": "2024-09-07 18:57:16.005000+00:00",
        "id": 1282052083176181863,
        "parent_id": null
    },
    {
        "author": "nazimgirach",
        "content": "Hey everyone, how do you add mock data to a class when passing it in a test?",
        "timestamp": "2024-09-07 18:26:32.946000+00:00",
        "id": 1282044352826314875,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "is there a way to add a description of a",
        "timestamp": "2024-09-07 01:13:24.334000+00:00",
        "id": 1281784353743179899,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "is there a way to add a description of a field in another field:\n\ne.g.\nclass Node {\n  id string \n  type string\n  description string @description(\"if possible, provide a brief description of id\")\n}",
        "timestamp": "2024-09-06 23:31:07.254000+00:00",
        "id": 1281758612963987458,
        "parent_id": null
    },
    {
        "author": "sudhanshug",
        "content": "does baml support assertion/expectations in tests? I only see test case execution",
        "timestamp": "2024-09-06 21:36:28.016000+00:00",
        "id": 1281729759348523169,
        "parent_id": null
    },
    {
        "author": "sudhanshug",
        "content": "can template strings be used across files? like can i have a `message.baml` and use it in `classifier.baml`?",
        "timestamp": "2024-09-05 22:42:44.694000+00:00",
        "id": 1281384050879107085,
        "parent_id": null
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Instantiating BAML classes in python code",
        "timestamp": "2024-09-05 21:39:28.890000+00:00",
        "id": 1281368130123337748,
        "parent_id": null
    },
    {
        "author": "ekp",
        "content": "Hi! I just started testing BAML for a new date time parsing feature. Thanks for the great tool!\n\nIâ€™m wondering if thereâ€™s any difference in accuracy between streaming vs without (practical or even theoretical).\n1. Will the final parsed response of a streamed function be identical to a function called without streaming?\n2. Once a KV pair is streamed, and assuming the stream isnâ€™t later determined unparseable, is it safe to assume the KV pair is final? Or is it possible for some later tokens to alter previously parsed value in the final response?",
        "timestamp": "2024-09-05 18:57:00.550000+00:00",
        "id": 1281327242621751316,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "parsing validation",
        "timestamp": "2024-09-05 13:41:48.497000+00:00",
        "id": 1281247919722332215,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I'd still like the rest of the schema if one field failed",
        "timestamp": "2024-09-05 11:07:49.166000+00:00",
        "id": 1281209167159230575,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Is parsing validation going to deal with parsing errors we get now (eg. Null value on non optional params)?",
        "timestamp": "2024-09-05 11:06:58.455000+00:00",
        "id": 1281208954462011393,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "openai structured output",
        "timestamp": "2024-09-04 15:47:17.957000+00:00",
        "id": 1280917112701063183,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Optional arrays",
        "timestamp": "2024-09-04 15:42:34.883000+00:00",
        "id": 1280915925402390573,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "batching requests",
        "timestamp": "2024-09-04 15:41:39.072000+00:00",
        "id": 1280915691314085894,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "What's the best way to do batched runs atm? I can read docs if it's in there, was in flight ðŸ˜€",
        "timestamp": "2024-09-04 09:36:40.048000+00:00",
        "id": 1280823840150388747,
        "parent_id": null
    },
    {
        "author": "foxicution",
        "content": "Question about https://openai.com/index/introducing-structured-outputs-in-the-api/ and BAML. As far as I understand BAML constructs the prompt, but doesn't use the OpenAI specific \"response_format\" parameter, thus the claim in the article of 100% reliability does not apply to BAML, even when using in conjunction with ChatGPT. Is my assumption correct? Is this a planned feature?",
        "timestamp": "2024-09-04 07:43:55.209000+00:00",
        "id": 1280795466359115776,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "why can't array types be optional also?",
        "timestamp": "2024-09-04 03:05:54.018000+00:00",
        "id": 1280725500372848701,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "maybe an enum of one element?",
        "timestamp": "2024-09-04 02:47:13.612000+00:00",
        "id": 1280720801049608273,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "looking to generate a list of operations as a result and i was hoping for some easy way to determine which among the set of possible operations was selected",
        "timestamp": "2024-09-04 02:44:17.268000+00:00",
        "id": 1280720061409267766,
        "parent_id": null
    },
    {
        "author": "airhorns",
        "content": "hey folks -- is there any support for constant/literal types and/or discriminated unions?",
        "timestamp": "2024-09-04 02:43:51.348000+00:00",
        "id": 1280719952692641873,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "organization",
        "timestamp": "2024-09-03 22:56:37.153000+00:00",
        "id": 1280662766734147704,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "What is the best way to manage many functions in a single file?\n\nI have a complex extraction that has 16 differ3nt schemas. The final output is a report that contains all that. Is my best wag of doing this a for loop over those? And how do people do this cleanly in code ðŸ˜€",
        "timestamp": "2024-09-03 22:33:18.774000+00:00",
        "id": 1280656901507383306,
        "parent_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Is it possible to do response prefilling with baml?",
        "timestamp": "2024-09-03 21:36:57.515000+00:00",
        "id": 1280642719479500860,
        "parent_id": null
    },
    {
        "author": "demontrius",
        "content": "Hi everyone. So I tried using baml on google colabs but got an error when running the code below:\n!baml-cli generate\n\nError generating clients Traceback (most recent call last): File \"/usr/local/bin/baml-cli\", line 8, in <module> sys.exit(invoke_runtime_cli()) baml_py.BamlError: error: client provider openai-generic not found. Did you mean one of these: `openai`, `azure-openai`, `anthropic`? --> ./baml_src/clients.baml:4 | 3 | client<llm> MyClient { 4 | provider openai-generic |\n\n#/content/baml_src/clients.baml\nclient<llm> MyClient { provider openai-generic options { base_url \"https://api.groq.com/openai/v1\" api_key env.GROQ_API_KEY model \"llama3-70b-8192\" } }",
        "timestamp": "2024-09-03 05:05:47.884000+00:00",
        "id": 1280393285768319038,
        "parent_id": null
    },
    {
        "author": "maweill.",
        "content": "Hi! In Nuxt 3 project I get ```X [ERROR",
        "timestamp": "2024-09-01 14:19:48.712000+00:00",
        "id": 1279807932174831693,
        "parent_id": null
    },
    {
        "author": "maweill.",
        "content": "Hi! In Nuxt 3 project I get ```X [ERROR] No loader is configured for \".node\" files: node_modules/.pnpm/@boundaryml+baml-win32-x64-msvc@0.54.0/node_modules/@boundaryml/baml-win32-x64-msvc/baml.win32-x64-msvc.node\n\nnode_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/native.js:96:23:\n      96 â”‚         return require('@boundaryml/baml-win32-x64-msvc')``` \nHow can I solve this?",
        "timestamp": "2024-09-01 14:04:03.648000+00:00",
        "id": 1279803968288985150,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Panic",
        "timestamp": "2024-08-30 00:29:40.447000+00:00",
        "id": 1278874245438374016,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "nonetheless, the rest of requests seem to be executing fine.",
        "timestamp": "2024-08-29 23:21:49.330000+00:00",
        "id": 1278857169936187464,
        "parent_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'm wondering what could throw this error:\n\nthread 'tokio-runtime-worker' panicked at baml-lib/jsonish/src/jsonish/parser/multi_json_parser.rs:33:56:\ncalled `Option::unwrap()` on a `None` value\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nProcessing Chunks:   0%|â–                                                                                                                             | 2/625 [02:25<11:34:26, 66.88s/it]\n\n\n*************Error, Request failed: rust future panicked: unknown error",
        "timestamp": "2024-08-29 23:20:43.322000+00:00",
        "id": 1278856893078704169,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Custom transforms",
        "timestamp": "2024-08-29 17:02:37.447000+00:00",
        "id": 1278761741622185988,
        "parent_id": null
    },
    {
        "author": "cat_ethos",
        "content": "for optional string field, sometime it returns `null` and another time `N/A`, maybe the parser can coerse those into one single output for better downstream processing",
        "timestamp": "2024-08-29 16:41:58.302000+00:00",
        "id": 1278756544271487119,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "deno + BAML",
        "timestamp": "2024-08-29 13:25:04.848000+00:00",
        "id": 1278706995053989923,
        "parent_id": null
    },
    {
        "author": "cat_ethos",
        "content": "I am trying to install BAML via deno `deno add npm:@boundaryml/baml ` and curiously getting the error `error: Could not find npm package '@boundaryml/baml-win32-arm64-msvc' matching '0.54.0'.` but I am in mac",
        "timestamp": "2024-08-29 11:37:47.744000+00:00",
        "id": 1278679995883192381,
        "parent_id": null
    },
    {
        "author": "nicarq",
        "content": "hi! is there any way to use BAML from Rust? I saw that the engine is in Rust",
        "timestamp": "2024-08-29 05:51:44.452000+00:00",
        "id": 1278592908324507727,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Langchain X BAML",
        "timestamp": "2024-08-28 18:54:23.752000+00:00",
        "id": 1278427482038730848,
        "parent_id": null
    },
    {
        "author": "mr_zoidberg_",
        "content": "Are there any good examples of Baml and Langchain integration? I have an app with long conversations and a need to sometimes (at certain steps of the conversations) return a specified structured response while maintaining a using textual conversation. Baml looks promising, but I'm lost in trying to figure out how to integrate it for this usecase.",
        "timestamp": "2024-08-28 14:35:14.910000+00:00",
        "id": 1278362265468272691,
        "parent_id": null
    },
    {
        "author": "gleed",
        "content": "OpenAI assistants",
        "timestamp": "2024-08-27 14:18:41.553000+00:00",
        "id": 1277995711161438261,
        "parent_id": null
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "BAML support  AWS Secrets Manager.",
        "timestamp": "2024-08-26 16:34:57.006000+00:00",
        "id": 1277667613631189054,
        "parent_id": null
    },
    {
        "author": "huythangphan",
        "content": "Hi <@99252724855496704> , Iâ€™d like to inquire if there are any plans for BAML to support JavaScript in the near future. Thank you",
        "timestamp": "2024-08-26 02:25:46.935000+00:00",
        "id": 1277453913544458313,
        "parent_id": null
    },
    {
        "author": "bsachs10",
        "content": "Hello! Is here an example of using `map`? <@99252724855496704> you mentioned a tip to me recently about using my own unique keys in my schema to get data back. Not sure how to implement that but very interested! Context: I'm making a tool to classify sales contracts based on whether the contract includes lagnuage offering a specific product. But there are 40-50 products. So I was thinking I could require it give me back an object where each key is the product name and then the property content for each key is whether the product was found, relevant text as proof, etc.",
        "timestamp": "2024-08-25 13:38:58.125000+00:00",
        "id": 1277260938608640081,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "new dynamic enums",
        "timestamp": "2024-08-24 13:14:36.008000+00:00",
        "id": 1276892418179989510,
        "parent_id": null
    },
    {
        "author": "gleed",
        "content": "Been playing around with BAML a bit, thanks for the project. I submitted that PR the other day about Gemini and might have some others for old docs etc. I might submit later.\n\nI'm trying to build a classifier. I read about enums in the documentation and examples, sounded pretty promising. I need to categorize with strings containing spaces, so the @alias functionality seemed useful as well. \n\nMy question is this, can I add @@dynamic enum fields and set the @alias as well? From what I can tell in the Python source, type builder really only allows you to add the enum value.\n\nOpen to other suggestions as well if there's another way to accomplish this.\n\nEx.\n```py\n#both diseases and categories are parsed from a file and can vary \ncategories=[\n\"Cardiovascular diseases\", \"Immunology\", \"Skin & Soft tissue\"]\n\ndiseases=[\"Melanoma\", \"Coronary artery disease\"]\n\n#call BAML here to categorize diseases\n```\nCurrently, I pass the list of strings in plain text and ask it to categorize, and cross my fingers that the LLM matches the category exactly",
        "timestamp": "2024-08-24 13:00:56.978000+00:00",
        "id": 1276888982919053343,
        "parent_id": null
    },
    {
        "author": "hitzor9",
        "content": "Hi! Recently, I needed to build a small prototype for extracting structured information from images using AI, and I decided to try using BAML. I didn't have much experience working with LLMs before, but with the help of your tool, I was able to build and test the prototype in just a few hours, so a huuuuge shotout for that!\n\nBefore going to production, I have a couple of questions. I'm using the Python SDK with an async client. \n\n1) When an error related to the LLM occurs, a `BamlError` is raised. I see that this error contains a textual representation of the `LLMResponse`, which includes the error code, message, and so on. Is there a way to access this object directly without trying to parse it from the text?\n\n2) As far as I can see in the OpenAI documentation, the server's response includes headers with usage information (`x-ratelimit-limit-requests`, `x-ratelimit-limit-tokens`). Is there any way I can access the raw response object or at least these headers? Or is there another unified way to obtain this information?\"",
        "timestamp": "2024-08-23 16:19:36.998000+00:00",
        "id": 1276576591241285766,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "exceptions",
        "timestamp": "2024-08-23 03:47:32.362000+00:00",
        "id": 1276387324799881242,
        "parent_id": null
    },
    {
        "author": "richardclove",
        "content": "won't this call the b.ClassifyMessage one-by-one before asyncio.gather gets to it?",
        "timestamp": "2024-08-23 03:43:38.048000+00:00",
        "id": 1276386342015602748,
        "parent_id": null
    },
    {
        "author": "richardclove",
        "content": "How do you do error handling in concurrent functions? Some errors that come up like found multiple words when trying to do classification using enums\n\nBamlError: (3 other previous tries)\nLLM call failed: LLMErrorResponse { client: \"Groq\", model",
        "timestamp": "2024-08-23 03:27:29.265000+00:00",
        "id": 1276382278645452821,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "BAML + Rust",
        "timestamp": "2024-08-22 13:23:44.024000+00:00",
        "id": 1276169940994359343,
        "parent_id": null
    },
    {
        "author": "cat_ethos",
        "content": "is there anyway i can use BAML directly from Rust",
        "timestamp": "2024-08-22 07:26:50.439000+00:00",
        "id": 1276080125909270559,
        "parent_id": null
    },
    {
        "author": "jawnathonjones",
        "content": "Has anyone tried hitting a fine-tuned OpenAI model with BAML?",
        "timestamp": "2024-08-21 19:35:41.138000+00:00",
        "id": 1275901157889413184,
        "parent_id": null
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "I also have a pydantic-centric multi-agent app that I am developing using langchain and langgraph.  I am trying to understand how BAML would fit into my app.  My agents typically have a prompt but also a pydantic class that are attached to the agent's model via functions.",
        "timestamp": "2024-08-15 20:23:33.989000+00:00",
        "id": 1273738880306380961,
        "parent_id": null
    },
    {
        "author": "richardclove",
        "content": "I want to write a preview so our app users can write their own prompts. Is there a way to output the compiled input prompt for Python? I know it showed up on the playground but is there a code version?",
        "timestamp": "2024-08-15 16:52:21.997000+00:00",
        "id": 1273685730119385210,
        "parent_id": null
    },
    {
        "author": "kdub03",
        "content": "Was there a sample or example on going from Pydantic -> BAML somewhere? We have a codebase of pydantic model extractions that I'd like to just test with BAML.",
        "timestamp": "2024-08-15 16:26:30.305000+00:00",
        "id": 1273679221851426908,
        "parent_id": null
    },
    {
        "author": "richardclove",
        "content": "Is it possible to put test cases in a different file? or better yet.. what do you suggest when we have really long tests or many test cases? What are the best practices here?",
        "timestamp": "2024-08-15 05:58:28.219000+00:00",
        "id": 1273521171727519817,
        "parent_id": null
    },
    {
        "author": "unsignedint.",
        "content": "Hey folks. Does anyone have an example of a test case that uses/extends a dynamic enum that they could share?",
        "timestamp": "2024-08-14 23:40:54.351000+00:00",
        "id": 1273426154518483043,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Debugging ClientRegistry",
        "timestamp": "2024-08-14 19:07:12.310000+00:00",
        "id": 1273357275485962365,
        "parent_id": null
    },
    {
        "author": ".alex4o",
        "content": "Also a side note it might be good to support generating imports that include the file extension for typescript because tsc does not want to compile the client",
        "timestamp": "2024-08-14 19:05:28.063000+00:00",
        "id": 1273356838242353204,
        "parent_id": null
    },
    {
        "author": ".alex4o",
        "content": "Hey I am trying to use BAML but I need to provide the api_key during the runtime because I package the client in a typescript library.\n```typescript\n  const clientRegistry = new ClientRegistry();\n\n  clientRegistry.addLlmClient(\"GPT-4o\", \"openai\", {\n    model: \"gpt-4o\",\n    temperature: 0.7,\n    api_key: key,\n  });\n\n  clientRegistry.setPrimary(\"GPT-4o\");\n\n  const res = await b.ExtractResume(\n    \"Mark gonzalez, mark@hello.com. python. 5 years.\",\n    { clientRegistry }\n  );\n\n  return res;\n```\nThis is what I do and I still get an error from open ai saying there is no bearer token",
        "timestamp": "2024-08-14 18:54:32.269000+00:00",
        "id": 1273354087643091035,
        "parent_id": null
    },
    {
        "author": "kdub03",
        "content": "Any thoughts on anthropic caching? https://www.anthropic.com/news/prompt-caching",
        "timestamp": "2024-08-14 18:22:07.256000+00:00",
        "id": 1273345929667411988,
        "parent_id": null
    },
    {
        "author": "feres0902",
        "content": "im trying to integrate baml with vllm and after hardcoding the baml pormpt the only thing left is to parse the llm answer, is there a way to do it in python",
        "timestamp": "2024-08-14 07:17:52.913000+00:00",
        "id": 1273178768437153833,
        "parent_id": null
    },
    {
        "author": "dokudoge",
        "content": "Hi, I was looking through the website, and for some reason the demo video isn't playing for me\n\nhttps://docs.boundaryml.com/docs/get-started/what-is-baml\n\nthanks!",
        "timestamp": "2024-08-13 22:10:35.267000+00:00",
        "id": 1273041037367181352,
        "parent_id": null
    },
    {
        "author": "mz5910",
        "content": "ClientRegistry parameters",
        "timestamp": "2024-08-12 19:57:01.783000+00:00",
        "id": 1272645038513324084,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Hi, I am suddenly getting this error,",
        "timestamp": "2024-08-09 17:01:24.951000+00:00",
        "id": 1271513680240185359,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "Hi, I am suddenly getting this error, but inconsistently, when testing out a prompt that previously worked, and it only fails some of the time. I am using temperature 0 so I don't think the output should be changing across different runs.\n\nUnspecified error code: 2\nFailed to parse event: Error(\"missing field `type`\", line: 0, column: 0)\n\nCheck the webview network tab for more details. Command Palette -> Open webview developer tools.",
        "timestamp": "2024-08-09 16:48:14.720000+00:00",
        "id": 1271510365770875005,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "maybe a naive question: If I set the max_tokens in the client, does the model take this into account when generating a response? IN other words, if i put 150 token maximum, will the model do it's best to constrain the output to 150 tokens or do I also need to prompt it for that",
        "timestamp": "2024-08-07 21:41:19.869000+00:00",
        "id": 1270859347500597288,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Benchmark thread",
        "timestamp": "2024-08-07 20:46:26.042000+00:00",
        "id": 1270845532188967071,
        "parent_id": null
    },
    {
        "author": "kdub03",
        "content": "Is the \"Strict\" mode comparing the new structured outputs from OpenAI? Is that function calling only? Is there a comparison with structured outputs w/o function calling, since I think that's closer aligned to what BAML is doing?\nhttps://discord.com/channels/1119368998161752075/1119375433666920530/1270775695420817489",
        "timestamp": "2024-08-07 20:35:49.355000+00:00",
        "id": 1270842861730009109,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Dyanmically settings API keys",
        "timestamp": "2024-08-06 19:49:56.172000+00:00",
        "id": 1270468926177939604,
        "parent_id": null
    },
    {
        "author": "mz5910",
        "content": "Hey. I am trying to use BAML inside an AWS Lambda function. How do I set the api_key from AWS Secrets Manager i.e modify this part?\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nCurrently, my Lambda's Python code does retrieve OPENAI_SECRET_KEY from AWS Secrets Manager. I am using BAML 0.44.0 with Python 3.10.",
        "timestamp": "2024-08-06 18:39:36.975000+00:00",
        "id": 1270451229583081615,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Is there a place in the docs where I can see which exceptions to expect from a baml call? Since I have my baml client in the worker, i got an error which automatically retried the worker when ideally i would've detected it were of type `ExceptionTypeA` and instead of retrying, i would do something differently",
        "timestamp": "2024-08-06 16:51:33.268000+00:00",
        "id": 1270424034944749638,
        "parent_id": null
    },
    {
        "author": "deoxykev",
        "content": "would it be reasonable to have multiple baml_src directories in a repo for organization purposes?",
        "timestamp": "2024-08-05 23:50:14.270000+00:00",
        "id": 1270167012198453259,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "If i removed the `BAML_LOG` environment variable, does this mean baml isn't logging anything anymore?",
        "timestamp": "2024-08-05 23:11:09.601000+00:00",
        "id": 1270157177943756851,
        "parent_id": null
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Maybe missing something super obvious: for tests, is there a way to provide what I expect the output should be? \n\nI have some inputs that I want the function to return `true` for, but I don't see a place for me to say I `expect` `true` as the output",
        "timestamp": "2024-08-05 23:10:47.023000+00:00",
        "id": 1270157083244757045,
        "parent_id": null
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Question about prompting for a true/false response: What's the best way to ask an LLM to answer a question as true or false and have parsing succeed? I did some testing, and found that if I try to use chain-of-thought with such a simple output schema, response coercion fails: https://www.promptfiddle.com/New-Project-GSO_w",
        "timestamp": "2024-08-05 21:52:28.676000+00:00",
        "id": 1270137376949276724,
        "parent_id": null
    },
    {
        "author": "kdub03",
        "content": "For the aws-bedrock provider, is there a way to have it use sso, or my authed aws client? \nhttps://docs.boundaryml.com/docs/snippets/clients/providers/aws-bedrock",
        "timestamp": "2024-08-01 01:23:43.470000+00:00",
        "id": 1268378599560577127,
        "parent_id": null
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Typing/output schema question: Is it possible to have a type with a field that I conditionally choose to turn off for some function calls? \n\nI have a BAML type I want to use throughout my code, but I only want one BAML function to try to fill in a particular field, not the other, so I don't want that field to show up in `{{ ctx.output_format }}`  for the first function at all. Is the only way to do this to define a completely new type with one fewer field?",
        "timestamp": "2024-07-31 18:59:33.986000+00:00",
        "id": 1268281923017506910,
        "parent_id": null
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Best practices w/ BAML question: Do namespaces exist as a concept in BAML, or is every function/type under different files combined to live under a global shared namespace? For context, I'm implementing another feature with BAML and had a name conflict with a similar type in another file -- then I considered importing it from there / making a separate file with shared types to organize them, but I could see that getting messy as I add more things in the future",
        "timestamp": "2024-07-30 21:01:16.732000+00:00",
        "id": 1267950165088534528,
        "parent_id": null
    },
    {
        "author": "bsachs10",
        "content": "Question: Is it possible to run BAML functions client side in NextJS? I know the security concerns about using API keys in browser, but I can mitigate that with a proxy server that uses session-specific authentication rather than API keys. But when I try to run any BAML function from client code (or simply import `b` from `@/baml_client`), I get an immediate error like this one:\n\n```\n./node_modules/@boundaryml/baml/async_context_vars.js:5:1\nModule not found: Can't resolve 'async_hooks'\n\nhttps://nextjs.org/docs/messages/module-not-found\n\nImport trace for requested module:\n./node_modules/@boundaryml/baml/index.js\n./src/baml_client/async_client.ts\n./src/baml_client/index.ts\n```",
        "timestamp": "2024-07-29 15:48:13.698000+00:00",
        "id": 1267508995468427264,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "custom jinja filters",
        "timestamp": "2024-07-29 15:33:48.453000+00:00",
        "id": 1267505366367993867,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Variables in functions",
        "timestamp": "2024-07-29 15:30:53.973000+00:00",
        "id": 1267504634545569849,
        "parent_id": null
    },
    {
        "author": "robert_hoenig",
        "content": "Another question: Is it possible to add variables?\n\nUse case: I'm implementing few-shot learning, and most of my examples are constructed from the same few structured documents. The documents have a type quite similar to the resume example:\n\n```\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n```\n\nI'd like to create a couple `Resume` instances and re-use them across all my BAML prompts. The `template_strings` mechanism suggested in the docs wouldn't be sufficient for this, because each prompt needs to see the Resume instance as a structured object, and not a string.",
        "timestamp": "2024-07-29 13:45:31.857000+00:00",
        "id": 1267478117669142570,
        "parent_id": null
    },
    {
        "author": "robert_hoenig",
        "content": "Is it possible to add custom Jinja filter functions?\n\nUse case: I'm implementing few-shot learning by adding some example outputs, and would like to control their formatting like so: (\"{{ example_output|format_example_output_in_json }}\". I discovered that in this specific instance,  pretty-printed json formatting can be achieved as \"{{ sample_resume_out|tojson(true) }}\", but I'm sure that at some point, I'll have a need for custom filters.",
        "timestamp": "2024-07-29 13:45:20.105000+00:00",
        "id": 1267478068377817261,
        "parent_id": null
    },
    {
        "author": "etbyrd",
        "content": "Is it possible to ask BAML to _not_ include certain inputs on clients when using the openai API format?\n\nContext: I am testing Fireworks using:\n```client<llm> FireworksLlama31_70b {\n  provider openai\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    api_key env.FIREWORKS_API_KEY\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n  }\n}```\n\n and I get:\n\n```\nRequest failed: {\"error\":{\"object\":\"error\",\"type\":\"invalid_request_error\",\"message\":\"Extra inputs are not permitted, field: 'stream_options'\"}}\n```",
        "timestamp": "2024-07-24 21:44:54.076000+00:00",
        "id": 1265786815831478273,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "is it possible to set the url for a llm resource, Iâ€™m trying to see if baml can work with Martian - I believe Martian matches the OpenAI api",
        "timestamp": "2024-07-24 15:17:04.605000+00:00",
        "id": 1265689216596050021,
        "parent_id": null
    },
    {
        "author": "energetic_axolotl_22123",
        "content": "I'm having trouble installing baml-py using poetry. It seems to detect 0.51.3/0.52.0 as the latest version, but that version is not actually installable? When I simply run pip install baml-py, I get baml-py v0.46.0. poetry add baml-py@0.46.0 works fine.",
        "timestamp": "2024-07-24 14:40:10.637000+00:00",
        "id": 1265679930541478120,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Does baml automatically detect whether",
        "timestamp": "2024-07-24 14:33:57.201000+00:00",
        "id": 1265678364237107256,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does baml automatically detect whether the context window limitation is exceeded? e.g. I have a fallback client which is gpt4 then claude sonnet. If i submit input which exceeds the 128k token limit, will baml automatically skip the gpt 4 client or will it try and fail based on my designated retry policy?",
        "timestamp": "2024-07-24 13:35:07.972000+00:00",
        "id": 1265663561577926726,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "iframe hyperlinks",
        "timestamp": "2024-07-23 05:39:19.353000+00:00",
        "id": 1265181432125460485,
        "parent_id": null
    },
    {
        "author": "deoxykev",
        "content": "Can I iframe the hyperlinks to prompt execution traces on the observability platform to internal tools?\n\nI would like to just iframe the prompt structures in grafana with the rest of the execution trace from otel",
        "timestamp": "2024-07-23 01:58:07.346000+00:00",
        "id": 1265125765293281290,
        "parent_id": null
    },
    {
        "author": "gintautas_turing",
        "content": "Are you going to support GPT 4o Mini that was released yesterday? Perhaps it is already supported?",
        "timestamp": "2024-07-19 09:35:58.952000+00:00",
        "id": 1263791438097809472,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "BAML vs Outlines vs Instructor",
        "timestamp": "2024-07-18 16:15:02.072000+00:00",
        "id": 1263529474956464176,
        "parent_id": null
    },
    {
        "author": "josephsirosh_22062",
        "content": "[Repost from <#1119375594984050779>] Clarification question: I'm comparing Outlines, Instructor, and BAML, and trying to understand the technical nuances that differentiate BAML. Especially over time, as both Outlines and Instructor are also continuously improving their codebases. What is enduringly different about the BAML approach?",
        "timestamp": "2024-07-18 14:53:41.119000+00:00",
        "id": 1263509002755510383,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "for some LLM calls, where I need to generate a lot of output, I'm running up against the bounds of the 4096 output token maximum. Does anyone have any good prompt engineering tips so that I can somehow recognize when the bounds have been reached to then pass this output as input to a subsequent continuation LLM call?",
        "timestamp": "2024-07-17 22:55:04.952000+00:00",
        "id": 1263267762466328638,
        "parent_id": null
    },
    {
        "author": "chocobeery",
        "content": "Hi, when I access a provided share link, I only see the default template. Is this the intended display or should it be showing someone else code?",
        "timestamp": "2024-07-17 15:16:39.678000+00:00",
        "id": 1263152396985696329,
        "parent_id": null
    },
    {
        "author": "robert_hoenig",
        "content": "How are the query costs in the observability platform calculated? In my case, I'm using the Huggingface Serverless Inference API Pro as a client, which has a fixed cost of $10 / month (but costs per query show up regardless).",
        "timestamp": "2024-07-17 08:44:34.037000+00:00",
        "id": 1263053723295416320,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "perhaps this is too specific but if i wanted to use baml with something like windmill, i would either have to manually deploy the baml generated python files or figure out a way for the worker to have called baml-cli generate",
        "timestamp": "2024-07-17 02:12:41.414000+00:00",
        "id": 1262955104206590034,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Restricting BAML output",
        "timestamp": "2024-07-16 13:08:02.830000+00:00",
        "id": 1262757642313400350,
        "parent_id": null
    },
    {
        "author": "neuralcorrelate",
        "content": "when I save my baml file in vs code, it is autogenerating a baml_client directory in the parent directory,  and not the directory that contains baml_src. How can I fix this?",
        "timestamp": "2024-07-15 15:49:32.243000+00:00",
        "id": 1262435894791766016,
        "parent_id": null
    },
    {
        "author": "kdub03",
        "content": "Is there thought/work towards how this could work with evaluations? I'm finding that defining my models in baml is more challenging to iterate my ground truth -> schema mapping, since the schema is abstracted and volatile.",
        "timestamp": "2024-07-15 14:33:40.167000+00:00",
        "id": 1262416802000928999,
        "parent_id": null
    },
    {
        "author": ".aaronv",
        "content": "Formatting",
        "timestamp": "2024-07-14 07:04:14.369000+00:00",
        "id": 1261941311380914258,
        "parent_id": null
    },
    {
        "author": "etbyrd",
        "content": "Are there any plans to support auto-formatting in VS Code?",
        "timestamp": "2024-07-14 05:20:26.760000+00:00",
        "id": 1261915190895579197,
        "parent_id": null
    },
    {
        "author": "joatmon.pockets",
        "content": "100% cpu usage",
        "timestamp": "2024-07-12 16:04:03.821000+00:00",
        "id": 1261352386858061860,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does baml offer some sort of timeout on clients? in development i noticed a boundary function led to 100% CPU usage for my worker and it's clearly stalled. Though I'm not seeing any options to set timeouts (which I would think can be irrespective of provider since it's controlled by the baml client)\n\nhttps://docs.boundaryml.com/docs/snippets/clients/overview",
        "timestamp": "2024-07-12 15:39:17.534000+00:00",
        "id": 1261346152918290508,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Where can I find the correct azure api_version I should be using?",
        "timestamp": "2024-07-11 23:05:05.449000+00:00",
        "id": 1261095953939693658,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Did you guys say something about rendering `\\n` characters not actually rendering newlines?",
        "timestamp": "2024-07-11 20:31:45.063000+00:00",
        "id": 1261057364723761152,
        "parent_id": null
    },
    {
        "author": "deoxykev",
        "content": "will the version after 0.49.0 be released today? I want to try out the dynamic clients",
        "timestamp": "2024-07-11 17:44:05.702000+00:00",
        "id": 1261015172705812510,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Anyone know whether Claude 3.5 Sonnet is actually better than Opus in every regard?",
        "timestamp": "2024-07-11 15:24:53.316000+00:00",
        "id": 1260980140259938344,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "VLLM support",
        "timestamp": "2024-07-11 14:15:51.374000+00:00",
        "id": 1260962767695777917,
        "parent_id": null
    },
    {
        "author": "feres0902",
        "content": "i want to change code and add a vllm provider how could i do it im new here",
        "timestamp": "2024-07-11 09:26:27.994000+00:00",
        "id": 1260889940401721474,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "I am processing a PDF with unstructured. At some points, I recognize that a Table is present. Unfortunately, Unstructured doesn't do a great job with preserving the format of the table/form. I have found that if I can pipe that table somehow to GPT4o, it does a really great job of outputting the table in markdown. Is there a clean way to accomplish something like this in Baml? I know the page number in the PDF where the table lives",
        "timestamp": "2024-07-10 19:39:05.808000+00:00",
        "id": 1260681725982544035,
        "parent_id": null
    },
    {
        "author": "dpaleka",
        "content": "hi! does this library support OpenAI-compatible APIs that do not have json/tools/functions mode?  i'm considering alternatives to `instructor` on a research project because i can't make it work with OpenRouter endpoints that do not support that param",
        "timestamp": "2024-07-09 00:46:19.412000+00:00",
        "id": 1260034266390073396,
        "parent_id": null
    },
    {
        "author": "etbyrd",
        "content": "How would you get baml running in just a basic new node project? Probably Webpack in a similar way to the NextJS setup but with using something like `node-loader` instead of `nextjs-node-loader`?",
        "timestamp": "2024-07-04 03:25:21.423000+00:00",
        "id": 1258262349157240913,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "Do the tests only test output schema being correct? Can you also test that the actual output matches what you would expect?",
        "timestamp": "2024-07-03 05:06:02.896000+00:00",
        "id": 1257925301065748561,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "can you pass in the client into a function? or specific client parameters? for example (1) if i wanted to switch between gpt4o and sonnet3.5,  i would need to copy paste and define multiple functions using different clients and (2) if i wanted to run sonnet3.5 but with 0 and 0.3 temperature, i would need to copy paste and define multiple functions using the same client but different settings?",
        "timestamp": "2024-07-03 04:39:53.368000+00:00",
        "id": 1257918717988311110,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "ah, i see it needs to be inside\n    generation_config {\n      temperature 0\n    }",
        "timestamp": "2024-07-03 04:04:15.836000+00:00",
        "id": 1257909752529293375,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "i get an error when I add it to options",
        "timestamp": "2024-07-03 04:02:04.931000+00:00",
        "id": 1257909203473924096,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "does Gemini support temperature?",
        "timestamp": "2024-07-03 04:01:56.479000+00:00",
        "id": 1257909168023539732,
        "parent_id": null
    },
    {
        "author": "deoxykev",
        "content": "is json schema conversion still being developed? Use case is dynamically converting unknown json schema into BAML for structured extraction.\n\nI was working on an implementation using the python client but it  looks like someone is working on a base rust implementation which might be better. https://github.com/BoundaryML/baml/pull/655",
        "timestamp": "2024-07-02 17:59:55.748000+00:00",
        "id": 1257757666697150485,
        "parent_id": null
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Hi team -- for deploying BAML, when do you think you'll  support alpine? We use alpine in our prod containers currently and are running into build errors",
        "timestamp": "2024-07-01 20:15:01.869000+00:00",
        "id": 1257429278367158284,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "are optional / default parameters in functions supported?",
        "timestamp": "2024-06-29 17:10:21.399000+00:00",
        "id": 1256658027776446494,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Defining Types in Python",
        "timestamp": "2024-06-29 07:37:02.494000+00:00",
        "id": 1256513748311478329,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "is it possible to define the data type inside python (and use it in a .baml file) instead of inside the baml file (which then flows to baml_client.types)",
        "timestamp": "2024-06-28 23:34:52.038000+00:00",
        "id": 1256392405184155759,
        "parent_id": null
    },
    {
        "author": "ashwin.a.kumar",
        "content": "How can we provide dynamic types to BAML functions? I'd like to use an `enum`, but it seems like I can only use this statically with the enum variants hardcoded in the file -- I'm guessing a workaround would be to type it `string` and create a prompt in the @description annotation, but I'd rather avoid this if possible\n\n(relevant empty docs page: https://docs.boundaryml.com/docs/calling-baml/dynamic-types)",
        "timestamp": "2024-06-25 23:47:40.856000+00:00",
        "id": 1255308466243899412,
        "parent_id": null
    },
    {
        "author": "elijas_ai",
        "content": "Helix",
        "timestamp": "2024-06-25 13:57:24.778000+00:00",
        "id": 1255159920446275620,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Is there support for classes with nested fields? i.e. something like \n\n```rust\nclass Section {\n   id string\n    text string\n    subsections Section[]\n}\n```",
        "timestamp": "2024-06-25 12:45:37.326000+00:00",
        "id": 1255141853683257394,
        "parent_id": null
    },
    {
        "author": "elijas_ai",
        "content": "My friend is using https://helix-editor.com/ IDE, what would be the best way to use BAML with it?",
        "timestamp": "2024-06-25 11:21:26.649000+00:00",
        "id": 1255120669608513641,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "do you commit the autogenerated python files from baml to git?",
        "timestamp": "2024-06-24 02:16:57.565000+00:00",
        "id": 1254621257673216001,
        "parent_id": null
    },
    {
        "author": "yungweedle",
        "content": "Hi, is there a built in way of providing sample outputs into prompts?",
        "timestamp": "2024-06-24 01:52:07.119000+00:00",
        "id": 1254615006289592453,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "I have a field which is a list of baml objects\n\n```rust\nclass Contact {\nname string?\nemail string?\n}\n\nclass Requirement {\n  text string\n  contacts Contact[]\n}\n```\nI assumed that if no contacts were able to be inferred, it would return an empty list. Instead, I am seeing the following from the output (ignore the `celery_worker_rfp-1`\n```\n\"contacts\": [\ncelery_worker_rfp-1           |           {\ncelery_worker_rfp-1           |             \"name\": null,\ncelery_worker_rfp-1           |             \"email\": null,\ncelery_worker_rfp-1           |             \"phone\": null\ncelery_worker_rfp-1           |           }\ncelery_worker_rfp-1           |         ],\n```\n\nAny idea why that is?",
        "timestamp": "2024-06-22 17:49:26.366000+00:00",
        "id": 1254131148221841428,
        "parent_id": null
    },
    {
        "author": "nebuleto",
        "content": "Is there any way to input any files(which is not image) to LLM with Baml?",
        "timestamp": "2024-06-21 23:29:55.420000+00:00",
        "id": 1253854446018887770,
        "parent_id": null
    },
    {
        "author": "elijas_ai",
        "content": "Can we pass Model name to be used through function argument?",
        "timestamp": "2024-06-21 22:05:24.476000+00:00",
        "id": 1253833176938188820,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Are there docs for implementing dynamic types? I would like to pass in an Enum I've defined in application code",
        "timestamp": "2024-06-21 18:12:07.795000+00:00",
        "id": 1253774470603083946,
        "parent_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does baml support datetimes? Or should we still use the `string` types with descriptions that specify ISO 8601 Timestamp format?",
        "timestamp": "2024-06-21 16:44:54.253000+00:00",
        "id": 1253752519537201263,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "Multi-agent",
        "timestamp": "2024-06-20 14:53:30.773000+00:00",
        "id": 1253362099124568137,
        "parent_id": null
    },
    {
        "author": "hellovai",
        "content": "using images",
        "timestamp": "2024-06-20 14:47:06.066000+00:00",
        "id": 1253360485546594357,
        "parent_id": null
    },
    {
        "author": "mariustrovik",
        "content": "Can I feed locally stored images to the LLM? If yes, could you give me some pointers on how to go about this (Class, function and main.py)? The goal would be to feed images of financial tables to the LLM and receive JSON back which I can store on my computer ðŸ™ \n\nI attempted something like this, but I cannot figure out how to pass the image data to the b.ExtractTable-function:\n\n```import asyncio\nimport json\nfrom pathlib import Path\nimport base64\nfrom baml_client import b\n\nasync def main():\n    try:\n        # Local path to the image\n        image_path = Path(\"C:/Users/Trovi/Documents/Projects/ML/baml_table_extraction/Page_131_High_Res_Image.png\")\n\n        # Read the image file\n        with open(image_path, 'rb') as image_file:\n            image_data = image_file.read()\n\n        # Encode the image to base64\n        image_base64 = base64.b64encode(image_data).decode('utf-8')\n\n        # Extract table from the image using the base64 string\n        table = await b.ExtractTable(image_base64) \n        \n        with open('extracted_table.json', 'w', encoding='utf-8') as file:\n            json.dump(table, file, ensure_ascii=False, indent=4)\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())`\n\n\n-------resume.baml--------\n// Function to extract the table from an image.\nfunction ExtractTable(first_image: image ) -> Table {\n  client GPT4o\n  prompt #\"\n    {{ _.role(\"user\")}}\n\n    Task: Analyze the provided image and extract the table data.\n\n    {{ first_image }}\n    {{ ctx.output_format }}\n\n  \"#\n}```",
        "timestamp": "2024-06-20 14:29:01.790000+00:00",
        "id": 1253355937763295232,
        "parent_id": null
    }
]