[
    {
        "thread_id": 1253355937763295232,
        "thread_name": "using images",
        "messages": [
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 14:29:01.790000+00:00",
                "content": "Can I feed locally stored images to the LLM? If yes, could you give me some pointers on how to go about this (Class, function and main.py)? The goal would be to feed images of financial tables to the LLM and receive JSON back which I can store on my computer üôè \n\nI attempted something like this, but I cannot figure out how to pass the image data to the b.ExtractTable-function:\n\n```import asyncio\nimport json\nfrom pathlib import Path\nimport base64\nfrom baml_client import b\n\nasync def main():\n    try:\n        # Local path to the image\n        image_path = Path(\"C:/Users/Trovi/Documents/Projects/ML/baml_table_extraction/Page_131_High_Res_Image.png\")\n\n        # Read the image file\n        with open(image_path, 'rb') as image_file:\n            image_data = image_file.read()\n\n        # Encode the image to base64\n        image_base64 = base64.b64encode(image_data).decode('utf-8')\n\n        # Extract table from the image using the base64 string\n        table = await b.ExtractTable(image_base64) \n        \n        with open('extracted_table.json', 'w', encoding='utf-8') as file:\n            json.dump(table, file, ensure_ascii=False, indent=4)\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())`\n\n\n-------resume.baml--------\n// Function to extract the table from an image.\nfunction ExtractTable(first_image: image ) -> Table {\n  client GPT4o\n  prompt #\"\n    {{ _.role(\"user\")}}\n\n    Task: Analyze the provided image and extract the table data.\n\n    {{ first_image }}\n    {{ ctx.output_format }}\n\n  \"#\n}```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 14:47:05.961000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 14:47:06.408000+00:00",
                "content": "great point, our docs are missing!\n\nHere's how you do it in python: \nYou can access example code in python here (this has a lot of examples we use in our unit tests): https://github.com/BoundaryML/baml/blob/b19f04a059ba18d54544cb278b6990b95170d3f3/integ-tests/python/test_functions.py#L96\n\n```python\n\n\nres = await b.TestImageInput(\n        img=baml_py.Image.from_url(\n\"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n        )\n    )\n```\n\nthere's also a `Image.from_base64` but that has a bug that we'll be patching in today's release! (`0.42`). \n\nInstead you can just do:\n```\nbaml_py.Image.from_url(f'data:{mime_type};base64,{base64_image}')\n```"
            },
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 17:09:12.493000+00:00",
                "content": "Thanks for the quick reply. This works!\n\nI have now set up my main.py and I am able to send the image to the LLM and get a response. I do have one more question though:\n\nWhy does the Playground give me perfect JSON, however the function output (result) in main.py gives me a list of list?"
            },
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 17:09:37.668000+00:00",
                "content": "```\n---resume.baml---\nclass Table {\n  row string[][] @description(#\"\n    The rows of the table, each containing the data for each column.\n  \"#)\n}\n\n// Function to extract the table from an image.\nfunction ExtractTable(first_image: image ) -> Table {\n  client GPT4o\n  prompt #\"\n    {{ _.role(\"user\")}}\n\n    Task: Analyze the provided image and extract the table data to JSON format:\n\n    {{ first_image }}\n\n    Extract this data in JSON format as described below:\n\n    {{ ctx.output_format }}\n\n    Before generating the output, validate the structure and content for accuracy and completeness.\n  \"#\n}\n\ntest table_extraction {\n  functions [ExtractTable]\n  args {\n    first_image { url \"https://i.postimg.cc/Xq4BjrSk/Page-131-High-Res-Image-Partial1.png\"}\n  }\n}\n```"
            },
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 17:10:02.658000+00:00",
                "content": "```\n---main.py---\nimport asyncio\nimport base64\nfrom pathlib import Path\nimport json\nimport baml_py\nfrom baml_client import b\nfrom baml_client.types import Table\n\nasync def main():\n    try:\n        # Local path to the image\n        image_path = Path(\"C:/Users/Trovi/Documents/Projects/ML/baml_table_extraction/Page_131_High_Res_Image_Partial1.png\")\n\n        # Read the image file\n        with open(image_path, 'rb') as image_file:\n            image_data = image_file.read()\n\n        # Encode the image to base64\n        base64_image = base64.b64encode(image_data).decode('utf-8')\n        mime_type = \"image/png\"  # Ensure this matches the actual image type\n\n        # Create an Image object using the from_url method with base64 data\n        Image = baml_py.Image.from_url(f'data:{mime_type};base64,{base64_image}')\n\n        result = await b.ExtractTable(Image)\n        \n        print(result)\n\n        # Save the result to a JSON file\n        output_path = Path(\"extracted_table.json\")\n        with open(output_path, 'w', encoding='utf-8') as json_file:\n            json.dump(result, json_file, ensure_ascii=False, indent=4)\n\n        print(f\"Results saved to {output_path}\")\n\n        assert isinstance(result, Table)\n\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 18:18:11.574000+00:00",
                "content": "can you share a bit more about what the outputs are?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 18:18:29.082000+00:00",
                "content": "like the raw output you get out of the prompt?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 18:18:37.952000+00:00",
                "content": "(in python vs the playground)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 18:18:43.501000+00:00",
                "content": "(screenshots are ok btw)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 18:29:59.863000+00:00",
                "content": "FYI, i created an issue after getting a partial repro on something similar: https://github.com/BoundaryML/baml/issues/702"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 18:30:12.450000+00:00",
                "content": "Will report back and figure out why the parser struggled!"
            },
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 18:42:05.577000+00:00",
                "content": "Thanks for looking into this üôè"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 19:22:11.755000+00:00",
                "content": "<@827968359856341023> i think you're actually getting what you want. We use pydantic (https://docs.pydantic.dev/2.7/) for all the models you generate. So you have a class called `Table` with a property called `row` of type `List[List[int]]`. \n\nTo print a pydantic model to json or as a dictionary:\n```\nresult.model_dump() -> key value dict\nresult.model_dump_json() -> str\n```"
            },
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 19:40:13.684000+00:00",
                "content": "Ah, very interesting. I had a feeling I was causing this myself. The syntax of building classes was a bit of a mystery and I tried my best to get something that would work. A list of lists gave no errors, so I stuck with it. I figured it would convert to JSON just like in the Playground. \n\nIt's great that the Playground is so quick and fail-proof; it gives the user an early sense of accomplishment. But do I understand correctly that there are some hidden \"One size fits all\"-parsing steps applied in the Playground that I have to create myself in the main.py? Such as the ones you suggest: \n¬¥result.model_dump() -> key value dict¬¥\n¬¥result.model_dump_json() -> str¬¥"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 19:42:46.803000+00:00",
                "content": "ah no, there's no extra work to be done in the python code.\n\nIn the playground we just are calling the equivalent of `model_dump()` for you in the UI üôÇ \n\nits more so the type of the function have you a `Table` class.\n\nwhen you do a `print(result)`, the default print method for the `Table` class uses the method pydantic defines (which does the `row=[ ...]`\n\nyou can still acess it via:\n```\nfor row in result.row:\n  for item in row:\n    print(item)\n  print(\"END\")\n```"
            },
            {
                "author": "mariustrovik",
                "timestamp": "2024-06-20 19:48:18.305000+00:00",
                "content": "I see, this makes sense. In the beginning I had a feeling that the magic of Baml was \"I will give you perfect JSON, no matter what, you just sit back and relax\", but now I see that there is a structure to it and \"things actually make sense\" (And that's preferable of course :D). Thank you for guiding me past through this bumpy patch in my Baml-journey.\n\nIf I can be of any help, please let me know. I do not believe I am up to par when it comes to contributing on the coding side, but perhaps user feedback or something could be of help."
            }
        ]
    },
    {
        "thread_id": null,
        "thread_name": null,
        "messages": [
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 14:47:06.066000+00:00",
                "content": "using images"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 13:57:24.778000+00:00",
                "content": "Helix"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 07:37:02.494000+00:00",
                "content": "Defining Types in Python"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:01:56.479000+00:00",
                "content": "does Gemini support temperature?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:02:04.931000+00:00",
                "content": "i get an error when I add it to options"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 14:15:51.374000+00:00",
                "content": "VLLM support"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-12 16:04:03.821000+00:00",
                "content": "100% cpu usage"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-14 07:04:14.369000+00:00",
                "content": "Formatting"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 16:15:02.072000+00:00",
                "content": "BAML vs Outlines vs Instructor"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 05:39:19.353000+00:00",
                "content": "iframe hyperlinks"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:33:57.201000+00:00",
                "content": "Does baml automatically detect whether"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:30:53.973000+00:00",
                "content": "Variables in functions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:33:48.453000+00:00",
                "content": "custom jinja filters"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 19:49:56.172000+00:00",
                "content": "Dyanmically settings API keys"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:46:26.042000+00:00",
                "content": "Benchmark thread"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-09 17:01:24.951000+00:00",
                "content": "Hi, I am suddenly getting this error,"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:07:12.310000+00:00",
                "content": "Debugging ClientRegistry"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 13:23:44.024000+00:00",
                "content": "BAML + Rust"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:47:32.362000+00:00",
                "content": "exceptions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:14:36.008000+00:00",
                "content": "new dynamic enums"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-28 18:54:23.752000+00:00",
                "content": "Langchain X BAML"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 13:25:04.848000+00:00",
                "content": "deno + BAML"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 17:02:37.447000+00:00",
                "content": "Custom transforms"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-29 23:21:49.330000+00:00",
                "content": "nonetheless, the rest of requests seem to be executing fine."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 00:29:40.447000+00:00",
                "content": "Panic"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-01 14:19:48.712000+00:00",
                "content": "Hi! In Nuxt 3 project I get ```X [ERROR"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-03 22:56:37.153000+00:00",
                "content": "organization"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 02:44:17.268000+00:00",
                "content": "looking to generate a list of operations as a result and i was hoping for some easy way to determine which among the set of possible operations was selected"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 02:47:13.612000+00:00",
                "content": "maybe an enum of one element?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:41:39.072000+00:00",
                "content": "batching requests"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:42:34.883000+00:00",
                "content": "Optional arrays"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:47:17.957000+00:00",
                "content": "openai structured output"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-05 11:07:49.166000+00:00",
                "content": "I'd still like the rest of the schema if one field failed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:41:48.497000+00:00",
                "content": "parsing validation"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 01:13:24.334000+00:00",
                "content": "is there a way to add a description of a"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:05:55.058000+00:00",
                "content": "Classes in test cases"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:45:06.915000+00:00",
                "content": "Pydantic description"
            },
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 21:37:07.479000+00:00",
                "content": "Does the BAML parser remove \\n from the response?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:07:19.474000+00:00",
                "content": "Any idea what sn empty error linting is?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:07:32.028000+00:00",
                "content": "I somehow can't make a previous successful client compile"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:07:52.233000+00:00",
                "content": "I literally added am empty line and saved"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 03:26:41.269000+00:00",
                "content": "Baml raw request"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 04:52:43.506000+00:00",
                "content": "optional list"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 13:36:32.192000+00:00",
                "content": "Hi guys, I love the project, it's really"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 14:03:58.775000+00:00",
                "content": "Session history in BAML"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-19 14:04:52.151000+00:00",
                "content": "maybe this will help https://docs.boundaryml.com/docs/snippets/clients/retry"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 14:15:31.506000+00:00",
                "content": "I was reading it, but it seems for network error"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 14:15:49.445000+00:00",
                "content": "No idea if internally do also the same"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 15:55:33.948000+00:00",
                "content": "Retry on parse failure"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 19:00:36.119000+00:00",
                "content": "Dashboard bug"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-19 19:34:48.153000+00:00",
                "content": "is there a way to easily define a python filtering function for each generated token? for example regex"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 19:37:09.849000+00:00",
                "content": "Prompting doesn‚Äôt work for you?"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 19:38:28.857000+00:00",
                "content": "You can do an after validation with pydantic or a dataclass if the word that appears is on the list, send the error and then try again using the feedback of the error"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-19 19:39:24.031000+00:00",
                "content": "not 100% of the time. i end up removing answers with keywords using jq"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-19 19:47:56.795000+00:00",
                "content": "i assume that pydantic validation happens after json of the structured response was completed. i filter after the response now already using jq (or python). what i was wondering is since baml works with streaming api (my assumption), it checks every generated token (to make sure json syntax is correct). so it would makes sense to regenerate a wrong token instead of the whole response. if there are multiple wrong tokens in the response, the probability of regecting response goes up"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-19 19:50:08.787000+00:00",
                "content": "another example of this problem is one of the simple task where all llms fail \n```\nWrite me a sentence without any words that appear in The Bible.\n\n```\nhttps://arxiv.org/html/2405.19616v2#S9.SS2.SSS8 see 9.2.8"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:51:41.304000+00:00",
                "content": "Reduce probability of each token"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 11:35:17.688000+00:00",
                "content": "Error message"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 02:52:38.309000+00:00",
                "content": "Is it possible to run tests within"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:25:41.727000+00:00",
                "content": "validations"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 13:28:02.503000+00:00",
                "content": "Schema robustness"
            },
            {
                "author": "tdn8",
                "timestamp": "2024-09-23 15:15:30.731000+00:00",
                "content": "When doing function/tool calling with"
            },
            {
                "author": "davidyoung",
                "timestamp": "2024-09-23 16:53:40.929000+00:00",
                "content": "Ignore, just saw you answered above üôÇ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-23 17:16:58.484000+00:00",
                "content": "Is there a way currently to have @@"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 20:05:49.975000+00:00",
                "content": "retry policy"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-29 02:13:10.659000+00:00",
                "content": "Default params"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:19:38.238000+00:00",
                "content": "Inputs rendering with aliases"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:20:54.422000+00:00",
                "content": "Input output tokens"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 18:58:06.405000+00:00",
                "content": "client graph"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-01 15:58:58.959000+00:00",
                "content": "Parens highlighting"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-10-01 20:34:25.543000+00:00",
                "content": "recursive types"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:57:23.504000+00:00",
                "content": "I asked about how to deal with"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:20:17.642000+00:00",
                "content": "is there a way to stop BAML logging (in the notebook). I upgraded to the version 0.57.1 and am noticing the logs/outputs are being printed:"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:20:34.381000+00:00",
                "content": "set BAML_LOG=warn"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:20:41.623000+00:00",
                "content": "thanks!"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-10-03 20:05:24.803000+00:00",
                "content": "Tokenize"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-06 01:02:37.911000+00:00",
                "content": "Recursion"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 17:24:54.288000+00:00",
                "content": "vllm with images"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:40:15.995000+00:00",
                "content": "Any plan to integrate with LMDeploy soon?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:10:02.576000+00:00",
                "content": "Hi! I'm tried running  unit test(s) from the prompt-shepards model using Llama3.1 70B-instruct but am getting the following error:\nFailed to coerce value: <root>: Expected Source enum value, got String(\"\")."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:32:28.204000+00:00",
                "content": "dynamic types"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:33:26.282000+00:00",
                "content": "getting raw json"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:33:19.259000+00:00",
                "content": "class alias"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 13:07:12.753000+00:00",
                "content": "bug"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 03:25:43.365000+00:00",
                "content": "Auto generated baml functions"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:00:45.754000+00:00",
                "content": "vim syntax"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:03:17.382000+00:00",
                "content": "BAML editor UI component"
            }
        ]
    },
    {
        "thread_id": 1253362099124568137,
        "thread_name": "Multi-agent",
        "messages": [
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 14:53:30.773000+00:00",
                "content": "Multi-agent"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 14:53:31.527000+00:00",
                "content": "<@346578058389749762> the way we do multi-agents in baml is more like code you would write, even if you weren't doing ai. If you wanted to do a router that takes a query then picks one of many agents to pass it down to you would do something like this:\n\n```rust\nenum Category {\n  TroubleShooting @description(#\"a short description\"#)\n  Buying @description(#\"a short description\"#)\n}\n\nfunction PickBestCategory(query: string) -> Category {\n  client GPT4o\n  prompt #\"\n    {{ ctx.output_format }}\n\n    {{ _.role('user') }}\n    {{query}}\n  \"#\n}\n```\n\nthen in python:\n\n```python\nfrom baml_client import b\nfrom baml_client.types import Category\n\nasync def pipeline(query: str):\n   category = await b.PickBestCategory(query)\n\n   if category == Category.TroubleShooting:\n      # code here to handle this however you want\n   elif category == Category.Buying\n      # So on...\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 15:01:42.266000+00:00",
                "content": "in typescript you would do:\n```typescript\nimport {b} from \"baml_client\"\nimport {Category} from \"baml_client/types\"\n\n\nasync function pipeline(query: string) {\n  const category = await b.PickBestCategory(query);\n  if (category == Category.TroubleShooting) {\n    return await TroubleShootingAgent(query);\n  }\n  if (category == Category.Buying) {\n    return await BuyingAgent(query);\n  }\n\n  return \"Sorry! I can't help you with that!\";\n}\n\n```\n\nLet me get you an example that also holds onto conversation history, so you can see what it looks like in a chat app"
            }
        ]
    },
    {
        "thread_id": 1253752519537201263,
        "thread_name": "date time",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-06-21 16:44:54.253000+00:00",
                "content": "Does baml support datetimes? Or should we still use the `string` types with descriptions that specify ISO 8601 Timestamp format?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 16:50:40.363000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 16:50:40.782000+00:00",
                "content": "string is the way!"
            }
        ]
    },
    {
        "thread_id": 1253774470603083946,
        "thread_name": "Dyanamic Enums",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-06-21 18:12:07.795000+00:00",
                "content": "Are there docs for implementing dynamic types? I would like to pass in an Enum I've defined in application code"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 18:14:33.499000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 18:14:33.993000+00:00",
                "content": "we're working on docs! Should out out by EOD.\n\ntl;dr:\n```python\nfrom baml_client.type_builder import TypeBuilder\n\ntb = TypeBuilder()\nfor _, row in df.iterrows():\n  if row[\"Parent\"] in root_categories:\n    tb.Tools.add_value(row['Categories'])\nselected = await b.Classify(tool, description, count=1, baml_options={ \"tb\": tb })\n```\n\n```rust\n// Defining a data model.\nenum Tools {\n  // We'll define these in python\n  @@dynamic\n\n  @@alias(ToolCategory)\n}\n\nclass Classification {\n  category Tools\n  reason string\n}\n\n\n\n// Creating a function to extract the resume from a string.\nfunction Classify(tool: string, description: string, count: int) -> Classification[] {\n  client FastOpenAI\n  prompt #\"\n    Given tools and their descriptions, classify the tools into categories.\n    {{ ctx.output_format(enum_value_prefix=null) }}\n\n    {% if count > 1 %}\n    Give me the {{ count }} best options.\n    {% endif %}\n\n    {{ _.role('user') }}\n    Tool: {{ tool }}\n    Description: {{ description }}\n  \"#\n}\n```\n\nHere's a repo example:\nhttps://github.com/BoundaryML/example-massive-categorizer/blob/main/classifier.ipynb"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-21 18:17:15.362000+00:00",
                "content": "what does it look like when baml returns the output, will it be the same type as the class I pass in?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 18:17:57.173000+00:00",
                "content": "the auto complete should work!\n\nThe return tyep of anything of type Tool auto becomes:\n```\nTools | str\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-21 18:20:38.308000+00:00",
                "content": "my case is\n\n# Application code\n```python\nclass DocumentType(Enum):\n   NOTION\n   GDRIVE\n```\n\n# Baml code\n```rust\nenum DocumentType {\n   @@dynamic\n}\n\nfunction getDocumentType(document_text: string) -> DocumentType {\n\n}\n```\n\nSo when I call the code in application, I can expect the return type to be the `DocumentType` I defined in Application code OR a `str`?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 18:21:01.279000+00:00",
                "content": "quick huddle?"
            }
        ]
    },
    {
        "thread_id": 1253833176938188820,
        "thread_name": "using client props in a function",
        "messages": [
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-21 22:05:24.476000+00:00",
                "content": "Can we pass Model name to be used through function argument?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:08:00.933000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:08:01.114000+00:00",
                "content": "what are you trying to do?\n\nWe definitely support accessing your client provider and name like this:\n\n{{ ctx.client.name }}"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:08:03.075000+00:00",
                "content": "in the prompt"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:08:18.481000+00:00",
                "content": "you can try adding {{ ctx.client }} and see the playgorund preview to see all the available properties"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-21 22:55:02.451000+00:00",
                "content": "I want to give user ability to select to gpt4o or sonnet3.5"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:55:17.331000+00:00",
                "content": "oh you want to change the client at runtime"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-21 22:55:21.218000+00:00",
                "content": "yes"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:55:58.767000+00:00",
                "content": "yes we have a way of doing it -- though it's going out on the next release (tonight). CC <@99252724855496704>"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-21 22:56:33.034000+00:00",
                "content": "Awesome! No rush, though"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-21 22:56:35.243000+00:00",
                "content": "Thank you"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-21 22:56:46.748000+00:00",
                "content": "üôå"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 22:57:22.030000+00:00",
                "content": "cool np. For now you could create 2 funcitons and copy the prompt over. We'll make it easier very very soon."
            }
        ]
    },
    {
        "thread_id": 1253854446018887770,
        "thread_name": "arbritrary files",
        "messages": [
            {
                "author": "nebuleto",
                "timestamp": "2024-06-21 23:29:55.420000+00:00",
                "content": "Is there any way to input any files(which is not image) to LLM with Baml?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:30:28.741000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:30:29.034000+00:00",
                "content": "hi haze, what kind of files are you talking about?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:30:36.348000+00:00",
                "content": "and what language are you trying to interface with"
            },
            {
                "author": "nebuleto",
                "timestamp": "2024-06-21 23:31:32.638000+00:00",
                "content": "I'm using typescript, and I want to input multiple pdf files or html files."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:31:56.282000+00:00",
                "content": "and what models are you planning on using?"
            },
            {
                "author": "nebuleto",
                "timestamp": "2024-06-21 23:33:27.913000+00:00",
                "content": "I'm planning to use Gemini 1.5 Flash/Pro, Claude 3 Haiku/3 Sonnet/3.5 Sonnet, GPT-4o. I think this models supports to input files."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-21 23:36:05.782000+00:00",
                "content": "Anthropic models only support text and images:\n\nBut gemini does support files! Vaibhav will chime in on that in a sec"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:36:40.545000+00:00",
                "content": "and the best way to do this, yea i think only gemini supports other files, but for now the method we recommend doing the file loading / OCR / convering to an image and loading prior to baml, then just passing in the files as a string param to a baml function.\n\n```rust\nclass File {\n  name string\n  content string\n}\n\nfunction DoSomething(goal: string, files: File[]) -> string {\n  client GPT4o\n  prompt #\"\n     Do {{ goal }}\n     \n     {% for file in files %}\n     {{ _.role('user') }}\n     Title: {{ file.name }}\n     {{ file.content }}\n     {% endfor %}\n  \"#\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:38:13.108000+00:00",
                "content": "then in typescript:\n\n```typescript\nimport {b} from 'baml_client'\nimport {File} from 'baml_client/types'\n\nasync function foo(files: string[]) {\n  const loaded_files: File[] = loadFiles(files)\n  await b.DoSomething(\"summarize the main points\")\n}\n```"
            },
            {
                "author": "nebuleto",
                "timestamp": "2024-06-21 23:38:44.042000+00:00",
                "content": "I can solve with this way. Thanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:38:53.178000+00:00",
                "content": "glad that helped! üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:39:12.098000+00:00",
                "content": "if you end up wanting to use images, you can make content a `string | image` type"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 23:39:19.703000+00:00",
                "content": "and it will just work as well FYI"
            }
        ]
    },
    {
        "thread_id": 1254131148221841428,
        "thread_name": "I have a field which is a list of baml",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:49:26.366000+00:00",
                "content": "I have a field which is a list of baml objects\n\n```rust\nclass Contact {\nname string?\nemail string?\n}\n\nclass Requirement {\n  text string\n  contacts Contact[]\n}\n```\nI assumed that if no contacts were able to be inferred, it would return an empty list. Instead, I am seeing the following from the output (ignore the `celery_worker_rfp-1`\n```\n\"contacts\": [\ncelery_worker_rfp-1           |           {\ncelery_worker_rfp-1           |             \"name\": null,\ncelery_worker_rfp-1           |             \"email\": null,\ncelery_worker_rfp-1           |             \"phone\": null\ncelery_worker_rfp-1           |           }\ncelery_worker_rfp-1           |         ],\n```\n\nAny idea why that is?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:52:58.612000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:52:59.044000+00:00",
                "content": "this is just prompt engineering. \n\nI think we can add an attribute to a field that helps you auto filter some of those out.\n(e.g. in class Contact)\n\n```\nclass Contact {\n  name string?\n  email string?\n  @@require_atleast_one_field\n}\n```\n(not the exact one, but you get the idea)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:53:14.688000+00:00",
                "content": "This already exists?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:53:24.122000+00:00",
                "content": "no it doesn't, just an idea of what we could do"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:55:09.990000+00:00",
                "content": "from a language perspective, and from the model's perspective, every field being Null is  a valid option since that is a valid class definition given the types of the field."
            }
        ]
    },
    {
        "thread_id": 1254615006289592453,
        "thread_name": "adding examples",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-06-24 01:52:07.119000+00:00",
                "content": "Hi, is there a built in way of providing sample outputs into prompts?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:55:24.087000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:55:24.394000+00:00",
                "content": "from <@201399017161097216> Not at the moment, the best way is to add an input argument with your examples. You would need to pass them in from your code and print them out in the prompt.\n\nfunction(arg1: string, examples: OutputType[]) -> OutputType"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-24 01:55:42.277000+00:00",
                "content": "Do I need to format it in a particular way"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-24 01:56:20.912000+00:00",
                "content": "Or is the str representation good enough"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:56:32.775000+00:00",
                "content": "then you can actually do:\n\n```\nfunction(arg1: string, examples: OutputType[]) -> OutputType {\n  client GPT4\n  prompt #\"\n  ...\n   {% for example in examples %}\n   example\n   {% endfor %}\n  \"#\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:56:45.132000+00:00",
                "content": "the playground should show you what the final prompt looks like!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:57:09.355000+00:00",
                "content": "docs on loops: https://docs.boundaryml.com/docs/snippets/prompt-syntax/loops"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:57:36.110000+00:00",
                "content": "also, as long as the type of the examples is the same as your output type, then it shouldn't matter too much"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 01:57:53.811000+00:00",
                "content": "do you have an example of the prompt you are able to share? Can give better advice with a more specific example"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-24 02:42:12.109000+00:00",
                "content": "is the playground in vscode? Or do you mean promptfiddle?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:42:29.585000+00:00",
                "content": "both üôÇ They are the same thing!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:42:36.367000+00:00",
                "content": "They run the same code*"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:47:11.262000+00:00",
                "content": "sorry in the loop it should be:\n```\n   {% for example in examples %}\n   {{ example }}\n   {% endfor %}\n```\n\nyou can do more interestuing thigns like:\n\n```\n   {% for example in examples %}\n   {{ _.role('user') }}\n   Example {{ loop.index }}:\n   {{ example }}\n   {% endfor %}\n```"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-24 02:58:12.611000+00:00",
                "content": "How do you use the playground in vscode?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:58:46.845000+00:00",
                "content": "there should be an open playground button over the baml function:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#opening-baml-playground"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-25 02:21:12.050000+00:00",
                "content": "When I pass in an object, it renders it in one line instead of pretty json format"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-25 02:21:31.383000+00:00",
                "content": "Is there a way of jsonify it like in jinja"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 03:05:46.493000+00:00",
                "content": "yes! You can do this:\n\n```\n{{ variable || pprint }}\n```\n\nhere's the complete list of built ins!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 03:05:47.915000+00:00",
                "content": "https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 03:05:55.261000+00:00",
                "content": "we should add these to our docs! ü•≤"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-25 03:24:54.981000+00:00",
                "content": "Is it good practice to do that so that it matches with the output format?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-25 03:25:05.115000+00:00",
                "content": "It‚Äôs more tokens right"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 03:26:27.723000+00:00",
                "content": "I think I would try and experiment. In some cases you will need the format but in others you may not!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 03:26:42.635000+00:00",
                "content": "It‚Äôs hard to say one or the other upfront"
            }
        ]
    },
    {
        "thread_id": 1254621257673216001,
        "thread_name": "do you commit the autogenerated python",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-06-24 02:16:57.565000+00:00",
                "content": "do you commit the autogenerated python files from baml to git?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:17:45.827000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:17:46.272000+00:00",
                "content": "some people do! but you can also .gitignore them and just run the generate command in your build script via `baml-cli generate`\n\nhttps://docs.boundaryml.com/docs/get-started/deploying/docker"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-10 20:05:26.920000+00:00",
                "content": "commiting autogenerated code sometimes causes annoying merge conficts if there are multiple people working on baml prompts..."
            }
        ]
    },
    {
        "thread_id": 1255120669608513641,
        "thread_name": "Helix",
        "messages": [
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 11:21:26.649000+00:00",
                "content": "My friend is using https://helix-editor.com/ IDE, what would be the best way to use BAML with it?"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 13:57:24.778000+00:00",
                "content": ""
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 13:57:25.579000+00:00",
                "content": "<@99252724855496704> i.e. is there maybe a Language Server I could connect?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:15:58.404000+00:00",
                "content": "Sorry I was reading the helix docs!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:16:56.732000+00:00",
                "content": "Right now we bundle our language server in with our extension. I think if we pull it out and ship it as a separate npm package it could work!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:17:44.702000+00:00",
                "content": "I‚Äôll take a stab at this over the weekend and get back to you! But it looks achievable."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:17:56.180000+00:00",
                "content": "This is the best workaround tho: https://docs.boundaryml.com/docs/get-started/quickstart/editors-other"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:22:01.431000+00:00",
                "content": "Do you know how you get syntax highlighting in helix"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:31:03.014000+00:00",
                "content": "Nvmind. Found it. Seems like tree sitter grammars"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 23:58:17.150000+00:00",
                "content": "This is super sweet, thanks for all the amazing work you do"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-01 18:53:12.111000+00:00",
                "content": "<@99252724855496704> Hey, do you have any updates?\nAny idea how hard it would be to add helix support ourselves?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 19:22:23.698000+00:00",
                "content": "Hey Elijas! Gave it a try and its a bigger refactor job and i thought for getting the full language server (with the playground) üò¶ \n\nBut here's what i recommend that would be a good stop gap:\n\n1. Add an auto compile hook in Helix for on save events in BAML file (run `baml-cli generate` from your python env or `npx baml-cli generate`)\n2. Adding a customer tree-sitter grammar file for syntax highlighting (attaching below - I'm still trying to figure out how helix actually runs the grammar, but was able to get the mostly grammar working atleast)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 19:23:34.305000+00:00",
                "content": "I was basing off of: https://github.com/tree-sitter/tree-sitter-javascript/blob/master/grammar.js"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 19:24:57.595000+00:00",
                "content": "For your team, whats the order of preference here in terms of most to least importance btw?\n\n1. playground\n2. syntax highlighting\n3. auto-generating the client code to keep python / TS in sync with BAML"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-01 22:20:04.271000+00:00",
                "content": "I highly appreciate for taking a look!\n\nI think (2) would the top priority, with (1) as second-place priority, and (3) least important due to file-save-hook doing most of the value\n\nHowever, I do completely understand the need for BAML to focus on core user base and features (VSCode), so definitely feel free to prioritize to maximize the overall success of BAML üôå \n\nThanks again üöÄ"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-01 22:21:26.337000+00:00",
                "content": "I'm personally using Cursor (LLM-enabled fork of VSCode) and not VSCode, though, but it has been baml extension has been 100% compatible so far"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 22:21:43.541000+00:00",
                "content": "awesome, in that case let me take another stab at the syntax highlighting w/ tree-sitter grammars.\n\nThe playground is something we're looking to enable for non-vscode devs via a:\n```\nbaml-cli playground \n```"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-01 22:21:55.403000+00:00",
                "content": "(My teammate is using Helix though)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 22:22:10.512000+00:00",
                "content": "ah yes, cursor should work! Do you have issues updating the BAML extension via cursor btw?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 22:22:23.364000+00:00",
                "content": "you should get them on cursor üòâ"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-01 22:23:45.077000+00:00",
                "content": "no issues, it's on latest 0.45.0"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 22:23:54.572000+00:00",
                "content": "nice!"
            }
        ]
    },
    {
        "thread_id": 1255141853683257394,
        "thread_name": "Is there support for classes with nested",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-06-25 12:45:37.326000+00:00",
                "content": "Is there support for classes with nested fields? i.e. something like \n\n```rust\nclass Section {\n   id string\n    text string\n    subsections Section[]\n}\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-25 12:45:50.733000+00:00",
                "content": ""
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-25 12:45:50.964000+00:00",
                "content": "I run into dependency cycle issues"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 13:04:37.015000+00:00",
                "content": "Sadly not. To support that we need to do a bit more work and testing on the prompt side as well"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-25 13:05:14.643000+00:00",
                "content": "rip"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 13:56:15.838000+00:00",
                "content": "<@1049713528170364968> \nyou could try to do with indexes\n\nclass Section {\n    id int\n    text string\n    subsection_ids int[]\n}"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-06-25 13:56:41.589000+00:00",
                "content": "(BAML would return Section[])"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-25 14:19:08.902000+00:00",
                "content": "How nested do you go? If it‚Äôs not infinite, you could just create a section and subsection class"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-26 13:02:13.924000+00:00",
                "content": "In the real application, there's no limit to how nested the final structure can be :yikes!:\n\nFor now I'm adding the constraint that there can only be 2 levels of nesting, though ideally it would be more dynamic. That said, I can't imagine more than 3 levels for 95% of cases"
            }
        ]
    },
    {
        "thread_id": 1255308466243899412,
        "thread_name": "How can we provide dynamic types to BAML",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-25 23:47:40.856000+00:00",
                "content": "How can we provide dynamic types to BAML functions? I'd like to use an `enum`, but it seems like I can only use this statically with the enum variants hardcoded in the file -- I'm guessing a workaround would be to type it `string` and create a prompt in the @description annotation, but I'd rather avoid this if possible\n\n(relevant empty docs page: https://docs.boundaryml.com/docs/calling-baml/dynamic-types)"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-25 23:48:00.806000+00:00",
                "content": ""
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-25 23:48:01.165000+00:00",
                "content": "In my use case, the user can define a couple possible values for a field at an administrative level, which are stored in a database and need to be dynamically pulled and provided to my BAML function."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-25 23:48:12.802000+00:00",
                "content": "this is totally possible -- do you use typescript or python?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-25 23:48:16.577000+00:00",
                "content": "Typescript!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-25 23:50:28.902000+00:00",
                "content": "Here is a code sample:\nhttps://github.com/BoundaryML/baml/blob/canary/integ-tests/typescript/tests/integ-tests.test.ts#L240\n\nthe BAML definition class just needs to have @@dynamic .Works for enums or classes. See here\n\nhttps://github.com/BoundaryML/baml/blob/canary/integ-tests/baml_src/test-files/functions/output/class-dynamic.baml\n\n```\nimport TypeBuilder from '../baml_client/type_builder'\n\n it('should work with dynamics', async () => {\n    let tb = new TypeBuilder()\n    tb.Person.addProperty('last_name', tb.string().optional())\n    tb.Person.addProperty('height', tb.float().optional()).description('Height in meters')\n    tb.Hobby.addValue('CHESS')\n    tb.Hobby.listValues().map(([name, v]) => v.alias(name.toLowerCase()))\n    tb.Person.addProperty('hobbies', tb.Hobby.type().list().optional()).description(\n      'Some suggested hobbies they might be good at',\n    )\n\n    const res = await b.ExtractPeople(\n      \"My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop.\",\n      { tb },\n    )\n    expect(res.length).toBeGreaterThan(0)\n    console.log(res)\n  })\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-25 23:50:59.760000+00:00",
                "content": "we are still working on docs. Let us know if you run into any issues -- this feature is relatively new. For example there is no way to test this using our Playground"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-25 23:51:53.793000+00:00",
                "content": "make sure you enable BAML_LOG=baml_events or BAML_LOG=info as an environment variable to see the prompt in your terminal as you run things"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-25 23:52:37.643000+00:00",
                "content": "Gotcha thanks, taking a look now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-25 23:53:23.408000+00:00",
                "content": "other solutions folks have done is to use a string[] -- but you will need to validate it. If you use our typebuilder we can at least guarantee one of the enum values you define will be returned"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 00:26:04.235000+00:00",
                "content": "the righ timport for typebuilder is:\n`import TypeBuilder from \"../../../baml_client/type_builder\"`"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 01:07:54.530000+00:00",
                "content": "Nice, got something working -- is it possible to dynamically create an `enum` and THEN dynamically add variants to that enum?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 01:09:08.930000+00:00",
                "content": "I found addEnum but I'm not sure how to then access that enum to add properties"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 01:12:20.187000+00:00",
                "content": "Best I can find is doing something hacky like  `tb.__tb().getEnum(key).addProperty(value)` which I doubt works"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 01:19:09.096000+00:00",
                "content": "yes you can create an enum dynamically -- it doesnt need to be in BAML. Your function returns a class or enum?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 01:22:29.595000+00:00",
                "content": "here is a full example:\n```\nimport TypeBuilder from \"../../../baml_client/type_builder\";\nimport { Role } from \"../../../baml_client/types\";\n\nexport async function GET(request: Request) {\n  const tb = new TypeBuilder();\n\n  tb.Person.addProperty(\"hair_color\", tb.string());\n  const roleEnum = tb.addEnum(\"Role\");\n  roleEnum.addValue(\"Software Engineer\");\n  roleEnum.addValue(\"Intern\");\n\n  tb.Person.addProperty(\"role\", roleEnum.type());\n\n  const result = await b.ExtractPerson(\n    \"My name is Harry. I have black hair. I love skiing. I am 25 years old.\",\n    { tb: tb }\n  );\n  console.log(JSON.stringify(result, null, 2));\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 01:23:04.274000+00:00",
                "content": "do use the import like this -- accessing the tb.__tb() may break for you in the future -- the underscore methods are internal"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 01:25:51.399000+00:00",
                "content": "you do need to declare an output class in BAML to do this, with @@dynamic attached. In this case here's our function:\n```\nclass Person {\n  @@dynamic\n}\n\nfunction ExtractPerson(input: string) -> Person {\n  client GPT4o\n  prompt #\"\n    Extract from this input:\n    {{ input }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n```\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 02:00:45.936000+00:00",
                "content": "Ooh I see I didn‚Äôt realize addEnum returned the enumüòÖ that makes sense"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 02:02:42.405000+00:00",
                "content": "It returns an array of a class ‚Äî this should be enough for me to get an MVP thx!"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-06-26 13:56:19.229000+00:00",
                "content": "Hi, I gave this a try where my class has an attribute person Person[], Person being the class with characteristics. Person has name Name, etc. where Name is an enum. What I am finding is that when I use this approach, it never returns more than one Person class, whereas when I use name string in the Person class as opposed to an name enum, it does. Any idea why?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 15:29:27.581000+00:00",
                "content": "can you show me the code?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 15:31:02.224000+00:00",
                "content": "you probably need to add a description on `Person`, if you are using this dynamic type builder you would need to do `addProperty(\"person\", tb.string().description(\"This can be many people\")`\n\nAnother option is to name this `people` not `person`. LLMs try to fix grammar like that."
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-06-26 17:19:44.618000+00:00",
                "content": "Thanks, I'll give those suggestions a try"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 18:27:33.281000+00:00",
                "content": "Re: call today:\n\n```\n// Dynamically construct LineItemMetadata type based on EntityMetadata table\nconst entityMetadata = await prisma.entityMetadata.findMany({\n  where: {\n    entityId,\n  },\n})\n\nlet tb = new TypeBuilder()\nfor (let { key, value } of entityMetadata) {\n  const fieldEnum = tb.addEnum(key)\n  for (let variant of value) {\n    fieldEnum.addValue(variant)\n  }\n  tb.LineItemMetadata.addProperty(key, fieldEnum.type())\n}\n\n...\n\nconst predictedLineItemMetadata = await b.PredictLineItemMetadata(llmPastInvoicesStr, llmCurrentInvoiceStr, { tb })\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 18:39:28.881000+00:00",
                "content": "Also here's the stacktrace you saw on the call -- may be helpful since it seems like the root error is happening in your source code:\n\n```\n FAIL  server/helpers/invoiceClassifiers.test.ts (5.177 s)\n  Identical Invoices (rename this when I get more test in this describe\n    ‚úï Identical Invoices (1 ms)\n\n  ‚óè Identical Invoices (rename this when I get more test in this describe ‚Ä∫ Identical Invoices\n\n    TypeError: this.classes.has is not a function\n\n      72 |\n      73 |     addEnum<Name extends string>(name: Name): EnumBuilder<Name> {\n    > 74 |         this.tb.addEnum(name);\n         |                 ^\n      75 |     }\n      76 | }\n\n      at TypeBuilder.addEnum (node_modules/@boundaryml/baml/type_builder.js:49:26)\n      at TypeBuilder.addEnum (baml_client/type_builder.ts:74:17)\n      at server/helpers/invoiceClassifiers.ts:21:26\n      at fulfilled (server/helpers/invoiceClassifiers.ts:5:58)\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 18:43:20.936000+00:00",
                "content": "i reproduced the bug, working on a fix"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 18:54:36.880000+00:00",
                "content": "fixed it! the fix will roll out in 20ish min"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 18:55:04.515000+00:00",
                "content": "Amazing, thank you!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 19:30:17.152000+00:00",
                "content": "try version 0.44.0 on both VSCode extension + the TS package"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 20:20:08.331000+00:00",
                "content": "That error is fixed now, but now I'm getting something else -- it seems like `tb.addEnum(key)` is silently returning undefined\n```\n FAIL  server/helpers/invoiceClassifiers.test.ts\n  Identical Invoices (rename this when I get more test in this describe\n    ‚úï Identical Invoices (15 ms)\n\n  ‚óè Identical Invoices (rename this when I get more test in this describe ‚Ä∫ Identical Invoices\n\n    TypeError: Cannot read properties of undefined (reading 'addValue')\n\n      24 |     console.log({ fieldEnum })\n      25 |     for (let variant in value) {\n    > 26 |       fieldEnum.addValue(variant)\n         |                 ^\n      27 |     }\n      28 |     tb.LineItemMetadata.addProperty(key, fieldEnum.type())\n      29 |   }\n\n      at server/helpers/invoiceClassifiers.ts:26:17\n      at fulfilled (server/helpers/invoiceClassifiers.ts:5:58)\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 20:20:11.514000+00:00",
                "content": "Same code as before"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 20:21:18.055000+00:00",
                "content": "Did you update the vscode extension?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 20:21:24.819000+00:00",
                "content": "Save a baml file first"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 20:21:27.956000+00:00",
                "content": "And try rerunning"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 20:21:36.003000+00:00",
                "content": "The generated code had one bug"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 20:22:13.466000+00:00",
                "content": "Yea updated vscode and ts, trying saving now"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 20:23:00.555000+00:00",
                "content": "Nice that error is gone now, thanks"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 20:26:25.401000+00:00",
                "content": "Does it work end to end?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 22:28:26.374000+00:00",
                "content": "There were some issues on my end, but now it mostly works -- there's some odd pathological cases I'm looking at now and trying to mitigate"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-26 22:37:06.247000+00:00",
                "content": "lmk if I can help with the prompt engineering"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-26 22:48:01.753000+00:00",
                "content": "For sure"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:03:40.387000+00:00",
                "content": "I actually have seen the problem of not generating the right number of metadata objects several times now, even after putting the index and a line in the prompt saying the model has to provide the correct number -- any ideas?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:04:51.894000+00:00",
                "content": "One hacky idea I have is to dynamically generate a unique property with a templated name for each line item I want on a custom `LineItemMetadataArray` and use that instead of `LineItemMetadata[]`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:04:54.029000+00:00",
                "content": "What schema are you using?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:05:16.919000+00:00",
                "content": "```\n// Defining a data model dynamically\nclass LineItemMetadata {\n  index int\n\n  @@dynamic\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:05:33.831000+00:00",
                "content": "Instead of index can you make that some other id?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:05:53.740000+00:00",
                "content": "Does it skip numbers?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:06:31.561000+00:00",
                "content": "If I give it indices 0, 1, 2, 3 it sometimes gives me 1, 2, 3 back"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:06:36.391000+00:00",
                "content": "Same if I use indices 1, 2, 3, 4"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:06:52.125000+00:00",
                "content": "But when it misses line items does it just map them incorrectly?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:07:18.947000+00:00",
                "content": "No the mapping is always correct, it just doesn't map items that it can and should"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:07:24.278000+00:00",
                "content": "```\nfunction PredictLineItemMetadata(past_invoices: string, current_invoice: string) -> LineItemMetadata[] {\n  client GPT4Turbo\n  prompt #\"\n    Parse the following history of invoices to learn how metadata fields correlate with line items.\n\n    Past Invoices\n    ---\n    {{ past_invoices }}\n    ---\n\n    Now, parse the following invoice's line items and return a structured representation of the most likely corresponding metadata fields in the schema below.\n    You MUST provide one metadata object for each line item provided in the current invoice.\n\n    Current Invoice\n    ---\n    {{ current_invoice }}\n    ---\n\n    {{ ctx.output_format }}\n  \"#\n}\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:07:28.887000+00:00",
                "content": "Full prompt ^"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:08:09.656000+00:00",
                "content": "Can you give me an example of a failing case with the expected value and what it gave you?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:08:18.080000+00:00",
                "content": "Sure, one sec"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:10:23.431000+00:00",
                "content": "```\nExpected:\n[\n  { index: 0, glAccountId: 'Savings', forceAlignment: 'Sith' },\n  { index: 1, glAccountId: 'Checking', forceAlignment: 'Sith' },\n  { index: 2, glAccountId: 'Savings', forceAlignment: 'Jedi' },\n  { index: 3, glAccountId: 'Checking', forceAlignment: 'Jedi' }\n]\n\nPredicted: \n[\n  { index: 1, glAccountId: 'Checking', forceAlignment: 'Sith' },\n  { index: 2, glAccountId: 'Savings', forceAlignment: 'Jedi' },\n  { index: 3, glAccountId: 'Checking', forceAlignment: 'Jedi' }\n]\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:13:01.239000+00:00",
                "content": "Ill take a look in a bit. Which model do you use?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:13:12.726000+00:00",
                "content": "I was using GPT4Turbo"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:13:22.970000+00:00",
                "content": "Oh dont use gpt4 turbo, use gpt4 or gpt4o"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:13:26.571000+00:00",
                "content": "Try gpt4 first"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:13:44.760000+00:00",
                "content": "Oh? I thought gpt4turbo was supposed to be better"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:14:03.741000+00:00",
                "content": "In order of goodness it s gpt4, gpt4o, gpt4turbo"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:14:10.945000+00:00",
                "content": "Interesting"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:14:30.212000+00:00",
                "content": "why is that?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:14:59.728000+00:00",
                "content": "Gpt4turbo was like alpha version of gpt4o"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:15:14.021000+00:00",
                "content": "Not sure about internals but it has been bad in our experience"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:15:33.144000+00:00",
                "content": "I see, that's surprising"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:15:46.733000+00:00",
                "content": "Either way ill check your prompt in a bit, i have some other ideas that have worked in the past"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:16:28.735000+00:00",
                "content": "Ran it a bunch of times and it hasn't failed yet, I'm gonna add some harder test cases and maybe try the custom array idea"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:17:00.604000+00:00",
                "content": "Yeah just fyi gpt4 is more expensive. See if gpt4o works"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:17:07.321000+00:00",
                "content": "Depends on your budget"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:17:22.097000+00:00",
                "content": "Ah I haven't tried looking for pricing info yet, where is that?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:17:48.985000+00:00",
                "content": "Should be in openai docs! Ill link it in a bit"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:18:16.079000+00:00",
                "content": "OH sorry I thought you meant a Boundary price for using baml"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 00:18:28.146000+00:00",
                "content": "Oh no you use your own api keys haha"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 00:18:39.093000+00:00",
                "content": "Right right that makes sense lol"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:40:28.483000+00:00",
                "content": "[warning long message] \n\nI ended up at this prompt after a while of tinkering with the schema:\n```\nParse the following history of invoices to learn how metadata fields correlate with line items.\nPast Invoices\n---\n[\n  {\n    lineItems: [\n      {\n        index: 0\n        amount: 1,\n        currency: USD,\n        name: null,\n        description: 'Anakin Skywalker',\n        quantity: 1,\n        unitPrice: 1,\n        metadata: {\n          glAccountId: 'Savings',\n          forceAlignment: 'Sith',\n        }\n      },\n\n      ...\n\n    ],\n  },\n]\n---\nNow, parse the following invoice's line items and return a structured representation of the most likely corresponding metadata fields in the schema below.\nYou MUST provide one metadata object for each line item provided in the current invoice.\nCurrent Invoice\n---\n{\n  lineItems: [\n    {\n      index: 0\n      amount: 1,\n      currency: USD,\n      description: 'Anakin Skywalker',\n      quantity: 1,\n      unitPrice: 1,\n    },\n    {\n      index: 1\n      amount: 100,\n      currency: JPY,\n      description: 'Darth Vader',\n      quantity: 10,\n      unitPrice: 10,\n    },\n    {\n      index: 2\n      amount: 100,\n      currency: JPY,\n      description: 'Obi-Wan Kenobi',\n      quantity: 10,\n      unitPrice: 10,\n    },\n    {\n      index: 3\n      amount: 100,\n      currency: JPY,\n      description: 'Qui-Gon Jinn',\n      quantity: 10,\n      unitPrice: 10,\n    },\n  ],\n},\n---\nAnswer in JSON using this schema:\n{\n  lineItem0: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n  lineItem1: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n  lineItem2: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n  lineItem3: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 01:41:36.612000+00:00",
                "content": "Did you build it with the dynamic typebuilder?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:41:47.219000+00:00",
                "content": "I think this is a little verbose and it took some time to hand-roll the dynamic key definitions, since you have to encode it this way and decode it -- but assuming BAML will never break the schema it means I don't have to validate the length of the array anymore"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:41:48.036000+00:00",
                "content": "Yep"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 01:42:00.066000+00:00",
                "content": "Yeah baml wont break it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 01:42:06.101000+00:00",
                "content": "Nice, i like the approach"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:42:36.010000+00:00",
                "content": "It'd be nice if a user could specify something like `LineItemMetadata[4]` and have you guys do it under the hood, this feels like a common use case/failure mode"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:43:06.624000+00:00",
                "content": "And I assume you'd be able to make something that doesn't cost as many tokens üòÖ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 01:43:30.627000+00:00",
                "content": "You probably dont need the index at this point i guess. And why do you need the account id there in the output schema too?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-27 01:44:03.541000+00:00",
                "content": "Oh nvm i think i misread it"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:44:23.394000+00:00",
                "content": "Yeah I just want to strongly type the whole thing based on what the user wants"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-27 01:44:26.903000+00:00",
                "content": "So it never breaks"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-28 06:35:27.854000+00:00",
                "content": "Forgot to mention this earlier today at work ‚Äî with this schema + GPT4o and some(many) bug fixes we got the line item classifier to a launchable state and have it set to go out to some customers tomorrow, thanks for your help! Wouldn‚Äôt have been able to figure out the dynamic type building / prompt engineering without it and y‚Äôall were super responsive throughout"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-28 06:36:01.272000+00:00",
                "content": "niiice! congrats!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-28 06:36:06.445000+00:00",
                "content": "lmk if you run into more issues"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-28 06:36:44.424000+00:00",
                "content": "üëÄ ur awake too üëÄ"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-28 06:36:53.304000+00:00",
                "content": "Haha thank you! And will do"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-28 06:37:10.244000+00:00",
                "content": "always"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-06-28 16:55:25.906000+00:00",
                "content": "Just launched the feature on LinkedIn and shouted y'all out! \nhttps://www.linkedin.com/posts/ashwin-a-kumar_today-mercoa-came-alive-for-the-first-time-activity-7212497434235584512-B4em?utm_source=share&utm_medium=member_desktop"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-06-28 17:07:11.702000+00:00",
                "content": "<@99252724855496704> niice can you repost?"
            }
        ]
    },
    {
        "thread_id": 1256392405184155759,
        "thread_name": "Defining Types in Python",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-06-28 23:34:52.038000+00:00",
                "content": "is it possible to define the data type inside python (and use it in a .baml file) instead of inside the baml file (which then flows to baml_client.types)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 07:37:02.494000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 07:37:04.365000+00:00",
                "content": "Sadly we don't support this as of yet. What some folks do is:\n\n1. define the types in BAML\n2. inherit from `baml_client.types` in their own python class so it keeps the data model in sync.\n\n(Also, we don't support all types in BAML yet (dict, tuple, set, and date))"
            }
        ]
    },
    {
        "thread_id": 1256658027776446494,
        "thread_name": "Optional parameters",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-06-29 17:10:21.399000+00:00",
                "content": "are optional / default parameters in functions supported?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 17:19:07.453000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 17:19:08.075000+00:00",
                "content": "Yes! Params can be ‚Äústring?‚Äù Or other variants of ?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-06-29 23:16:56.085000+00:00",
                "content": "what about default values"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 23:47:09.689000+00:00",
                "content": "We don‚Äôt support those yet!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-29 23:47:35.692000+00:00",
                "content": "We only have default values for optionala for which it defaults to none"
            }
        ]
    },
    {
        "thread_id": 1257429278367158284,
        "thread_name": "Hi team -- for deploying BAML, when do",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-01 20:15:01.869000+00:00",
                "content": "Hi team -- for deploying BAML, when do you think you'll  support alpine? We use alpine in our prod containers currently and are running into build errors"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-01 20:17:39.215000+00:00",
                "content": ""
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-01 20:17:39.464000+00:00",
                "content": "We can *probably* switch to a non-alpine distro without breaking things so hopefully that works in the meantime, but ideally we can just add it to the existing configuration and have it work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-01 20:20:31.610000+00:00",
                "content": "We should be able to add this in within 2 weeks! I‚Äôll confirm after we do scheduling on Thursday!"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-01 20:21:46.127000+00:00",
                "content": "Gotcha, appreciate it!"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-06 00:46:08.558000+00:00",
                "content": "Circling back on this -- does BAML support alpine linux now / plan to soon?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 14:48:19.134000+00:00",
                "content": "Hi Ashwin, sorry for the delay on this. Some good news, we had a build compiling this AM for `aarch64-unknown-linux-musl` (Whic is what alpine uses - assuming you're using a aarch64 not x86 container). \n\nWe should know soon if it works on alpine."
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-06 16:01:14.166000+00:00",
                "content": "Sounds good, thx"
            }
        ]
    },
    {
        "thread_id": 1257757666697150485,
        "thread_name": "JSON Schema -> Type definition converter",
        "messages": [
            {
                "author": "deoxykev",
                "timestamp": "2024-07-02 17:59:55.748000+00:00",
                "content": "is json schema conversion still being developed? Use case is dynamically converting unknown json schema into BAML for structured extraction.\n\nI was working on an implementation using the python client but it  looks like someone is working on a base rust implementation which might be better. https://github.com/BoundaryML/baml/pull/655"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-02 18:02:15.700000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-02 18:02:16.112000+00:00",
                "content": "is this something that may help you? https://docs.boundaryml.com/docs/calling-baml/dynamic-types\n\nOr do you want to use json schemas specifically?"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-02 18:02:37.756000+00:00",
                "content": "yeah I am parsing a json schema and creating the dynamic type using that feature"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-02 18:03:07.245000+00:00",
                "content": "ahh gotcha, you just want a faster way rather than build it all out manually. Let me get back to you on the date for this, one sec"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-02 18:03:59.412000+00:00",
                "content": "yeah the domain i'm working in has a well-defined ontology with json schemas for everything already"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-02 18:22:11.720000+00:00",
                "content": "our current estimate is for completing this is end of next week (Fri July 12) -- sorry we cant get to it earlier! That PR definitely is still a work in progress"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-02 19:18:56.399000+00:00",
                "content": "<@1062441178022289479> could you share some samples of the JSONschemas you have? I want to make sure we can get it working for the strucutres you have!"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-02 20:27:56.503000+00:00",
                "content": "<@99252724855496704> certainly!\n\nschema_1: https://build.fhir.org/episodeofcare.schema.json.html\n\nschema_1_sample: https://build.fhir.org/episodeofcare-example.json.html\n\n---\n\nschema_2: https://build.fhir.org/patient.schema.json.html\n\nschema_2_sample: https://build.fhir.org/patient-example-b.json.html\n\n---\n\nand  behold: \nhttp://build.fhir.org/fhir.schema.json.zip\n\nthe mother of all json schemas\n\nthis should be an excellent test bench"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 19:27:58.540000+00:00",
                "content": "Hey there! I'm picking the branch back up and just wanted to double-check - you can load `fhir.schema.json` into your code, right?\n\nThe reason I ask is because `patient.schema.json` and `episodeofcare.schema.json` don't seem to actually contain enough information to be fully resolveable, since they reference other root schemas but don't actually have metadata for how to find those other roots"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-06 13:24:58.223000+00:00",
                "content": "Hey, I‚Äôm sorry I did not check. Usually these schemas are part of a larger standard, which contain the missing references"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-08 22:40:27.474000+00:00",
                "content": "So I have JSON schema loading working with dynamic types, and am currently working on getting `fhir.schema.json` to load, and wanted to check whether or not you're OK with doing some pre-processing of it before feeding it into BAML.\n\n---\n\nContext: the FHIR schemas have some cycles in the type definitions, e.g. they have something that looks like this:\n\n```\nclass Reference {\n  identifier Identifier\n}\n\nclass Identifier {\n  assigner Reference\n}\n```\n\nBAML currently doesn't support this, and I'm wondering if you'd be OK with modifying the JSON schema that you feed into BAML to strip out these type cycles.\n\n---\n\nLet me know if this question makes sense, or if you'd be interested in hopping on a call to chat more about your use case"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-08 22:53:58.030000+00:00",
                "content": "A more specific example is `#/definitions/Extension`, which I suspect for your case you can just replace with `\"type\": \"string\"`\n\n```\n    \"Extension\": {\n      \"description\": \"Optional Extension Element - found in all resources.\",\n      \"properties\": {\n        ...,\n        \"extension\": {\n          \"description\": \"May be used to represent additional information that is not part of the basic definition of the element. To make the use of extensions safe and managable, there is a strict set of governance applied to the definition and use of extensions. Though any implementer can define an extension, there is a set of requirements that SHALL be met as part of the definition of the extension.\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Extension\"\n          },\n          \"type\": \"array\"\n        },\n    }\n```\n\nYou can diff these two files to see an example of what I mean: https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/fhir-no-cycles.schema.json and https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/fhir-original.schema.json"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-08 23:40:32.942000+00:00",
                "content": "I think you're definitely going to want to modify the JSON schemas a bit before you feed them into BAML- here's the output format we would generate for Patient: https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/patient-schema.txt"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-08 23:42:14.708000+00:00",
                "content": "Here's what it is for EpisodeOfCare: https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/episode-of-care.schema.txt"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:56:39.673000+00:00",
                "content": "Deoxy what we can do here for you is to actually give you some python code to generate BAML schemas + prompts, because you may actually need to change the definitions slightly (e.g. prompt engineer), and it's really hard to do if we just give you a straight up `typebuilder.from_json_schema`  that generates the output schema that Sam posted above. For example, you may want to remove all the redundant `id` descriptions.\n\nWe can give you a python script that generates the files if you are interested. I assume that these schemas probably don't change much.\n\nIf you want to chat more about your usecase in a quick call and see how we can help you get better results or learn about your problems definitely let us know."
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-09 13:47:11.667000+00:00",
                "content": "Yes, preprocessing is totally ok!"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-09 13:50:36.861000+00:00",
                "content": "Yeah absolutely I can try this out and give feedback"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-09 13:56:03.334000+00:00",
                "content": "I would be interested in a call, I think our entire team is interested. Can I get back to you with some availability slots?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-09 13:56:43.581000+00:00",
                "content": "would be happy to! I take it you are Europe timezones?"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-09 14:29:44.751000+00:00",
                "content": "No, we are on American central time zone"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-09 14:30:38.061000+00:00",
                "content": "Oh, you're just an early riser! üôÇ \n\nIf its more helpful here's a calendly btw! https://calendly.com/boundary-founders/connect-45\n\nOtherwise feel free to suggest times!"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-09 14:31:01.284000+00:00",
                "content": "Are you a US company?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-09 15:08:52.535000+00:00",
                "content": "Yep! Based in Seattle!"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-09 15:36:44.718000+00:00",
                "content": "Cool, I sent an invite at 4 cst with our team. Looking forward to it!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-09 21:51:12.414000+00:00",
                "content": "https://github.com/BoundaryML/baml-examples/blob/b685347967b9ff956d68be7c516b536ae663f988/nextjs-starter/baml_src/rag.baml"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-10 17:51:03.120000+00:00",
                "content": "<@711679663746842796> left some feedback on this https://github.com/BoundaryML/baml/issues/765#issuecomment-2221112650"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 18:02:21.942000+00:00",
                "content": "This was really well thought out! üôÇ Quite appreciate the thoughts on this. Will be sharing the spec with you over the weekend sometime!"
            }
        ]
    },
    {
        "thread_id": 1257909752529293375,
        "thread_name": "ah, i see it needs to be inside",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:04:15.836000+00:00",
                "content": "ah, i see it needs to be inside\n    generation_config {\n      temperature 0\n    }"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:13:39.327000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:13:39.635000+00:00",
                "content": "yea you'll need to pass in params exactly how gemini wants them! We just do passthrough of all params! Glad you got it! We should make a PR on our docs to fix this."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:13:50.866000+00:00",
                "content": "to help others*"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:20:40.803000+00:00",
                "content": "i'm having trouble passing in safety_settings and response_mime_type"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:20:49.832000+00:00",
                "content": "response_mime_type is supposed to be in generation_config"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:21:04.218000+00:00",
                "content": "safety_settings is supposed to be on the same level as generation_config"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:21:27.966000+00:00",
                "content": "have you tried the raw_curl request option the playground? That can help debug the issue."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:21:57.530000+00:00",
                "content": "let me also try meanwhile!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:22:54.930000+00:00",
                "content": "when i look at the docs: https://ai.google.dev/api/rest/v1/GenerationConfig\n\nI don't see response_mime_type"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:24:24.374000+00:00",
                "content": "https://ai.google.dev/api/rest/v1beta/GenerationConfig"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:24:39.569000+00:00",
                "content": "it is reflected as such in the playground"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:24:51.159000+00:00",
                "content": "ah maybe the issue is that is v1beta api instead of v1"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 04:25:48.570000+00:00",
                "content": "ah yes, we can get an easy patch in that allows you to configure base_url which would make it possible to edit that.\n\nWe're doing a release tonight, so i'll include that change! You can expect it tmrw."
            }
        ]
    },
    {
        "thread_id": 1257918717988311110,
        "thread_name": "Dynamic clients",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:39:53.368000+00:00",
                "content": "can you pass in the client into a function? or specific client parameters? for example (1) if i wanted to switch between gpt4o and sonnet3.5,  i would need to copy paste and define multiple functions using different clients and (2) if i wanted to run sonnet3.5 but with 0 and 0.3 temperature, i would need to copy paste and define multiple functions using the same client but different settings?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 04:43:11.075000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 04:43:11.417000+00:00",
                "content": "This feature is coming in the next 4 hours or so"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 04:44:15.005000+00:00",
                "content": "üôÇ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 04:44:34.824000+00:00",
                "content": "Currently you will need to use different functions. You could use a template_string to define yojr prompt only one time. But yes we will release a dynamic client capability shortly"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 18:52:10.582000+00:00",
                "content": "we had some delays on this -- new ETA is tomorrow -- apologies on the delay!"
            }
        ]
    },
    {
        "thread_id": 1257925301065748561,
        "thread_name": "Test assertions",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 05:06:02.896000+00:00",
                "content": "Do the tests only test output schema being correct? Can you also test that the actual output matches what you would expect?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 05:07:12.613000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 05:07:13.116000+00:00",
                "content": "Not yet! We will add assertion capabilities soon, but i dont currently have a timeline. Do you use python or typescript? You can also write programmatic tests"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 05:07:33.785000+00:00",
                "content": "And yes they only validate the schema"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-03 05:07:36.384000+00:00",
                "content": "python currently"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 05:08:14.726000+00:00",
                "content": "Gotcha, yeah for the moment you may want to write your own pytests. We will adds docs on that soon"
            }
        ]
    },
    {
        "thread_id": 1258262349157240913,
        "thread_name": "BAML + Node",
        "messages": [
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:25:21.423000+00:00",
                "content": "How would you get baml running in just a basic new node project? Probably Webpack in a similar way to the NextJS setup but with using something like `node-loader` instead of `nextjs-node-loader`?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:26:12.535000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:26:12.791000+00:00",
                "content": "are you trying to run it in an express app? How do you deploy it?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:26:35.864000+00:00",
                "content": "The goal is actually to use sst and deploy AWS apis"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:27:02.267000+00:00",
                "content": "But I bet if I can get it working in a simple node project I could figure out the other way"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:29:33.462000+00:00",
                "content": "If you aren't familiar, https://docs.sst.dev/ - it's basically a nice wrapper around the CDK"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:31:55.104000+00:00",
                "content": "the webpack thing is only needed cause the project's getting bundled, but if it's just a simple node server with no bundling you shouldn't have to do anything fancy. I'm not sure if this is what theyre using to bundle things https://github.com/sst/sst/blob/master/packages/sst/src/runtime/handlers/node.ts"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:31:55.224000+00:00",
                "content": "Yeah, I think sst uses express under the hood"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:33:12.948000+00:00",
                "content": "the basic gist is that BAML's @boundaryml/baml package just has a native node module that needs to be included if bundling."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:33:32.965000+00:00",
                "content": "if you try their basic Api construct and add @boundaryml/baml perhaps it may work out of the box (?)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:33:55.558000+00:00",
                "content": "`pnpm add @boundaryml/baml` \n`npx baml-cli init`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:34:54.998000+00:00",
                "content": "tomorrow we'll have some time to help you set that up"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:35:00.678000+00:00",
                "content": "I see - I should install it in the specific package that it is using it probably, not the root? (packages/functions/)"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:35:06.993000+00:00",
                "content": "awesome, thanks!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:35:54.756000+00:00",
                "content": "exactly, it would be in packages/functions"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:35:57.638000+00:00",
                "content": "in that package.json"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:36:07.762000+00:00",
                "content": "Yeah, trying to just add it and init the cli gives:\n\n```\nnode:internal/modules/cjs/loader:1143\n  throw err;\n  ^\n\nError: Cannot find module './native'\nRequire stack:\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:37:34.474000+00:00",
                "content": "I am on Windows, btw, if that causes weirdness"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:38:45.683000+00:00",
                "content": "ok for now just copy this in a baml_src folder in packages/functions:\n\nmain.baml\n```\n// This is a BAML config file, which extends the Jinja2 templating language to write LLM functions.\n\nclass Resume {\n  name string\n  education Education[] @description(\"Extract in the same order listed\")\n  skills string[] @description(\"Only include programming languages\")\n}\n\nclass Education {\n  school string\n  degree string\n  year int\n}\n\nfunction ExtractResume(resume_text: string) -> Resume {\n  // see clients.baml\n  client GPT4o\n\n  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!\n  prompt #\"\n    Parse the following resume and return a structured representation of the data in the schema below.\n\n    Resume:\n    ---\n    {{ resume_text }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\ntest Test1 {\n  functions [ExtractResume]\n  args {\n    resume_text #\"\n      John Doe\n\n      Education\n      - University of California, Berkeley\n        - B.S. in Computer Science\n        - 2020\n\n      Skills\n      - Python\n      - Java\n      - C++\n    \"#\n  }\n}\n\n\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model gpt-4o\n    api_key env.OPENAI_API_KEY\n  }\n}\n\ngenerator default {\n  output_type typescript\n  output_dir ../\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:39:17.914000+00:00",
                "content": "and try to save it using the vscode extension. A `baml_client` should generate"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:44:17.825000+00:00",
                "content": "The extension is working as expected and generating the files, so I tried to import and run the BAML in the API and got this when building:\n\n```\n‚úñ  Build failed packages/functions/src/lambda.handler\n   Could not resolve \"@boundaryml/baml\"\n   packages/functions/baml/baml_client/client.ts\n   18 ‚îÇ import { BamlRuntime, FunctionResult, BamlCtxManager, BamlStream, Image } from \"@boundaryml/baml\"\n   Could not resolve \"@boundaryml/baml\"\n   packages/functions/baml/baml_client/globals.ts\n   18 ‚îÇ import { BamlCtxManager, BamlRuntime } from '@boundaryml/baml'\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:45:27.884000+00:00",
                "content": "That package.json is:\n\n```\n{\n  \"name\": \"@baml-sst-test/functions\",\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"test\": \"sst bind vitest\",\n    \"typecheck\": \"tsc -noEmit\"\n  },\n  \"devDependencies\": {\n    \"@types/aws-lambda\": \"^8.10.140\",\n    \"@types/node\": \"^20.14.9\",\n    \"sst\": \"^2.43.3\",\n    \"vitest\": \"^1.6.0\"\n  },\n  \"dependencies\": {\n    \"@boundaryml/baml\": \"^0.47.0\"\n  }\n}\n```\n\nSo I imagine that the baml generate needs to be a part of the build step at some point?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:48:03.195000+00:00",
                "content": "i dont think it can find the native node module"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:51:54.933000+00:00",
                "content": "yeah, in the nextjs project I am using BAML in, I can see the `baml.win32-x64-msvc.node` in the node_modules, but not in this project"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:53:24.193000+00:00",
                "content": "are you open to using a container?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:53:29.607000+00:00",
                "content": "I copied it over, and now I get:\n\n```\n No loader is configured for \".node\" files: packages/functions/node_modules/@boundaryml/baml-win32-x64-msvc/baml.win32-x64-msvc.node\n   packages/functions/node_modules/@boundaryml/baml/native.js\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:53:50.851000+00:00",
                "content": "yeah their webpack loader probably would need to be modified"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:53:56.879000+00:00",
                "content": "that's what we did with the nextjs one"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:55:47.245000+00:00",
                "content": "I suppose, sst seems to have easy enough support for that: https://docs.sst.dev/containers\n\nBut I feel like there is probably a simple-ish setup with webpack to get this working. It would probably be almost exactly the same as a `npm init` project"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:58:26.924000+00:00",
                "content": "probably something like this: https://docs.sst.dev/constructs/Function#install"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:58:36.623000+00:00",
                "content": "try adding @boundaryml/baml there"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 03:58:44.092000+00:00",
                "content": "and / or also add the .node loader"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:59:15.856000+00:00",
                "content": "oh very interesting, I'll try that out"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 03:59:30.167000+00:00",
                "content": "Thanks for the support, I wish I knew more about this and could be more helpful lol"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 04:00:59.385000+00:00",
                "content": "```\nnodejs: {\n                    loader: {\n                        '.node': 'copy',\n                    },\n                    install: ['@boundaryml/baml']\n                }\n```\ntry it without the `install` and with it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 04:01:31.824000+00:00",
                "content": "relevant https://discord.com/channels/983865673656705025/1206068849313718302 (i searched for \"no loader is configured for \".node\" files\")"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 04:01:40.287000+00:00",
                "content": "lmk if that works"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 04:01:49.051000+00:00",
                "content": "if it does we'll add it to your docs üôÇ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 04:01:51.583000+00:00",
                "content": "our*"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 04:59:33.135000+00:00",
                "content": "Hmm, so the Function construct seems to be the way to deploy a lambda locally? I would need to use the Api construct in order to get it to build and run when deployed? Or I am misunderstanding?\n\nLet me read through that other thread more and see"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:01:42.458000+00:00",
                "content": "what construct are you using?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:06:53.840000+00:00",
                "content": "API, something like this in the most basic example:\n\n```\nimport { Api, StackContext } from \"sst/constructs\";\n\nexport function API({ stack }: StackContext) {\n  const api = new Api(stack, \"api\", {\n    routes: {\n      \"GET /\": \"packages/functions/src/lambda.handler\"\n    }\n  });\n\n  stack.addOutputs({\n    ApiEndpoint: api.url,\n  });\n}\n\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:10:26.094000+00:00",
                "content": "'nodejs' isn't a property on API, so I probably have to go through a function but I am trying to figure out how. Currently looking through some more threads on sst"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:10:33.618000+00:00",
                "content": "an api wraps around a function:\n\n`https://docs.sst.dev/constructs/Api#addroutes`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:11:38.405000+00:00",
                "content": "```\nconst func = new Function(stack, \"MyFunction\", {\n  handler: \"src/lambda.handler\",\n  nodejs: {\n    // stuff here!\n  }\n});\n\nAPI(....) {\nroutes: {\n  \"GET /\": func\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:12:17.709000+00:00",
                "content": "ohhh, duh, that makes so much sense"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:12:40.556000+00:00",
                "content": "(edited above block with the nodejs thing)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:13:02.059000+00:00",
                "content": "also i havent tried this, was just reading the parameters etc"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:13:06.194000+00:00",
                "content": "and their discord"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:13:53.895000+00:00",
                "content": "right, but this totally makes sense, and I do see that is how people were able to get native modules running with other configurations so I bet this will work, trying it out now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:18:04.433000+00:00",
                "content": "https://github.com/winklerj/console/blob/c4373bd8991c84eb763a2588e23c7d10bd353db2/stacks/api.ts#L121"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:18:51.452000+00:00",
                "content": "you may not even need \"Api\". Just the raw lambda function. def take a look at other projects"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:19:13.525000+00:00",
                "content": "good search query: https://github.com/search?q=sst+%22new+Function%22+nodejs+language%3ATypeScript+&type=code&p=3 . Tomorrow ill have more time to take a look"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:19:46.360000+00:00",
                "content": "Progress: seemingly getting a BAML error:\n\n```\nError: Cannot read properties of undefined (reading 'bind')\n       at <anonymous> (C:\\...\\packages\\functions\\baml\\baml_client\\tracing.ts:22:71)\n       at ModuleJob.run (node:internal/modules/esm/module_job:195:25)\n       at async ModuleLoader.import (node:internal/modules/esm/loader:337:24)\n       at async file:///C:/.../node_modules/.pnpm/sst@2.43.3_@aws-sdk+client-sso-oidc@3.609.0/node_modules/sst/support/nodejs-runtime/index.mjs:46:15\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:20:07.273000+00:00",
                "content": "Thanks so much for your time! I'll see if I can get it working tonight"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:20:47.086000+00:00",
                "content": "can you post the contents of baml_client/tracing.ts ?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:21:03.293000+00:00",
                "content": "and your @boundaryml/baml is 0.47.0 right?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:21:06.826000+00:00",
                "content": "```\n/*************************************************************************************************\n\nWelcome to Baml! To use this generated code, please run one of the following:\n\n$ npm install @boundaryml/baml\n$ yarn add @boundaryml/baml\n$ pnpm add @boundaryml/baml\n\n*************************************************************************************************/\n\n// This file was generated by BAML: do not edit it. Instead, edit the BAML\n// files and re-generate this code.\n//\n// tslint:disable\n// @ts-nocheck\n// biome-ignore format: autogenerated code\n/* eslint-disable */\nimport { BamlLogEvent } from '@boundaryml/baml';\nimport { DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX } from './globals';\n\nconst traceAsync =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnAsync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst traceSync =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnSync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst setTags =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.upsertTags.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst flush = () => {\n  DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.flush.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)()\n}\nconst onLogEvent = (callback: (event: BamlLogEvent) => void) =>\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.onLogEvent(callback)\n\nexport { traceAsync, traceSync, setTags, flush, onLogEvent }\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:21:14.530000+00:00",
                "content": "yep!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:21:55.947000+00:00",
                "content": "can you try just commenting this:\n\n`  DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.flush.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)()\n`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:22:07.361000+00:00",
                "content": "inside the flush = () =>"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:22:51.264000+00:00",
                "content": "Seems like the same errror"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:23:11.684000+00:00",
                "content": "which is the line 22 in your file?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:23:50.670000+00:00",
                "content": "```\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnAsync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:24:02.047000+00:00",
                "content": "But I bet this is a red herring and it is just being built correctly"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:24:17.231000+00:00",
                "content": "ok just comment out the whole file and do:\n\nexport {}"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:24:19.568000+00:00",
                "content": "for now"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:24:24.902000+00:00",
                "content": "because if I comment out everything, I get another error for ```\nError: this.ctx_manager.cloneContext is not a function\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:24:30.299000+00:00",
                "content": "if you save a .baml file it will get overwritten again fyi"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:24:30.508000+00:00",
                "content": "in client.ts"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:24:38.356000+00:00",
                "content": "hmm"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:25:20.006000+00:00",
                "content": "yeah same error, I will try messing around with the loader based on what I find and see"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:25:42.738000+00:00",
                "content": "hmm this may be an issue on our end"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:25:43.466000+00:00",
                "content": "I am also not using the example baml, I am using some from my other project, so maybe I should switch it to the boilerplate while I test this"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:25:43.879000+00:00",
                "content": "one sec"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:27:56.419000+00:00",
                "content": "did you try both removing the `install` and adding it back in?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:28:01.749000+00:00",
                "content": "in nodejs setting"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:28:07.231000+00:00",
                "content": "yep!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:28:13.020000+00:00",
                "content": "and same error?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:28:22.353000+00:00",
                "content": "yeah, seemingly"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:28:26.347000+00:00",
                "content": "let me make sure"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:29:34.765000+00:00",
                "content": "yep, exact same"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:30:08.729000+00:00",
                "content": "oks, yeah i would probably just wait until we can repro on our end -- this is kind of hairy stuff"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:30:35.319000+00:00",
                "content": "yeah for sure, it isn't a blocker or anything for me, just really wanted to deploy it this way if possible üòÑ"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 05:30:37.682000+00:00",
                "content": "Thanks for the help!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 05:30:57.598000+00:00",
                "content": "no problem!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 06:44:29.183000+00:00",
                "content": "I got it working:\n\nyou'll need @boundaryml/baml version 0.48.0\n\nAlso:\nExampleStack:\n```\nimport { Api, StackContext } from \"sst/constructs\";\n\nexport function ExampleStack({ stack }: StackContext) {\n  // Create the HTTP API\n  const api = new Api(stack, \"Api\", {\n    routes: {\n      // \"GET /notes\": \"packages/functions/src/list.handler\",\n      \"GET /notes/{id}\": {\n        function: {\n          handler: \"packages/functions/src/get.handler\",\n          environment: {\n            OPENAI_API_KEY: \"...\",\n          },\n          nodejs: {\n            install: [\"@boundaryml/baml\"],\n            esbuild: {\n              loader: {\n                \".node\": \"file\",\n              },\n            },\n          },\n        },\n      },\n\n      //\"packages/functions/src/get.handler\",\n      // \"PUT /notes/{id}\": \"packages/functions/src/update.handler\",\n    },\n  });\n\n  // Show the API endpoint in the output\n  stack.addOutputs({\n    ApiEndpoint: api.url,\n  });\n}\n```\npackages/functions/src/get.ts:\n```\nimport { APIGatewayProxyHandlerV2 } from \"aws-lambda\";\nimport { b } from \"../baml_client\";\n\nexport const handler: APIGatewayProxyHandlerV2 = async (event) => {\n  try {\n    const res = await b.ExtractResume(\n      \"Mark gonzalez, mark@hello.com. python. 5 years.\"\n    );\n    console.log(res);\n  } catch (e) {\n    console.log(e);\n  }\n  return {\n    statusCode: 200,\n    body: JSON.stringify({ message: \"Hello from get!\" }),\n  };\n};\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 06:44:55.122000+00:00",
                "content": "you cna run:\n`npm uninstall @boundaryml/baml`\n`npm install @boundaryml/baml`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 06:45:35.776000+00:00",
                "content": "not sure how you can safely add the environment variables -- you may want to look into that (instead of hardcoding it in here)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 06:46:11.162000+00:00",
                "content": "`curl -X GET -H \"Content-Type: application/json\" https://id123.execute-api.us-east-1.amazonaws.com/notes/123`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 06:48:17.900000+00:00",
                "content": "I ran it with ‚Äúnpm run dev‚Äù"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:26:27.631000+00:00",
                "content": "This is awesome, thanks! It works!\n\nWith 0.48, it seems like I just need:\n\n```\nconst aFunction = new Function(stack, \"myFunction\", {\n    handler: \"packages/functions/src/lambda.handler\",\n    nodejs: {\n      loader: {\n        '.node': 'copy',\n      }\n    },\n    environment: {\n      OCTOKIT_API_KEY: \"...\",\n    }\n  })\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 19:26:46.110000+00:00",
                "content": "Awesome"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 19:27:15.627000+00:00",
                "content": "The ‚Äúinit‚Äù command should worknow as well"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:27:21.150000+00:00",
                "content": "Btw if you are interested, you can use AWS SSM with SST to store the secrets"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 19:27:30.120000+00:00",
                "content": "Nice"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:27:55.276000+00:00",
                "content": "This is seriously great, I really appreciate you taking so much time to help with this!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:44:25.040000+00:00",
                "content": "Ah, \n\n```\nWe do not currently support any other mechanisms for providing authorization credentials, including but not limited to:\n\nexchanging refresh tokens for ephemeral authorization tokens\nfetching credentials from a secret storage service, such as AWS Secrets Manager or HashiCorp Vault\n```\n\nSo BAML won't let you change the api key at runtime - so I need to figure out a way to pass in the API key in the environment in the stack without storing it in the code"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 19:56:13.608000+00:00",
                "content": "<@711679663746842796> mind taking a look?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 19:56:41.294000+00:00",
                "content": "Maybe we need the dynamic client thing to work?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:56:45.468000+00:00",
                "content": "Sorry, please hold - I think there is a way to use env"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 19:56:58.404000+00:00",
                "content": "Youre using bedrock no?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:57:22.918000+00:00",
                "content": "I will be in on-prem distributions, but using OctoAI atm"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:59:18.539000+00:00",
                "content": "Alrighty, env does work. Just need to have env in the root, and pass:\n\n```\nenvironment: {\n      OCTOKIT_API_KEY: process.env.OCTOKIT_API_KEY ?? \"\"\n    }\n```\n\nand we are good!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:59:41.233000+00:00",
                "content": "also make sure `\"types\": [\"node\"],` is in the tsconfig"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 19:59:57.950000+00:00",
                "content": "I can fully use baml in the pipeline now üòÑ"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-04 20:02:03.385000+00:00",
                "content": "I should probably write this up and have a basic sst +baml example available"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:26:16.045000+00:00",
                "content": "Yeah we can add it to our docs for sure"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 08:46:41.910000+00:00",
                "content": "Sorry for rezzing this old thread but when you said you got this working did you actually deploy it or just use locally with npm run dev?\n\nI got distracted by some other things over the last few days and I realized that I never actually tested it working in a deployment, and unfortunately it doesn‚Äôt work - same missing binary error. \n\nI thought it could be because I am deploying from Windows but I deployed from Linux (WSL but should be fine?) and ensured that BAML was bundling its Linux binary. I tried tons of different configurations on the function but still to no avail. \n\nI know I could probably use a docker container but I‚Äôm wary about start times because this function will be called a ton. But if that is the only way for now, so be it - but I really doubt it‚Ä¶there has to be a way to bundle native node modules to lambda somehow.\n\nIf there is any way you could get a minimal deployed node18+ running that would be amazing‚Ä¶and maybe I could see what I did wrong."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 15:37:11.749000+00:00",
                "content": "Ill take a look at this today!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 15:38:43.934000+00:00",
                "content": "It worked with npm run dev but i think npm run dev actually deploys a stack"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 16:48:04.110000+00:00",
                "content": "Awesome, thanks!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 16:48:53.724000+00:00",
                "content": "I believe it does deploy a stack but I think that the lambda functions are run in local debug mode by default"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 19:04:46.609000+00:00",
                "content": "So I even tried deploying from an Amazon Linux 2 EC2 machine just to make sure it wasn't an issue with windows and I still get:\n\n```\nERROR    Unhandled Promise Rejection     \n{\n    \"errorType\": \"Runtime.UnhandledPromiseRejection\",\n    \"errorMessage\": \"Error: Failed to load native binding\",\n    \"reason\": {\n        \"errorType\": \"Error\",\n        \"errorMessage\": \"Failed to load native binding\",\n        \"stack\": [\n            \"Error: Failed to load native binding\",\n            \"    at Object.<anonymous> (/var/task/node_modules/@boundaryml/baml/native.js:359:11)\",\n            \"    at Module._compile (node:internal/modules/cjs/loader:1364:14)\",\n            \"    at Module._extensions..js (node:internal/modules/cjs/loader:1422:10)\",\n            \"    at Module.load (node:internal/modules/cjs/loader:1203:32)\",\n            \"    at Module._load (node:internal/modules/cjs/loader:1019:12)\",\n            \"    at Module.require (node:internal/modules/cjs/loader:1231:19)\",\n            \"    at require (node:internal/modules/helpers:177:18)\",\n            \"    at Object.<anonymous> (/var/task/node_modules/@boundaryml/baml/index.js:4:16)\",\n            \"    at Module._compile (node:internal/modules/cjs/loader:1364:14)\",\n            \"    at Module._extensions..js (node:internal/modules/cjs/loader:1422:10)\"\n        ]\n    },\n    \"promise\": {},\n    \"stack\": [\n        \"Runtime.UnhandledPromiseRejection: Error: Failed to load native binding\",\n        \"    at process.<anonymous> (file:///var/runtime/index.mjs:1276:17)\",\n        \"    at process.emit (node:events:517:28)\",\n        \"    at emit (node:internal/process/promises:149:20)\",\n        \"    at processPromiseRejections (node:internal/process/promises:283:27)\",\n        \"    at process.processTicksAndRejections (node:internal/process/task_queues:96:32)\"\n    ]\n}\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 19:07:02.596000+00:00",
                "content": "This is with the most basic repro I can with just:\n\n```\nimport { StackContext, Api, Function } from \"sst/constructs\";\n\nexport function API({ stack }: StackContext) {\n  const theAnalyzeFunction = new Function(stack, \"analyzerFunction\", {\n    handler: \"packages/functions/src/analyzer.handler\",\n    nodejs: {\n      install: [\"@boundaryml/baml\"],\n      loader: {\n        '.node': 'copy',\n      }\n    },\n    environment: {\n      OCTOKIT_API_KEY: process.env.OCTOAI_API_KEY ?? \"\"\n    }\n  })\n\n  const api = new Api(stack, \"api\", {\n    routes: {\n      \"PUT /analyze\": theAnalyzeFunction,\n    },\n  });\n\n  stack.addOutputs({\n    ApiEndpoint: api.url,\n  });\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 19:07:22.108000+00:00",
                "content": "cool, ill start taking a look in around 2 hours"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 19:07:23.100000+00:00",
                "content": "It runs locally with `npm run dev` but of course won't run in an deployment"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 19:07:40.449000+00:00",
                "content": "Awesome, no rush - just wanted to provide more context üòÑ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 19:08:04.689000+00:00",
                "content": "cool, wanna make sure you dont waste a lot of time on it since it's probably something ive seen before"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 19:08:40.479000+00:00",
                "content": "make sense - I will stop messing with it for now then lol"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 22:57:26.956000+00:00",
                "content": "I think I‚Äôm gonna try deploying with Python - it might just work out of the box"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:00:56.418000+00:00",
                "content": "my bad, shouldve noticed this earlier:\n\nyour config should be:\n```\n nodejs: {\n            install: [\"@boundaryml/baml\"],\n            esbuild: {\n              loader: {\n                \".node\": \"file\",\n              },\n            },\n          },\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:08:29.098000+00:00",
                "content": "im testing my config now in prod"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:08:55.001000+00:00",
                "content": "That doesn‚Äôt seem to work, I have tried file and copy, with and without install, as well as changing some esbuild options"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:10:38.919000+00:00",
                "content": "oks i reproduced the error, seeing if i can fix it"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:10:55.427000+00:00",
                "content": "awesome, thanks again for taking the time to look into this!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:16:40.327000+00:00",
                "content": "i figured out the likely issue"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:16:42.706000+00:00",
                "content": "Yeah, you can look in the deployed lambda function and see that the binary is not there"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:16:44.250000+00:00",
                "content": "working on a fix"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:16:45.323000+00:00",
                "content": "yep"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:17:04.310000+00:00",
                "content": ""
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:38:05.516000+00:00",
                "content": "Progress - it still errors, but I can confirm that doing this:\n`install: [\"@boundaryml/baml\", \"@boundaryml/baml-linux-x64-gnu\"],`\n\nwill actually copy the node binary"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:38:23.530000+00:00",
                "content": "how did you confirm it?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:38:44.108000+00:00",
                "content": "function size jumped from 16kb to 9.8mb and then I downloaded the zip and checked"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:39:44.340000+00:00",
                "content": ""
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:40:00.820000+00:00",
                "content": "That is the unzipped structure of the function now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:40:07.022000+00:00",
                "content": "yeah im wondering what binary our script is trying to get inside the lambda"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:40:10.056000+00:00",
                "content": "will add some more logs"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:48:55.606000+00:00",
                "content": "So one issue looks like process.platform and process.arch are empty when requireNative checks. I then forced `return require(\"../baml-linux-x64-gnu\")` which gives:\n\n `/lib64/libm.so.6: version GLIBC_2.29 not found (required by /var/task/node_modules/@boundaryml/baml-linux-x64-gnu/baml.linux-x64-gnu.node)`\n\nSo, more progress lol"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:51:21.038000+00:00",
                "content": "Lambda only seems to have glic 2.26"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:51:32.330000+00:00",
                "content": "ohh interesting"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:51:40.919000+00:00",
                "content": "did you modify the code and reupload it to test?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:51:47.453000+00:00",
                "content": "Yep!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:51:55.375000+00:00",
                "content": "niiiice"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:52:07.230000+00:00",
                "content": "ok the glibc version we can fix on our end, let me take a look at our build configs"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:52:49.120000+00:00",
                "content": "nice - the platform and arch being empty is not great....must be a weird node issues somewhere"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:53:32.724000+00:00",
                "content": "i think we can patch our index.js script to account for lambda runtimes"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 23:53:41.725000+00:00",
                "content": "but for now glibc issue is the blocker"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-09 23:55:52.304000+00:00",
                "content": "makes sense!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:05:46.034000+00:00",
                "content": "can you try with :\n\n```\n function: {\n          handler: \"packages/functions/src/get.handler\",\n          environment: {\n            OPENAI_API_KEY: \"...\",\n          },\n          runtime: \"nodejs20.x\",\n          nodejs: {\n            install: [\n              \"@boundaryml/baml\",\n              // \"baml-linux-x64-gnu\",\n              \"@boundaryml/baml-linux-x64-gnu\",\n            ],\n            esbuild: {\n              loader: {\n                \".node\": \"file\",\n              },\n            },\n          },\n        },\n      },\n\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:05:59.815000+00:00",
                "content": "basically nodejs20 runtime uses a newer version of glibc"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:06:14.159000+00:00",
                "content": "(from amazon linux 2023) https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html#runtimes-supported"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:06:27.010000+00:00",
                "content": "with the same hack on the script you did to load the module directly"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:16:40.063000+00:00",
                "content": "what CLI command did you use to download the source code?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:16:58.152000+00:00",
                "content": "oh nvm!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:17:10.298000+00:00",
                "content": "Sorry, was afk - I just used the Lambda UI"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:24:02.911000+00:00",
                "content": "seems to have worked out of the box for me now with nodejs20"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:24:06.001000+00:00",
                "content": "like no hacks"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:24:15.634000+00:00",
                "content": "just the nodejs install / loader config we had there"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:25:42.264000+00:00",
                "content": "lmk if it works for you! We can definitely publish a lambda layer soon to make the invocation faster"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:26:04.771000+00:00",
                "content": "the good news is sst supports it: https://docs.sst.dev/advanced/lambda-layers"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:31:10.783000+00:00",
                "content": "It is definitely working now! Sorry for delay, there was another small issue on my end"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:31:24.276000+00:00",
                "content": "but I cannot believe I didn't just try using node20 -_-"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:31:52.752000+00:00",
                "content": "A lambda layer would be great, though - I already use that for accessing git at runtime"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:32:11.830000+00:00",
                "content": "Haha no worries. And yeah ill take a look at the layer config either tonight or tomorrow"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:33:29.371000+00:00",
                "content": "Awesome! I can also try writing a small doc on using SST/CDK to deploy but it really seems like it is jus this simple:\n\n```\nruntime: \"nodejs20.x\", \n    nodejs: {\n      install: [\"@boundaryml/baml\", \"@boundaryml/baml-linux-x64-gnu\"],\n      loader: {\n        '.node': 'file',\n      }\n    },\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:33:37.030000+00:00",
                "content": "And you might not even need to install baml itself"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:35:23.683000+00:00",
                "content": "Also, I can confirm that deploying from Windows works perfectly fine"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:39:24.891000+00:00",
                "content": "Yeah, you just need to install \"@boundaryml/baml-linux-x64-gnu\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 00:55:01.582000+00:00",
                "content": "Niiiice. Thanks for all the patience in getting this to work. Lmk if you have specific prompt engineering wuestions or whatnot at some point. Ill update our docs with this info and hopefully we can get the lambda layer working tomorrow"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-10 00:56:43.212000+00:00",
                "content": "Of course! And thanks for your help! I guess this shows how badly I didn't want to have to write parsing/testing code and use BAML instead lol"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 18:59:58.372000+00:00",
                "content": "Here's a version of that SST project with the lambda layer:\n\nhttps://github.com/BoundaryML/baml-examples/tree/main/node-aws-lambda-sst\n\nIf you end up using something other than SST it will work very similarly"
            }
        ]
    },
    {
        "thread_id": 1260034266390073396,
        "thread_name": "Alternate providers",
        "messages": [
            {
                "author": "dpaleka",
                "timestamp": "2024-07-09 00:46:19.412000+00:00",
                "content": "hi! does this library support OpenAI-compatible APIs that do not have json/tools/functions mode?  i'm considering alternatives to `instructor` on a research project because i can't make it work with OpenRouter endpoints that do not support that param"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:47:22.048000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:47:22.236000+00:00",
                "content": "yeah, it does! We have docs on this: https://docs.boundaryml.com/docs/snippets/clients/providers/other\n\nOur playground has a \"raw curl\" toggle where you can see the actual request that will be sent"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:47:53.324000+00:00",
                "content": "let us know if you run into any issues, we try to respond pretty damn quickly"
            },
            {
                "author": "dpaleka",
                "timestamp": "2024-07-09 00:49:27.931000+00:00",
                "content": "ok i can test it on the models that don't work with `instructor` and report back if it fails"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:49:44.705000+00:00",
                "content": "yeah definitely let us know"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-11 17:26:19.310000+00:00",
                "content": "Hi, were you able to test out BAML? Do you have any feedback on your experience? We'd love to improve the product / address any issues!"
            }
        ]
    },
    {
        "thread_id": 1260681725982544035,
        "thread_name": "Parsing pdfs with tables",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 19:39:05.808000+00:00",
                "content": "I am processing a PDF with unstructured. At some points, I recognize that a Table is present. Unfortunately, Unstructured doesn't do a great job with preserving the format of the table/form. I have found that if I can pipe that table somehow to GPT4o, it does a really great job of outputting the table in markdown. Is there a clean way to accomplish something like this in Baml? I know the page number in the PDF where the table lives"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:41:44.031000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:41:44.458000+00:00",
                "content": "Heres my ideas:\n\nlikely need a few functions to do a good job here:\n\n1. ExtractTableHeaders(page: string | image) -> TableHeader[]\n2. ExtractTable(page: string | image, headers: TableHeader[]) -> string  // This is a markdown string"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:42:28.777000+00:00",
                "content": "also if you want, you can pass in all the pages as parameters with another param `selected_page`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:42:57.721000+00:00",
                "content": "or did you have another thought in mind about what about would be a better more BAML-native way to do this?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 19:45:30.634000+00:00",
                "content": "well, do you guys support ingesting a PDF?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 19:45:38.385000+00:00",
                "content": "these PDFs can be hundreds of pages long"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 19:45:46.969000+00:00",
                "content": "so ideally, i just take the page where the table exists that I want to extract"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:52:45.342000+00:00",
                "content": "not a PDF directly, but we support an image type.\n\nwe haven't built in file types yet as most models don't support them.\n\nBut i think you should be able to write a pretty small class that can get the right page out of a PDF:\n\nIt sounds like the library you'd need to use is: fitz (PyMuPDF)\n\n```python\nimport fitz  # PyMuPDF\nimport base64\nfrom io import BytesIO\nfrom baml_py import Image\n\ndef pdf_page_to_base64(pdf_path: str, page_number: int = 0, zoom: int = 2) -> Image:\n    \"\"\"\n    Convert a specified page of a PDF to a base64-encoded image.\n\n    :param pdf_path: Path to the PDF file.\n    :param page_number: Page number to convert (0-indexed).\n    :param zoom: Zoom factor for rendering the page.\n    :return: Baml Image\n    \"\"\"\n    # Open the PDF file\n    pdf_document = fitz.open(pdf_path)\n    \n    # Get the specified page\n    page = pdf_document.load_page(page_number)\n    \n    # Render page to a pixmap\n    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))\n    \n    # Convert pixmap to bytes\n    img_bytes = pix.tobytes(\"png\")\n    \n    # Encode the image to base64\n    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n\n    return Image.from_base64(\"image/png\", img_base64)\n```\n\nhttps://pymupdf.readthedocs.io/en/latest/pixmap.html#Pixmap.tobytes"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:54:55.342000+00:00",
                "content": "given that openai prefers image dimensions in mutliples of 512, i would highly recommend playing around with zoom factor to optimize for that"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:55:36.508000+00:00",
                "content": "You can then also use the PIL library to dump out the images to disk if you want to debug it better"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:56:50.867000+00:00",
                "content": "```diff\n+ from PIL import Image as PILImage\n\ndef ...\n  # Convert pixmap to bytes\n  img_bytes = pix.tobytes(\"png\")\n    \n+ # Save the image to disk using PIL\n+ image = PILImage.open(BytesIO(img_bytes))\n+ image.save(output_image_path, format=\"PNG\")\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 19:57:00.489000+00:00",
                "content": "Ok, yea I was considering using something like this to turn the page into an image. I believe Unstructured even supports writing the tables to IMages itself"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 19:57:34.899000+00:00",
                "content": "Then I guess use BAML to call GPT4o with the image and ask it to return the table in markdown (or some class I define)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:57:42.427000+00:00",
                "content": "yep! basically!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 19:57:56.996000+00:00",
                "content": "good to know for us, i should write a cookbook on this!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:15:12.691000+00:00",
                "content": "ok I am able to get images. Is there any suggestion for the baml class? I'm comfortable with just markdown but wondering if you have seen others have success with other prompts?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:15:52.653000+00:00",
                "content": "it really depends on how complicated the table is"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:16:41.226000+00:00",
                "content": "I'm thinking i'll just ask it to return in markdown which downstream LLMs can use"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:19:51.073000+00:00",
                "content": "one idea:\n\nif you run the first call to get TableHeaders[] you can have it spit out:\n\n```rust\nclass Header {\n  name string\n  type ValueType\n}\n\nenum ValueType {\n  String\n  Int\n  Float\n  StringList\n  IntList\n  ...\n}\n```\n\nThen you can technically use dynamic types on the second call to return a MyResponseType[]\n\nWhich then is built to check the types of each item."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:20:38.254000+00:00",
                "content": "then you can render reach row as such."
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:23:38.147000+00:00",
                "content": "That's interesting... I'll need to think about how I can architect our system to handle that. I would need to persist this class to my DB but without a predefined schema, it might not be possible"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:23:59.614000+00:00",
                "content": "though as long as I can get it to markdown, then I can dump it into the actual text and downstream LLMs will likely parse it better as opposed to nonsensical text"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:26:29.800000+00:00",
                "content": "Is GPT4o the only model that supports images?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:26:39.067000+00:00",
                "content": "sonnet 3.5 also supports images"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:26:52.427000+00:00",
                "content": "oh baby"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:27:00.956000+00:00",
                "content": "and gemini üòâ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:27:25.907000+00:00",
                "content": "gemini is kinda trash tbh but i havent tested all their models"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:28:05.896000+00:00",
                "content": "ya this image --> markdown is waaay better. look at this:\n\n# before \n```\nProposal Volumes Volume Title Page Limitation Volume I Volume II Volume III Executed Solicitation Documents (Section K) and SF 33 N/A Executive Summary (1 page) Technical Approach (15 pages) Experience (5 pages) Resumes of Key Personnel not subject to page limitations Management Approach (14 pages) Past Performance Questionnaires and References (with Narrative) Cost/Price Proposal 35 Pages Total N/A Volume IV N/A\n```\n\n# after\n```\n| Proposal Volumes | Volume Title                                                                                | Page Limitation |\n|------------------|---------------------------------------------------------------------------------------------|-----------------|\n| Volume I         | Executed Solicitation Documents (Section K) and SF 33                                       | N/A             |\n| Volume II        | Executive Summary (1 page) Technical Approach (15 pages) Experience (5 pages) Resumes of Key Personnel not subject to page limitations Management Approach (14 pages) | 35 Pages Total  |\n| Volume III       | Past Performance Questionnaires and References (with Narrative)                             | N/A             |\n| Volume IV        | Cost/Price Proposal                                                                         | N/A             |\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:29:12.643000+00:00",
                "content": "LFG üôÇ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:29:12.973000+00:00",
                "content": "do you have a link  to the `unstructured` stuff you were using?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:29:27.649000+00:00",
                "content": "https://docs.unstructured.io/open-source/introduction/quick-start#installation"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:29:30.779000+00:00",
                "content": "I use the open source stuff"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:29:32.266000+00:00",
                "content": "not their API"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:29:45.310000+00:00",
                "content": "so definitely a lot of finnagling and diving into their repo"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:29:57.406000+00:00",
                "content": "oh interesting, you use their `system dependencies` yourself to OCR then"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:30:09.409000+00:00",
                "content": "ya everything is done locally"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:30:11.832000+00:00",
                "content": "so it's a bit slow"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:30:14.787000+00:00",
                "content": "gotcha"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:30:28.182000+00:00",
                "content": "alternative was their API or AWS Textract but it's wildly expensive"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:30:35.847000+00:00",
                "content": "at least given how many pages i need to process"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:31:20.429000+00:00",
                "content": "i see, i have heard textract is really damn good but makes sense"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:31:43.337000+00:00",
                "content": "I did test it since you can use a demo version"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:31:55.187000+00:00",
                "content": "it is very good, though honestly this combo is just as good and (i imagine) a lot cheaper"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:32:05.432000+00:00",
                "content": "plus the local-inference is huge"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:32:21.178000+00:00",
                "content": "wait gpt4o isn't local tho"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:32:24.381000+00:00",
                "content": "correct"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:32:29.305000+00:00",
                "content": "or do you mean local for everything but that?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:32:39.621000+00:00",
                "content": "but idk man sometimes when you're talking to customers, certain data privacy things are ok and others aren't"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:32:45.705000+00:00",
                "content": "lol"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:33:30.429000+00:00",
                "content": "btw did you end up using the codesnippet I had for converting to baml Image?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:33:34.733000+00:00",
                "content": "or something else?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:33:58.500000+00:00",
                "content": "no, in the instructured library there is a way to specify saving tables as images"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:34:09.439000+00:00",
                "content": "whats that lib?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:34:17.453000+00:00",
                "content": "wait do you save to disk?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:34:35.973000+00:00",
                "content": "yep"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:34:42.035000+00:00",
                "content": "in a temporary file location"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 20:34:49.234000+00:00",
                "content": "oh interesting, yea makes sense!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:46:00.752000+00:00",
                "content": "Maybe a silly question but in the docs I only see balml Images loaded from URL or base64, is there a way to do it when i have a saved .jpg file?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:46:51.708000+00:00",
                "content": "We dont support loading from a file yet :(, only base64 and image url. We have file uris in our roadmap"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-10 20:47:31.977000+00:00",
                "content": "But only for playground. In the actual code youll likely always need base64 or image url"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 20:48:34.955000+00:00",
                "content": "ok, let me see how well the base64 approach works"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:24:08.934000+00:00",
                "content": "```python\nfrom baml_py import Image\nfrom baml_client import b\n\nasync def test_image_input():\n  # from URL\n  res = await b.TestImageInput(\n      img=Image.from_url(\n          \"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n      )\n  )\n\n  # Base64 image\n  image_b64 = \"iVBORw0K....\"\n  res = await b.TestImageInput(\n    img=Image.from_base64(\"image/png\", image_b64)\n  )\n```\n\nDoes it have to be `image/png` or what are the options for the base64?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:24:12.816000+00:00",
                "content": "i believe it's default saved as jpg"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:24:47.743000+00:00",
                "content": "it should support any media type! We just add it into the CURL request as the model requests it"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:25:59.327000+00:00",
                "content": "dumb question but what are the options then?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:26:57.469000+00:00",
                "content": "I'm not sure what Openai/Anthropic explicitly support, but the copmlete list is here:\n\nhttps://www.iana.org/assignments/media-types/media-types.xhtml#image"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:27:32.561000+00:00",
                "content": "thnaks!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:31:30.586000+00:00",
                "content": "Would baml support test cases if i input a base64 string?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:31:37.672000+00:00",
                "content": "that would be great Lol"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:31:54.434000+00:00",
                "content": "what do you mean?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:32:09.596000+00:00",
                "content": "can I write test cases in my Baml file for functions that accept images as inputs"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:33:05.490000+00:00",
                "content": "ah yes!  \n\nhttps://docs.boundaryml.com/docs/snippets/test-cases#images"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:33:17.546000+00:00",
                "content": "legendary"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:33:18.427000+00:00",
                "content": "ty"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:41:56.565000+00:00",
                "content": "I have a fallback which starts with claude 3.5. sonnet and then gpt4o, seems like the test is defaulting to gpt 4o"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:42:00.470000+00:00",
                "content": "does this mean sonnet isn't supported?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:42:34.856000+00:00",
                "content": "easy thing to do is click on `raw curl` toggle, try the request in browser"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:42:39.730000+00:00",
                "content": "Looking at boundary dashboard, this is the return value:\n```\nRequest failed: {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"messages.0.content.1.image.source.base64.data: The image was specified using the image/png media type, but does not appear to be a valid png image\"}}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:42:42.408000+00:00",
                "content": "in curl*"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:42:49.109000+00:00",
                "content": "^ from claude 35"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:43:10.473000+00:00",
                "content": "that seems to be a message directly from claude's servers"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:44:07.225000+00:00",
                "content": "so it is almost def the fact that claude may need a different serialization, is it png or jpeg?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:44:42.941000+00:00",
                "content": "fixed it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:44:46.345000+00:00",
                "content": "üôÇ"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:44:49.985000+00:00",
                "content": "`image/jpeg` lol"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:44:55.670000+00:00",
                "content": "üòâ"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-10 22:45:06.868000+00:00",
                "content": "I appreciate you guys having patience lol"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:45:39.177000+00:00",
                "content": "not an issue at all, i gotta update the docs on testing images now cause it took me more than 5 mins to find it when you asked"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-10 22:45:44.073000+00:00",
                "content": "so its helpful for us too üôÇ"
            }
        ]
    },
    {
        "thread_id": 1260889940401721474,
        "thread_name": "VLLM support",
        "messages": [
            {
                "author": "feres0902",
                "timestamp": "2024-07-11 09:26:27.994000+00:00",
                "content": "i want to change code and add a vllm provider how could i do it im new here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 14:15:51.208000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 14:15:51.726000+00:00",
                "content": "Hi Feres, to add VLLM support we can do it a few ways:\n\n1. add official provider in rust (if you are familiar with it, I can point you to the code)\n2. Use VLLM's openai compatibility and it should work out of the box with BAML\n\n\nHere's how you can do it for 2:\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n\nUsing other openai compatible providers:\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/other\n\n```bash\npython -m vllm.entrypoints.openai.api_server --model NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key $VLLM_API_KEY\n```\n\n```rust\nclient<llm> MyVLLM {\n  provider openai\n  options {\n    base_url \"http://localhost:8000/v1\"\n    api_key env.VLLM_API_KEY\n    model \"NousResearch/Meta-Llama-3-8B-Instruct\"\n  }\n}\n```"
            },
            {
                "author": "feres0902",
                "timestamp": "2024-08-13 14:15:05.653000+00:00",
                "content": "sorry for late replay, \nThank you so much, yes i did make it work like this, the only issue now is how to send a batch requests so vlmm deal with them at very short time as the call now are made throw baml python code, the vllm openai compatible client also accespt multiple requests at the same time and the response time is very short"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 14:22:37.468000+00:00",
                "content": "perfect. i think batching is quite interesting of a concept, i wonder if we can support this more natively. \n\nIt doesn't look like their native sdk supports batched inference via the openai compatiable one:\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n\nWe would likely need to create a new instance of batch that is supported here. \nI wonder if it may be easier for these kinds of use cases for us to have an API in our SDK that does something like the following:\n\n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = b.parts.prompt.MyFunction(...)\n# you call VLLM directly\noutputs = llm.generate(prompts, sampling_params)\n# you parse it using baml\nfor output in outputs:\n   parsed = b.parts.parse.MyFunction(output)\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 14:24:27.994000+00:00",
                "content": "native batch support will be tricky as openai's batching inference is quite different than how VLLM does it."
            },
            {
                "author": "feres0902",
                "timestamp": "2024-08-14 07:21:08.759000+00:00",
                "content": "Yes agree on the logic; it should send a batch of prompts and then loop to parse (using baml parser) \n, i do vllm inference this way:\nclient = OpenAI(\n                    base_url=\"http://localhost:8000/v1\",\n                    api_key=\"token-abc123\",\n                )\n\ncompletion = client.completions.create(\n        model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n        prompt=list_of_prompts,\n        max_tokens= 1000 # chane this if you notice that the model output is not complete\n        )"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 07:27:42.880000+00:00",
                "content": "awesome, let me chat with the team tmrw and figure out what this would take for us to expose for you!"
            }
        ]
    },
    {
        "thread_id": 1260980140259938344,
        "thread_name": "Claude 3.5 Sonnect vs Opus",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 15:24:53.316000+00:00",
                "content": "Anyone know whether Claude 3.5 Sonnet is actually better than Opus in every regard?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 15:27:18.958000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 15:27:19.264000+00:00",
                "content": "Here's some benchmarks:\nhttps://gorilla.cs.berkeley.edu/leaderboard.html\n\nhttps://klu.ai/glossary/mmlu-eval\n\nIt seems like yes from these cases fyi!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-11 17:12:08.718000+00:00",
                "content": "someone also did this recently: https://vlmsareblind.github.io/ but it's not peer reviewed fyi (and their prompts may be bad)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 17:26:12.109000+00:00",
                "content": "https://chat.lmsys.org/?leaderboard as well"
            }
        ]
    },
    {
        "thread_id": 1261015172705812510,
        "thread_name": "will the version after 0.49.0 be",
        "messages": [
            {
                "author": "deoxykev",
                "timestamp": "2024-07-11 17:44:05.702000+00:00",
                "content": "will the version after 0.49.0 be released today? I want to try out the dynamic clients"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:44:36.042000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:44:36.400000+00:00",
                "content": "yes! we're working on the release this AM."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 19:19:41.056000+00:00",
                "content": "version 0.50 is out which now supports ClientRegistry. \n\nhttps://docs.boundaryml.com/docs/calling-baml/client-registry"
            }
        ]
    },
    {
        "thread_id": 1261057364723761152,
        "thread_name": "Did you guys say something about",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 20:31:45.063000+00:00",
                "content": "Did you guys say something about rendering `\\n` characters not actually rendering newlines?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 20:32:42.630000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 20:32:42.970000+00:00",
                "content": "I would recommend block strings isntead `#\"..\"#` with actual new lines. \n\nWe're working on patching a bug in our quoted string `\"...\"` so it does correct escaping!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 20:34:08.427000+00:00",
                "content": "But is this for test cases or for the actual function ?"
            }
        ]
    },
    {
        "thread_id": 1261095953939693658,
        "thread_name": "Where can I find the correct azure api_",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 23:05:05.449000+00:00",
                "content": "Where can I find the correct azure api_version I should be using?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 23:07:24.742000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 23:07:25.006000+00:00",
                "content": "https://docs.boundaryml.com/docs/snippets/clients/providers/azure"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 23:08:04.018000+00:00",
                "content": "to be clear: I'm trying to figure out what the right api_version is in azure haha, was wondering if someone knew where in the azure portal I could find that"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 23:08:20.403000+00:00",
                "content": "ahh"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 23:08:54.091000+00:00",
                "content": "lemme check"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 23:09:35.798000+00:00",
                "content": "I found this? https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 23:10:42.837000+00:00",
                "content": "none of us know where in the azure portal it is"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-11 23:11:07.542000+00:00",
                "content": "(we're setting up for an event right now, hence why we're a bit slow to respond right now)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 23:11:18.787000+00:00",
                "content": "you're good, p sure the link i dropped is the one"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-12 01:40:58.431000+00:00",
                "content": "Here‚Äôs our working azure config\n\nclient<llm> AzureGPT4o {\n  provider azure-openai\n  options {\n    temperature 0.0\n    seed 42\n    base_url \"https://mycustomdomain.openai.azure.com/openai/deployments/gpt-4o\"\n    api_version \"2024-02-01\"\n    api_key env.AZURE_OPENAI_API_KEY\n  }\n}"
            }
        ]
    },
    {
        "thread_id": 1261346152918290508,
        "thread_name": "100% cpu usage",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-12 15:39:17.534000+00:00",
                "content": "Does baml offer some sort of timeout on clients? in development i noticed a boundary function led to 100% CPU usage for my worker and it's clearly stalled. Though I'm not seeing any options to set timeouts (which I would think can be irrespective of provider since it's controlled by the baml client)\n\nhttps://docs.boundaryml.com/docs/snippets/clients/overview"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-12 16:04:03.821000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-12 16:04:04.107000+00:00",
                "content": "this is almost definitely the tracing bug you‚Äôre hitting, unfortunately- you can use atexit.unregister(flush) to disable this"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-12 17:21:05.822000+00:00",
                "content": "<@711679663746842796> can you add a feature request for adding timeouts?"
            },
            {
                "author": "sduck.is",
                "timestamp": "2024-07-16 08:15:02.300000+00:00",
                "content": "I'm using BAML with `llama-3-8b-instruct` model. It works well overall, but sometimes it comes out as Giverish. I need a way to control this because the response lasts indefinitely."
            },
            {
                "author": "sduck.is",
                "timestamp": "2024-07-16 08:15:37.271000+00:00",
                "content": "Anyway, it's a great library. Thank you for the hard work of the baml team. üëç"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-16 13:09:24.567000+00:00",
                "content": "Answered in <#1262757642313400350>"
            }
        ]
    },
    {
        "thread_id": 1261915190895579197,
        "thread_name": "Formatting",
        "messages": [
            {
                "author": "etbyrd",
                "timestamp": "2024-07-14 05:20:26.760000+00:00",
                "content": "Are there any plans to support auto-formatting in VS Code?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-14 07:04:14.248000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-14 07:04:15.023000+00:00",
                "content": "Yes! It is on our roadmap! But we dont have an exact date just yet"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-15 00:35:21.877000+00:00",
                "content": "Awesome, thanks!"
            }
        ]
    },
    {
        "thread_id": 1262416802000928999,
        "thread_name": "Validations",
        "messages": [
            {
                "author": "kdub03",
                "timestamp": "2024-07-15 14:33:40.167000+00:00",
                "content": "Is there thought/work towards how this could work with evaluations? I'm finding that defining my models in baml is more challenging to iterate my ground truth -> schema mapping, since the schema is abstracted and volatile."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:42:05.297000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:42:05.756000+00:00",
                "content": "Your timing is fortuitous!  I just started my draft of this feature!\n\nhttps://github.com/orgs/BoundaryML/discussions/786"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:42:41.337000+00:00",
                "content": "Oh wait. You mean asserts in test cases!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:44:02.563000+00:00",
                "content": "Sorry misread. That feature is also coming up shortly! We have a spec on it that we will post for review and then implement shortly! It will be something akin to adding a the assert keyword in a test case"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:44:51.378000+00:00",
                "content": "What would be very helpful here is if you could share a few examples of ground turth you‚Äôd like to assert!"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-07-15 14:46:21.353000+00:00",
                "content": "Does my problem space make sense? So I have 10x ground truth examples. I think my ideal workflow would be to iterate on extraction schemas, and test them against my GT data. The challenge is that I have to continually adapt my GT data to match my extraction schema, since the two don't necessarily align, and any changes I make to extraction would require changes on each GT case."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:47:23.138000+00:00",
                "content": "that makes sense. whats hard is not having access to some map based operator to help you do transformations?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:48:13.463000+00:00",
                "content": "curious, can you share with me a code sample of how the two end up diverting?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 14:48:31.443000+00:00",
                "content": "and which binding are you using? TS or Python or ruby?"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-07-15 14:52:22.045000+00:00",
                "content": "Python:\n\nI'm working with commercial insurance data, so I represent different types of coverages, limits, endorsements as various classes. I find I need to iterate on things being like, 4 booleans, vs an enum. Or how detailed an endorsement extraction is. As I iterate on those, I would typically make changes to my GT to reflect what I want.\nWith the abstraction from baml, its harder to \"sync\" my GT data to the schema I need to compare against."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 15:00:22.517000+00:00",
                "content": "got it! thats very helpful. thanks for sharing. If it understand correctly the issue boils down to:\n1. Due to LLMs, you end up. changing the datamodel quite a lot\n2. the requires you to modify the output quite a lot to get the final schema you want.\n\nI think we have a couple of things in store that will help with this (evaluations in BAML is one, but i think its insufficient to address all your needs here, you likely need something like the `@compute` and `@no_binding` idea we briefly touch on in the spec i listed).\n\nHere's my suggestion to help unblock you wihtout any new features:\n\nI would write a simple test suite in python like the following:\n\n```python\n# test_my_type.py\n\nfrom baml_client import b\nfrom baml_client.types import MyLLMResponseType\n\nfrom app.somewhere import MyGTType\n\n# Update this function every time you modify MyLLMResponseType\ndef validate(res: MyLLMResponseType, expected: MyGTType):\n  assert res.foo == expected.bar\n\nasync def test_my_function():\n  params, expected = load_from_file(\"./mydata.json\")\n  res = await b.MyFunction(**params)\n  validate(res, expected)\n```"
            }
        ]
    },
    {
        "thread_id": 1262435894791766016,
        "thread_name": "when I save my baml file in vs code, it",
        "messages": [
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-15 15:49:32.243000+00:00",
                "content": "when I save my baml file in vs code, it is autogenerating a baml_client directory in the parent directory,  and not the directory that contains baml_src. How can I fix this?"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-15 15:54:52.717000+00:00",
                "content": ""
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-15 15:54:53.181000+00:00",
                "content": "or better yet, can I disable the autogenerate on save in vscode?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 16:17:53.688000+00:00",
                "content": "we're actually working on adding the capability to disable auto-generate. Should land either today or tmrw! <@201399017161097216> in on check of this.\n\n`baml_client` should actually always be generated outside of the `baml_src` folder as its a binding from baml -> language of your choice, so we opt to not include it as a part of the `baml` project.\n\nthat said, you can modify the `generator` block that was auto-generated for you to change the path where `baml_client` is generated. Or if you don't want any bindings, you can just remove that block all together.\n\nIf the block is not present that should disable auto-generate as well.\n\nThen you can see the `baml-cli generate --help` for instructions on how to generate if the block is not present."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 16:18:49.312000+00:00",
                "content": "Out of curiosity, can you share why you don't want it to auto-generate? Is it beacuse you are just testing `baml` out for now or just you don't like auto-generate and prefer to have more control over when it runs?"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-15 16:44:43.993000+00:00",
                "content": "if this is my directory structure ./parent_dir/package/baml_src I am finding that baml_client is being generated here ./parent_dir/baml_client instead of ./parent/package/baml_client"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-15 16:44:56.519000+00:00",
                "content": "it works correctly with baml-cli generate, but not when it is autogenerated on saves"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-15 16:45:49.142000+00:00",
                "content": "Oh! Thats great to know. We'll also file that bug, but for now disabling it seems like the easiest fix."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-16 07:27:33.025000+00:00",
                "content": "FYI we sadly couldn‚Äôt land it in today, but it should come in tmrw! We‚Äôre just doing some final testing. \n\nhttps://github.com/BoundaryML/baml/pull/791"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-19 23:28:03.820000+00:00",
                "content": "It's ready, see https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#vscode-generator-settings !"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-19 23:28:24.221000+00:00",
                "content": "Version 0.51.0 of VSCode (update baml-py or the TS dependency as well to that version)"
            }
        ]
    },
    {
        "thread_id": 1262757642313400350,
        "thread_name": "Restricting BAML output",
        "messages": [
            {
                "author": "hellovai",
                "timestamp": "2024-07-16 13:08:02.830000+00:00",
                "content": "Restricting BAML output"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-16 13:08:05.960000+00:00",
                "content": "<@222588854308175872> \n> I'm using BAML with llama-3-8b-instruct model. It works well overall, but sometimes it comes out as Giverish. I need a way to control this because the response lasts indefinitely.\n\nCould you further clarify? Also how are you calling llama-3? Ollama provider?\n\nCould you consider putting a max-token limit on the output?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-16 13:08:58.895000+00:00",
                "content": "Also, it would be helpful to see an example of a good vs bad output. You may be able to abort for example and kill the network request as well."
            },
            {
                "author": "sduck.is",
                "timestamp": "2024-07-17 10:40:10.483000+00:00",
                "content": "I'm serving the model using vLLM, connecting with vLLM OpenAI Compatible API."
            },
            {
                "author": "sduck.is",
                "timestamp": "2024-07-17 10:42:52.076000+00:00",
                "content": "Since `max_tokens` is set up, streaming doesn't go on indefinitely in Giverish case."
            },
            {
                "author": "sduck.is",
                "timestamp": "2024-07-17 10:46:50.249000+00:00",
                "content": "I'm trying to use it to refine roleplay responses. And this is good and bad response examples.\n\n**Good**\n```\n{\n  \"actor\": \"Harumi\",\n  \"why\": \"to respond to Ken\",\n  \"actions\": [\n    {\n      \"move\": \"She closes her book and puts it on the table\"\n    },\n    {\n      \"emotion\": \"pleasant surprise\",\n      \"speak\": \"Welcome to the club, Ken. We don't have any members yet, so we can start from scratch. What makes you interested in literature?\"\n    }\n  ]\n}\n```\n\n**Bad**\n```\nI I am: I: Harumi: I am: ... (ongoing like this)\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:30:10.269000+00:00",
                "content": "Oh that‚Äôs an interesting scenario. How did you handle this in the past?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:31:41.889000+00:00",
                "content": "How often does this happen?\nOne idea I have is perhaps a timeout?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:36:31.594000+00:00",
                "content": "It‚Äôs a tricky problem as technically the LLM could eventually dump out a valid json after some amount of I I am:‚Ä¶  one way I could imagine is you calling the stream based function and then if you don‚Äôt get a parseabke response within the first 100, then it‚Äôs not a good request. (Almost like a timeout?)"
            }
        ]
    },
    {
        "thread_id": 1262955104206590034,
        "thread_name": "Widmill deployment",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-07-17 02:12:41.414000+00:00",
                "content": "perhaps this is too specific but if i wanted to use baml with something like windmill, i would either have to manually deploy the baml generated python files or figure out a way for the worker to have called baml-cli generate"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-17 02:14:36.321000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-17 02:14:36.722000+00:00",
                "content": "Yes you would need to include the generated files or do the recommended way which is to generate them at build time"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-17 02:23:37.087000+00:00",
                "content": "Too complicated with windmill, you have to run your baml-cli in a docker container then run everything inside"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-17 02:24:55.717000+00:00",
                "content": "I would probably just use instructor to keep things simple in windmill tbh"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 02:42:49.922000+00:00",
                "content": "Agreed. This is not really a well supported path! (Ps there is a way if you are incredibly committed to do it without baml-cli generate, but we don‚Äôt have great docs for it. But it is technicallly possible.)\nIf you‚Äôre interested I can post a Jupyter notebook example when I‚Äôm home! But you wouldnt have the same level of autocomplete capabilities that baml-cli generate offers. Hence why instructor may be a better call."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 02:43:42.860000+00:00",
                "content": "Alternatively you host a python server with fly.io or something that hosts baml endpoints that you then call from windmill"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-17 03:12:08.516000+00:00",
                "content": "thanks, given instructor doesn't fuzzy parse the LLM outputs i would have to use temperature > 0 for the retries to work? tbh i don't know what % of time this is even an issue but i have rather large prompt contexts / outputs so i don't want a stray comma or quote to break instructor"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-17 03:13:58.333000+00:00",
                "content": "separately, are there any other workflow tools that you like other than windmill? i was just trying to set up a simple pipeline with user QA / inputs at various points"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-07-17 04:34:27.872000+00:00",
                "content": "I‚Äôm using temporal for workflow management with similar use case including QA like you\n\nFits well with BAML I think"
            }
        ]
    },
    {
        "thread_id": 1263053723295416320,
        "thread_name": "How are the query costs in the",
        "messages": [
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-17 08:44:34.037000+00:00",
                "content": "How are the query costs in the observability platform calculated? In my case, I'm using the Huggingface Serverless Inference API Pro as a client, which has a fixed cost of $10 / month (but costs per query show up regardless)."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 08:46:55.070000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 08:46:55.344000+00:00",
                "content": "we've actually hard coded it atm for certain models (we assume gpt-3.5 in the case of not one of the ones we support i beleive)! We're working on adding pricing to the baml client files actually! So its easier for users to control it."
            }
        ]
    },
    {
        "thread_id": 1263152396985696329,
        "thread_name": "Hi, when I access a provided share link",
        "messages": [
            {
                "author": "chocobeery",
                "timestamp": "2024-07-17 15:16:39.678000+00:00",
                "content": "Hi, when I access a provided share link, I only see the default template. Is this the intended display or should it be showing someone else code?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:23:49.678000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:23:50.068000+00:00",
                "content": "you will need to switch files ovrer to the main.baml file! We will patch that tho!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:24:18.484000+00:00",
                "content": "can you also post the link you are trying to access which isn't working?"
            },
            {
                "author": "chocobeery",
                "timestamp": "2024-07-17 15:29:39.590000+00:00",
                "content": "here you go, https://www.promptfiddle.com/gpt4o-maxes-at-4096-output-tokens-pkc3i"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:37:59.578000+00:00",
                "content": "ah yes, i think it works, you just need to switch files to main.baml"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:38:01.376000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 15:39:01.291000+00:00",
                "content": "in that specific links case the difference between that an the default tempalte is this bit i believe"
            },
            {
                "author": "chocobeery",
                "timestamp": "2024-07-18 01:25:49.106000+00:00",
                "content": "ic! tks for the fast response"
            }
        ]
    },
    {
        "thread_id": 1263267762466328638,
        "thread_name": "LLM cut short",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 22:55:04.952000+00:00",
                "content": "for some LLM calls, where I need to generate a lot of output, I'm running up against the bounds of the 4096 output token maximum. Does anyone have any good prompt engineering tips so that I can somehow recognize when the bounds have been reached to then pass this output as input to a subsequent continuation LLM call?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 22:59:30.045000+00:00",
                "content": ""
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 22:59:30.239000+00:00",
                "content": "Alternatively, is there a way to tell in Baml when the parsed response was cut short due to max tokens being reached? This could be an indicator for me that I need to pump the output back into a continuation call"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 23:07:52.804000+00:00",
                "content": "^we are thinking of ways to do taht actually! \n\nAdding a trigger into BAML that auto calls the llm when a parameter is explictly set the client.\n\nsomething like:\n\n```rust\nclient<llm> SomeClient {\n  provider openai\n  options {\n     max_tokens_behaviour 'continue' | 'stop'\n  }\n}\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:09:09.942000+00:00",
                "content": "does that mean there's no way to see the stop reason for my LLM call today?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 23:13:50.471000+00:00",
                "content": "sadly not really. only in the dashboard. \n\nI can see if we can release this within the next few days?\n\nreason why its not very visible:\n- we only expose the parsed data type and we'd have to change the function's callstack interface to patch this. We are working on a more transparent interface that is a bit more cumbersome to call, but gives you the raw openai response we made."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 23:14:18.508000+00:00",
                "content": "But that won't land in the next two weeks. The max_tokens is more likely to be done."
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:14:31.341000+00:00",
                "content": "i hear you. I mean idrc what the solution is, though my problem is I don't know whether the LLM gave me my complete list or not üò¶"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:14:50.451000+00:00",
                "content": "manually inspecting the raw LLM response it's clear that it was cut part way through"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 23:15:16.245000+00:00",
                "content": "Oh see, basically we're parsing it for you, but programatically you have no signal about if we ran into max_tokens or not"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:15:24.822000+00:00",
                "content": "correct"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:15:31.116000+00:00",
                "content": "because my parsed response is basically a list"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:15:42.414000+00:00",
                "content": "so i think i get the n-1 first entries parsed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 23:15:42.542000+00:00",
                "content": "curious, would you rather have us raise an error when max_tokens is hit? (also can talk on discord really quick"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 23:15:48.571000+00:00",
                "content": "let's chat"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 23:32:30.138000+00:00",
                "content": "https://www.promptfiddle.com/GPT4-32k-tokens-kdE9P\n\n```\nclient<llm> GPT4 {\n  // Use one of the following: https://docs.boundaryml.com/docs/snippets/clients/providers/openai\n  provider openai\n  // You can pass in any parameters from the OpenAI Python documentation into the options block.\n  options {\n    model gpt-4-32k-0613\n    max_tokens 8192\n    api_key env.OPENAI_API_KEY\n  }\n} \n```\n\nbut it is *expensive*"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 23:36:38.651000+00:00",
                "content": "there are 32k ones in azure https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-17 23:37:35.027000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/client-registry\n\n```python\nfrom baml_py import ClientRegistry\nasync def run():\n    cr = ClientRegistry()\n    # Sets MyAmazingClient as the primary client (MyAmazingClient defined in BAML)\n    cr.set_primary('MyAmazingClient')\n    # ExtractResume will now use MyAmazingClient as the calling client\n    res = await b.ExtractResume(\"...\", { \"client_registry\": cr })\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 23:43:11.710000+00:00",
                "content": "fyi there's also `gpt-3.5-turbo-16k-0613`"
            }
        ]
    },
    {
        "thread_id": 1263509002755510383,
        "thread_name": "BAML vs Outlines vs Instructor",
        "messages": [
            {
                "author": "josephsirosh_22062",
                "timestamp": "2024-07-18 14:53:41.119000+00:00",
                "content": "[Repost from <#1119375594984050779>] Clarification question: I'm comparing Outlines, Instructor, and BAML, and trying to understand the technical nuances that differentiate BAML. Especially over time, as both Outlines and Instructor are also continuously improving their codebases. What is enduringly different about the BAML approach?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 16:15:02.072000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 16:15:02.474000+00:00",
                "content": "Thats a great question. \n\nFundamentally, its all just code. So there is never really anything that differentiates one peice of software from another. For example, one could say you can't have fast code in python, but technically, you can write a library in C++ with bindings to python (e.g. numpy), and make python pretty fast. \n\nGiven that disclaimer, I think of the goal of Instructor / Outlines very different than that of BAML. All of them have a shared goal of providing an interface to communicate with LLMs.\n\nBAML has two additional explicit goals built into its design: provide a great developer experience and complete transparency.\n\n- Providing complete transparency\n\n1. This includes our tools which **visualize the prompt** PRIOR to execution\n2. The ability to **see the actual CURL request** we are building to the LLM PRIOR to us making it\n3. (soon!) The ability to see the exact order of how different LLMs are called given complex retry policies, fallbacks.\n4. **Complete control over the prompt** (BAML encodes no special text or prefix)\n\n- Providing a great developer experience\n\n1. Includes tools like the VSCode playground where you can **run tests easily** and visualize almost every detail about the function are you working on. \n2. **Special prompt syntax** like `@alias` for Enum Values which is not really doable in python, or even comments in prompts. \n3. A much more **powerful type library**. For example: we are able toreturn a list of objects without wrapping them in another object.\n4. **BAML's stability across languages**. By nature of implementing BAML in rust and providing a thin interface to Python / Typescript / Ruby / etc, we have almost no bugs that are only in one language. However, Instructor / Outlines would basically have to be re-written every time they need to support another language which means there will be more drift and bugs."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 16:15:06.387000+00:00",
                "content": "And lastly, a big difference in my mind from a technical capability is the algorithmic focus on our team. We hire mostly folks with deep background in algorithm design, which allows us to invent new techniques like our fuzzy parsing. \n\nAnyways, long ramble üôÇ If you would actually be open to it, I'd love to hear in your perspective what you find the differences are as a developer. I'd be happy to hop into the office hours channel."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-18 16:23:37.558000+00:00",
                "content": "In addition to everything Vaibhav said above:\n\n- outlines (and sglang and guidance) takes the constrained output approach, which makes it harder to do chain of thought (you'd have to somehow model that in the output grammar)\n- instructor's selling point is that you can use the underlying SDK directly, and it wraps the underlying SDK with wrapper transforms, which it achieves by (silently!) transforming the prompt for the user\n\nWe wrote about this a bit in https://www.boundaryml.com/blog/structured-output-from-llms"
            },
            {
                "author": "josephsirosh_22062",
                "timestamp": "2024-07-18 17:35:04.964000+00:00",
                "content": "Follow up question: With e.g. OpenAI, can you handle OpenAI Assistant runs (present or in the future)? Or only chat-completions? I'm building with OpenAI assistants."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-18 17:38:49.259000+00:00",
                "content": "No support right now, am looking through the API and figuring out how hard it would be for us to add support"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-18 17:39:32.007000+00:00",
                "content": "Are you on the v1 API or the v2?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 17:56:31.496000+00:00",
                "content": "<@1191794836085424149> for some context, we aren't yet ruling out supporting OpenaI Assistant API yet, however one thing we try and do with BAML is support virtually every model / LLM provider.\n\nAssistants API is very, very tied to openai and its hard to deciefer what it would mean to support that for other models. However, if it becomes a trend that others are starting to implement it, then yes, we would definetely support it.\n\nAn example of this is function calling. Originally, it was just openai, but now its looking like all models are starting to support this more natively, that means its a great case for BAML to include offical support for it.\n\nAs a general rule of thumb, we are likely never going to adopt every new capability that everyone adds immediately, we prefer to wait a bit and then see how the API pans out.\n\nTo help us better understand your use case of the assistants API, whats your favorite part of using that interface compared to the chat / completions? (is it the abstractions around RAG / context?)"
            },
            {
                "author": "josephsirosh_22062",
                "timestamp": "2024-07-18 18:49:01.310000+00:00",
                "content": "The file_search tool with vector stores is incredibly useful. That is the standout feature for me."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 05:55:12.940000+00:00",
                "content": "Thats good to know. That's 100% one of the areas we're lacking. We mostly aim for providing full control to the developer, but we may benefit from providing some higher level building blocks like a vector store search. I'll share this during our bi-weekly roadmap sync and see what we think!\n\ni wouldn't rule it out, but likely a bit out of scope for the next few months. However, we may put together a quick python helper library or something to accompany baml."
            }
        ]
    },
    {
        "thread_id": 1263791438097809472,
        "thread_name": "GPT 4o Mini",
        "messages": [
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 09:35:58.952000+00:00",
                "content": "Are you going to support GPT 4o Mini that was released yesterday? Perhaps it is already supported?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 09:36:40.052000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 09:36:40.574000+00:00",
                "content": "Already supported!\n\njust change the model parameter in your client and it'll \"just work\""
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 09:37:30.726000+00:00",
                "content": "shoulld it be like this?\n\n```\nclient<llm> GPT4Mini {\n  provider openai\n  options {\n    model \"gpt-4.0-mini\"\n    api_key \"my api key\"\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 09:37:37.271000+00:00",
                "content": "nailed it!"
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 09:37:53.611000+00:00",
                "content": "but is the model string correct tho? model \"gpt-4.0-mini\""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 09:38:43.214000+00:00",
                "content": "yep! `\"gpt-4o-mini\"`\n\neffectively we support any model that you can pass in from here: https://platform.openai.com/docs/models/gpt-4o-mini"
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 09:40:04.066000+00:00",
                "content": "Thank you Vaibhav!"
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 09:40:09.531000+00:00",
                "content": "You're such a great help!"
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 10:10:31.194000+00:00",
                "content": "By the way, do you know where I can securely set the env variables for the BAML to pick up?\n\n```\nclient<llm> Claude {\n  provider anthropic\n  options {\n    model \"claude-3-opus-20240229\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n```\n\nANTHROPIC_API_KEY\n\nsetting this key in the .env file in the baml_src does not seem to work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 10:12:02.877000+00:00",
                "content": "yes! we recommend using something like `dotenv -e .env python your_python.py` or a secrets manager.\n\nYou have some other options here as well:\nhttps://docs.boundaryml.com/docs/calling-baml/set-env-vars"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 10:12:29.243000+00:00",
                "content": "in VSCode, you can also set up env variables for the playground:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#setting-env-variables"
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 10:51:48.633000+00:00",
                "content": "Do you have perhaps a project/demo with the env variables, as setting \n\n```\nimport dotenv\ndotenv.load_dotenv()\n```\nbefore importing baml does not seem to work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 10:53:03.927000+00:00",
                "content": "thats odd! Can you try the following:\n\n```sh\npip install dotenv-cli\ndotenv python my_app.py\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 10:53:38.599000+00:00",
                "content": "I'll file a github issue regarding the dotenv approach directly in python not working"
            },
            {
                "author": "gintautas_turing",
                "timestamp": "2024-07-19 10:54:43.033000+00:00",
                "content": "I can provide info in that github issue in case you need it, just paste the link here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 10:56:44.388000+00:00",
                "content": "https://github.com/BoundaryML/baml/issues/805"
            }
        ]
    },
    {
        "thread_id": 1265125765293281290,
        "thread_name": "iframe hyperlinks",
        "messages": [
            {
                "author": "deoxykev",
                "timestamp": "2024-07-23 01:58:07.346000+00:00",
                "content": "Can I iframe the hyperlinks to prompt execution traces on the observability platform to internal tools?\n\nI would like to just iframe the prompt structures in grafana with the rest of the execution trace from otel"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 05:39:19.353000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 05:39:19.712000+00:00",
                "content": "There should be nothing specific that blocks that!\n\nAre you unable to? I can double check with our auth provider to see what would work"
            }
        ]
    },
    {
        "thread_id": 1265663561577926726,
        "thread_name": "Does baml automatically detect whether",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-24 13:35:07.972000+00:00",
                "content": "Does baml automatically detect whether the context window limitation is exceeded? e.g. I have a fallback client which is gpt4 then claude sonnet. If i submit input which exceeds the 128k token limit, will baml automatically skip the gpt 4 client or will it try and fail based on my designated retry policy?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:33:57.055000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:33:57.815000+00:00",
                "content": "no, we'll call each one subsequently, but that does sound like something we could do! (the tokenizer isn't always revealed, so we may need to think a bit more creatively to consider if we run it or not)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-24 15:09:11.596000+00:00",
                "content": "ya that's fair, maybe it makes sense for me to keep that logic in application code and use the client registry then"
            }
        ]
    },
    {
        "thread_id": 1265679930541478120,
        "thread_name": "Installation issues baml-py",
        "messages": [
            {
                "author": "energetic_axolotl_22123",
                "timestamp": "2024-07-24 14:40:10.637000+00:00",
                "content": "I'm having trouble installing baml-py using poetry. It seems to detect 0.51.3/0.52.0 as the latest version, but that version is not actually installable? When I simply run pip install baml-py, I get baml-py v0.46.0. poetry add baml-py@0.46.0 works fine."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:40:54.373000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:40:54.879000+00:00",
                "content": "That's odd! what operating system are you on?"
            },
            {
                "author": "energetic_axolotl_22123",
                "timestamp": "2024-07-24 14:41:34.767000+00:00",
                "content": "I tested on WSL first, then logged on a VPS server (Ubuntu) same issue. Python3.11 in both environments."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:42:15.637000+00:00",
                "content": "```python\nvbv@Vaibhavs-MacBook-Pro-2 super-easy-demo % poetry add baml-py@latest\nUsing version ^0.52.0 for baml-py\n\nUpdating dependencies\nResolving dependencies... (0.5s)\n\nPackage operations: 0 installs, 1 update, 0 removals\n\n  - Updating baml-py (0.51.3 -> 0.52.0)\n```\n\nThis is what i'm see on my macbook pro.\n\nAre you on Arm64?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:42:49.753000+00:00",
                "content": "alternatively, would you be open to quickly getting on office hours and seeing if you can screenshare?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:42:54.334000+00:00",
                "content": "Might make it faster!"
            },
            {
                "author": "energetic_axolotl_22123",
                "timestamp": "2024-07-24 14:43:16.032000+00:00",
                "content": "I can try"
            },
            {
                "author": "energetic_axolotl_22123",
                "timestamp": "2024-07-24 14:45:08.235000+00:00",
                "content": "sorry I'm very new to discord, let me try from the iPad"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:45:29.472000+00:00",
                "content": "No worries, let me send a zoom link!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 14:45:42.115000+00:00",
                "content": "I just DM'ed you!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 17:16:04.259000+00:00",
                "content": "Can you check now <@1171670007588065293> ? <@711679663746842796> released 0.52.1!"
            },
            {
                "author": "energetic_axolotl_22123",
                "timestamp": "2024-07-24 20:26:50.447000+00:00",
                "content": "Installation working well! Thanks so much!"
            }
        ]
    },
    {
        "thread_id": 1265689216596050021,
        "thread_name": "is it possible to set the url for a llm",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-07-24 15:17:04.605000+00:00",
                "content": "is it possible to set the url for a llm resource, I‚Äôm trying to see if baml can work with Martian - I believe Martian matches the OpenAI api"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 15:19:22.649000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 15:19:23.147000+00:00",
                "content": "yes in the playground! There's a raw curl button that will show you the the request we are making"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-24 15:23:42.333000+00:00",
                "content": "can I define a custom resource in clients"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 18:25:28.524000+00:00",
                "content": "Yeah, you can add arbitrary `client` definitions in `clients.baml` - is that what you're asking about, or are you asking about something else?"
            }
        ]
    },
    {
        "thread_id": 1265786815831478273,
        "thread_name": "Is it possible to ask BAML to _not_",
        "messages": [
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:44:54.076000+00:00",
                "content": "Is it possible to ask BAML to _not_ include certain inputs on clients when using the openai API format?\n\nContext: I am testing Fireworks using:\n```client<llm> FireworksLlama31_70b {\n  provider openai\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    api_key env.FIREWORKS_API_KEY\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n  }\n}```\n\n and I get:\n\n```\nRequest failed: {\"error\":{\"object\":\"error\",\"type\":\"invalid_request_error\",\"message\":\"Extra inputs are not permitted, field: 'stream_options'\"}}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 21:48:27.706000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 21:48:28.227000+00:00",
                "content": "oh interesting!! Right now no. Let me chat with the team and find a way to expose this properly! I'll get back to you by EOD!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 21:48:45.046000+00:00",
                "content": "is fireworks high priority for you?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 21:49:13.693000+00:00",
                "content": "seems like someone had a similar issue recently https://github.com/FlowiseAI/Flowise/issues/2866"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:49:59.269000+00:00",
                "content": "Thanks! Actually, it looks like I can do this:\n\n```\nclient<llm> FireworksLlama31_70b {\n  provider ollama\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n    headers {\n        \"Authorization\" \"Bearer <API_KEY>\"\n    }\n  }\n}\n```\n\nBut then I actually have to put the API key there - is there way to grab it from env with string interpolation from there?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:50:34.395000+00:00",
                "content": "They currently have a very good 3.1 price and I was going to divert some prod traffic to them to see how they handle it if I can üòÑ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 21:51:36.982000+00:00",
                "content": "you can just do:\n\n```\nclient<llm> FireworksLlama31_70b {\n  provider ollama\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n    headers {\n        \"Authorization\" env.MyKey\n    }\n  }\n}\n```\n\n```\nexport MyKey = \"Bearer <API_KEY>\"\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 21:51:47.469000+00:00",
                "content": "i think that should work!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:52:36.154000+00:00",
                "content": "Actually, there's a better solution than this for your problem"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:52:40.869000+00:00",
                "content": "Pulling up the docs, one sec"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:53:05.866000+00:00",
                "content": "I'd still have to export it with the secret, right? I was hoping for something like in TS with `Bearer ${env.FIREWORKS_API_KEY}`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:53:14.870000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/client-registry is what you want"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:53:41.572000+00:00",
                "content": "so you can write TS code like this:\n\n```async function run() {\n    const cr = new ClientRegistry()\n    // Creates a new client\n    cr.addLlmClient({ name: 'MyAmazingClient', provider: 'openai', options: {\n        model: \"gpt-4o\",\n        temperature: 0.7,\n        api_key: \"sk-...\"\n    }})\n    // Sets MyAmazingClient as the primary client\n    cr.setPrimary('MyAmazingClient')\n    // ExtractResume will now use MyAmazingClient as the calling client\n    const res = await b.ExtractResume(\"...\", { clientRegistry: cr })\n}\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:53:45.716000+00:00",
                "content": "for fireworks:"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:53:51.226000+00:00",
                "content": "Oh! Very nice"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:54:03.466000+00:00",
                "content": "so you can imagine how that'd work for fireworks"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:54:27.112000+00:00",
                "content": "+ with this approach you don't have to redefine all your functions"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:55:02.402000+00:00",
                "content": "Does this handle what you need to do?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:55:59.311000+00:00",
                "content": "Yeah, that should work!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:56:20.467000+00:00",
                "content": "But surprisingly, this also works:\n\n```\nclient<llm> FireworksLlama31_70b {\n  provider ollama\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n    headers {\n        \"Authorization\" env.FIREWORKS_API_KEY\n    }\n  }\n}\n```\n\nIt doesn't seem to actually need 'bearer' as part of the auth key"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:56:29.652000+00:00",
                "content": "lmao"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:56:47.424000+00:00",
                "content": "don't you love when things just work out? lol"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:56:47.513000+00:00",
                "content": "i would stick with whatever the fireworks docs say"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:57:18.758000+00:00",
                "content": "it's possible that it's a thing they don't realize $underlying-authz-framework supports and that they might break it at any moment"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 21:57:49.991000+00:00",
                "content": "glad we could get you unblocked!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:57:54.863000+00:00",
                "content": "yeah, that is a very good point, and it would be a pain in the ass to debug because I would have 100% forgotten about this converstation by that point..."
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 21:58:00.012000+00:00",
                "content": "Thanks so much guys!"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 22:49:34.881000+00:00",
                "content": "So this works in a local client & playground:\n\n```\nclient<llm> FireworksLlama31_405b {\n   provider ollama\n   options {\n     base_url \"https://api.fireworks.ai/inference/v1/\"\n     model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n     headers {\n         \"Authorization\" env.FIREWORKS_API_KEY\n     }\n   }\n }\n```\n\nBut trying to convert to using ClientRegistry:\n\n```\nconst cr = new ClientRegistry();\ncr.addLlmClient(\"Fireworks_Llama31_405b\", \"ollama\", {\n    base_url: \"https://api.fireworks.ai/inference/v1/\",\n    model: \"accounts/fireworks/models/llama-v3p1-405b-instruct\",\n    headers: {\n        Authorization: \"Bearer \" + FIREWORKS_API_KEY\n    }\n})\n\ncr.setPrimary(\"Fireworks_Llama31_405b\");\n\ntry {\n    result = await b.AnalyzeFileChangeBAML(patch, { clientRegistry: cr });\n} catch (error: any) {\n    console.error(\"Could not use Fireworks, error:\", error);\n}\n```\n\nI get (truncated):\n\n```\n\"Request failed: \", code: Other(404) }\n\n    at BamlAsyncClient.AnalyzeFileChangeBAML (file:///var/task/packages/analyzing/src/analyzeConsumer.mjs:14501:16)\n    at async Runtime.handler (file:///var/task/packages/analyzing/src/analyzeConsumer.mjs:14635:22) {\n  code: 'GenericFailure'\n}\n```\n\nunfortuntely I can't easily check the actual url called in my current environment"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 22:50:38.697000+00:00",
                "content": "what version of `@boundaryml/baml` are you on? the latest is 0.52.1"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 22:51:24.200000+00:00",
                "content": "you can also set `BAML_LOG=debug` in the env to try to get logs out"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 22:51:29.449000+00:00",
                "content": "```\n'@boundaryml/baml':\n        specifier: ^0.52.0\n        version: 0.52.1\n```"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 22:52:23.514000+00:00",
                "content": "interesting - will that make the client be more verbose?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-24 22:53:08.029000+00:00",
                "content": "yes- it's not a stable logging format, but it's our internal logging control and will give you a lot of information in `stderr` about what's happening under the hood"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 22:59:49.144000+00:00",
                "content": "Make sense, thanks. I see this:\n\n```\nbuilt request: Request { method: POST, url: Url { scheme: \"https\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"api.fireworks.ai\")), port: None, path: \"/inference/v1//chat/completions\", query: None, fragment: None }, headers: {\"authorization\": \"Bearer <my_actual_token>\", \"baml-original-url\": \"https://api.fireworks.ai/inference/v1/\", \"content-type\": \"application/json\"} } ```\n\nhmmm, it seems like the correct url"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:00:23.505000+00:00",
                "content": "hmm theres an extra \"/\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:00:31.972000+00:00",
                "content": "seems like we don't fix that in our requestbuilder"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:00:46.597000+00:00",
                "content": "not sure why it would be inconsistent. There's a /inference/v1//"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:01:01.388000+00:00",
                "content": "ahh your baseurl has a dangling \"/\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:01:08.424000+00:00",
                "content": "we'll make it more resilient to this kind of issue"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 23:01:08.595000+00:00",
                "content": "Thats true - but the raw CURL in the playground has the same and works:\n\n\"curl -X POST 'https://api.fireworks.ai/inference/v1//chat/completions'\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:01:37.331000+00:00",
                "content": "hmm, does it work in your code if you remove the dangling \"/\"?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:01:50.656000+00:00",
                "content": "does that curl request run? if you ru it in the terminal"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:03:14.797000+00:00",
                "content": "the playground proxies the request through a localhost vscode server (to get rid of CORS issues), so i think it may be removing the extra \"/\", will take a look. My guess is that curl request doesn't actually execute if you copy it in the terminal"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 23:04:14.051000+00:00",
                "content": "oh interesting - that makes sense."
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 23:04:21.134000+00:00",
                "content": "It does work when removing the trailing slash üòÑ"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 23:09:07.711000+00:00",
                "content": "thanks again everyone!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-24 23:10:04.638000+00:00",
                "content": "np, will squash this trailing slash bug soon, ty for the patience"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-24 23:14:12.243000+00:00",
                "content": "no worries at all!"
            }
        ]
    },
    {
        "thread_id": 1267478068377817261,
        "thread_name": "custom jinja filters",
        "messages": [
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-29 13:45:20.105000+00:00",
                "content": "Is it possible to add custom Jinja filter functions?\n\nUse case: I'm implementing few-shot learning by adding some example outputs, and would like to control their formatting like so: (\"{{ example_output|format_example_output_in_json }}\". I discovered that in this specific instance,  pretty-printed json formatting can be achieved as \"{{ sample_resume_out|tojson(true) }}\", but I'm sure that at some point, I'll have a need for custom filters."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:33:48.341000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:33:48.751000+00:00",
                "content": "This is something we are still debating on. In order to property support this we'd need a lot more complex expressions, so as of now, there is no plans to allow this. \n\nAs you come accross some filters you'd like to add, if you can share them, we'd benefit from understanding them and perhaps theres a good pattern we can see.\n\nThe recommended workaround in case of missing this is to just write any necessary logic in the language of your choice and pass in string paramters to baml."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:34:38.304000+00:00",
                "content": "(I think you can also do a lot of manipulation with template strings, howevery they don't support generics yet, so it may require some duplicate code)"
            },
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-29 16:37:47.251000+00:00",
                "content": "Thanks! Right, doing heavy string lifting in Python works. The downside is just that those changes would no longer show up in the playground, right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 16:38:51.122000+00:00",
                "content": "you would just need to pass in the same strings as paramters to test cases, but yes sadly that custom python code wouldn't run in the playground üò¶ \n\nDo you have examples of some of the heavy string lifting you are doing?"
            },
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-29 16:43:37.407000+00:00",
                "content": "Not yet! So far our only use case is the conversion of objects to json, which the in-built filter tojson handles well üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 16:44:03.791000+00:00",
                "content": "you may also want to use the `pprint` filter fyi!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 16:44:40.379000+00:00",
                "content": "the complete list we support is here:\nhttps://docs.rs/minijinja/latest/minijinja/filters/index.html#functions\n\nWe'll be adding these to our docs soon!"
            }
        ]
    },
    {
        "thread_id": 1267478117669142570,
        "thread_name": "Variables in functions",
        "messages": [
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-29 13:45:31.857000+00:00",
                "content": "Another question: Is it possible to add variables?\n\nUse case: I'm implementing few-shot learning, and most of my examples are constructed from the same few structured documents. The documents have a type quite similar to the resume example:\n\n```\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n```\n\nI'd like to create a couple `Resume` instances and re-use them across all my BAML prompts. The `template_strings` mechanism suggested in the docs wouldn't be sufficient for this, because each prompt needs to see the Resume instance as a structured object, and not a string."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:30:53.973000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:30:54.476000+00:00",
                "content": "not yet! However, you can achieve this by adding additional paramters to functison\n\n```\nfunction FooBar(param:type, resumes: Resume[]) {\n  prompt #\" {{ resumes|tojson(true) }} \"#\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:31:08.141000+00:00",
                "content": "would that work as a temporary hold?"
            },
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-29 16:35:17.979000+00:00",
                "content": "that's indeed what I had in mind for using BAML functions. However, without variable support, the crucial limitation would be that you couldn't use the same set of examples in both the playground and in production: \n\nEither, you add the few-shot resume instances to a test case for FooBar and can see them in the playground. Then, however, there's no way of re-using the few-shot resume instances when invoking FooBar from Python.\n\nOr, you add the few-shot resume instances to your Python code. But then, there's no way to see the full prompt (with the few-shot instances) in the playground.\n\nA third alternative would be to directly write the few-shot instances inside the FooBar prompt. But then, the examples would no longer be typed.\n\nVariables would solve all of this. Do you agree / do the things I say make sense?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 16:37:48.272000+00:00",
                "content": "oh you're 100% right, we're working on variable support atm üôÇ Just will take a bit to get there.\n\nAs a part of adding in asserts (see <#1265356689796890820> ), we are going to be updating our compiler so that adding things like variables and type aliases will be trivial. \n\nI would expect this to land by end of august!"
            },
            {
                "author": "robert_hoenig",
                "timestamp": "2024-07-29 16:44:41.829000+00:00",
                "content": "Awesome, sounds great! I guess this particular feature is the most important one for us right now -- for the other's I've been posting, we've found workarounds üôÇ"
            }
        ]
    },
    {
        "thread_id": 1267508995468427264,
        "thread_name": "BAML client side",
        "messages": [
            {
                "author": "bsachs10",
                "timestamp": "2024-07-29 15:48:13.698000+00:00",
                "content": "Question: Is it possible to run BAML functions client side in NextJS? I know the security concerns about using API keys in browser, but I can mitigate that with a proxy server that uses session-specific authentication rather than API keys. But when I try to run any BAML function from client code (or simply import `b` from `@/baml_client`), I get an immediate error like this one:\n\n```\n./node_modules/@boundaryml/baml/async_context_vars.js:5:1\nModule not found: Can't resolve 'async_hooks'\n\nhttps://nextjs.org/docs/messages/module-not-found\n\nImport trace for requested module:\n./node_modules/@boundaryml/baml/index.js\n./src/baml_client/async_client.ts\n./src/baml_client/index.ts\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:52:10.689000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:52:10.885000+00:00",
                "content": "Interesting! We currently don't support this due to security concerns as you mentioned. \n\nThe `async_hooks` is how we handle observability and such in an automated way. That said, techncially it can run (we use a Wasm version of BAML to execute BAML functions on a browser on promptfiddle.com).\n\nIf this becomes important, we can look into seeing how much work it is to create a generator for a browser (something like `typescript/browser`)."
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-29 15:54:57.489000+00:00",
                "content": "Useful for me because Netlfiy is our platform of choice and its server actions have relatively short timeout periods compared to Vercel. We also use a proxy server for our own observability, so there's just no need to have streams be repeated through server actions. But may not be important for others!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:55:15.441000+00:00",
                "content": "Alternatively, if we hosted your baml functions as endpoints for you and provided you with a typesafe interface to them, would that be better?"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-29 15:56:11.567000+00:00",
                "content": "Meaning we could run them client side by authenticating sessions with you as the host? An equally good solution (albeit with vendor lock-in)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:56:51.828000+00:00",
                "content": "thanks for sharing! I'll get back with a response after chatting with the team!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:57:19.592000+00:00",
                "content": "and then we should know what we can support easily."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-29 15:58:04.101000+00:00",
                "content": "(The tricky part about browser based is how we need to set up env vars, we do quite a bit of hacking in our current solution)"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-29 15:59:02.952000+00:00",
                "content": "That makes sense. I use a custom registry so I didn't need to use ENV vars in BAML itself"
            }
        ]
    },
    {
        "thread_id": 1267950165088534528,
        "thread_name": "Namespaces",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-30 21:01:16.732000+00:00",
                "content": "Best practices w/ BAML question: Do namespaces exist as a concept in BAML, or is every function/type under different files combined to live under a global shared namespace? For context, I'm implementing another feature with BAML and had a name conflict with a similar type in another file -- then I considered importing it from there / making a separate file with shared types to organize them, but I could see that getting messy as I add more things in the future"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-30 21:01:56.291000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-30 21:01:56.586000+00:00",
                "content": "we are drafting a spec for a module system! but for now we have a global namespace!"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-30 21:02:23.019000+00:00",
                "content": "Nice!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-30 21:02:31.276000+00:00",
                "content": "we're thinking through it and will share a draft with you shortly. Its great to see you ahve this problem tho üòâ"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-30 21:03:01.826000+00:00",
                "content": "Haha appreciate it"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-30 21:03:26.645000+00:00",
                "content": "Also talked to Mihir this morning, sounds like he really liked chatting with you -- super excited to see you at Michigan in the fall!"
            }
        ]
    },
    {
        "thread_id": 1268281923017506910,
        "thread_name": "dynamic field",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 18:59:33.986000+00:00",
                "content": "Typing/output schema question: Is it possible to have a type with a field that I conditionally choose to turn off for some function calls? \n\nI have a BAML type I want to use throughout my code, but I only want one BAML function to try to fill in a particular field, not the other, so I don't want that field to show up in `{{ ctx.output_format }}`  for the first function at all. Is the only way to do this to define a completely new type with one fewer field?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 19:08:40.295000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 19:08:40.558000+00:00",
                "content": "you can make that field be dynamic with https://docs.boundaryml.com/docs/calling-baml/dynamic-types\n\nWe don't currently have a way to dynamically turn on or off each field in a BAML file unfortunately. This will need to happen at runtime."
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 19:09:39.571000+00:00",
                "content": "Gotcha, that should be fine"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 19:10:03.837000+00:00",
                "content": "Is this limited to creating fields, or would I be able to delete fields with this too?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 19:11:37.312000+00:00",
                "content": "Basically what I'd want to be able to do is have:\n\n```\nclass LineItem {\n  description string?\n  quantity int?\n  unitPrice float?\n  totalAmount float?\n  name string?\n  currency string?\n  metadata LineItemMetadata?\n  @@dynamic\n}\n```\n\nand then do something like\n\n```\ntb.LineItem.deleteProperty('metadata')\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 19:28:37.197000+00:00",
                "content": "sadly you can't delete stuff dynamically"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 19:28:50.804000+00:00",
                "content": "as that would break the type-safety types we have in your `LineItem` type in typescript"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 19:29:02.739000+00:00",
                "content": "only deletion is allowed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 19:30:06.936000+00:00",
                "content": "that said, it sounds like what you want is some sort of generic composition.\n\nWhere we could know that the types for these two functions are similar but have some key differences. I wonder if unions and more complex type-algebra would make this easier. (The caveat being naming types would harder üôÇ )"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 19:41:14.984000+00:00",
                "content": "Gotcha, that makes sense"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 19:51:29.521000+00:00",
                "content": "Hmm, I'm not sure I see exactly what you mean with union types -- without an actual inheritance/composition system, the best thing I can think of is something like this (with more names xD), which requires the non- \"toggleable\" fields to be another level. deeper:\n\n```\nclass LineItemData {\n  description string?\n  quantity int?\n  unitPrice float?\n  totalAmount float?\n  name string?\n  currency string?\n}\n\nclass LineItemWithoutMetadata {\n  data LineItemData\n}\n\nclass LineItemWithMetadata {\n  data LineItemData\n  metadata LineItemMetadata?\n}\n```\n\nI guess there's nothing strictly incorrect with this approach,  but I'd rather not privilege the `metadata` field in terms of nesting depth compared to others when I'm using the BAML type in my other business logic, or write a mapper to fix the nesting depth because that partially defeats the purpose of having a nice BAML type at all"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 20:14:43.763000+00:00",
                "content": "I agree that this isn't ideal, the dynamic types approach is better.\n\nI should have implemented a code sample with it:\n\n```rust\nclass LineItem {\n  description string?\n  quantity int?\n  unitPrice float?\n  totalAmount float?\n  name string?\n  currency string?\n}\n\ntype AnotherType = LineItem & {\n  metadata LineItemMetadata?\n}\n```\n\nThen you could use `AnotherType` in the function that needs it"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-31 20:20:12.816000+00:00",
                "content": "Ohh I see, that's better for sure -- FWIW I probably would've figured out `type` and `&`  are supported in BAML had I actually tried making the type myself, but I didn't think BAML supported defining a new type with `type` or using `&`  from reading the docs (I was looking at `|` instead)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 20:37:02.827000+00:00",
                "content": "they actually aren't supported yet üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 20:37:22.490000+00:00",
                "content": "we are gonna be working on that after we are able ship the assert feature"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:38:45.515000+00:00",
                "content": "Circling back to this, just wanted to bump feedback that this is actually a pain point I've hit a second distinct time:\n\nFor the OCR pipeline I'm building, I conditionally want to sometimes parse out the invoice line items in a BAML call and sometimes not, depending on the length of the invoice. The cleanest workaround I can find right now now is defining a new BAML `InvoiceWithoutLineItems` type, and then writing Typescript mappers that convert `InvoiceWithoutLineItems` and `LineItem[]` into the `Invoice` type I'm currently using in business logic"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:39:39.979000+00:00",
                "content": "This *works* but it's a lot more cumbersome than the capability to dynamically turn off a field in a BAML type for one BAML function call would be"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:40:52.896000+00:00",
                "content": "^which importantly wouldn't have to break type safety, since the only thing that would functionally change is the prompt sent to the LLM -- the return type could be the same type as the predefined one, just with the disabled field always set to null"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 22:41:52.718000+00:00",
                "content": "thanks for the feedback, we'll see what solutions we can give you and come back to you"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:42:29.755000+00:00",
                "content": "Sounds good, thanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-02 07:39:11.095000+00:00",
                "content": "we support the `@skip` keyword for enums already. I think we can support `@skip` for optional fields. that would likely solve this!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-02 07:39:21.978000+00:00",
                "content": "this is a good point on how its annoying"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-02 07:39:30.125000+00:00",
                "content": "and we should be able to solve this for you üôÇ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 14:41:17.989000+00:00",
                "content": "<@99252724855496704> you mean re adding overrides? üòÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-02 15:53:47.776000+00:00",
                "content": "Oh yea. We‚Äôd also need that capability as well. I guess both of those together would solve it"
            }
        ]
    },
    {
        "thread_id": 1268378599560577127,
        "thread_name": "aws-bedrock ‚Äî BAML Documentation",
        "messages": [
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:23:43.470000+00:00",
                "content": "For the aws-bedrock provider, is there a way to have it use sso, or my authed aws client? \nhttps://docs.boundaryml.com/docs/snippets/clients/providers/aws-bedrock"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:24:30.714000+00:00",
                "content": ""
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:24:30.852000+00:00",
                "content": "client<llm> BedrockClaude {\n  provider aws-bedrock\n  options {\n    AWS_PROFILE \"mgmt\"\n    model_id \"anthropic.claude-3-5-sonnet-20240620-v1:0\n    temperature 0.1\n  }\n}\nis what I was trying."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-01 01:26:07.208000+00:00",
                "content": "yes! I think you can do this using env variables! \n\n<@711679663746842796> can you double check if we're missing docs here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-01 01:29:13.287000+00:00",
                "content": "could you share how your auth'ed client is defined?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-01 01:30:04.651000+00:00",
                "content": "The library we use def supports sso (https://docs.rs/aws-config/latest/aws_config/index.html)\n\nbut just a mater of making sure we have plumbed it through correctly"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:31:43.522000+00:00",
                "content": "I get this error:\nUnspecified error code: 2\n\"AWS_REGION, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY must be set in the environment\""
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:32:40.135000+00:00",
                "content": "I have my config file for aws setup with profile \"name\" for various environments. I'm not sure if there's a pass through. Using like boto3, I can specify the profile name, and it will pop sso if needed, or use the cached credentials."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-01 01:33:10.293000+00:00",
                "content": "is this in the playground?\n\n(I think as of now, our playground only supports that method)\n\nOur playground is implement in WebAssembly (AKA doesn't have access to the file system - only a web browser).\n\nWe added SSO for Vertex, but i think we should be able to augment this for Bedrock as well."
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:33:29.402000+00:00",
                "content": "I can't use permanent credentials in my environment, so I either get a 1 hour key to set, or SSO."
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:33:34.955000+00:00",
                "content": "Oh I could probably use it outside the playground."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-01 01:35:48.818000+00:00",
                "content": "i think so! I think the python / typescript bindings should \"just work\" but if not let me know!\n\nWe were waiting to see how long until someone has an issue like this in the playground, and it seems like its come up üôÇ We'll report the issue and get back with an ETA of how long it will take to solve for the playground shortly."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-01 01:36:31.615000+00:00",
                "content": "fyi, we load default files and env variables that AWS supports\n\nhttps://docs.aws.amazon.com/sdkref/latest/guide/file-location.html\nhttps://docs.aws.amazon.com/sdkref/latest/guide/environment-variables.html\n\nSo. as long some of the default ones are set, i think SSO auth should work."
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-01 01:37:16.062000+00:00",
                "content": "I did get it to work now restarting the playground w/ the temp creds. I'll try w/ the SSO later. \n\nThanks for the response!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-01 01:46:23.512000+00:00",
                "content": "howdy! vaibhav‚Äôs response is on the nose- the generated code will work just fine with `aws sso login` but the playground will need a long-lived access token"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-01 01:49:45.615000+00:00",
                "content": "we can provide instructions for how to provision a narrowly scoped token for playground use, if you think those would help- otherwise we‚Äôll look into how we can make things work as pong as you‚Äôre signed into sso"
            }
        ]
    },
    {
        "thread_id": 1270137376949276724,
        "thread_name": "Prompt engineering for true/false questions",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 21:52:28.676000+00:00",
                "content": "Question about prompting for a true/false response: What's the best way to ask an LLM to answer a question as true or false and have parsing succeed? I did some testing, and found that if I try to use chain-of-thought with such a simple output schema, response coercion fails: https://www.promptfiddle.com/New-Project-GSO_w"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 21:52:56.053000+00:00",
                "content": ""
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 21:52:56.660000+00:00",
                "content": "The use case is I want to have a very simple function that determines if a PDF is an invoice or not -- curious what the limitations on response coercion are"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 21:54:07.626000+00:00",
                "content": "Prompt engineering for true/false questions"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 22:03:06.829000+00:00",
                "content": "ah it seems like our coercion isnt powerful enough in this case. If you need COT and a boolean just wrap it around an object for now, sorry we will create an issue for this"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 22:03:55.107000+00:00",
                "content": "https://github.com/BoundaryML/baml/issues/860"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 22:04:58.050000+00:00",
                "content": "Gotcha makes sense, nw -- I don't need COT right now, was just testing the limits of coercion\n\nMy prompt right now is \"Return true if and only if the following PDF is an invoice.\", but I could see the LLM saying something like \"not true\" in response, so i was testing out different prompts and different distances between `bool` and the LLM's response"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 22:05:18.893000+00:00",
                "content": "https://www.promptfiddle.com/New-Project-8WL2u"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 22:05:26.003000+00:00",
                "content": "This is another way to do this"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 22:06:04.095000+00:00",
                "content": "You can swap `Yes` or `No` to `true` and `false` respectively, but atm you'd still have to swap it to a boolean programtically"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 22:06:18.322000+00:00",
                "content": "Gotcha I see"
            }
        ]
    },
    {
        "thread_id": 1270157083244757045,
        "thread_name": "Maybe missing something super obvious:",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 23:10:47.023000+00:00",
                "content": "Maybe missing something super obvious: for tests, is there a way to provide what I expect the output should be? \n\nI have some inputs that I want the function to return `true` for, but I don't see a place for me to say I `expect` `true` as the output"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:11:35.432000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:11:35.816000+00:00",
                "content": "we're actively building this atm! check out <#1265356689796890820>"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 23:13:50.248000+00:00",
                "content": "Hmm, that looks interesting but don't think I'm talking about the same thing -- I mean the ability to write something like:\n\n```\ntest SimilarVendorName1 {\n  functions [VendorIsCorrect]\n  args {\n    existingVendor {\n      name \"Epoxy Tech\"\n    }\n    invoiceVendor {\n      name \"EpoxyTech\"\n    }\n  }\n  expect true\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:19:01.382000+00:00",
                "content": "ah yes sorry üôÇ\n\nthe field validation concept will also be applicable to tests shortly after their release so you will be able to write asserts on the test as well."
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-05 23:30:04.149000+00:00",
                "content": "Gotcha ok"
            }
        ]
    },
    {
        "thread_id": 1270157177943756851,
        "thread_name": "BAML_LOG env vars",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:11:09.601000+00:00",
                "content": "If i removed the `BAML_LOG` environment variable, does this mean baml isn't logging anything anymore?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:12:20.889000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:12:21.174000+00:00",
                "content": "no, it will still log WARN / ERROR by default. do you want no logging?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:12:47.896000+00:00",
                "content": "yes"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:12:56.048000+00:00",
                "content": "the boundary log is too large and it's crashing my application"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:14:35.436000+00:00",
                "content": "`BAML_LOG=error`\nShould do the trick.\n\nAlso, I'll patch this in tonight so you can put in `disabled` as well"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:14:48.627000+00:00",
                "content": "hmm well now i'm thinking there's a different issue\nhttps://app.boundaryml.com/dashboard/projects/proj_4d5d8c28-b9df-435e-8b6e-48f2566cadbb/drilldown?start_time=2024-07-22T23%3A14%3A00.731Z&end_time=2024-08-05T23%3A14%3A01.606Z&eid=74b44362-7ba4-415e-b732-e360c244ee74&s_eid=17aae156-569e-471c-95a9-3eb6c35f2010&test=false&onlyRootEvents=true"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:14:54.388000+00:00",
                "content": "or we'll just truncate super long logs"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:15:10.268000+00:00",
                "content": "ya so boundary error log is really long, but also i'm not quite sure what the error is"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:16:03.927000+00:00",
                "content": "theres a missing field `citations`. We're going to truncate these logs"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-05 23:16:15.235000+00:00",
                "content": "lol vscode crashed when i opened that file as well"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:16:43.758000+00:00",
                "content": "for the client?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:20:39.723000+00:00",
                "content": "in the output"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:21:00.092000+00:00",
                "content": "that's not a field..."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:21:24.606000+00:00",
                "content": "does your schema have a field called \"citations\"?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:21:31.913000+00:00",
                "content": "the output"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:21:59.884000+00:00",
                "content": "nope"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:23:22.074000+00:00",
                "content": "it also doesn't fail for the previous invocations of the function... just this one specific call"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:27:33.226000+00:00",
                "content": "do you use vertex or gemini api?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:28:02.973000+00:00",
                "content": "gemini api"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:28:31.720000+00:00",
                "content": "ok we figured it out, we'll patch it (and sorry for this long-ass error, it makes no sense to render the whole input on these)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:28:44.527000+00:00",
                "content": "ok, is there something I can do on my end to fix it?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:28:46.207000+00:00",
                "content": "this is urgent"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:29:09.079000+00:00",
                "content": "it's an internal failure of us not serializing the response. I can patch it now and release, but will take 25min"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:29:18.872000+00:00",
                "content": "ok, mind pinging me when it's available?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:29:24.106000+00:00",
                "content": "yeah"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-05 23:29:27.421000+00:00",
                "content": "appreciate you"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:53:45.213000+00:00",
                "content": "the release is kicked off. ETA is like 25min from now https://github.com/BoundaryML/baml/actions/runs/10257711404\nill follow this release with another one to not print out the whole prompt on those errors (and also fix the dashboard not marking that as an error)."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-06 00:11:49.663000+00:00",
                "content": "it's done: 0.53.0"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-06 00:12:46.500000+00:00",
                "content": "the bug is that gemini api docs are wrong so we expected a field to be there but it wasn't. LMK if it's still failing after this"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 00:26:26.293000+00:00",
                "content": "classic gemini"
            }
        ]
    },
    {
        "thread_id": 1270167012198453259,
        "thread_name": "multiple baml_src",
        "messages": [
            {
                "author": "deoxykev",
                "timestamp": "2024-08-05 23:50:14.270000+00:00",
                "content": "would it be reasonable to have multiple baml_src directories in a repo for organization purposes?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:51:54.957000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:51:55.261000+00:00",
                "content": "the problem is that you end up with multiple baml_clients. If you are ok with each baml_client generated being for some subset of all your prompts I don't see an issue with that.\n\nif not, perhaps we can add a flag to our generator to point to more than 1 BAML_SRC directory to compile all baml files"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-08-05 23:52:23.007000+00:00",
                "content": "multiple baml clients should be fine, as long as they can be imported separately"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-05 23:56:00.005000+00:00",
                "content": "yeah they totally can be! each baml_src can just have a different `output_dir` configuration in the `generator` block you declare. We'll add an integration test on our end to ensure this kind of setup works well"
            },
            {
                "author": "deoxykev",
                "timestamp": "2024-08-05 23:56:18.886000+00:00",
                "content": "i'll play around with it! thank you"
            }
        ]
    },
    {
        "thread_id": 1270424034944749638,
        "thread_name": "Exception Handling",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 16:51:33.268000+00:00",
                "content": "Is there a place in the docs where I can see which exceptions to expect from a baml call? Since I have my baml client in the worker, i got an error which automatically retried the worker when ideally i would've detected it were of type `ExceptionTypeA` and instead of retrying, i would do something differently"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 16:52:16.941000+00:00",
                "content": ""
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 16:52:17.152000+00:00",
                "content": "Looks like it's just of type `BamlError`?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-06 16:58:53.636000+00:00",
                "content": "Unfortunately we don't expose more nuance about error causes to you, but this is something we've talked about! I've gone ahead and filed https://github.com/BoundaryML/baml/issues/866"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 17:37:09.817000+00:00",
                "content": "Is there something I could use in my baml retry policies? this way i don't need to worry about the client exposing something"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 17:51:30.931000+00:00",
                "content": "What do you mean? by in the retry policies?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 17:59:42.806000+00:00",
                "content": "I have a baml function specific retry policy, then i have my celery worker retry policy. A few months back, when I first onboarded to baml, there was a way to configure special retry logic based on the error codes from the LLM client (iirc)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-06 18:00:04.761000+00:00",
                "content": "wondering whether that's still something i can work with, though maybe in this case i still need an application level exception handler"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 18:00:33.914000+00:00",
                "content": "Ah yes, even in that case i think you'd need application level handler. We don't currently have a way to do that in the current retry policies"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 18:04:15.434000+00:00",
                "content": "we're doing planning today, so i'll get you a timeline on exceptions!"
            }
        ]
    },
    {
        "thread_id": 1270451229583081615,
        "thread_name": "Dyanmically settings API keys",
        "messages": [
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 18:39:36.975000+00:00",
                "content": "Hey. I am trying to use BAML inside an AWS Lambda function. How do I set the api_key from AWS Secrets Manager i.e modify this part?\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nCurrently, my Lambda's Python code does retrieve OPENAI_SECRET_KEY from AWS Secrets Manager. I am using BAML 0.44.0 with Python 3.10."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 19:49:56.172000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 19:49:56.763000+00:00",
                "content": "hi <@929667691525050370> ! Yes this is possible, (you'll need to update to the latest version of BAML 0.53)\n\nYou can then modify this code in ptyhon via:\n\n```python\nfrom baml_py import ClientRegistry\nfrom baml_client import b\n\ncr = ClientRegistry()\n    # Creates a new client\ncr.add_llm_client(name='GPT4o', provider='openai', options={\n    \"model\": \"gpt-4o\",\n    \"api_key\": \"SOME KEY FROM AWS\"\n})\n\n\ndef foo():\n  res = b.MyFunction(..., { \"client_registry\": cr })\n```\n\nsee https://docs.boundaryml.com/docs/calling-baml/client-registry"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 19:56:36.511000+00:00",
                "content": "Thanks <@99252724855496704> . My org uses Python 3.10 and the latest version of BAML supported with that is 0.44.0 so I won't be able to upgrade to 0.53. Is there any other work around?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 19:57:38.504000+00:00",
                "content": "Oh thats odd, we hsould be supporting 0.53 with 3.8+ I'll double if thats the case. If it isn't, we'll ship and update by end of day."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 19:58:03.293000+00:00",
                "content": "(We have local tests for 3.8+ that we use üôÇ so its just an update to the release script if anything)"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 19:58:23.658000+00:00",
                "content": "Great if you could confirm. I was initially using 0.52.x but downgraded to be compatible with Python 3.10."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-06 19:58:23.853000+00:00",
                "content": "Alternatively, https://docs.aws.amazon.com/secretsmanager/latest/userguide/retrieving-secrets_lambda.html is also an option!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 19:59:08.018000+00:00",
                "content": "can you confirm if you aren't able to pip-install? \n\nhttps://pypi.org/project/baml-py/0.53.0/#files"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 20:09:32.529000+00:00",
                "content": "I am just working on something. Will revert."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:17:14.242000+00:00",
                "content": "Back now. So I upgraded the BAML version to 0.53.0 and also the VS Code Extension version. However, I am faced with this error:\nfunctions/llm_model_execution/baml_client/types.py:16: in <module>\n    import baml_py\nE   ModuleNotFoundError: No module named 'baml_py'"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:17:33.879000+00:00",
                "content": "<@201399017161097216> can you help here?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-06 21:18:21.239000+00:00",
                "content": "how do you setup your python environment?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-06 21:18:28.823000+00:00",
                "content": "and how do you run it?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-06 21:26:10.148000+00:00",
                "content": "I just tested our example https://github.com/BoundaryML/baml-examples/tree/main/python-fastapi-starter with 0.53.0 and it seems to be running well -- perhaps it's the python environment?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:29:14.007000+00:00",
                "content": "I am using Python 3.10 and BAML 0.53.0 in VS Code."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:30:07.878000+00:00",
                "content": "We can hop on office hours very quick and screenshare (that may be faster!)"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:31:21.025000+00:00",
                "content": "I am using makefile to run some pytest tests. So when I do make test, this is the error I get."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:31:42.600000+00:00",
                "content": "Also, this is my organization's code, so I won't be able to share my screen."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:32:29.432000+00:00",
                "content": "Ah no worries.\nAre you able to run the tests w/o make?\n\ne.g. directly run `baml-cli --version`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:32:48.913000+00:00",
                "content": "also can you run the following:\n\n```\npip show baml-py\n```"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:49:22.655000+00:00",
                "content": "`baml-runtime 0.53.0`"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:49:27.781000+00:00",
                "content": "is what the version gave."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:50:10.313000+00:00",
                "content": "`pip3 show baml-py`\n`Name: baml-py`\n`Version: 0.53.0`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:51:52.068000+00:00",
                "content": "interesting, so you def have it.\n\n```\npython -c 'import baml_py; print(baml_py)'\n```\n\ndoes that work for you?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:52:46.714000+00:00",
                "content": "`<module 'baml_py' from '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/baml_py/__init__.py'>`"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:52:54.146000+00:00",
                "content": "Yes it does."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:53:07.372000+00:00",
                "content": "my gut says that somehow the mechanism your make pytest is using is importing the wrong python env (leading to wrong baml_py)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:53:27.787000+00:00",
                "content": "one last thing to try is to regen the baml client.\n\n```\nbaml-cli generate\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:53:48.642000+00:00",
                "content": "then retry running pytest"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 21:55:15.191000+00:00",
                "content": "we can also do the following:\n\n```\nimport baml_py\nprint(baml_py)\n```\n\nat the very top of your pytest file (before any other imports)"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-06 21:57:34.663000+00:00",
                "content": "Generating the client again also gave the same error."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 22:09:41.684000+00:00",
                "content": "wait, you mean running:\n`baml-cli generate` causes the same bug? or that after running with the new generated client, you have that issue?\n\ncan you run the `pip show baml-cli` command as a part of your make file?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-06 22:10:04.686000+00:00",
                "content": "and also `python -c 'import baml_py; print(baml_py)'` in your make file after pip show"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 16:44:13.020000+00:00",
                "content": "Hi Vaibhav. This is the result I got by running the commands in the makefile above. The client has been generated:\n`make check_baml`\n`pip3 show baml-cli`\n`WARNING: Package(s) not found: baml-cli`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 16:44:48.821000+00:00",
                "content": "ah! It looks like you are using a different python env here. You may need to do `pip3 install baml-py`"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 16:45:03.024000+00:00",
                "content": "Yes, I have done that."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 16:59:11.610000+00:00",
                "content": "A follow up question I had was that for BAML 0.53.0, I notice this as the arhictecture:\n`baml_py-0.53.0-cp38-abi3-manylinux_2_28_aarch64.whl`. Is there a reason why you switched to this?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-07 17:00:19.690000+00:00",
                "content": "whats your OS? Linux arm64?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:01:12.822000+00:00",
                "content": "macOS 14.4.1 (23E224)"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:02:22.246000+00:00",
                "content": "So the last version that was working for me was BAML 0.44.0 which uses this:\n`baml_py-0.44.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl `"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:02:56.664000+00:00",
                "content": "I realize that I may need to update my docker image. But wanted to confirm if there was a specific reason to switch to `baml_py-0.53.0-cp38-abi3-manylinux_2_28_aarch64.whl`?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-07 17:03:11.857000+00:00",
                "content": "which docker image are you using as well?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 17:04:01.993000+00:00",
                "content": "oh i see. It appears you have glibc version < 2.28!\n\nCan you run:\n\n```\nldd --version\n```"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:11:28.569000+00:00",
                "content": "Yes, we use a version less than 2.28."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 17:11:54.165000+00:00",
                "content": "perfect! We found the issue. We should be able to release a version that updates this!\n\nCan you share what version you are?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:14:06.442000+00:00",
                "content": "Just verifying and will get back to you. Was there a specific reason to switch to 2.28?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:14:34.413000+00:00",
                "content": "Is it possible to stay at 2.17 which was the version used with BAML 0.44.0?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 17:14:58.088000+00:00",
                "content": "Its just a build script thing things üôÇ we should be able to support 2.17 <@711679663746842796> can you take this on?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-07 17:18:47.486000+00:00",
                "content": "<@929667691525050370> thanks for your patience, and for highlighting the discrepancy!"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 17:20:04.284000+00:00",
                "content": "Thank you for your support! Appreciate it."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:45:07.111000+00:00",
                "content": "Hi <@929667691525050370> , is there any chance you'd be able to use glibc > 2.28+? It looks like we added a core indirect dependency (a low level web sockets library in RUST), that causes this issue."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 21:48:12.875000+00:00",
                "content": "Let me check and get back to you?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 22:25:15.145000+00:00",
                "content": "Is using glibc 2.26 an option?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-07 22:25:47.285000+00:00",
                "content": "let me give you a wheel and see if it works for you"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-07 22:27:05.743000+00:00",
                "content": "ok- can you download this and try to `pip install ./baml_py-0.53.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl` in the image?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-07 22:33:05.539000+00:00",
                "content": "alt. if you can tell us what base image you're using, i can also take a look at that"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 22:34:18.506000+00:00",
                "content": "Ok. Let me get back to you."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 22:35:55.011000+00:00",
                "content": "This is our base image: `public.ecr.aws/lambda/python:3.10-arm64`"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 22:36:43.481000+00:00",
                "content": "I tried doing this and got this error:\n`ERROR: baml_py-0.53.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl is not a supported wheel on this platform.`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-07 22:40:04.755000+00:00",
                "content": "Ah, that makes a lot of sense. Will dig into this and let you know what I come up with."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 22:40:47.396000+00:00",
                "content": "Appreciate it, thanks!"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-07 23:21:32.314000+00:00",
                "content": "I am trying to change my base image to 86x64 and will see if that works."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-07 23:21:37.285000+00:00",
                "content": "OK, let's try this:\n\n- download the attached zip and extract it\n- in the directory where you extract it, run `docker build -t baml-py-lambda .` \n- then run `docker run -it baml-py-lambda` and inside resulting bash shell, `BAML_LOG=debug OPENAI_API_KEY=sk..... python app.py`\n\nAnd let me know which step fails for you and how. It works for me, but it sounds like it might fail for you and I'm confused about what would cause that."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-07 23:22:05.941000+00:00",
                "content": "> I am trying to change my base image to 86x64 and will see if that works.\nif you're on an arm mac, i don't think that will work, sadly"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-08 22:29:00.133000+00:00",
                "content": "Hi everyone. Just wanted to give an update that I changed the base image arch to x86_64 and its now working."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-08 22:29:20.559000+00:00",
                "content": "oh, awesome!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-08 22:29:47.087000+00:00",
                "content": "awesome news -- sorry you had to spent so long fixing this. We'll eventually run integ tests using Amazon Linux 2 on ARM instances and make sure that works for future users"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-08 22:30:39.330000+00:00",
                "content": "No worries - thanks for the support!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-08 22:30:45.694000+00:00",
                "content": "^ we've also merged a change that brings our compatibility level down to glibc 2.24, which should work with the image you mentioned (it did on my machine), so that'll also be going out in the next release"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-08 22:43:23.919000+00:00",
                "content": "Thanks again. A question I had was that can I make my BAML object returned JSON serializable?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-08 22:44:30.056000+00:00",
                "content": "if you're in python, all objects are pydantic models. Which has good JSON compat.\n\nSee https://docs.pydantic.dev/latest/concepts/serialization/"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-08 22:45:03.381000+00:00",
                "content": "Thanks! I'll have a look."
            }
        ]
    },
    {
        "thread_id": 1270842861730009109,
        "thread_name": "Benchmark thread",
        "messages": [
            {
                "author": "kdub03",
                "timestamp": "2024-08-07 20:35:49.355000+00:00",
                "content": "Is the \"Strict\" mode comparing the new structured outputs from OpenAI? Is that function calling only? Is there a comparison with structured outputs w/o function calling, since I think that's closer aligned to what BAML is doing?\nhttps://discord.com/channels/1119368998161752075/1119375433666920530/1270775695420817489"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:46:26.042000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:46:26.486000+00:00",
                "content": "We'll be running that soon (the benchmark doesn't currently support comparing it w/o function calling). However, you can see what structured does vs non-structured on this post:\n\nSee JSON mode vs Function calling here.\nhttps://www.boundaryml.com/blog/schema-aligned-parsing#json-mode\nhttps://www.boundaryml.com/blog/schema-aligned-parsing#function-calling\n\nTLDR, my gut says that function calling will likely perform better than JSON mode since json more will almsot FORCE the model to produce an output."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:47:18.636000+00:00",
                "content": "JSON mode and function calling are almost exactly the same thing, function calling just has a special way to trigger when JSON mode starts."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:47:45.780000+00:00",
                "content": "hope that helps!"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-07 20:48:39.126000+00:00",
                "content": "so basically hard forcing the response format seemingly has negative impact on the response?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:48:47.733000+00:00",
                "content": "mhm"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:49:03.836000+00:00",
                "content": "glad to explain why over on office hours if you'd like!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:49:14.051000+00:00",
                "content": "(we'll have a more detailed post on thsi later this week or early next!)"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-07 20:50:22.920000+00:00",
                "content": "I think that makes sense. I'd love to chat though. Do you do office hours every day? I'm working mapping extraction functions to openapi specs, and leaning towards baml, or part of baml as part of the implementation."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:50:54.992000+00:00",
                "content": "We're usually on most days!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 20:51:04.374000+00:00",
                "content": "online now if you'd like!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:34:24.224000+00:00",
                "content": "fyi here's my calendly!\n\nhttps://calendly.com/boundary-founders/connect-45"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:34:32.073000+00:00",
                "content": "looking forward to chatting tmrw"
            }
        ]
    },
    {
        "thread_id": 1270859347500597288,
        "thread_name": "what does max toakesn do",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 21:41:19.869000+00:00",
                "content": "maybe a naive question: If I set the max_tokens in the client, does the model take this into account when generating a response? IN other words, if i put 150 token maximum, will the model do it's best to constrain the output to 150 tokens or do I also need to prompt it for that"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:43:01.445000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:43:01.894000+00:00",
                "content": "nope! It won't take this into account. its purely a limitor on the output tokens"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 21:44:04.459000+00:00",
                "content": "any tips for constraining a model's response to a word count limit? \n\nSo far i've just been doing it with naive prompting and then having a naive checker LLM which then says \"too short\" or \"too long\" and regenerates. I guess baml doesn't natively support something like this?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:45:26.487000+00:00",
                "content": "how many words do you want"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 21:45:43.449000+00:00",
                "content": "it's dynamic, i feed it as a parameter ot the function"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:45:48.603000+00:00",
                "content": "you can do `string[]`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:45:55.665000+00:00",
                "content": "and set the description dynamically"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 21:46:02.843000+00:00",
                "content": "that's an interesting idea"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:47:07.994000+00:00",
                "content": "typebuilder to the rescue üòâ \n\nin BAML\n```\nclass Foo {\n  words string[]\n  @@dyanmic\n}\n```\n\nin python:\n```\ntb = TypeBuilder()\ntb.Foo.words.description(f\"at most {count} words\")\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 21:47:31.557000+00:00",
                "content": "hmmm is that really better than just throwing it into the prompt?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 21:47:40.760000+00:00",
                "content": "i think the `string[]` is interesting if that will impact something"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 21:48:14.815000+00:00",
                "content": "as of right now, likely yes, but soon we'll allow variables in descriptions directly in BAML and that will make life better"
            }
        ]
    },
    {
        "thread_id": 1271510365770875005,
        "thread_name": "Hi, I am suddenly getting this error,",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-08-09 16:48:14.720000+00:00",
                "content": "Hi, I am suddenly getting this error, but inconsistently, when testing out a prompt that previously worked, and it only fails some of the time. I am using temperature 0 so I don't think the output should be changing across different runs.\n\nUnspecified error code: 2\nFailed to parse event: Error(\"missing field `type`\", line: 0, column: 0)\n\nCheck the webview network tab for more details. Command Palette -> Open webview developer tools."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-09 17:01:24.951000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-09 17:01:25.508000+00:00",
                "content": "hi <@152300469094580224> if possible, can you open developer tools in vscode:\nhttps://docs.boundaryml.com/docs/get-started/debugging/vscode-playground#tests-failing-to-run\n\nThen you can look in the network call and see what request we are making.\n\nAlso one surefire way to check is the button:\n\n```\nRaw CURL\n```\n\nthen you can try running that in terminal a few times. if that works, then we have some odd error"
            },
            {
                "author": "anish.pi",
                "timestamp": "2024-08-09 17:55:21.735000+00:00",
                "content": "What provider are you using?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-08-09 20:11:12.519000+00:00",
                "content": "Claude 3.5"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-08-09 20:20:51.423000+00:00",
                "content": "ok, it looks like it is an error on anthropic side: {\"client_error\":false,\"code\":529,\"detail\":\"Overloaded\"}"
            }
        ]
    },
    {
        "thread_id": 1272645038513324084,
        "thread_name": "ClientRegistry parameters",
        "messages": [
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 19:57:01.783000+00:00",
                "content": "ClientRegistry parameters"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 19:57:03.014000+00:00",
                "content": "Hi all. I am using the following code in Python to generate a new client:\n```python\ncr.add_llm_client(\nname=\"OpenAIClient\",\nprovider=\"openai\",\noptions={\n\"api_key\": auth,\n\"model\": model_name,\n\"messages\": [{\"role\": \"assistant\"}],\n\"temperature\": temperature,\n\"max_tokens\": max_tokens,\n\"top_p\": top_p,\n\"frequency_penalty\": frequency_penalty,\n\"presence_penalty\": presence_penalty,\n},\n)\n```\n\nThen later on, I am using this client like this:\n```python\nresult = b.my_function(prompt, {\"client_registry\": cr})\n```\nJust wanted to confirm if the role and other parameters will be sent to the GPT call I make?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 19:57:45.357000+00:00",
                "content": "ah, you can, but messages isn't supported!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 19:57:58.611000+00:00",
                "content": "the prompt is assembled from the template string you write in BAML"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 19:59:20.111000+00:00",
                "content": "if you want to tune that, you can actually passs in `_.role('user') ` or any other roles in your prompt directly.\n\nSee for what parameters we support for `options`:\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/openai\n\nSee here for how to add chat roles to the prompt: https://docs.boundaryml.com/docs/snippets/prompt-syntax/roles"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 20:00:32.820000+00:00",
                "content": "Currently, it's this:\n```function my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ input }}\n\n    {{ ctx.output_format}}\n  \"#\n}```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 20:01:16.221000+00:00",
                "content": "if you want to modify the role for that prompt, you can do dsomething liek this:\n\n```\nfunction my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ _.role('system') }}\n    {{ ctx.output_format}}\n\n    {{ _.role('user') }}\n    {{ input }}\n  \"#\n}\n```"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 20:34:56.426000+00:00",
                "content": "The documentation at the end says to refer to OpenAI documentation for other options and that includes temperature etc. But are those currently not supported?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 20:35:56.078000+00:00",
                "content": "Is there a reason why you have put system as the role for output_format and user for input?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 21:33:15.559000+00:00",
                "content": "Also, I noticed that there is a default_role option so if I set that to 'User' that should be same as setting the role to user as in the code above?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 21:57:22.137000+00:00",
                "content": "yea, all other options aside from those we mention as `DO NOT USE` are supported as passthrough paramters."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 22:00:43.058000+00:00",
                "content": "in this example, I used teh output format in system prompt and the user's content as user. \n\nThe reason is that the model is trained to understand instructions in the system prompt, so it allows us to take advantage of that training (at least for the model).\n\nAlso, yes, if you set up the `default_role` as `user` it will make the two below exactly the same:\n\n```\nfunction my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ _.role('user') }}\n    {{ ctx.output_format}}\n \n    {{ input }}\n  \"#\n}\n```\n\n\n```\nfunction my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ ctx.output_format}}\n \n    {{ input }}\n  \"#\n}\n```"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 22:08:04.445000+00:00",
                "content": "Sorry, could you clarify more what you mean by passthrough paramaters? Can I set specific values for these as I have done above?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-12 22:08:31.619000+00:00",
                "content": "yep! what you're doing in your example is completely ok.\n\nI would just remove teh messages paramter, and it should \"just work\""
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-12 22:09:46.559000+00:00",
                "content": "Ok, thanks!"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 19:27:49.918000+00:00",
                "content": "Hey. I am following up regarding parameters to the LLM provider. For a prompt and the parameters below, I am getting a very detailed response from directly calling ChatGPT (role is 'assistant'). However, when I make the same call using BAML, I am getting a very concise response. Could you let me know why that may be? Is there something happening with BAML under the hood?\n\n```python\n        \"model\": \"gpt-4o\",\n        \"default_role\": 'assistant',\n        \"temperature\": 0.7,\n        \"max_tokens\": 512,\n        \"top_p\": 1,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0, ```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 19:29:48.927000+00:00",
                "content": "thats ineteresting. BAML has a quick: \"Raw curl\" button.\n\nAre you saying the Raw CURL is much more verbose than after BAML returns it?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 19:38:18.339000+00:00",
                "content": "(the reason is likely thanks to the BAML parser)\n\nThat parser is able to remove anything but the structured response you want.\n\ne.g. the model includes some prefix text prior to your actually structure, the parser will remove the prefix text.\n\nfor more introspection on BAML, you can set the env var: `BAML_LOG=info` and that will dump out more information for you"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:00:36.849000+00:00",
                "content": "Thanks! I should be seeing the parameters I passed i.e. temperature etc in the raw CURL right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:01:07.025000+00:00",
                "content": "yes! but if you are using the client registry, then you won't see it. Then you'll have to dump out the logs yourself with the env var that i said"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:01:18.309000+00:00",
                "content": "Ok thanks!"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:01:41.013000+00:00",
                "content": "I am using the client registry so let me set the environment variable."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:04:43.579000+00:00",
                "content": "I should set the BAML_LOG variable in the playground?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-13 20:04:56.480000+00:00",
                "content": "no, that one already shows you the raw output in ther esults"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:05:28.623000+00:00",
                "content": "just in your shell environment  (wherever you're running your python code)"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:06:00.060000+00:00",
                "content": "Ok!"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:07:24.759000+00:00",
                "content": "How will I see the output of the log?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:09:31.246000+00:00",
                "content": "```python\n    os.environ['BAML_LOG'] = info\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:29:16.529000+00:00",
                "content": "it should dump into your terminal."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:29:38.011000+00:00",
                "content": "can you type this in your terminal:\n\n```\nexport BAML_LOG=info\n```"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:30:50.808000+00:00",
                "content": "I have done that.."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:31:04.527000+00:00",
                "content": "is that not working when you run the logs?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:31:16.177000+00:00",
                "content": "i can hop on Office hours and take a look!"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:33:12.736000+00:00",
                "content": "How do I run the logs?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:34:25.199000+00:00",
                "content": "oh,, I see. \n\nwhen you run your python function, it will dump it into termianl.\nyou should see something like this:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:34:50.976000+00:00",
                "content": "(in this case it shows BAML failed to parse, but in your case, it will show the actual parsed response)"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:36:22.950000+00:00",
                "content": "I don't see that in the terminal. I actually print `raw` in `sync_client.py` to get the response you are showing."
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:37:27.159000+00:00",
                "content": "And when I print raw, I actually do not see the parameters etc like temperature that I passed."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:38:29.283000+00:00",
                "content": "oh, ok! got it.\n\nThat's odd. I'll file a bug and see why thats the case.\n\nCan you share your current baml version so i can include that on the ticket.\nAlong with python version you have?"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:38:35.073000+00:00",
                "content": "Happy to jump on office hours is easier to solve?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:38:42.837000+00:00",
                "content": "yea that might be easier üôÇ"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-13 20:39:02.593000+00:00",
                "content": "Let me know how to join pls?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 20:39:24.866000+00:00",
                "content": "on the left side panel, just click on there and it should join"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-14 19:10:58.442000+00:00",
                "content": "Hi Vaibhav! Thanks for the help again yesterday. I had a question. My output currently looks like this:\n```{\n  \"title\": \"Executive Update on National Security and Defense\",\n  \"sections\": [\n    {\n      \"title\": \"Appropriations Bills\",\n      \"summary\": \"**Appropriations Bills**: The U.S. Congress is considering three critical appropriations bills for fiscal year 2025: the Homeland Security Appropriations Act, the Defense Appropriations Act, and the State Foreign Operations and Related Programs Appropriations Act. These bills are designed to enhance border security, strengthen national defense, and improve the U.S.'s global standing.\"\n    },\n    {\n      \"title\": \"International Defense Cooperation\",\n      \"summary\": \"**Czech Republic**: The U.S. is supporting a major defense acquisition by the Czech Republic to bolster its homeland defense and deter regional threats, enhancing the security of this key NATO ally.\"\n    }\n  ]\n}\n```\n\nIs there a way I can get it to return like this:\n```**Executive Update on National Security and Defense**\n\n1. **Appropriations Bills**: The U.S. Congress is considering three critical appropriations bills for fiscal year 2025: the Homeland Security Appropriations Act, the Defense Appropriations Act, and the State Foreign Operations and Related Programs Appropriations Act. These bills are designed to enhance border security, strengthen national defense, and improve the U.S.'s global standing.\n\n2. **International Defense Cooperation**:\n   - **Czech Republic**: The U.S. is supporting a major defense acquisition by the Czech Republic to bolster its homeland defense and deter regional threats, enhancing the security of this key NATO ally.\n```"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-14 19:11:41.861000+00:00",
                "content": "I tried to manipulate template string but that didn't seem to work."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:11:44.546000+00:00",
                "content": "yes! you can just change your return type of the function to be a `string` instead of a class and then you can prompt engineer it to return markdown"
            },
            {
                "author": "mz5910",
                "timestamp": "2024-08-14 19:12:17.882000+00:00",
                "content": "Thanks!"
            }
        ]
    },
    {
        "thread_id": 1273041037367181352,
        "thread_name": "video on docs!",
        "messages": [
            {
                "author": "dokudoge",
                "timestamp": "2024-08-13 22:10:35.267000+00:00",
                "content": "Hi, I was looking through the website, and for some reason the demo video isn't playing for me\n\nhttps://docs.boundaryml.com/docs/get-started/what-is-baml\n\nthanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 22:11:46.013000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 22:11:46.284000+00:00",
                "content": "oh no! thanks for flagging. it seems to be working on my end, I'll get you a direct link üôÇ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-13 22:12:28.331000+00:00",
                "content": "does this work? https://boundaryml.wistia.com/medias/5fxpquglde"
            },
            {
                "author": "dokudoge",
                "timestamp": "2024-08-13 22:16:16.084000+00:00",
                "content": "ya that works, tysm!"
            },
            {
                "author": "dokudoge",
                "timestamp": "2024-08-13 22:16:39.544000+00:00",
                "content": "ya I'm hotspot rn, so maybe that's it, but definitely worth looking into"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-13 22:16:59.719000+00:00",
                "content": "thanks for the heads-up!"
            }
        ]
    },
    {
        "thread_id": 1273178768437153833,
        "thread_name": "vllm + baml",
        "messages": [
            {
                "author": "feres0902",
                "timestamp": "2024-08-14 07:17:52.913000+00:00",
                "content": "im trying to integrate baml with vllm and after hardcoding the baml pormpt the only thing left is to parse the llm answer, is there a way to do it in python"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 07:18:51.740000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 07:18:52.199000+00:00",
                "content": "hi feres, could you outline what you mean by do it in python? Do you mean use only the parser?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 07:19:55.352000+00:00",
                "content": "as of right now, we don't explictly expose the parser, but we should be able to do this for you!"
            },
            {
                "author": "feres0902",
                "timestamp": "2024-08-14 08:32:35.881000+00:00",
                "content": "Yes exactly, i want to use the baml parser only, \ni had 2 usecases: \n1rst predict summary and solution from chat and i was able to do it by using baml extenstion to get the baml prompt ( + small adjustments like adding the <s>[INST] {prompt} [/INST] ) \nthen apply as a function to all chats to get list of prompts that i sent to vllm opeai compatible client and get list of reposnes then to parse those i just load the text to json and use a pydantic class to get thel to the exact object i want, this works welll for this usecase and i alsmost get summaries and  solution for every chat, \nin my second usecase im trying to perform multilabel classfication and in this case the pydantic + json parsing solution stops working alsmost and i want to use Baml parser as its just much better at dealing with raw llm outputs"
            },
            {
                "author": "feres0902",
                "timestamp": "2024-08-26 06:57:38.089000+00:00",
                "content": "hello <@99252724855496704> hope you doing well, \nis there any news about the parser availability?"
            }
        ]
    },
    {
        "thread_id": 1273345929667411988,
        "thread_name": "anthropic caching",
        "messages": [
            {
                "author": "kdub03",
                "timestamp": "2024-08-14 18:22:07.256000+00:00",
                "content": "Any thoughts on anthropic caching? https://www.anthropic.com/news/prompt-caching"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 18:22:47.242000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 18:22:47.638000+00:00",
                "content": "yes! expect BAML to support this by next monday!\n\n```\n_.role(\"user\", cache_control={...})\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 18:24:05.835000+00:00",
                "content": "in the future, you'll be able to add arbitrary keys here (not just anthropic)"
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-14 18:27:25.743000+00:00",
                "content": "BAML is not using tools for anthropic, correct? \n        |Cache prefixes are created in the following order: tools, system, then messages."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 18:27:41.881000+00:00",
                "content": "yep! we dont support using tools atm"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 18:28:18.691000+00:00",
                "content": "so it should work for all messages in system / your prompt.\n\nThat said we'll be optionally supporting tools API soon and you should see that land in about 2 weeks."
            },
            {
                "author": "kdub03",
                "timestamp": "2024-08-14 18:28:25.394000+00:00",
                "content": "I haven't tested it yet, but it reads like they do cache tools, which is what I don't want. Since I want to basically cache a long context, then hit it with many structured outputs."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 18:29:43.486000+00:00",
                "content": "ah great.  I think in that case the approach here will work for you\n```\n{{ _.role(\"user\") }}\nYour uncached content\n{{ _.role(\"user\", cache_control= {...}) }}\ncached content\n{{ _.role(\"user\") }}\nYour uncached content\n```"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-08-19 17:18:05.462000+00:00",
                "content": "cache_control parameter needs the update to work?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:04:02.587000+00:00",
                "content": "this actually isn't released yet! We were hoping to get it out today, but it should go out in 0.54 (planned for tmrw or day after!)"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-08-22 22:00:34.124000+00:00",
                "content": "is this one coming out soon üòÄ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 22:00:56.663000+00:00",
                "content": "yes! its pretty much working now üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 22:01:01.657000+00:00",
                "content": "(from my machine)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 22:01:12.378000+00:00",
                "content": "(on local build)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 22:01:56.951000+00:00",
                "content": "but I'm just doing a few small things like fixing the UI so that when you have an openai model it shows you that we ignore cache_policy (BAML already does that, but just making the playground visualize it correctly atm)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 14:45:41.035000+00:00",
                "content": "https://github.com/BoundaryML/baml/pull/893\n\nPr is finally out and likely will land today!"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-08-24 16:15:27.594000+00:00",
                "content": "is there a way to access the other items from the response, like anthropic returns number of cache tokens etc"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 16:17:04.102000+00:00",
                "content": "We're working on such things and providing the raw HTTP response back. we expect that to be merged in in about 2 weeks"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 16:17:41.108000+00:00",
                "content": "FYI, while the PR is merged in, we still have one bug we're tracking to figure out before we can release (unrelated to this). I'll message on announcements when we update and issue a release"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 16:26:57.107000+00:00",
                "content": "(this is live on promptfiddle already tho so you can test it there)"
            }
        ]
    },
    {
        "thread_id": 1273354087643091035,
        "thread_name": "Debugging ClientRegistry",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 18:54:32.269000+00:00",
                "content": "Hey I am trying to use BAML but I need to provide the api_key during the runtime because I package the client in a typescript library.\n```typescript\n  const clientRegistry = new ClientRegistry();\n\n  clientRegistry.addLlmClient(\"GPT-4o\", \"openai\", {\n    model: \"gpt-4o\",\n    temperature: 0.7,\n    api_key: key,\n  });\n\n  clientRegistry.setPrimary(\"GPT-4o\");\n\n  const res = await b.ExtractResume(\n    \"Mark gonzalez, mark@hello.com. python. 5 years.\",\n    { clientRegistry }\n  );\n\n  return res;\n```\nThis is what I do and I still get an error from open ai saying there is no bearer token"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:07:12.166000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:07:12.718000+00:00",
                "content": "Let me take a look at this! can you try running with the following env variable in your shell\n\n```\nBAML_LOG=debug\n```\n\nThat should print out the raw request we are making"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 19:09:07.960000+00:00",
                "content": "should I do it when generating the client or when executing?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:09:15.325000+00:00",
                "content": "during execution"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:09:33.136000+00:00",
                "content": "also glad to hop on Office hours real quick and see if theres something small being missed"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 19:17:23.836000+00:00",
                "content": "thank you BAML_LOG helped me fix it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:17:30.832000+00:00",
                "content": "woo! üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:17:34.258000+00:00",
                "content": "what was the bug btw?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 19:18:24.218000+00:00",
                "content": "i had a typo in the openai key env var"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 19:18:29.062000+00:00",
                "content": "ah üôÇ"
            }
        ]
    },
    {
        "thread_id": 1273356838242353204,
        "thread_name": "TS imports",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 19:05:28.063000+00:00",
                "content": "Also a side note it might be good to support generating imports that include the file extension for typescript because tsc does not want to compile the client"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-14 19:06:23.678000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-14 19:06:24.168000+00:00",
                "content": "what are your TS config settings? And what version of TS are you on? Are you also using nextjs?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-14 19:06:52.322000+00:00",
                "content": "have you tried using a library like https://ui.shadcn.com/docs/components/dialog ? I'm guessing you have to modify those imports as well no?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 19:08:21.703000+00:00",
                "content": "Yes we did have to modify them because we use esm. \nI fixed it by doing:\n```\n    \"module\": \"Preserve\",\n    \"moduleResolution\": \"Bundler\",\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-14 19:09:53.726000+00:00",
                "content": "interesting -- so with this you're unblocked right? I'll take a look at our esm compatibility! Thanks for reporting"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-08-14 19:10:54.680000+00:00",
                "content": "yes, I just wanted to report it"
            }
        ]
    },
    {
        "thread_id": 1273426154518483043,
        "thread_name": "dynamic enum example",
        "messages": [
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-14 23:40:54.351000+00:00",
                "content": "Hey folks. Does anyone have an example of a test case that uses/extends a dynamic enum that they could share?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:41:23.049000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:41:23.293000+00:00",
                "content": "sharing a repo! one sec"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:43:01.969000+00:00",
                "content": "https://github.com/BoundaryML/example-massive-categorizer/blob/main/classifier.ipynb\n\nsepcifically:\n\n```python\n@trace\nasync def classify(tool: str, description: str) -> Classification:\n    root_categories = get_best_categories(tool, description)\n    # Filter for all categories which are children of the root categories\n    tb = TypeBuilder()\n    for _, row in df.iterrows():\n        if row[\"Parent\"] in root_categories:\n            tb.Tools.add_value(row['Categories'])\n    selected = await b.Classify(tool, description, count=1, baml_options={ \"tb\": tb })\n    if len(selected) == 0:\n        return None\n    return selected[0]\n```\n\nthe baml defintion:\n```\n// Defining a data model.\nenum Tools {\n  // We'll define these in python\n  @@dynamic\n\n  @@alias(ToolCategory)\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:43:44.124000+00:00",
                "content": "`tb.Tools.add_value(new_value)`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:44:03.182000+00:00",
                "content": "oh do you mean a test case within BAML itself?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:45:43.615000+00:00",
                "content": "If so, we have an open issue for this (we're hoping to close this by mid sept - theres a few more high prio items atm)\nhttps://github.com/BoundaryML/baml/issues/711\n\nBut the recommendation for testing dynamic types is to write the tests in a python/ts script for now."
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-14 23:49:53.215000+00:00",
                "content": "> If so, we have an open issue for this (we're hoping to close this by mid sept - theres a few more high prio items atm)\n\nYep, cool.\n\nFor context: we have a junior-engineer playing with prompting and writing test cases in the playground. It's been really useful for experimentation. The goal was to lock in the BAML with playground-driven development, then generate client and move to python when we're ready to build the pipelines."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-14 23:51:33.650000+00:00",
                "content": "Agreed üôÇ we really wish we could do extentions of types trivially in tests but the syntax for it was sadly a bit more nuanced than we expected and requires a full spec similar to <#1265356689796890820>  (hence the delay here).\n\nif it would help, I'd be glad to share some quick and dirty python scripts for easy testing"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-15 00:12:31.399000+00:00",
                "content": "Good for now.\n\nThat's an interesting proposal. I'm no PL designer, but I wonder if there is a lot going on, with the predicate loaded into in-line annotations. Looking at the Address example..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 00:15:55.901000+00:00",
                "content": "I agree on that.\n\nif you'd like to chat more about the spec and offer some thoughts, would be very happy to hear your thoughts. We're in the middle of building it and if we have some thoughts that can help shape its direction, would be more than glad."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 00:16:11.249000+00:00",
                "content": "(nothing official, just free form thoughts would be appriciated)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 00:16:38.477000+00:00",
                "content": "(available in the office hours channel if you'd like to talk through it, or just write things down in this thread)"
            }
        ]
    },
    {
        "thread_id": 1273521171727519817,
        "thread_name": "Tests",
        "messages": [
            {
                "author": "richardclove",
                "timestamp": "2024-08-15 05:58:28.219000+00:00",
                "content": "Is it possible to put test cases in a different file? or better yet.. what do you suggest when we have really long tests or many test cases? What are the best practices here?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 06:00:01.084000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 06:00:01.456000+00:00",
                "content": "Yeah! You can put tests wherever. Some people create one file per test or others create a directory for all the tests. We dont have an official guideline but splitting things up into different files or a separate long test file can make things more mnageable"
            },
            {
                "author": "richardclove",
                "timestamp": "2024-08-15 16:54:57.273000+00:00",
                "content": "I understand. It's automatically imported. Thanks"
            }
        ]
    },
    {
        "thread_id": 1273679221851426908,
        "thread_name": "pydantic -> BAML",
        "messages": [
            {
                "author": "kdub03",
                "timestamp": "2024-08-15 16:26:30.305000+00:00",
                "content": "Was there a sample or example on going from Pydantic -> BAML somewhere? We have a codebase of pydantic model extractions that I'd like to just test with BAML."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 16:29:20.288000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 16:29:20.747000+00:00",
                "content": "sadly we don't have a quick way to do this. How complex is your schema?\n\nfor most schemas, I usually just dump some BAML schemas into chat gpt as context then I say:\n\n```\n<baml schemas>\n\ntranslate this pydantic code to the above format\n<pydantic models>\n```"
            }
        ]
    },
    {
        "thread_id": 1273685730119385210,
        "thread_name": "use prompts",
        "messages": [
            {
                "author": "richardclove",
                "timestamp": "2024-08-15 16:52:21.997000+00:00",
                "content": "I want to write a preview so our app users can write their own prompts. Is there a way to output the compiled input prompt for Python? I know it showed up on the playground but is there a code version?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 16:57:09.764000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 16:57:10.042000+00:00",
                "content": "hi! Yes its def possible. what kinds of prompts do you want to support?\n\nDo you mean full BAML files? With the schema and prompt syntax? Or just adding some additional context into prompts you have?"
            },
            {
                "author": "richardclove",
                "timestamp": "2024-08-15 16:58:31.784000+00:00",
                "content": "I want the user to provide context  (domain knowledge specific to user/org) and i can use it as input/parameter for the LLM function"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 16:58:42.608000+00:00",
                "content": "also since this is a more complex usecase, glad to talk in office hours and share more information?"
            },
            {
                "author": "richardclove",
                "timestamp": "2024-08-15 16:58:53.354000+00:00",
                "content": "That'll be great"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 16:59:05.122000+00:00",
                "content": "online on office hours channel! üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-15 17:00:19.155000+00:00",
                "content": "if you click on it, you should be able to join"
            }
        ]
    },
    {
        "thread_id": 1273738880306380961,
        "thread_name": "pydantic / langchain",
        "messages": [
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 20:23:33.989000+00:00",
                "content": "I also have a pydantic-centric multi-agent app that I am developing using langchain and langgraph.  I am trying to understand how BAML would fit into my app.  My agents typically have a prompt but also a pydantic class that are attached to the agent's model via functions."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 20:25:41.460000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 20:25:41.691000+00:00",
                "content": "You can just create specific BAML functions that plug in to the rest of your langchain code.\n\nYou can do any kind of tool calling in those BAML function definitions like this:\nhttps://docs.boundaryml.com/docs/snippets/functions/function-calling#choosing-multiple-tools \n\nFor any types you define in BAML it generates pydantic models for you to leverage elsewhere"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 20:26:21.344000+00:00",
                "content": "if you can show me what kind of code you're trying to write I can give you more examples -- do you need an example using langgraph specifically? Or LCEL from langchain?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 20:27:27.321000+00:00",
                "content": "And what are the kinds of features you find useful from Langgraph? Agents at the end of the day are just \"LLM functions\" chained together running various prompts in sequence (or in a graph), potentially inside some while-loop"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 21:55:04.513000+00:00",
                "content": "Thanks, <@201399017161097216> .  Agree that \"Agents at the end of the day are just \"LLM functions\" chained together running various prompts in sequence (or in a graph), potentially inside some while-loop.\""
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 21:55:26.854000+00:00",
                "content": "Here's a simplified diagram of my setup"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 21:55:43.175000+00:00",
                "content": ""
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 21:56:21.936000+00:00",
                "content": "The LangGraph part is not important"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 21:57:05.624000+00:00",
                "content": "Given a langchain runnable that has a prompt and a function-enabled LLM (function is a pydantic class)...."
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 21:59:46.808000+00:00",
                "content": "I am trying to wrap my head around how I swap in BAML in place of the Pydantic classes."
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 22:00:14.859000+00:00",
                "content": "The good news is I think I try BAML on one of the simpler agents to test it out."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 22:02:48.689000+00:00",
                "content": "ok one sec let me see what bind_functions does"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 22:10:20.711000+00:00",
                "content": "so this is the same thing as AgentA:\n-An LLM that calls a tool  (WeatherAPI), which always gets returned\n\nSo in your code you just call it like:\n```python\nweather_api_params = b.UseTool(\"...\")\ncall_weather_api(weather_api_params.city, weather_api_params.timeOfDay)\n```\n\nIf your Agent A uses more than one tool then you can still have the LLM BAML Fucntion return two different responses instead using a union type (Tool1 | Tool2), you jus thave to check which one got returned:\n```\nres  = b.UseTools(\"...\")\nif (isinstance(res, WeatherApi):\n  // call the weather api with the params\nelse:\n...\n```\n\nIf you need more in depth guidance id be happy to chat on a call if that's easier, or just lmk what details you're looking for"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-15 22:11:43.232000+00:00",
                "content": "remember all the `class` definitions in .baml files get converted to pydantic models that you can access from your baml responses, and import the type using `from baml_client.types import MyType`"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 22:56:12.034000+00:00",
                "content": "Thanks, <@201399017161097216> .  As my diagram shows, the langchain runnable (a python function) uses a system prompt + a model that has a pydantic class embedded in it as a function.  Check out this code snippet:"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 22:56:23.191000+00:00",
                "content": "```from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\n    \ndef my_runnable():\n  llm: ChatOpenAI = ChatOpenAI(model=\"gpt-4o\", temperature=0,           api_key=settings.OPENAI_API_KEY)\n    \n  functions = [MyPydanticClass]\n  llm_embedded_with_pydantic_class = llm.bind_functions(functions,   function_call=\"MyPydanticClass\")\n                \n  my_prompt_template = ChatPromptTemplate.from_messages(\n      [\n         (\"system\", my_system_prompt),\n         MessagesPlaceholder(variable_name=\"messages\"),\n      ]\n  )\n    \n  inputs = {\"messages\": message_buffer}\n    \n  # LCEL stuff\n  chain = my_prompt_template | llm_embedded_with_pydantic_class\n  response = chain.invoke(inputs)  # This is the model invocation\n```"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 23:01:22.449000+00:00",
                "content": "my LCEL chain invokes the model with both the system prompt and the embedded pydantic class"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 23:03:08.025000+00:00",
                "content": "If I'm not mistaken, BAML invokes the model when you call the function you define, e.g., UseTools()."
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 23:10:20.731000+00:00",
                "content": "I guess I could call a BAML function I define inside of my_runnable()"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-15 23:11:53.855000+00:00",
                "content": "How would I pass in the message_buffer?  ü§î"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-16 00:39:03.963000+00:00",
                "content": "So here's how you do this with BAML:\n\nBAML function:\n```\nclass MyPydanticClass {\n hello string\n}\n\nfunction MyFunction(messages: string[]) -> MyPydanticClass {\n  client GPT4o // your client def here\n  prompt #\"\n    Extract the following data. < any thing else you wanna add here>\n    \n    {{ ctx.output_format }}\n    \n    {{ _.role('user') }}\n    {% for m in messages %}\n    - {{ m }}\n    {% endfor %}\n  \"#\n}\n```\n\nNote that i added the user role there for your messages and thats not something you currently do, but its something we've seen improve teh accuracy of the model. Similarly, instead of dumping the messages array directly, I unrolled the array into bullet points.\n\nPython code to call this\n```\nfrom baml_client import b\n\nresponse = b.MyFunction(messages: message_buffer)\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-16 00:39:17.669000+00:00",
                "content": "does that make sense?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-16 00:39:42.409000+00:00",
                "content": "the response will be of type `MyPydanticClass`"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-16 15:16:54.592000+00:00",
                "content": "Thanks, <@201399017161097216> .  The BAML part makes perfect sense.  I'll try to integrate into my langchain/langgraph plumbing today."
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:01:51.903000+00:00",
                "content": "Hi <@201399017161097216>  - I am up and going with BAML and have replaced one of my pydantic classes (doing one class at a time).  Question:  Sometimes my BAML function takes a parameter that will be None  'cal_events'.  I want to represent the variable conditionally in the prompt.  \n```\nfunction Converse(prompt: string, cal_events: string, messages: string[]) -> FutureTripsConversation {\n  client GPT4o\n  prompt #\"\n    \n    {{ _.role('user') }}\n    {% for m in messages %}\n    - {{ m }}\n    {% endfor %}\n\n    {{ _.role('system') }}\n    {{ prompt }}\n\n    {% if cal_events != null %}\n      Calendar events in JSON format:\n      {{ cal_events }}\n    {% endif %}\n\n    Extract the following data:\n    {{ ctx.output_format }}\n\n  \"#\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:03:02.536000+00:00",
                "content": "you can make cal_events of type `string?`"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:04:48.339000+00:00",
                "content": "it's a json string"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:05:11.584000+00:00",
                "content": "sometimes, it is None, in which case I don't want it included in the prompt"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:05:43.076000+00:00",
                "content": "is my test for null in the jinja correct?"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:06:44.118000+00:00",
                "content": "it must not be correct because I am getting:\n\nbaml_py.BamlError:   Error: cal_events: Expected type String, got `Null`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:06:59.148000+00:00",
                "content": "hmm let me check"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:08:53.953000+00:00",
                "content": "use this:\n```\n{% if thing %}\n    {{ thing }}\n    {% endif %}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:09:11.297000+00:00",
                "content": "or {% if thing != none %}"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:09:23.225000+00:00",
                "content": "jinja has this \"none\" type thing that all nulls get converted to"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:09:40.350000+00:00",
                "content": ""
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:19:43.287000+00:00",
                "content": "hmm, neither of these work"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:20:45.070000+00:00",
                "content": "is there type-checking or validation happening in the entry to the function?"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:21:32.475000+00:00",
                "content": "I call it like this:\n\nresponse = b.Converse(prompt=future_trips_agent_system_prompt, cal_events=None, messages=message_buffer_strs)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-20 21:22:31.146000+00:00",
                "content": "<@99252724855496704> will jump in on this"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-20 21:22:58.188000+00:00",
                "content": "hey <@1195491584066724011> this looks rather odd, mind if we hop on a quick call and we can look at some debug logs?"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-20 21:23:10.467000+00:00",
                "content": "sure"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-20 21:23:33.268000+00:00",
                "content": "sent a DM with zoom link!"
            }
        ]
    },
    {
        "thread_id": 1275901157889413184,
        "thread_name": "Finetuned OpenAI models with BAML",
        "messages": [
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 19:35:41.138000+00:00",
                "content": "Has anyone tried hitting a fine-tuned OpenAI model with BAML?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-21 19:36:42.916000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-21 19:36:43.079000+00:00",
                "content": "Yess, one of our paying customers in the healthcare space is using fine-tuned gpt4 models. All you need to change is the model name to your finetuned model"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 19:42:42.221000+00:00",
                "content": "<@201399017161097216> I think you might be my new best friend üòÇ"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 19:42:52.338000+00:00",
                "content": "Is it supported in prompt fiddle?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-21 19:43:17.799000+00:00",
                "content": "no, since promptfiddle uses our own API keys in our own org, but if you download the VSCode extension you can setup your own API keys there"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 19:43:52.028000+00:00",
                "content": "Riiiight ofc ofc. Thanks!"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 19:44:05.401000+00:00",
                "content": "This is super helpful!"
            }
        ]
    },
    {
        "thread_id": 1276080125909270559,
        "thread_name": "BAML + Rust",
        "messages": [
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-22 07:26:50.439000+00:00",
                "content": "is there anyway i can use BAML directly from Rust"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 13:23:44.024000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 13:23:44.377000+00:00",
                "content": "We're close to releasing a version of BAML that generates an OpenAPI spec which will then work with every language. As of now, we don't have a rust crate we expose üò¢"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 13:24:54.662000+00:00",
                "content": "(if you are really committed, what one team did is clone BAML and use it directly! but we don't provide any guarantees on a stable interface there)"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-22 14:48:45.247000+00:00",
                "content": "i think i might just need the parsing part in production"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-22 15:02:35.852000+00:00",
                "content": "by OpenAPI spec do you mean there will be some kind of web service that all language can issue http call to?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 15:49:41.693000+00:00",
                "content": "yep! and its a service that you'll be able to run via something like: `baml-cli serve --port 3000`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 15:50:13.285000+00:00",
                "content": "parsing isn't availabe as a module because its deeply coupled to BAML syntax sadly"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 15:51:00.561000+00:00",
                "content": "there's a folder called JSONish in engine you can copy and paste into your repo, but that would mean managing updates is on you!"
            }
        ]
    },
    {
        "thread_id": 1276382278645452821,
        "thread_name": "exceptions",
        "messages": [
            {
                "author": "richardclove",
                "timestamp": "2024-08-23 03:27:29.265000+00:00",
                "content": "How do you do error handling in concurrent functions? Some errors that come up like found multiple words when trying to do classification using enums\n\nBamlError: (3 other previous tries)\nLLM call failed: LLMErrorResponse { client: \"Groq\", model"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:47:32.362000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:47:32.811000+00:00",
                "content": "hi <@832768104585101322>, we'll be releasing proper exceptions soon! (likely 1-2 weeks) but for now you just need to wrap it around a try-catch. We're also working on a concept like default value.\n\nIn python you can use `await asyncio.gather(*tasks, return_exceptions=True)`\n\nhttps://docs.python.org/3/library/asyncio-task.html#running-tasks-concurrently"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:47:41.578000+00:00",
                "content": "that will return all exceptions to you"
            },
            {
                "author": "richardclove",
                "timestamp": "2024-08-23 03:52:14.525000+00:00",
                "content": ".. but, once one of them throw an exception, all of the other ones are no longer running"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:53:14.152000+00:00",
                "content": "are you using the async version btw?"
            },
            {
                "author": "richardclove",
                "timestamp": "2024-08-23 03:54:18.662000+00:00",
                "content": "Yes, I just switched to async right now"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:55:13.242000+00:00",
                "content": "hmm, thats odd, let me try and repro rq"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:57:49.011000+00:00",
                "content": "hmm:\n\n```python\nimport asyncio\n\nasync def task1():\n    await asyncio.sleep(1)\n    return \"Task 1 completed\"\n\nasync def task2():\n    await asyncio.sleep(2)\n    raise ValueError(\"Task 2 failed\")\n\nasync def task3():\n    await asyncio.sleep(1)\n    return \"Task 3 completed\"\n\nasync def task4():\n    await asyncio.sleep(2)\n    return \"Task 4 completed\"\n\nasync def task5():\n    await asyncio.sleep(1)\n    raise RuntimeError(\"Task 5 failed\")\n\nasync def main():\n    results = await asyncio.gather(\n        task1(),\n        task2(),\n        task3(),\n        task4(),\n        task5(),\n        return_exceptions=True\n    )\n\n    for i, result in enumerate(results, 1):\n        if isinstance(result, Exception):\n            print(f\"Task {i} failed with exception: {result}\")\n        else:\n            print(f\"Task {i} result: {result}\")\n\nasyncio.run(main())\n```\n\nthis code works for returning exceptions (just ran it)\n\nOne way to quickly test is to put the baml function in one of the tasks and see if that issue fails"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 04:40:55.115000+00:00",
                "content": "did that work?"
            }
        ]
    },
    {
        "thread_id": 1276386342015602748,
        "thread_name": "async in python",
        "messages": [
            {
                "author": "richardclove",
                "timestamp": "2024-08-23 03:43:38.048000+00:00",
                "content": "won't this call the b.ClassifyMessage one-by-one before asyncio.gather gets to it?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:49:47.550000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:49:47.798000+00:00",
                "content": "yes! but you can change your generator to have the `default_client_mode` set to `async`\n\nor you can directly import the. async version of b from:\n\n```python\nfrom baml_client.async_client import b\n```\nor for sync\n```python\nfrom baml_client.sync_client import b\n```\n\n```python\n# this is just shorthand for whichever is set as default_client_mode\nfrom baml_client import b \n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:50:26.671000+00:00",
                "content": "sorry for the poor docs here! We'll take a note of this and work on improving the docs for this. Must've missed this in the update"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:50:40.639000+00:00",
                "content": "You can read more here: https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#best-practices"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 03:50:59.429000+00:00",
                "content": "and if you've got other questions, will be on for a bit"
            }
        ]
    },
    {
        "thread_id": 1276576591241285766,
        "thread_name": "Going into production",
        "messages": [
            {
                "author": "hitzor9",
                "timestamp": "2024-08-23 16:19:36.998000+00:00",
                "content": "Hi! Recently, I needed to build a small prototype for extracting structured information from images using AI, and I decided to try using BAML. I didn't have much experience working with LLMs before, but with the help of your tool, I was able to build and test the prototype in just a few hours, so a huuuuge shotout for that!\n\nBefore going to production, I have a couple of questions. I'm using the Python SDK with an async client. \n\n1) When an error related to the LLM occurs, a `BamlError` is raised. I see that this error contains a textual representation of the `LLMResponse`, which includes the error code, message, and so on. Is there a way to access this object directly without trying to parse it from the text?\n\n2) As far as I can see in the OpenAI documentation, the server's response includes headers with usage information (`x-ratelimit-limit-requests`, `x-ratelimit-limit-tokens`). Is there any way I can access the raw response object or at least these headers? Or is there another unified way to obtain this information?\""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 16:23:50.168000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-23 16:23:50.529000+00:00",
                "content": "Hi <@591299942820216869>, thanks for giving BAML a try. Very excited that this worked for you, and hope we can continue to learn from you to make it better.\n\n1. parsing exceptions. This is def a major issue on our end, and we're working on releasing exceptions that are objects. We have a spec for this already and I will share the preview docs with you. (note its not yet implemented but should be live by mid september). Until then sadly you'll have to parse the text.\n\n2.As of right now, sadly not. But we are working on a spec for a thing like `b.raw.YourFunction` which will not only return the raw HTTP request, but a bunch of other metadata that BAML was able to collect (including all retry policys etc). This should also come in the mid sept release.\n\nSpeaking of, you may want to take a look at fallbacks and retry_policies to make your app more resiliant.\nhttps://docs.boundaryml.com/docs/snippets/clients/retry"
            }
        ]
    },
    {
        "thread_id": 1276888982919053343,
        "thread_name": "new dynamic enums",
        "messages": [
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:00:56.978000+00:00",
                "content": "Been playing around with BAML a bit, thanks for the project. I submitted that PR the other day about Gemini and might have some others for old docs etc. I might submit later.\n\nI'm trying to build a classifier. I read about enums in the documentation and examples, sounded pretty promising. I need to categorize with strings containing spaces, so the @alias functionality seemed useful as well. \n\nMy question is this, can I add @@dynamic enum fields and set the @alias as well? From what I can tell in the Python source, type builder really only allows you to add the enum value.\n\nOpen to other suggestions as well if there's another way to accomplish this.\n\nEx.\n```py\n#both diseases and categories are parsed from a file and can vary \ncategories=[\n\"Cardiovascular diseases\", \"Immunology\", \"Skin & Soft tissue\"]\n\ndiseases=[\"Melanoma\", \"Coronary artery disease\"]\n\n#call BAML here to categorize diseases\n```\nCurrently, I pass the list of strings in plain text and ask it to categorize, and cross my fingers that the LLM matches the category exactly"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:14:36.008000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:14:36.374000+00:00",
                "content": "Firstly, thanks for the PR, please do continue to do so! we appreciate it very much.\n\nAnd yess you can do create new dyanmic enums as well!\n\nhttps://docs.boundaryml.com/docs/calling-baml/dynamic-types#creating-new-dynamic-classes-or-enums-not-in-baml\n\nOne thing we're working on is literals as well."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:15:33.695000+00:00",
                "content": "also you can always construct an enum that's dynamic that has 0 values described in BAML, then only add them dynamically"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:16:52.714000+00:00",
                "content": "Yes I was planning to take that approach but thought I might be limited by the requirement that the classifications have strings with spaces, while the enums can't have spaces"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:17:20.399000+00:00",
                "content": "you can have spaces by using alias fyi!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:17:51.378000+00:00",
                "content": "so you can have a value that has no spaces, but then alias it so you can render spaces for the LLM"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:17:57.995000+00:00",
                "content": "Yes, but I can't add an alias with a dynamic enum right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:18:06.805000+00:00",
                "content": "oh you can do that as well üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:18:17.930000+00:00",
                "content": "I think you should be able to at least"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:18:37.333000+00:00",
                "content": "I was looking at type_builder and it didn't seem immediately obvious to me that I could"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:19:49.076000+00:00",
                "content": "I think our docs for typebuilder need some work\n\n```\n  hobbiesEnum = tb.add_enum('Hobbies')\n  hobbiesEnum.add_value('Soccer').alias(..)\n  hobbiesEnum.add_value('Reading').description(..)\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:19:57.569000+00:00",
                "content": "those should work as well"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:20:10.073000+00:00",
                "content": "the top level enum can also have alias as well i think"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:20:11.297000+00:00",
                "content": "Oh perfect, there it is!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:20:14.758000+00:00",
                "content": "It should auto complete üôÇ"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:20:16.396000+00:00",
                "content": "Thanks"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:20:21.619000+00:00",
                "content": "if it doesn't, it won't work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-24 13:20:29.343000+00:00",
                "content": "but yea we 100% need to improve the docs around it"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-24 13:21:02.181000+00:00",
                "content": "Will give it a shot, thanks a ton"
            }
        ]
    },
    {
        "thread_id": 1277260938608640081,
        "thread_name": "map",
        "messages": [
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 13:38:58.125000+00:00",
                "content": "Hello! Is here an example of using `map`? <@99252724855496704> you mentioned a tip to me recently about using my own unique keys in my schema to get data back. Not sure how to implement that but very interested! Context: I'm making a tool to classify sales contracts based on whether the contract includes lagnuage offering a specific product. But there are 40-50 products. So I was thinking I could require it give me back an object where each key is the product name and then the property content for each key is whether the product was found, relevant text as proof, etc."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 13:41:08.649000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 13:41:09.163000+00:00",
                "content": "seems like a hole in the docs!\n\nhttps://docs.boundaryml.com/docs/snippets/supported-types#map\n\nyou can do this:\n`map<string, ProductInfo>`\n\nThen add a description where you say `@description(\"Key is the name\")`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 13:41:28.624000+00:00",
                "content": "we'll add more examples for the docs to show this"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 17:44:23.221000+00:00",
                "content": "Thanks - I'll give it a try!"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 18:06:28.417000+00:00",
                "content": "I should probably be able to work this out on my own, but embarrassingly, I am a bit stumped here. I figured I would try to specify my actual product names as keys somehow, perhaps as an enum, and pass that to the map definition. But I got `Maps may only have strings as keys`, so obviously my intuition was wrong there. \n\n```\nenum ProductEntitlements {\n    Aerospace_Defense @description( \"Aerospace & Defense\" )\n    Healthcare @description( \"Healthcare\" )\n    Geotechnology @description( \"Geotechnology\" )\n    Industrial_Consumer @description( \"Industrial & Consumer\" )\n}\n\nclass ContractProductMappingValue {\n    confidence float\n    relevantLanguage string\n}\n\nclass ContractProductMappingResult {\n    Products map<ProductEntitlements, ContractProductMappingValue>\n}\n```\n\nMy intention is to get back an object with **every** product listed a a key so that I am assured the LLM gives me its analysis of each. I have 40 or so of those, so ultimately, I'll have to do this with dynamic types I imagine. But for testing, I was trying a few manual entries."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 18:07:30.771000+00:00",
                "content": "oh yea, we don't have enum support in maps yet! <@711679663746842796> can work on this shortly, i think we should be able to land it in this week (likely ~1 day of work)"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 18:08:12.691000+00:00",
                "content": "Got it! So in the meantime, is there a programmatic workaround with dynamic types? I think you mentioned something like this before on my last project."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 18:09:14.535000+00:00",
                "content": "for now can you make a new object instead?\n\n```\nclass ProductEntitlements {\n    air ContractProductMappingValue @alias( \"Aerospace & Defense\" )\n    health ContractProductMappingValue @alias( \"Healthcare\" )\n    geotech ContractProductMappingValue @alias( \"Geotechnology\" )\n    consumer ContractProductMappingValue @alias( \"Industrial & Consumer\" )\n}\n```\n\nAlternatively you can construct that class dynamically as well"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 18:09:29.692000+00:00",
                "content": "Ha I literally just figured that out. \n\n```\nclass ContractProductMappingResult {\n    Industrial_Consumer ContractProductMappingValue @description( \"Industrial & Consumer\" )\n    Aerospace_Defense ContractProductMappingValue @description( \"Aerospace & Defense\" )\n    Healthcare ContractProductMappingValue @description( \"Healthcare\" )\n    Geotechnology ContractProductMappingValue @description( \"Geotechnology\" )\n}\n```"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 18:09:34.329000+00:00",
                "content": "Duh!"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 18:09:41.996000+00:00",
                "content": "Is alias better than description?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 18:10:22.375000+00:00",
                "content": "alias will use less tokens! I recommend taking a look at the final prompt to see the differences.\n\nThough if you use alias you may prefer to use no spaces in key names"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 18:10:44.953000+00:00",
                "content": "Awesome - thanks as always!"
            }
        ]
    },
    {
        "thread_id": 1277453913544458313,
        "thread_name": "JS support",
        "messages": [
            {
                "author": "huythangphan",
                "timestamp": "2024-08-26 02:25:46.935000+00:00",
                "content": "Hi <@99252724855496704> , I‚Äôd like to inquire if there are any plans for BAML to support JavaScript in the near future. Thank you"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:27:32.616000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:27:33.165000+00:00",
                "content": "Hey John! We're actually releasing a version of BAML this upcoming week that will work as follows:\n\n```\n# write your BAML files\n\n# then run\nbaml-cli serve --port 3000\n```\n\nThat will also generate an `openapi.yaml` file you can use to generate clients to any language. That way until we build a more native JS option, you'll still be able to ping BAML functions locally."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:27:37.360000+00:00",
                "content": "Would that work for you?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:28:25.604000+00:00",
                "content": "(otherwise, also happy to have you set up a tsc command to compile BAML typescript code -> JS code. I suspect this should be just 15 mins of work)"
            },
            {
                "author": "huythangphan",
                "timestamp": "2024-08-26 02:31:20.207000+00:00",
                "content": "That's great to hear! We‚Äôre looking to use JavaScript for faster coding and a more efficient review process. However, the new version coming next week is definitely something to look forward to."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:32:16.925000+00:00",
                "content": "awesome. It will add an extra step FYI to your deploy process where you'll have to have a microservice running `baml-cli serve` but we'll be offering that shorlty after as well"
            },
            {
                "author": "huythangphan",
                "timestamp": "2024-08-26 02:33:48.751000+00:00",
                "content": "I believe the additional microservice won‚Äôt be costly, so it should be fine."
            },
            {
                "author": "huythangphan",
                "timestamp": "2024-08-26 02:33:57.015000+00:00",
                "content": "Thank you"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:34:13.982000+00:00",
                "content": "yep! its super lightweight. Glad to hear it'll work for you. Will DM you when its ready"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:34:24.894000+00:00",
                "content": "Tagging <@711679663746842796> for context who's working on this."
            }
        ]
    },
    {
        "thread_id": 1277667613631189054,
        "thread_name": "BAML support  AWS Secrets Manager.",
        "messages": [
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-26 16:34:57.006000+00:00",
                "content": "BAML support  AWS Secrets Manager."
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-08-26 16:34:57.446000+00:00",
                "content": "Hi @Vaibhav @aaronv  Hope you had a good weekend.  We're moving the first bits of our BAML implementation to production in AWS.  We use AWS Secrets Manager and trying to figure out how if BAML will work with it - https://docs.boundaryml.com/docs/calling-baml/set-env-vars - it's not clear to me it does."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 17:40:33.608000+00:00",
                "content": "Hi Jay! the easier way to do this is to use our client registry feature. \n\nThis will let you define clients using runtime values.\ne.g. set env vars for your bedrock client using the secrets manager.\n\nSee docs here: https://docs.boundaryml.com/docs/calling-baml/client-registry\n\nLet me know if that unblocks you! We're still looking for a way to have the entire object be reset with new env vars, but for now this is the current working solution."
            }
        ]
    },
    {
        "thread_id": 1277995711161438261,
        "thread_name": "OpenAI assistants",
        "messages": [
            {
                "author": "gleed",
                "timestamp": "2024-08-27 14:18:41.553000+00:00",
                "content": "OpenAI assistants"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-27 14:18:43.984000+00:00",
                "content": "I've recently run into a case where the I'd like the LLM to be able to run code to help with answer accuracy, and I found this does exist to an extent with the OpenAI Assistants API. <https://platform.openai.com/docs/assistants/quickstart?lang=curl&context=without-streaming>\n\nFor example, I'd like to ask gpt-4o \"Solve for x: 3x^2 + 5 = 10.\" and be confident the result will be mapped to a pydantic model like is done in BAML, but such a query would be too complex without giving the LLM the power of code interpreter. And I don't think the basic openai client allows use of tools like code interpreter.\n\nI can accomplish this with assistants API with something like this currently:\n\n```py\nfrom openai import OpenAI\nclient = OpenAI()\n\n# Create an assistant\nassistant = client.beta.assistants.create(\n    name=\"Math Solver\",\n    instructions=\"You are a math problem solver. Solve mathematical equations step by step.\",\n    tools=[{\"type\": \"code_interpreter\"}],\n    model=\"gpt-4\"\n)\n\n# Create a thread\nthread = client.beta.threads.create()\n\n# Add a message to the thread\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"Solve for x: 3x^2 + 5 = 10\"\n)\n\n# Run the assistant\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id\n)\n\n# Retrieve the assistant's response\nif run.status == 'completed':\n    messages = client.beta.threads.messages.list(\n        thread_id=thread.id\n    )\n    # The last message in the thread will be the assistant's response\n    assistant_response = messages.data[0].content[0].text.value\n    print(assistant_response)\nelse:\n    print(f\"Run status: {run.status}\")\n```\nIs there a place in this code I could leverage an existing function/class from BAML to be able to map the output to a defined pydantic model? I know the client/function/model logic in BAML is pretty intertwined so I doubt it, but thought I'd check. üôÇ"
            },
            {
                "author": "gleed",
                "timestamp": "2024-08-27 14:23:39.447000+00:00",
                "content": "Looks like this has been brought up to some extent here <https://discord.com/channels/1119368998161752075/1263509002755510383/1263555015851643047>. I figured assistant api integration wasn't planned but I thought maybe there'd still be some functionality from baml that could help optimize a prompt, for example, even if i perform the LLM call outside of baml."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-27 15:03:51.473000+00:00",
                "content": "Hmm. I think we had a few other requests for assistants api and from what i was speaking to another customer, they mentioned they might implement in BAML. Let me chat with them and see what the conclusion was. Out of curiosity, for your end, how would you see yourself improving the above code with BAML"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-09-04 01:24:17.716000+00:00",
                "content": "Hey checking in on this, was there ever an update?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 22:48:52.356000+00:00",
                "content": "<@761090952239906828> not yet, we haven't yet found a good way to incorporate the assistants API, but we have built some document processing API's we'll be releasing shortly. Those may help"
            },
            {
                "author": "huythangphan",
                "timestamp": "2024-09-23 09:38:19.977000+00:00",
                "content": "Hello <@99252724855496704>, any update on incorporating the Assistants API? Thanks."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 23:04:45.942000+00:00",
                "content": "sadly not, this work hasn't been prioritized for a bit, that said if someone is willing to help make a new provider for `openai-assistant-api`  I'd be happy to walk someone through the code where to do this.\n\nWe were hoping for someone in our community to do this, but they had some other urgent priorities happen and slipped in lower priority for them as well.\n\nif someone is willing to, just reply here and I'll be glad to hop on a meeting and show how the code works.\n\nIf we do it ourselves, we'd likely need to see a large volume of people upvote the assistants API and then prioritize it accordingly.\n\nFor a workaround, we currently recommend:\n1. calling the assistants API in python/ts\n2. passing those values back to a BAML function to get structured data out"
            }
        ]
    },
    {
        "thread_id": 1278362265468272691,
        "thread_name": "Langchain X BAML",
        "messages": [
            {
                "author": "mr_zoidberg_",
                "timestamp": "2024-08-28 14:35:14.910000+00:00",
                "content": "Are there any good examples of Baml and Langchain integration? I have an app with long conversations and a need to sometimes (at certain steps of the conversations) return a specified structured response while maintaining a using textual conversation. Baml looks promising, but I'm lost in trying to figure out how to integrate it for this usecase."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-28 18:54:23.310000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-28 18:54:25.023000+00:00",
                "content": "Hello! welcome! Yes, what kind of Langchain things are you using? Their conversation Memory and stuff like that?"
            }
        ]
    },
    {
        "thread_id": 1278592908324507727,
        "thread_name": "Rust support",
        "messages": [
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 05:51:44.452000+00:00",
                "content": "hi! is there any way to use BAML from Rust? I saw that the engine is in Rust"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-29 05:54:22.987000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-29 05:54:23.337000+00:00",
                "content": "No native rust support at the moment. But by monday (or maybe tomorrow) we will have a server you can instantiate using our CLI to call functions via http.\n\nWe can then generate OpenApi clients for any language.\n\nWould that work?"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 05:58:53.100000+00:00",
                "content": "I'm trying to have everything embedded in Rust so I can compile it to multiple architectures. For context, my project is Shinkai (https://www.shinkai.com)  Shinkai is a two-click install app that sets up AIs to run locally on your device. It can tap into a decentralized network where AI agents can pay for services from other AI agents or APIs using microtransactions.\n\nDo you know if I could compile the server to different architectures? Currently we are already embedding Ollama and a couple of other tools. I was planning to take a deeper look tomorrow"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:01:41.587000+00:00",
                "content": "We are already running local tools (Rust and JS using Node v22 before we were using rquickjs / llrt). A lot of the models that run locally are not thaaat great. I'm super excited about integrating with BAML"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:02:18.550000+00:00",
                "content": "What list of platforms are you planning to target?"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:03:27.334000+00:00",
                "content": "https://github.com/dcSpark/shinkai-node/blob/main/.github/workflows/build-binaries.yml#L14"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:03:38.950000+00:00",
                "content": "```\n  - arch: x86_64-unknown-linux-gnu\n            os: ubuntu-22.04\n          - arch: aarch64-apple-darwin\n            os: macos-14\n          - arch: x86_64-pc-windows-msvc\n            os: windows-2022\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:08:09.807000+00:00",
                "content": "BAML builds on all of those already! üôÇ\n\nWe don't currently ship any binary targets, although I'll be adding one for our internal use as part of the current work - we might be able to commit to supporting it"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:12:45.465000+00:00",
                "content": "that would be really handy bc it would make embedding BAML into applications much much easier"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:13:59.539000+00:00",
                "content": "It's still WIP - we're close, but there's still a bit of work we need to do to lock down the APIs - but you can TAL at\n\n- https://github.com/BoundaryML/baml/pull/908\n- `engine/baml-runtime` is the binary target\n- docs/docs/get-started/quickstart/openapi.mdx will have instructions for how to use it"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:14:19.983000+00:00",
                "content": "thank so much! we will play with it tomorrow"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:14:43.847000+00:00",
                "content": "we did recently put up some instructions for how you can use promptfiddle with ollama though: https://www.boundaryml.com/blog/ollama-structured-output"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:15:20.509000+00:00",
                "content": "(we had some entertaining - and surprisingly usable! - results with even phi3. haven't tried with phi3.5 yet.)"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:17:00.998000+00:00",
                "content": "that's amazing. we tested a bunch of them and we were coming up with potential ideas on how to make them fail less (aka at least work some times)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:18:40.365000+00:00",
                "content": "You're also welcome to just pull the crate in directly as a dependency (it's all MIT licensed), but the reason we're explicitly supporting the RESTful/OpenAPI route right now is because we care a lot about the DX of our users (aka you) and it's the most effective way for us to scale the surface area of what we support"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:20:29.872000+00:00",
                "content": "makes sense. we are also already embedding ollama and even node v22 in the shinkai app so communication through the RESTFul api also works for me"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:21:46.166000+00:00",
                "content": "Oh, if that's the case, then you won't need to pull in a custom build at all"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 06:22:15.669000+00:00",
                "content": "Our NPM build targets all the platforms you mentioned, and `baml-cli serve` will be available using the CLI we ship in the NPM package"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-08-29 06:24:18.716000+00:00",
                "content": "aha that's great. So the plan is going to be to go that route then. I will keep a close eye on that PR for the baml-cli serve üôÇ"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-29 16:21:11.871000+00:00",
                "content": "hi I am also interested in using BAML in rust and i am wondering what do you mean by embedding the REST api inside rust. is it spawning a thread to run the external command or simple starting the external api separately, be it within the same machine or not"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-29 17:02:40.310000+00:00",
                "content": "this is what the process model will be - you'll be able to run `baml-cli serve` any way you want: as a subprocess, as a separate Docker container, as a microservice, whichever way you want"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-03 19:09:40.297000+00:00",
                "content": "Just following up- we're getting closer and closer to the release! Expect it some time this week; we'll update this thread when we do.\n\nTo give you a sense of the current status, we're doing final passes now (e.g. reviewing generated code and making sure that we can preserve backwards compatibility for future releases).\n\nThings you can expect:\n\n- we've implemented OpenAPI codegen that can also run `openapi-generator-cli` for you automatically\n- we've implemented a server with hot reload that will re-generate everything when you edit BAML files\n- we've added not only quickstart docs, but also a `docker-compose` example ([preview](https://boundary-preview-1eabadb4-99c1-45a6-8131-2277b2373fe9.docs.buildwithfern.com/docs/get-started/deploying/openapi))\n- we've published example code for Go, Java, PHP, Rust at https://github.com/BoundaryML/baml-examples"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-09 05:35:14.747000+00:00",
                "content": "<@711679663746842796> hey sam. im getting this error creating a new baml project\n\n```\nnpx @boundaryml/baml init \\\n  --client-type rest/openapi --openapi-client-type rust\n\nerror: unexpected argument '--openapi-client-type' found\n\n  tip: a similar argument exists: '--client-type'\n\nUsage: baml-cli init <--dest <DEST>|--client-type <CLIENT_TYPE>>\n\nFor more information, try '--help'.\n```\nhttps://docs.boundaryml.com/docs/get-started/quickstart/openapi#create-a-new-baml-project"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-09 05:35:43.837000+00:00",
                "content": "i have the previous setup working with typescript but I wanted to try to make it work with Rust and the cli server"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-09 05:37:09.415000+00:00",
                "content": "maybe i need to wait for a new version of the npm package? https://www.npmjs.com/package/@boundaryml/baml?activeTab=versions"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 15:13:48.912000+00:00",
                "content": "Hi- yes, we haven‚Äôt released yet! Will update this thread when we do!"
            },
            {
                "author": "saiko9729",
                "timestamp": "2024-09-09 18:59:20.797000+00:00",
                "content": "Heeey <@711679663746842796> I was playing-around/reading-code about serve function. I think it's pretty useful.\n\nAs some feedback/questions, why serve needs the BAML files (throught the --from param) in place of being fully generic?\nLet's say, on every network request to the BAML Rest service, besides:\n- function (in the route param)\n- args (in the body)\n\nThe client (IE, typescript client) could send the BAML files to the endpoint so we could:\n- Creates a BAML Runtime dynamically\n- Execute the param parsing pipeline\n- Execute the function\n- Collect results\n- Send response\n\n\nI'm not sure about how inefficient could be stay instantiating a new BAML Runtime on every request but maybe it could be an improve over the first use case or even a completely different use case where we can share/scale the same instance of BAML Http Service throught different set of tools with no need to have them tied ü§î"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 19:00:28.622000+00:00",
                "content": "(let's start a new üßµ for this)"
            },
            {
                "author": "saiko9729",
                "timestamp": "2024-09-09 19:00:29.026000+00:00",
                "content": "For sure, this is something we can develop on our side but idk if this is what we expect... ending creating a custom BAML Client and a custom BAML Server ü§î"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 21:47:36+00:00",
                "content": "<@526965020031188994> BAML 0.55.0 is out, which means you can now try out BAML from Rust!\n\n- [announcement](https://discord.com/channels/1119368998161752075/1119375433666920530/1282818005084016721)\n- [quickstart docs](https://docs.boundaryml.com/docs/get-started/quickstart/openapi)\n- [baml-examples](https://github.com/BoundaryML/baml-examples/tree/main/rust-openapi-starter)\n\nExample code:\n\n```\n    {\n        let image = BamlImage::BamlImageUrl(BamlImageUrl {\n            url: \"https://i.redd.it/adzt4bz4llfc1.jpeg\".to_string(),\n            media_type: None,\n        }.into()).into();\n        let req = ExtractReceiptRequest {\n            receipt: ExtractReceiptRequestReceipt::BamlImage(image).into(),\n        };\n        let resp = b::extract_receipt(&config, req).await?;\n        dbg!(resp);\n    }\n```"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-09 21:49:14.177000+00:00",
                "content": "thanks <@711679663746842796> ! we are working with <@198831631602024450> with the other approach"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 21:49:44.599000+00:00",
                "content": "Ahh- I didn't realize you two were on the same team, this makes more sense now"
            }
        ]
    },
    {
        "thread_id": 1278679995883192381,
        "thread_name": "deno + BAML",
        "messages": [
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-29 11:37:47.744000+00:00",
                "content": "I am trying to install BAML via deno `deno add npm:@boundaryml/baml ` and curiously getting the error `error: Could not find npm package '@boundaryml/baml-win32-arm64-msvc' matching '0.54.0'.` but I am in mac"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 13:25:04.848000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 13:25:10.422000+00:00",
                "content": "does it work after that error? i think it should work regardless of that one error. We're wroking on patching it! (also not syet 100% sure if works on the deno runtime)"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-29 15:49:56.418000+00:00",
                "content": "i change to Bun runtime after the error and it works....it was for demo purpose and I am not js/ts person so i avoid using npm in the documentation just because i don't want to deal with typescript compiler"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 15:55:41.297000+00:00",
                "content": "Ah got it. As a non TS person myself, I also agree on the state of npm üòÖ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 15:55:55.969000+00:00",
                "content": "Zig and rust are so much better"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 15:56:14.304000+00:00",
                "content": "I‚Äôll figure out what it would take for us to officially support deno"
            }
        ]
    },
    {
        "thread_id": 1278756544271487119,
        "thread_name": "Custom transforms",
        "messages": [
            {
                "author": "cat_ethos",
                "timestamp": "2024-08-29 16:41:58.302000+00:00",
                "content": "for optional string field, sometime it returns `null` and another time `N/A`, maybe the parser can coerse those into one single output for better downstream processing"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 17:02:37.335000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-29 17:02:38.125000+00:00",
                "content": "That‚Äôs a cool idea. I‚Äôll share a doc soon for how we are thinking of generalizing these kinds of transformations in the future so you can control it even at an individual field level"
            }
        ]
    },
    {
        "thread_id": 1278856893078704169,
        "thread_name": "Panic",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-29 23:20:43.322000+00:00",
                "content": "i'm wondering what could throw this error:\n\nthread 'tokio-runtime-worker' panicked at baml-lib/jsonish/src/jsonish/parser/multi_json_parser.rs:33:56:\ncalled `Option::unwrap()` on a `None` value\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nProcessing Chunks:   0%|‚ñç                                                                                                                             | 2/625 [02:25<11:34:26, 66.88s/it]\n\n\n*************Error, Request failed: rust future panicked: unknown error"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 00:29:40.447000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 00:29:40.947000+00:00",
                "content": "What version of baml are you on?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-30 01:17:28.789000+00:00",
                "content": "for context, this was fixed in 0.54+. we had removed most `.unwraps` in the pipeline, but had missed one!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-30 01:18:10.316000+00:00",
                "content": "(it used to happen when the LLM produced some garbage text that started with a `]` or `}`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-30 01:18:22.273000+00:00",
                "content": "without any matching opening braces"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-05 00:35:00.239000+00:00",
                "content": "0.53.0"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 00:54:19.586000+00:00",
                "content": "FYI, we've got a patch that will make it in by 9 pm today that will patch this for anyone unable to update to 0.54+ (some linux users may be impacted)"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-05 00:58:17.360000+00:00",
                "content": "okay thanks"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 23:18:33.140000+00:00",
                "content": "<@1041386380732923964> were you able to get the latest?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-06 23:23:28.858000+00:00",
                "content": "yes"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-06 23:23:30.745000+00:00",
                "content": "thanks"
            }
        ]
    },
    {
        "thread_id": 1279803968288985150,
        "thread_name": "Nuxt 3 Error",
        "messages": [
            {
                "author": "maweill.",
                "timestamp": "2024-09-01 14:04:03.648000+00:00",
                "content": "Hi! In Nuxt 3 project I get ```X [ERROR] No loader is configured for \".node\" files: node_modules/.pnpm/@boundaryml+baml-win32-x64-msvc@0.54.0/node_modules/@boundaryml/baml-win32-x64-msvc/baml.win32-x64-msvc.node\n\nnode_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/native.js:96:23:\n      96 ‚îÇ         return require('@boundaryml/baml-win32-x64-msvc')``` \nHow can I solve this?"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-01 14:19:48.712000+00:00",
                "content": ""
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-01 14:19:54.268000+00:00",
                "content": "Fixed by adding ```vite: {\n    optimizeDeps: {\n      exclude: [\"@boundaryml/baml\"],\n    },\n  },```\nto `nuxt.config.ts` but now getting this error on page open: ```globals.ts:18 Uncaught SyntaxError: The requested module '/_nuxt/node_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/index.js?v=a5f5513e' does not provide an export named 'BamlCtxManager' (at globals.ts:18:10)```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-01 14:20:34.351000+00:00",
                "content": "This is an <@201399017161097216> question! Expect a response around PST 11am!"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-01 14:22:01.934000+00:00",
                "content": "Nuxt 3 Error"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 18:24:26.586000+00:00",
                "content": "<@329919568950591489>  Hi! It should work with this config:\n```\n// https://nuxt.com/docs/api/configuration/nuxt-config\nexport default defineNuxtConfig({\n  compatibilityDate: '2024-04-03',\n  devtools: { enabled: true },\n  vite: {\n    optimizeDeps: {\n      exclude: [\"@boundaryml/baml\"],\n    },\n    build: {\n      target: 'esnext',\n    },\n    ssr: {\n      noExternal: ['@boundaryml/baml'],\n    },\n  },\n})\n\n```\n\nLet me know where you're trying to deploy this -- I highly recommend deploying to your server and making sure everything is good there as well (not just in dev server)"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:19:10.349000+00:00",
                "content": "<@201399017161097216> Getting the same error unfortunately ```Uncaught SyntaxError: The requested module '/_nuxt/node_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/index.js?v=ada1cd1b' does not provide an export named 'BamlCtxManager' (at globals.ts:18:10)```\n\nI am trying to run it on dev.\nMy `nuxt.config.ts`: ```// https://nuxt.com/docs/api/configuration/nuxt-config\nexport default defineNuxtConfig({\n  compatibilityDate: \"2024-04-03\",\n  devtools: { enabled: true },\n  modules: [\n    \"@nuxt/ui\",\n    \"@nuxt/eslint\",\n    \"@pinia/nuxt\",\n    \"@formkit/auto-animate\",\n    \"@vueuse/nuxt\",\n    \"@nuxtjs/mdc\",\n  ],\n  plugins: [\"~/plugins/shiki.ts\"],\n  runtimeConfig: {\n    public: {\n      anthropicBaseUrl: process.env.NUXT_ANTHROPIC_BASE_URL,\n      anthropicApiKey: process.env.NUXT_ANTHROPIC_API_KEY,\n    },\n  },\n  ssr: false,\n  mdc: {\n    components: {\n      prose: true,\n    },\n    highlight: false,\n  },\n  vite: {\n    optimizeDeps: {\n      exclude: [\"@boundaryml/baml\"],\n    },\n    build: {\n      target: 'esnext',\n    },\n    ssr: {\n      noExternal: ['@boundaryml/baml'],\n    },\n  },\n});\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:20:36.267000+00:00",
                "content": "how do you call the baml function in your app?"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:21:53.095000+00:00",
                "content": "name-chat.baml: ```class Chat {\n    title string\n    icon string\n}\n\nfunction TitleChat(chat_content: string) -> Chat {\n    client Claude\n\n    prompt #\"\n        Given the following chat content, create a short, descriptive title (maximum 5 words) and choose a single appropriate emoji that represents the main theme or topic of the conversation. The title should be clear and informative to simplify user navigation between multiple chats.\n\n        Chat content:\n        ---\n        {{ chat_content }}\n        ---\n\n        {# special macro to print the output instructions. #}\n        {{ ctx.output_format }}\n\n        JSON:\n    \"#\n}```\n\nchatService.ts: ```import { useChatsStore } from \"~/stores/chatsStore\";\nimport { useMessagesStore } from \"~/stores/messagesStore\";\nimport type { ChatState } from \"~/types\";\nimport { b } from \"~/baml_client\";\n\nexport const useChatService = () => {\n  const chatsStore = useChatsStore();\n  const messagesStore = useMessagesStore();\n\n  const createChat = async ({\n    chat,\n    initialMessageContent,\n  }: {\n    chat: Omit<ChatState, \"id\" | \"title\">;\n    initialMessageContent: string;\n  }) => {\n    const bamlChat = await b.TitleChat(initialMessageContent);\n    const newChat = await chatsStore.addChat({\n      ...chat,\n      title: bamlChat.title,\n    });\n    return newChat;\n  };\n\n  const deleteChat = async (id: string) => {\n    await messagesStore.deleteMessagesForChat(id);\n    await chatsStore.deleteChat(id);\n  };\n\n  const renameChat = async (id: string, title: string) => {\n    await chatsStore.updateChat(id, { title });\n  };\n\n  const setActiveChat = (id: string | null) => {\n    chatsStore.setActiveChat(id);\n  };\n\n  return {\n    createChat,\n    deleteChat,\n    renameChat,\n    setActiveChat,\n  };\n};```"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:22:44.162000+00:00",
                "content": "using `    \"@boundaryml/baml\": \"^0.54.0\",` version"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:23:35.077000+00:00",
                "content": "ok I gotcha, the issue is that you can't call baml functions from your frontend"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:24:26.465000+00:00",
                "content": "oh wait maybe im reading this incorrectly, is this a react hook?"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:26:11.049000+00:00",
                "content": "just a lamda in vue. i am calling it on frontend, yes. It is not possible? Maybe some workaround?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:26:56.188000+00:00",
                "content": "here's my nuxt starter:\nhttps://github.com/BoundaryML/baml-examples/tree/main/nuxt-starter\nyou can create a server function:\nhttps://github.com/BoundaryML/baml-examples/blob/main/nuxt-starter/server/extract.get.ts\n\nthat you call in your vue script:\nhttps://github.com/BoundaryML/baml-examples/blob/main/nuxt-starter/app.vue#L3"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:28:15.073000+00:00",
                "content": "since the client also uses your API keys, and those probably are stored somewhere in your backend"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:28:21.427000+00:00",
                "content": "(on the server-side)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:28:52.719000+00:00",
                "content": "unless you're letting users put their API keys via a UI on the client side, then we'd need to give you a javascript-sdk"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:29:33.213000+00:00",
                "content": "I am making client-only app with setting API keys via a UI, yep"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:31:25.909000+00:00",
                "content": "do you want it to be all client-side? or would you be open to sending those api keys to your backend?"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:32:57.733000+00:00",
                "content": "the idea is to run all logic only on client-side"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:35:15.398000+00:00",
                "content": "we currently don't have a frontend-only SDK yet. We may be able to provide you with a web-assembly SDK that you can use to call functions that way, but it may be until the end of this week or the next. The alternative for now is to send API keys and call a function on your backend unfortunately."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:35:55.802000+00:00",
                "content": "the way our playground works is that we use web assembly to call the functions (but the interface to call the functions is more barebones and we dont have types etc)"
            },
            {
                "author": "maweill.",
                "timestamp": "2024-09-02 16:37:39.800000+00:00",
                "content": "ah, okay. Would love to get web-assembly SDK in the future. Thanks for help!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-02 16:38:18.893000+00:00",
                "content": "we'll definitely add it to our roadmap, and sorry about that, our docs should be more specific about Javascript / Browser compatibility and the fact that you do need Node to run it"
            }
        ]
    },
    {
        "thread_id": 1280393285768319038,
        "thread_name": "OpenAI generic error",
        "messages": [
            {
                "author": "demontrius",
                "timestamp": "2024-09-03 05:05:47.884000+00:00",
                "content": "Hi everyone. So I tried using baml on google colabs but got an error when running the code below:\n!baml-cli generate\n\nError generating clients Traceback (most recent call last): File \"/usr/local/bin/baml-cli\", line 8, in <module> sys.exit(invoke_runtime_cli()) baml_py.BamlError: error: client provider openai-generic not found. Did you mean one of these: `openai`, `azure-openai`, `anthropic`? --> ./baml_src/clients.baml:4 | 3 | client<llm> MyClient { 4 | provider openai-generic |\n\n#/content/baml_src/clients.baml\nclient<llm> MyClient { provider openai-generic options { base_url \"https://api.groq.com/openai/v1\" api_key env.GROQ_API_KEY model \"llama3-70b-8192\" } }"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 05:06:30.868000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 05:06:31.585000+00:00",
                "content": "Hi! Can you try with just OpenAI?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 05:06:37.090000+00:00",
                "content": "Lowercase*"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 05:07:12.612000+00:00",
                "content": "We‚Äôll hot fix openai-generic shortly! Seems we missed an edge case in the compiler"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 05:07:47.735000+00:00",
                "content": "Sorry, with groq, can you use ‚Äúollama‚Äù not openai."
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-03 11:56:50.034000+00:00",
                "content": "I don't have ollama but I can try with openai"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:45:04.038000+00:00",
                "content": "did this work?"
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-12 12:08:45.379000+00:00",
                "content": "Sorry for the late response. Got distracted with other projects"
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-12 12:09:41.049000+00:00",
                "content": "I haven't retried this... Will likely try using baml again but in vscode and not colabs when I get the time."
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-12 12:10:04.956000+00:00",
                "content": "Nice demo at the AI hackerspace last Friday üëå"
            }
        ]
    },
    {
        "thread_id": 1280642719479500860,
        "thread_name": "Prefilling w/ BAML",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-03 21:36:57.515000+00:00",
                "content": "Is it possible to do response prefilling with baml?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 21:37:19.085000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 21:37:19.301000+00:00",
                "content": "what do you mean by this?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-03 21:40:35.995000+00:00",
                "content": "Oh wait is it as simple as adding `{` after the ctx.schema?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 21:40:52.783000+00:00",
                "content": "oh, you can try that!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 21:41:01.083000+00:00",
                "content": "yes, that should work"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-03 21:41:03.159000+00:00",
                "content": "Cool ye lol might be just thay"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 21:41:23.862000+00:00",
                "content": "but i'm not sure if the parser will work in that scenario (it may, but it may not as well)."
            }
        ]
    },
    {
        "thread_id": 1280656901507383306,
        "thread_name": "organization",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-03 22:33:18.774000+00:00",
                "content": "What is the best way to manage many functions in a single file?\n\nI have a complex extraction that has 16 differ3nt schemas. The final output is a report that contains all that. Is my best wag of doing this a for loop over those? And how do people do this cleanly in code üòÄ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-03 22:56:37.153000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-03 22:56:37.579000+00:00",
                "content": "You can add subsets of your schema in other files if needed.\n\nI'm guessing your final output object just has a ton of fields? Or the function returns one out of many other types?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-03 22:57:11.998000+00:00",
                "content": "all types are available in any baml file (everything is globally scoped)"
            }
        ]
    },
    {
        "thread_id": 1280719952692641873,
        "thread_name": "Literals and descriminated unions",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 02:43:51.348000+00:00",
                "content": "hey folks -- is there any support for constant/literal types and/or discriminated unions?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 02:47:24.943000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 02:47:25.408000+00:00",
                "content": "(On my phone will reply with workaround when I‚Äôm at my desk)\nShort answer now, long answer you can do this with unions or types with an enum of one element"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 03:05:48.450000+00:00",
                "content": "thanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:49:07.346000+00:00",
                "content": "<@262014624122011648> would you also like anonymous / unnamed types like Typescript? As we are re-amping our typesystem, i'd actually love to get some thoughts around what you find to be some of the most powerful features here"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 18:42:55.692000+00:00",
                "content": "havent come up against the need for that yet, i dont having to name everything to be honest"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 18:43:32.476000+00:00",
                "content": "when workin with zod schemas for openai structured outputs i found that adding descriptions to all of them really helped with output quality, so if i am gonna add a description to just about every type i am fine adding a name too"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 18:44:30.325000+00:00",
                "content": "yep! one thing we recommend in general is don't add a description to every field FYI.\n\nits kinda like when you code, if a field name is self describing, don't add the same thing twice"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 19:39:22.523000+00:00",
                "content": "is that just to keep the token count low?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 19:39:31.374000+00:00",
                "content": "or cause it actually raises output quality"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 21:17:45.942000+00:00",
                "content": "its two fold. Effectively the LLM is attempting to understand information in your prompt to then produce some tokens that are nice.\n\nideally, you'd like that information to be compressed as much as possible into as few tokens as possible, so the network can quickly come to conclusion on what the best next token in.\n\nA practical example is the same reason we prefer slides with bullet points instead of paragraphs. When its in paragraph form, we need to do work to remove otherwise unnecessary tokens. Transform models benefit from the same context"
            }
        ]
    },
    {
        "thread_id": 1280725500372848701,
        "thread_name": "Optional arrays",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 03:05:54.018000+00:00",
                "content": "why can't array types be optional also?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:42:34.735000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:42:35.169000+00:00",
                "content": "tehcnically there's a workaround: `string[] | null`\n\nIn practice, during parsing, you'll almost always get an `[]` instead as we can infer that even if the array value isn't set."
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-04 18:43:44.482000+00:00",
                "content": "ah that makes sense"
            }
        ]
    },
    {
        "thread_id": 1280795466359115776,
        "thread_name": "openai structured output",
        "messages": [
            {
                "author": "foxicution",
                "timestamp": "2024-09-04 07:43:55.209000+00:00",
                "content": "Question about https://openai.com/index/introducing-structured-outputs-in-the-api/ and BAML. As far as I understand BAML constructs the prompt, but doesn't use the OpenAI specific \"response_format\" parameter, thus the claim in the article of 100% reliability does not apply to BAML, even when using in conjunction with ChatGPT. Is my assumption correct? Is this a planned feature?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:47:17.957000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:47:18.711000+00:00",
                "content": "yep! we don't do what openai does for a few reasons:\n\n1. openai uses a technique called constrained generation which reduces the accuracy of the model. While openai does provide a 100% gurantee of parseability, that does not mean 100% on correctness. \n\n2. openai's approach has many restrictions and doesn't allow for many schemas\n\nTo have a better intuition think of it this way:\n```\nLets start with just a hypothetical model phi420. Phi420 is completely nonsense and produces tokens randomly (its basically a rand(1, num tokens)). In this case, you can use a constrained generation technique like outlines does, and it will technically produce parseable JSON. The JSON still doesn't mean anything useful even if its valid and matches the schema.\n\nParseable != useful\n\nThe implication that the model is able to output something close enough to a schema that we are able to parse gives the confidence that the model is able to understand the task / inputs.\n\nMore practical example: you are parsing a resume object from a OCR'ed PDF. however a user uploads an invoice pdf. Constrained generation will still output a resume, but parsing will correctly raise an exception.\n```\n\nYou can read more here:\nhttps://www.boundaryml.com/blog/sota-function-calling?q=0\n\nYou'll notice that:\n* BAML + gpt-4o-mini we actually outperform Structured output + GPT-4o. \n* GPT-4o + structured output is actually worse than even GPT-4o + BAML\n\nDoes that help?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:47:55.294000+00:00",
                "content": "(That said we aren't against adding support for this officially, but we have seen so far most people havne't had parsing issues with BAML. If that changes we'd prioritize this differently)."
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-09-04 17:26:14.017000+00:00",
                "content": "Thanks <@99252724855496704> \nI've been also following this"
            }
        ]
    },
    {
        "thread_id": 1280823840150388747,
        "thread_name": "batching requests",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-04 09:36:40.048000+00:00",
                "content": "What's the best way to do batched runs atm? I can read docs if it's in there, was in flight üòÄ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:41:38.890000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-04 15:41:40.069000+00:00",
                "content": "we recommend doing `asyncio.gather`\n\nthen using semaphores to do max ratelimiting:\n\n```python\nimport asyncio\n\nasync def worker(semaphore, num):\n    async with semaphore:\n        print(f\"Worker {num} starting\")\n        await asyncio.sleep(2)  # Simulate some async work\n        print(f\"Worker {num} finished\")\n\nasync def main():\n    semaphore = asyncio.Semaphore(5)  # Limit to 5 tasks running in parallel\n\n    # Create a list of tasks\n    tasks = [worker(semaphore, i) for i in range(20)]\n\n    # Gather the tasks and run them\n    await asyncio.gather(*tasks)\n\n# Run the main function\nasyncio.run(main())\n```"
            }
        ]
    },
    {
        "thread_id": 1281208954462011393,
        "thread_name": "parsing validation",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-05 11:06:58.455000+00:00",
                "content": "Is parsing validation going to deal with parsing errors we get now (eg. Null value on non optional params)?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:41:48.370000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:41:48.837000+00:00",
                "content": "it can! We have two different kinds of validation:\n\n@assert - will raise an exception and not give any partial values\n@check - will give metadata for a field saying which assertions failed\n\n\n```ts\nclass FooBar {\n  foo int? @check(this != null, \"non_null\")\n}\n```\n\nthis will in python provide metadata such as this:\n\n```python\nres: FooBar\n\nres.foo.value: int | None\nres.foo.checks: [(\"non_null\", bool)]\n```"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-05 22:09:10.235000+00:00",
                "content": "Does this allow my parsing to still succeed though or do I need to smh predict where that can happen and make it optional?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-05 22:09:23.834000+00:00",
                "content": "In new use cases I end up making almost everything optional at first atm üòÑ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 22:43:51.252000+00:00",
                "content": "yes! so parsing will still work as long as you use @check instead of @assert.\n\n@assert raises ParsingExceptions\n\n@check gives you an error mesmage"
            }
        ]
    },
    {
        "thread_id": 1281327242621751316,
        "thread_name": "streaming: partial vs final response",
        "messages": [
            {
                "author": "ekp",
                "timestamp": "2024-09-05 18:57:00.550000+00:00",
                "content": "Hi! I just started testing BAML for a new date time parsing feature. Thanks for the great tool!\n\nI‚Äôm wondering if there‚Äôs any difference in accuracy between streaming vs without (practical or even theoretical).\n1. Will the final parsed response of a streamed function be identical to a function called without streaming?\n2. Once a KV pair is streamed, and assuming the stream isn‚Äôt later determined unparseable, is it safe to assume the KV pair is final? Or is it possible for some later tokens to alter previously parsed value in the final response?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 19:00:29.377000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 19:00:29.737000+00:00",
                "content": "There's always a possibility of it parsing differently in subsequent tokens:\n\nWhen we stream we return a `Partial<T>` but when its complete we return a `T` type.\n\nSo if your type is:\n\n```ts\nclass Foo {\n  bar: int\n  car: float\n}\n```\n\nduring streaming:\n```ts\n{ car: 0.1 }\n```\n\nwould work, but when you get the final response:\n```ts\n<error>\n```\n\nas your return type is a `Foo` type but a `Foo` requires both bar and car. However a `Partial<Foo>` can be just car.\n\nThis is then true recursively"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 19:01:46.685000+00:00",
                "content": "that means if youre return type is actually: \n```ts\nclass Wrapper {\n  foo?: Foo\n}\n```\n\nthen during streaming you could get:\n```ts\n{\n  foo: { car: 0.1 }\n}\n```\n\nbut finally you could get:\n```ts\n{ foo: null }\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 19:02:18.945000+00:00",
                "content": "does that help clarify the difference?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 19:02:58.729000+00:00",
                "content": "You can mitigate this difference by making each field optional in the actual class itself"
            }
        ]
    },
    {
        "thread_id": 1281368130123337748,
        "thread_name": "Instantiating BAML classes in python code",
        "messages": [
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-09-05 21:39:28.890000+00:00",
                "content": "Instantiating BAML classes in python code"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-09-05 21:39:34.779000+00:00",
                "content": "<@201399017161097216> <@99252724855496704> Hey Guys, After getting some of the basics of BAML going in my project, I am pushing my usage a bit further.  Am I able to instantiate the classes I declare in my BAML files outside of the BAML functions (e.g. in my regualr python code)?  I figured out the importing (from baml_client.types import ClassName), but I am getting an exception when I say:\n\nmyClass = ClassName()"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-05 21:40:01.763000+00:00",
                "content": "what kind of error do you get?"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-09-05 21:41:09.234000+00:00",
                "content": "I am trying to track it down - so far it is just an exception that gets trapped."
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-09-05 21:41:20.595000+00:00",
                "content": "Should I be able to do this?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-05 21:42:10.395000+00:00",
                "content": "yes you should be able to initialize classes like this:"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-05 21:42:32.586000+00:00",
                "content": "where `Message` was a BAML class, and `Role` was a BAML enum"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-05 21:43:10.076000+00:00",
                "content": "feel free to DM the class information, maybe I can try and reproduce -- does the exception happen everytime you instantiate?"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-09-06 22:04:35.286000+00:00",
                "content": "<@201399017161097216> quick follow-up question:  I have been using the optional operator '?' on the BAML class fields, though I seem to be having trouble using it on fields that are arrays, e.g.,  preferred_seats string[]?  or preferred_seats string?[]"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-06 22:05:25.051000+00:00",
                "content": "to get around that you can try \"string[] | null\". In general we opted for making arrays non-optional since we'll fill in an empty array if we didnt find anything for that field"
            },
            {
                "author": "noble_fawn_80154_44873",
                "timestamp": "2024-09-06 22:06:19.875000+00:00",
                "content": "got it - I'll tray thanks and have a great weekend."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-06 22:06:25.638000+00:00",
                "content": "likewise!"
            }
        ]
    },
    {
        "thread_id": 1281384050879107085,
        "thread_name": "can template strings be used across",
        "messages": [
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-05 22:42:44.694000+00:00",
                "content": "can template strings be used across files? like can i have a `message.baml` and use it in `classifier.baml`?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 22:45:18.626000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 22:45:18.977000+00:00",
                "content": "yes!"
            }
        ]
    },
    {
        "thread_id": 1281729759348523169,
        "thread_name": "test asserts",
        "messages": [
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-06 21:36:28.016000+00:00",
                "content": "does baml support assertion/expectations in tests? I only see test case execution"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-06 21:37:12.925000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-06 21:37:13.156000+00:00",
                "content": "Not yet! But we are working on it. Those will land in about 4 weeks"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-06 21:37:21.205000+00:00",
                "content": "alrighty"
            }
        ]
    },
    {
        "thread_id": 1281758612963987458,
        "thread_name": "is there a way to add a description of a",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-06 23:31:07.254000+00:00",
                "content": "is there a way to add a description of a field in another field:\n\ne.g.\nclass Node {\n  id string \n  type string\n  description string @description(\"if possible, provide a brief description of id\")\n}"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 01:13:24.216000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 01:13:25.191000+00:00",
                "content": "not yet, we almost will be able to once we release variables.\n\nyou want to do something like this right?\n\n```\nclass Node {\n  id string @description(\"if possible, provide a brief description of id\")\n  type string\n  description string @description(id.description)\n}\n```"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-07 18:07:45.042000+00:00",
                "content": "yes, something like that"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-07 18:49:27.458000+00:00",
                "content": "that'd be quite useful"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:06:57.579000+00:00",
                "content": "Perfect. We‚Äôre working on variables. But that‚Äôs likely about 2 months away."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:07:16.335000+00:00",
                "content": "For now sadly copy and paste"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-09 16:45:14.065000+00:00",
                "content": "looking forward to it!"
            }
        ]
    },
    {
        "thread_id": 1282044352826314875,
        "thread_name": "Classes in test cases",
        "messages": [
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 18:26:32.946000+00:00",
                "content": "Hey everyone, how do you add mock data to a class when passing it in a test?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:05:55.058000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:05:55.735000+00:00",
                "content": "<@711679663746842796> can you share an example"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 20:30:55.105000+00:00",
                "content": "Howdy! Here's one of the examples we have over [in our docs](https://docs.boundaryml.com/docs/snippets/test-cases):\n\n```baml\nclass Message {\n  user string\n  content string\n}\n\nfunction ClassifyMessage(messages: Messages[]) -> Category {\n...\n}\n\ntest Test1 {\n  functions [ClassifyMessage]\n  args {\n    messages [\n      {\n        user \"hey there\"\n        content #\"\n          You can also add a multi-line\n          string with the hashtags\n          Instead of ugly json with \\n\n        \"#\n      }\n    ]\n  }\n}\n```"
            }
        ]
    },
    {
        "thread_id": 1282052083176181863,
        "thread_name": "Pydantic description",
        "messages": [
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 18:57:16.005000+00:00",
                "content": "I also noticed that the description I add to the BAML code doesn't carry over to the generated Python types in Pydantic."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:45:06.915000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:45:07.314000+00:00",
                "content": "Hey Nazim, do you mind explaining what you want to use the descriptions for to understand your usecase?"
            },
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 19:47:08.697000+00:00",
                "content": "I noticed that using descriptions with Pydantic improves my results, so I added descriptions to most of the BAML classes I created. However, when I checked the Python types file, I realized that none of the description data was generated in Python."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:47:39.079000+00:00",
                "content": "ah, so when you add descriptions we still add them to the prompt :). If you open up the Playground in VSCode, you'll see the full prompt with the descriptions inlined"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:48:06.113000+00:00",
                "content": "All this information is sent to our parser + clients for you -- we don't use pydantic for this. The pydantic class is just used to store the final data"
            },
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 19:48:26.369000+00:00",
                "content": "Got it, that gives me some peace of mind haha"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:48:55.142000+00:00",
                "content": "have you clicked on this button -- \"open playground\" ?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-07 19:49:26.904000+00:00",
                "content": "(from within a BAML file)"
            }
        ]
    },
    {
        "thread_id": 1282092946673631293,
        "thread_name": "Does the BAML parser remove \\n from the response?",
        "messages": [
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 21:39:38.622000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 21:42:27.168000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 21:42:27.295000+00:00",
                "content": "Taking a look..."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 21:47:56.344000+00:00",
                "content": "OK, it looks like we do not:"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 21:48:15.036000+00:00",
                "content": "We _do_ have a rendering bug in the playground here, and I totally get why this would be concerning!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 21:49:33.831000+00:00",
                "content": "Filed https://github.com/BoundaryML/baml/issues/928 for us to go and fix this"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-07 21:50:12.132000+00:00",
                "content": "In the meantime, you can also set `BAML_LOG=info` while running your app to see what I screenshotted above"
            },
            {
                "author": "nazimgirach",
                "timestamp": "2024-09-07 22:20:44.935000+00:00",
                "content": "I can confirm it works with python. Looks like a VS code extension bug."
            }
        ]
    },
    {
        "thread_id": 1282523040026198089,
        "thread_name": "Weird bug",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:08:40.868000+00:00",
                "content": "Also, the playground is now gone. Wonder if I did smth to my vscode setup smh"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:09:37.406000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:09:37.868000+00:00",
                "content": "Hi gabriel, can you do a reload on vscode?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:10:02.770000+00:00",
                "content": "Tried one, let me try again"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:10:48.258000+00:00",
                "content": "Also are any baml files red in the side panel? There‚Äôs a problems tab you can see."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:11:06.861000+00:00",
                "content": "Alternatively, are you able to run baml-cli generate from terminal"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:11:25.501000+00:00",
                "content": "That may help find any syntax errors easier"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:11:55.607000+00:00",
                "content": "No red files. These were working last time I was in this project"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:12:19.352000+00:00",
                "content": "Cli says error generating clients at invoke_runtjme_cli()"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:12:31.336000+00:00",
                "content": "Oh! What‚Äôs the error it gives?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:12:46.364000+00:00",
                "content": "Weirdly playground says no function available when next to functions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:13:09.100000+00:00",
                "content": "Yea. There‚Äôs likely some syntax error that is causing BAML not to compile"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:13:09.772000+00:00",
                "content": "baml_py.BamlError"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:13:17.340000+00:00",
                "content": "I guess üôÇ there is no other trace"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:13:20.838000+00:00",
                "content": "Mind hopping on office hours?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:13:30.568000+00:00",
                "content": "Sure one sec"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:44:23.349000+00:00",
                "content": "Just noticed code is smh now structured with spaces vs tabs in one file. Wonder if that could do it?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-09 02:44:43.093000+00:00",
                "content": "It actually works though even with spaces in other files I think"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:50:13.570000+00:00",
                "content": "Oh damn"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:50:15.248000+00:00",
                "content": "Yes"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:50:18.606000+00:00",
                "content": "That will do it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:50:27.373000+00:00",
                "content": "I think we require spaces"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:50:31.307000+00:00",
                "content": "Not tabs"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 02:50:37.836000+00:00",
                "content": "ü§¶‚Äç‚ôÇÔ∏è"
            }
        ]
    },
    {
        "thread_id": 1282712552941289543,
        "thread_name": "azure",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 14:41:44.268000+00:00",
                "content": "Hey guys, quick question:\nHow can I modify the azure-openai provider to use the government cloud? All that's needed is to change the endpoint from \n`<base>.openai.azure.com/` to `<base>.openai.azure.us/`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 14:48:08.905000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 14:48:09.347000+00:00",
                "content": "https://docs.boundaryml.com/docs/snippets/clients/providers/azure\n\nI thinik you can just set base_url"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 14:50:20.803000+00:00",
                "content": "Is there a syntax for reading env variables in that base URL?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 14:50:30.442000+00:00",
                "content": "actually i can just make a net new env for base url"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 14:51:58.035000+00:00",
                "content": "sadly not yet, we're working on that still!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 15:00:05.810000+00:00",
                "content": "Where would I find the deployment ID? I can see the deployment name though not sure if that's the same in Azure"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 15:01:18.132000+00:00",
                "content": "Based on the azure playground code, i can see the base URL appears to be the correct one, can share over DM if that's easier"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 16:05:43.116000+00:00",
                "content": "Did you find it out?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 16:11:52.999000+00:00",
                "content": "yes!"
            }
        ]
    },
    {
        "thread_id": 1282713630969958441,
        "thread_name": "other questions",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 14:46:01.290000+00:00",
                "content": "Follow up questions:\n1. Is https://app.boundaryml.com still the correct dashboard URL? \n2. Running a test case in my BAML Playground is stalling, not sure how to see the what's going on"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 14:48:52.931000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 14:48:53.465000+00:00",
                "content": "1. yes! is there any issue\n2. click on all tests, then back to test results"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-09 14:49:32.621000+00:00",
                "content": "weird... it was recursively loading for me but now it's good"
            }
        ]
    },
    {
        "thread_id": 1282777791573786657,
        "thread_name": "baml-over-http design hypotheticals",
        "messages": [
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 19:00:58.370000+00:00",
                "content": "<@198831631602024450> re your question:\n\n> Heeey <@711679663746842796> I was playing-around/reading-code about serve function. I think it's pretty useful.\n> \n> As some feedback/questions, why serve needs the BAML files (throught the --from param) in place of being fully generic?\n> Let's say, on every network request to the BAML Rest service, besides:\n> - function (in the route param)\n> - args (in the body)\n> \n> The client (IE, typescript client) could send the BAML files to the endpoint so we could:\n> - Creates a BAML Runtime dynamically\n> - Execute the param parsing pipeline\n> - Execute the function\n> - Collect results\n> - Send response\n> \n> \n> I'm not sure about how inefficient could be stay instantiating a new BAML Runtime on every request but maybe it could be an improve over the first use case or even a completely different use case where we can share/scale the same instance of BAML Http Service throught different set of tools with no need to have them tied ü§î"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 19:03:10.804000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 19:03:11.255000+00:00",
                "content": "It's an interesting idea!\n\nMy initial reaction is that while we can definitely support this in the future, I don't know if I would want this to be the default: it would impose a lot of burden on the user for packaging their `baml_src/` into their own applications, though, which can get pretty frustrating if, say, you're in an AWS Lambda"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 19:04:13.159000+00:00",
                "content": "Do you have a use case in mind that's driving this request?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 19:04:34.131000+00:00",
                "content": "Would your users write BAML schemas for you? We've done something like this on TypeScript and Python. It is doable to create a baml runtime for languages where we have SDKs already (Python, Typescript) and execute things that way. The generated code is just a wrapper around that"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 19:06:07.800000+00:00",
                "content": "if you want a small example on how to do this clearly in typescript or python, let me know, but this stuff is very different from the `baml serve` feature, which creates a custom server with REST endpoints for each of your baml functions"
            },
            {
                "author": "saiko9729",
                "timestamp": "2024-09-09 19:15:11.874000+00:00",
                "content": "This is my current use case:\n\n- Our team and a community write \"Tools\" using typescript (think about them as functions)\n- Every tool is bundled as a single file\n- Tools are distributed and then executed by our app (multiplatform and client side)\n\nWe created a tool that uses baml but when we tried to bundle it as a single file we found that baml requires some .node files per platform that could be hard to bundle because it means that every tool that use baml will get some additional megabytes.\n\nThen we found the PR about Serve which is really cool because we can have baml as a side-executor, distributed for every platform and working as a HTTP API but is not exactly what we need because the \"baml executor\" is tied to the tools it can run..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 19:24:28.849000+00:00",
                "content": "This sounds really interesting! Why don't we grab some time to chat about this later this week.\n\nwe've got a calendly up and running on our web site (book a time with a founder button at the bottom)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 19:24:43.873000+00:00",
                "content": "want to meet sometime this week and discuss in more details"
            }
        ]
    },
    {
        "thread_id": 1283094057828089907,
        "thread_name": "baml-cli broken",
        "messages": [
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 15:57:42.121000+00:00",
                "content": "hey guys, just a heads up - baml-cli is broken on 0.55.0 version release, but 0.54 works though!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 16:00:05.271000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 16:00:05.662000+00:00",
                "content": "Thanks for letting us know, will look asap. Are you using Python or NPM, and is it a macbook arm or something else?"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 16:00:39.495000+00:00",
                "content": "Thanks for the quick response! I am using python"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 16:01:37.081000+00:00",
                "content": "its a windows pc with intel i5 x86"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 16:02:20.380000+00:00",
                "content": "```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\Users\\<my_name>\\.virtualenvs\\grammarcheck-SxcXB2v8\\Scripts\\baml-cli.exe\\__main__.py\", line 4, in <module>\n```"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 16:02:27.018000+00:00",
                "content": "the exact error that I get"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 16:15:23.661000+00:00",
                "content": "ah i see it, thanks"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 16:15:55.121000+00:00",
                "content": "happy to help :)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 16:43:19.738000+00:00",
                "content": "Fix is in and will be out within the hour- thanks for letting us know!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 18:16:33.770000+00:00",
                "content": "Took longer than anticipated - ran into a hiccup along the way - but fix is out now. Let me know if you still have any issues!"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-10 18:19:55.759000+00:00",
                "content": "It works great now! good work :)"
            }
        ]
    },
    {
        "thread_id": 1283256668393701419,
        "thread_name": "Baml raw request",
        "messages": [
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 02:43:51.501000+00:00",
                "content": "is there a way to get the token usage info besides logging?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 03:26:41.269000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 03:26:42.007000+00:00",
                "content": "Hi there, mind sharing more about your usecase? We‚Äôd love to learn what you use this metadata for. Would you say this is a blocker for using BAML longterm or just a desired feature?"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 03:31:16.409000+00:00",
                "content": "we are running workloads from clients and would like to make sure fair usage, so need to track how many tokens each clients are using"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 03:31:46.471000+00:00",
                "content": "currently i think we can use the oneventlog callback"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 03:32:09.730000+00:00",
                "content": "would like a easier solution in the long term"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 03:33:58.066000+00:00",
                "content": "Gotcha, we are planning or roadmap soon so we will update you on this. Unfortunately that callback doesn‚Äôt yet expose the number of tokens though it d be easy to add them there"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 03:35:14.162000+00:00",
                "content": "yeah now i am getting the raw output and prompt and use the tiktoken to get the token counts"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 03:35:32.221000+00:00",
                "content": "Ahh, smart üôÇ"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 03:35:56.197000+00:00",
                "content": "looking forward to your future roadmap"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 03:36:40.632000+00:00",
                "content": "Any other features or things youd like to see improved at the moment? Could be anything like the developer experience etc"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-11 03:40:48.287000+00:00",
                "content": "can't think of any now but sure i will share them whenever i have some ideas, so far the openapi server feature is really nice"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 13:50:12.474000+00:00",
                "content": "btw <@635507634342068228> question question re: token usage. BAML also offers provider strategies like retry_policy and fallback. In that scenario, how would you want token usage to be filled out? Would you want all of requests made and their token usage? Or would you want only the last successful request and its token usage?"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-16 14:41:28.416000+00:00",
                "content": "in my case, i want token usage of all the requests. because by looking at the total usage, i can better tweak the what providers and how the retry strategy should be"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 14:42:01.217000+00:00",
                "content": "got it! Is this for real-time decision making? Or more so for analysis btw?"
            },
            {
                "author": "cat_ethos",
                "timestamp": "2024-09-16 14:42:40.134000+00:00",
                "content": "analysis for now"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 14:42:50.390000+00:00",
                "content": "üëçüèæ"
            }
        ]
    },
    {
        "thread_id": 1283424410367299645,
        "thread_name": "Rate limits",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 13:50:24.304000+00:00",
                "content": "how does BAML manage LLM provider rate limits if at all?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 13:53:33.197000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 13:53:33.510000+00:00",
                "content": "We don‚Äôt yet at the moment. Is there a particular implementation you liked from another framework or library? \n\nThe issue is that for many users in serverless contexts or multi-container contexts, per-process rate limiting tends to not be as useful."
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 13:56:25.945000+00:00",
                "content": "agreed with that for sure -- i more mean that if BAML is doing its own retries internally to manage schema violations, and the first couple requests work but the third is rate limited, that will throw and the message chain is just lost right?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 14:10:55.295000+00:00",
                "content": "What do you mean by ‚Äúrequests‚Äù? If the http request to the llm api succeeds and the response is valid we always return it. If the http request succeeds but it fails to parse we will retry according to the retry policy or the fallback config"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:15:05.842000+00:00",
                "content": "yeah, thats what i mean, if it fails to parse. i was under the impression that thats why BAML makes its own HTTP requests -- so that it can implement niceities like retries and whatnot"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:15:45.084000+00:00",
                "content": "are the retries just \"run the request again\" or is it \"produce a message that explains the schema errors, add it to the list of messages, and run that\"? i've found that the latter works a lot better than the former"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 18:50:40.620000+00:00",
                "content": "we don't retry with a modified prompt at the moment -- we just retry again with the same request\n\nWe are tracking this since I believe that we can implement this as well, and I agree it can help fix more complex mistakes that perhaps keep popping up even with normal retries. https://github.com/BoundaryML/baml/issues/840"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 18:57:35.994000+00:00",
                "content": "For now the workaround is to just write a \"FixMyPrompt(..)\" baml function that can do this for you. Once we implement this more natively we can help do this with a one-liner or some configuration."
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 19:49:59.704000+00:00",
                "content": "how would one get the messages that have already been exchanged in order to do that?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 19:50:51.733000+00:00",
                "content": "My bad, yes you would need the full prompt. We have a festure we will release to give you all the request metadata"
            }
        ]
    },
    {
        "thread_id": 1283644524761055294,
        "thread_name": "optional list",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-12 04:25:03.665000+00:00",
                "content": "Any way to assign a list of types as optional?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 04:52:43.386000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 04:52:44.193000+00:00",
                "content": "not currently, but we can make that happen!\n\nthe reason is that in the parser‚Äôs current error-correcting behavior, an unset parameter is coerced into an empty list"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-12 08:39:17.331000+00:00",
                "content": "Sounds good, would be nice if possible"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-12 08:39:42.148000+00:00",
                "content": "Also, not sure if another pattern is better, like do idk making a list of optional variables smh?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 16:54:51.971000+00:00",
                "content": "you're not the only one who's asked for this - I expect we'll add this soon!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 16:59:17.580000+00:00",
                "content": "Filed https://github.com/BoundaryML/baml/issues/948, we'll update you if/when we get around to this!"
            }
        ]
    },
    {
        "thread_id": 1283938452714553496,
        "thread_name": "conditional logic inside schemas",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-12 23:53:01.550000+00:00",
                "content": "We discussed at some point about conditional logic inside the schemas, is that close?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 23:54:39.801000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 23:54:40.045000+00:00",
                "content": "Still a few weeks out, sadly - we want it as much as you do though!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 23:58:38.296000+00:00",
                "content": "Happy to help figure out a workaround for you if you need something today though - perhaps [dynamic types](https://docs.boundaryml.com/docs/calling-baml/dynamic-types)?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-13 00:24:54.874000+00:00",
                "content": "No worries! I just did an in context thing and hope for the best üòÄ"
            }
        ]
    },
    {
        "thread_id": 1284534046508191894,
        "thread_name": "Inconsistencies in responeses",
        "messages": [
            {
                "author": "charizard_98",
                "timestamp": "2024-09-14 15:19:42.176000+00:00",
                "content": "I'm at a loss here:\n\nIn the playground, I get the intended results under lookup_list.\n\nBut when I call the same function with the same user prompt from my application, I don't get both the items in the list\n\nDo you guys know if there is a way to keep it consistent?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:20:50.819000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:20:51.084000+00:00",
                "content": "can you check what the temperature is? openai sets it to 1 by default, so you'll need to explicitly set it to 0."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:21:01.285000+00:00",
                "content": "that is likely impacting this"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-14 15:21:40.773000+00:00",
                "content": "That was going to be my next question üòÖ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:21:50.403000+00:00",
                "content": "we should document that üôÇ"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-14 15:22:37.550000+00:00",
                "content": "Do you know how I can set the temperature in BAML? I wasn't able to find it in the docs."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:23:48.924000+00:00",
                "content": "```\nclient<llm> Foo {\n  provider openai\n  options {\n    temperature 0.0\n  }\n}\n```"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-14 15:25:38.569000+00:00",
                "content": "Ah okay. I had not used that syntax before. I was just using `client \"openai/gpt-4o\"` till now. Thanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:26:20.310000+00:00",
                "content": "got it! Yep, this is our more explicit client specification"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-14 15:26:45.949000+00:00",
                "content": "then you'd do the following:\n\n```\nfunction DoSomething(...) -> SomeType {\n   client Foo\n   prompt #\"...\"#\n}\n```"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 04:13:42.063000+00:00",
                "content": "Hey <@99252724855496704> . I'm still getting the same thing after changing the temperature. Do you have any other ideas why this might happen?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:14:31.257000+00:00",
                "content": "I‚Äôm out at dinner! Let me share some ideas when I‚Äôm back. Can you share with me a min repro on prompt fiddle?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:14:38.220000+00:00",
                "content": "(There‚Äôs a share link)"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 04:14:54.287000+00:00",
                "content": "Sure! Thanks."
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 04:19:37.858000+00:00",
                "content": "https://www.promptfiddle.com/MDA-example---Hari-DbHWI"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 04:28:35.592000+00:00",
                "content": "Just for more context. \nIn this prompt, \n\nRecord a sale of 10 chairs to Jordan LLC on 10th this month, paid in cash\n\nwhen I change Jordan LLC to Hari, I'm getting the same output in both BAML playground and go. That is why I'm really confused."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:12:06.108000+00:00",
                "content": "odd, it works ok for me. Things to try: can you check what version of BAML you have in python vs in the playground"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:12:11.620000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:12:29.129000+00:00",
                "content": "in python it'll be `baml-cli --version`"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 05:28:54.433000+00:00",
                "content": "I'm using go. What do I do then?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:29:38.412000+00:00",
                "content": "oh in go, its the same thing: can you check what version of `baml-cli` you have?"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 05:37:34.030000+00:00",
                "content": ""
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 05:39:20.985000+00:00",
                "content": "My VSCode runtime is 0.55.3 as well"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 05:50:36.857000+00:00",
                "content": "This is weird because I changed chairs to Chairs and it is giving me the same results in go and the playground. For some customer names, it gives the same result and for some it changes. Do you know if there is any way to control this?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:51:24.232000+00:00",
                "content": "do you have BAML_LOG enabled?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:51:36.892000+00:00",
                "content": "i'm wondering if its the LLM messing up or our parser"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 05:51:45.659000+00:00",
                "content": "I don't think so."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:51:56.723000+00:00",
                "content": "export BAML_LOG=info"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:52:05.226000+00:00",
                "content": "then run `baml-cli dev"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 06:03:57.258000+00:00",
                "content": "Is there anything specfic you are looking for in the log?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:05:41.377000+00:00",
                "content": "i would just look to see if the RAW Response is different than the Parsed Response"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:09:39.991000+00:00",
                "content": "have you tried our observability dashbaord"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:09:59.573000+00:00",
                "content": "you can sign up over at: app.boundaryml.com"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:10:07.406000+00:00",
                "content": "then i think it'll be a bit easier to debug"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:14:40.580000+00:00",
                "content": "I can hop on a quick call and check it out with you if that would be easier"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:22:19.409000+00:00",
                "content": "just shoot me over a calendar invite for a time that works for you! vbv@boundaryml.com and I'll be glad to help debug this"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 23:18:11.120000+00:00",
                "content": "Sure! Not sure if you are available in the weekend. If not, we can meet anytime on Monday."
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 23:45:17.556000+00:00",
                "content": "I don't think the parser is messing up. I am able to see the LLM REPLY and Parsed Response in the logs now."
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 23:45:41.564000+00:00",
                "content": ""
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-20 23:50:02.537000+00:00",
                "content": "This is from the playground:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 05:38:41.091000+00:00",
                "content": "Hmm but the python code has or doesn‚Äôt have this issue?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 23:05:19.382000+00:00",
                "content": "<@410093421420871680> did you find out if this was just happening in the playground or the parser as well?"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-23 23:39:12.981000+00:00",
                "content": "I tried switching to 4o from 4o-mini and 4o did much better I think. By parser do you mean when I call from the function from go?"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-23 23:41:42.427000+00:00",
                "content": "I don't think there is any problem with the parser. It must be the LLM or my system prompt. I was just confused why I'd get different answers when I call the BAML function from go vs the playground."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-24 03:34:27.003000+00:00",
                "content": "ah got it. I think its likely the prompt sadly. models are pretty inconsistent sadly, even on temperature 0 üò¶"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-24 03:34:40.099000+00:00",
                "content": "(i can do a smaller write up about why in a bit)"
            }
        ]
    },
    {
        "thread_id": 1285148836066754634,
        "thread_name": "Hi guys, I love the project, it's really",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-16 08:02:39.429000+00:00",
                "content": "Hi guys, I love the project, it's really awesome. I havea question, is therea way to access the raw response from the model?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 13:36:31.997000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 13:36:34.688000+00:00",
                "content": "Thanks <@619130668567494656> , this is similar to the request we've had about tokens and we're still designing the right interface here. Out of curiosity, whats your use case of this data?\n\nThe reason i ask is that w/ BAML its a bit tricky to just give the raw response due to our offerings of provider stratigies like: `retry_policy` and `fallback` ."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 13:37:11.912000+00:00",
                "content": "e.g. if we had to try 3 different models to get the response, would you prefer that you only get the final HTTP resposne? Or all 3?"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-16 21:03:10.013000+00:00",
                "content": "That's a very good question. My principal use case for this request is fine-tuning. I'm finding BAML really amazing for structuring output and doing prompt engineering that is portable, fast, iterable, and easy to test. I'm trying to fine-tune using the thinking process of the model and its reflection. For this, I would need both the raw output and the structured output.\n\nRegarding your question, I wasn't thinking that deeply, but for example, with Instructor you get the final output (after revalidation and retrying) and also the sum of the total tokens that were expended between iterations. I guess this would fulfill the previous request, and also my purposes since I want the final output."
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-16 21:07:27.316000+00:00",
                "content": "(With instructor you can get this overwriting the class, but since Baml is compiled in rust and works really different, is not doable as my understanding)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-16 22:06:59.464000+00:00",
                "content": "Thanks for the input! We are planning our roadmap very soon so this feedback is super useful."
            }
        ]
    },
    {
        "thread_id": 1285358631415447595,
        "thread_name": "getting starting",
        "messages": [
            {
                "author": "demontrius",
                "timestamp": "2024-09-16 21:56:18.538000+00:00",
                "content": "Hi guys.... Been reading the docs and still not sure how to use BAML on my own. Is there some starter video? Tried following the doc but not sure I am getting the sequence"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 21:57:05.576000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 21:57:06.032000+00:00",
                "content": "Hey <@865664087345725508> appreciate that note. We should make a video for this.\n\nBut what language are you using?"
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-16 21:57:41.518000+00:00",
                "content": "Python"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 21:57:56.934000+00:00",
                "content": "awesome, let me record a quick python video and send it to you!"
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-16 21:58:15.777000+00:00",
                "content": "Much appreciated"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 22:56:34.484000+00:00",
                "content": "https://www.youtube.com/watch?v=MITj2ukpB-s"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 22:56:38.861000+00:00",
                "content": "hope its good!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 22:56:48.354000+00:00",
                "content": "I alos linked the getting started guide FYI"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 22:59:05.302000+00:00",
                "content": "just make sure you do: `pip install baml-py`"
            },
            {
                "author": "demontrius",
                "timestamp": "2024-09-16 23:32:20.127000+00:00",
                "content": "Thanks.... Will get on this in a bit"
            }
        ]
    },
    {
        "thread_id": 1285388973761626122,
        "thread_name": "langgraph flow -> BAML",
        "messages": [
            {
                "author": "faizansattar",
                "timestamp": "2024-09-16 23:56:52.717000+00:00",
                "content": "Hey team! Do you have any recommendations on how to convert a langgraph flow to BAML?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 23:57:24.498000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 23:57:24.820000+00:00",
                "content": "can you post some sample code? I can convert it for you! üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-16 23:57:50.390000+00:00",
                "content": "I'll also then make some docs about this ü§£"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-17 00:07:21.451000+00:00",
                "content": "Here you go: https://gist.github.com/fsattar/17dc476650636e7aff7de73a46587cd0\n\nScrubbed away some of our internal stuff but this is roughly what we are trying to do. Its a ReAct agent.\n\nWe want to migrate to BAML so we can stream back a structured UI response. I can add that after you convert it as another tool call."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:13:59.139000+00:00",
                "content": "perfect, I'll go ahead and take a look tonight (at an event tonight)"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-17 01:20:47.382000+00:00",
                "content": "cool - I might also take a stab at it, will keep you posted if I end up coming up with a solution"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-17 01:21:02.491000+00:00",
                "content": "just trying to figure out the best alternativ ReAct framework"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-17 01:23:47.321000+00:00",
                "content": "Thinking of just using something like this: https://til.simonwillison.net/llms/python-react-pattern"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-17 03:47:29.323000+00:00",
                "content": "<@99252724855496704> Its a bit unclear to me how best to structure the baml dsl for the Agent. I'll wait to hear back from you pausing for now."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 06:32:28.354000+00:00",
                "content": "ok back! so finally take a look athe code and I'll send you some scripts that do this."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 13:59:31.248000+00:00",
                "content": "ok sorry I passed out last night. I see why this is tricky! But i've got a somewhat working thing.\n\nWhat I'm working on is putting a full example togehter so you can see it"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-17 14:50:14.988000+00:00",
                "content": "<@99252724855496704> I got something working as well. Will share over DM shortly."
            }
        ]
    },
    {
        "thread_id": 1285447761034084402,
        "thread_name": "baml via wasm",
        "messages": [
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 03:50:28.695000+00:00",
                "content": "if we are using the wasm lib to generate the schemas, how difficult it would be to extend it so it also can run code using the runtime? it seems that it's already doing it for some test cases. I saw that's not quite there right now but wondering how much extra work it could be and if someone like me (external) could be able to do it"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 03:54:07.442000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 03:54:07.993000+00:00",
                "content": "Hmm- can you elaborate on your use case?\n\nIt‚Äôs entirely doable; the challenge for us is primarily maintaining a stable API contract there and providing a good developer experience. There‚Äôs a lot of little details that get annoying and tricky to reason about in the wasm interface today."
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 03:55:25.814000+00:00",
                "content": "it's so much easier to have something crossplatform that can take dynamic (as in on-demand baml schemas) using wasm"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 04:56:23.522000+00:00",
                "content": "made it work!"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 04:56:34.022000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 04:59:57.913000+00:00",
                "content": "haha, amazing!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 05:04:46.084000+00:00",
                "content": "fair warning: we have no stability guarantees on the wasm interface right now, and we‚Äôll definitely look into providing them but right now it‚Äôs very much ‚Äúat your own risk‚Äù"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 05:05:34.620000+00:00",
                "content": "yeah no prob! i've been warned"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-17 15:59:52.440000+00:00",
                "content": "btw did you build the wasm lib yourself or did you grab the version published from npm?"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 16:41:55.222000+00:00",
                "content": "i modified the wasm code, so i had to build it myself"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 16:42:09.886000+00:00",
                "content": "it didn't work on my mac so i switched to linux"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-17 17:32:19.289000+00:00",
                "content": "gotcha, niice"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 02:22:16.735000+00:00",
                "content": "<@711679663746842796> <@201399017161097216> if you guys think it could be helpful i can create 3 public repos with examples for typescript + ollama (with that specific conf for some models), one for wasm and another one for rust.  it is always good to have more examples but the downside is that it could make people try to use wasm or rust and it is not really intended to be the use case right now."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-18 02:28:51.600000+00:00",
                "content": "Im down to have more reference examples! We can add disclaimers. And we can link to them if necessary"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-18 02:29:11.034000+00:00",
                "content": "We have baml-examples repo so we might add the ollama one there"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 02:46:29.495000+00:00",
                "content": "great! i will create the examples in the next couple of days and share them with you. for the ollama one i will create a small pr"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 02:47:13.167000+00:00",
                "content": "i will make it different than the other one so it's also helpful with another stuck, so it will be more barebones using typescript and node without extra frameworks like nextjs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-18 02:57:41.645000+00:00",
                "content": "Sounds perfect! Thanks a bunch"
            }
        ]
    },
    {
        "thread_id": 1285448584397066414,
        "thread_name": "dynamic descriptions",
        "messages": [
            {
                "author": "sidd065",
                "timestamp": "2024-09-17 03:53:45+00:00",
                "content": "Hey guys, Is it possible to change the value of the description of a property inside a class via python?\nExample:\n```class Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n  extra_data string @description(\"Description for what extra data to extract, passed as a variable\")\n}```\nHere could I set the description of `extra_data` dynamically using a function call or some other way?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 03:55:44.827000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 03:55:45.168000+00:00",
                "content": "Yes it is! https://docs.boundaryml.com/docs/calling-baml/dynamic-types"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-17 03:56:22.297000+00:00",
                "content": "awesome, thank you!"
            }
        ]
    },
    {
        "thread_id": 1285548388972236842,
        "thread_name": "Session history in BAML",
        "messages": [
            {
                "author": "brandburner",
                "timestamp": "2024-09-17 10:30:20.265000+00:00",
                "content": "I am LOVING using BAML so far for generating structured metadata from drama transcripts. With Sonnet 3.5 and prompt caching I'm able to process a script scene by scene and extract all the entities, dramatic themes etc. So I'm curious if BAML natively maintains any kind of session history when executing on functions, or whether each call is independent of the previous? For my use case, it would help the consistency of responses if the model could see its previous few responses"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 14:03:58.775000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 14:04:00.981000+00:00",
                "content": "Awesome ‚ù§Ô∏è \n\nThere's some ways to do history if you'd like:\n\n```rust\nclass Message {\n  role string\n  content string\n}\n\ntemplate_string RenderMessage(m: Message) #\"\n  {{ _.role(m.role) }}\n  {{ m.content }} \n\"#\n\nfunction ProcessScene(script: string, messages: Message[]) -> History {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Given this script.\n    {{ script }}.\n\n    {{ ctx.output_format }}\n\n    {% for m in messages %}\n    {{ RenderMessage(m) }}\n    {% endfor %}\n  \"#\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 14:04:29.875000+00:00",
                "content": "Would this be what you're looking for?"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-17 14:06:32.573000+00:00",
                "content": "ahh nice - thanks! I'll see if i can make it work with my main scene processing loop"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-09-18 01:50:57.735000+00:00",
                "content": "Do you have any guidance for how to deal with history that would contain previous structured responses? ex. i ask for a list of things, the content of that is returned as structured output. So the content of that message is technically what? a pydantic model? json?. We need this so that subsequent questions may be asked about the prior content (in this case previous items as structured output)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:33:20.166000+00:00",
                "content": "yep! for that you can just pass it in state as well <@1150651185167085589>  and just update RenderMessage function accoridingly\n\n```\nclass Message {\n  role string\n  content string | History\n}\n\nfunction ProcessScene(script: string, messages: Message[]) -> History {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Given this script.\n    {{ script }}.\n\n    {{ ctx.output_format }}\n\n    {% for m in messages %}\n    {{ RenderMessage(m) }}\n    {% endfor %}\n  \"#\n}\n```"
            }
        ]
    },
    {
        "thread_id": 1285956356691460178,
        "thread_name": "Commet Opik",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:31:27.349000+00:00",
                "content": "Hi guys, is there a way to use the new commet Opik with BAML?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:34:16.269000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:34:16.539000+00:00",
                "content": "can you share what that is?"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:34:28.516000+00:00",
                "content": "https://www.comet.com/docs/opik/tracing/integrations/openai/"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:35:03.773000+00:00",
                "content": "As you see you would monkey patch openai for it"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:35:08.250000+00:00",
                "content": "but since it is handle internally"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:35:40.770000+00:00",
                "content": "Is there a way to monkey patch the client?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:36:11.621000+00:00",
                "content": "I see"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:36:34.357000+00:00",
                "content": "there's not directly a way to monkey patch,  but i think they do some work to basically forward requests"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:36:41.683000+00:00",
                "content": "Let me try and see how they do this"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:37:51.944000+00:00",
                "content": "hmm it doesn't look they they document how they make the webrequest"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:38:04.406000+00:00",
                "content": "if you can share how they do that, then it may be possible to plug that in"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:56:44.633000+00:00",
                "content": "opik uses a decorator to wrap the openai client.  It takes an instance of the OpenAI client and replaces it to include the tracking functionality."
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:57:06.993000+00:00",
                "content": "def track_openai(openai_client):\n    decorator = OpenaiTrackDecorator()\n    wrapper = decorator.track(\n        type=\"llm\",\n        name=\"chat_completion_create\",\n        generations_aggregator=chunks_aggregator.aggregate,\n    )\n    openai_client.chat.completions.create = wrapper(\n        openai_client.chat.completions.create\n    )\n    return openai_client"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 13:57:10.460000+00:00",
                "content": "this is how they do it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:57:40.944000+00:00",
                "content": "I see, sorry i meant like the web requests they actually make to track all the content. We could technically add a hook for someone to upload all the requests our way"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 14:27:17.561000+00:00",
                "content": "Got it, I was reviewing the code, and what Opik does is essentially capture the input prompt and parameters before making the request, and once the response comes back, it captures the output. It doesn't directly intercept or modify the underlying HTTP request itself but wraps the method that handles the request"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 14:27:58.622000+00:00",
                "content": "Then after they send this data to the deployment, i'm hosting it locally and it allows you to test the prompts and use llms as judges easily"
            }
        ]
    },
    {
        "thread_id": 1285964626898456708,
        "thread_name": "Passing headers in to providers",
        "messages": [
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:04:19.120000+00:00",
                "content": "Hi all! First of all, am having a blast with baml!\nI have a question about serving baml as an API: **Is it possible to have headers from the request to the served baml API be passed through to the LLM provider?**\n\nI have two use cases for this:\n1. The LLM provider is azure-openai \"like\", but requires an OAuth token that has to be refreshed every 12h. Being able to forward a token header from the request to the baml API would mean that the requesting client could take care of having a non-expired token\n2. The client that calls the baml API also wants to add some headers to the LLM provider that let it know who is making the request (required by the provider for metering). \n\nHappy to give more details if needed."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:07:58.882000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:07:59.113000+00:00",
                "content": "so glad you're loving BAML!\n\nQuick question, are you using BAML over HTTP, or native python, or native TS version of BAML?"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:24:04.120000+00:00",
                "content": "I am using BAML over HTTP, ie running using `baml-cli`. \nWith the native generated clients this would be quite simple with the `ClientRegistry` I think"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:25:08.646000+00:00",
                "content": "ah got it yes that would work.\n\n<@711679663746842796> trackign for you to share with <@438416839224197132> when we can ship client registry and typebuilder for BAML over HTTP.\n\nMy thoughts are we are about 2 weeks away from this going live (maybe sooner, but that would be the safe version)."
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:28:30.708000+00:00",
                "content": "That's awesome!\nJust to make sure I understand it correctly, this means there would be (a) endpoints exposed for creating a custom client (which could include the custom headers), (b) a way to specify which client to use in the request to the `/call/*` endpoint (likely as a header, since the body is just the input data itself).\nIs that right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:29:58.734000+00:00",
                "content": "it would likely look something like this:\n\n```\ncurl -X POST 'https://yourbamlendpoint.com/call/function' -H \"content-type: application/json\" -d '{\n  param1: ...,\n  __baml_options__: {\n     client_registry: { <info goes here> }\n  }\n}'\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:30:20.174000+00:00",
                "content": "so then you'd be able to construct any client as is possible with client registry"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:30:25.648000+00:00",
                "content": "Makes sense, even better to have it in one request."
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:30:36.419000+00:00",
                "content": "Is there a PR/issue open already tracking this?"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:31:35.174000+00:00",
                "content": "and on that note, is there an OpenAPI.json spec for the BAML HTTP server available somewhere?\nScouring the repository, I was only able to find the endpoints `/_debug/ping` and `/call/<fn>`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:31:40.063000+00:00",
                "content": "not yet, we have an internal workitem though! <@711679663746842796>  can you ticket this so we can share this"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:31:48.068000+00:00",
                "content": "we are still working on that üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:32:21.198000+00:00",
                "content": "btw what language are you using with BAML over HTTP?"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:33:02.076000+00:00",
                "content": "Prototyped in Python (which, ofc, has a nice generated client), now to be built¬†properly in Java"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:33:45.226000+00:00",
                "content": "And since there's no JVM generated client available yet (afaict), the decision to host baml externally and call it through the API"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:39:18.768000+00:00",
                "content": "I like how you said `build properly` üòÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:39:25.627000+00:00",
                "content": "I too agree, python is not a proper language XD"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:39:49.671000+00:00",
                "content": "Are you using the generated JavaSDK for the HTTP request?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:42:28.100000+00:00",
                "content": "```\n// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator java_sdk {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"rest/openapi\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../java\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.55.3\"\n\n    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client\n    // This command will be run from within $output_dir\n    on_generate \"npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g java -o . --additional-properties invokerPackage=com.boundaryml.baml_client,modelPackage=com.boundaryml.baml_client.model,apiPackage=com.boundaryml.baml_client.api,java8=true && cd ../baml_client && mvn clean install\"\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:43:13.995000+00:00",
                "content": "Then you can plug anything you want for `output_dir \"../java\"\n` and it'll dump java files + openapi spec for your specific BAML functions"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:52:20.105000+00:00",
                "content": "> I like how you said build properly\nI wouldn't say that Java is my favorite language either, but in the end there is supposed some consistency in the tech stack üòÑ"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 14:53:08.983000+00:00",
                "content": "> Are you using the generated JavaSDK for the HTTP request?\n\nI was not, since I wasn't aware of this snippet üòÆ trying it out now!"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 15:02:32.261000+00:00",
                "content": "That works for me, though I had to change `-i openapi.yaml` to `-i baml_client/openapi.yaml` since that is where the yaml file is generated for me. If this is part of an example that maybe no longer works, I'll happily open up a documentation PR to fix it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 15:19:32.108000+00:00",
                "content": "oh nice, if you want, you can actually open up a real pr to fix it üòâ \n\nhttps://github.com/BoundaryML/baml/blob/46322c394dd7a0920874fafab2f61d1f888daa9b/engine/baml-runtime/src/cli/init.rs#L130"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 15:19:36.615000+00:00",
                "content": "its just one line of change"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 15:19:44.519000+00:00",
                "content": "(otherwise i'll patch it)"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-18 15:26:06.659000+00:00",
                "content": "Never mind that one is correct, however the comment \"This command will be run from within $outputdir\", should be \"This command will be run from within baml_client/outputdir\". I'll find the place that comment is placed and update it."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 15:26:36.785000+00:00",
                "content": "oh awesome, thanks <@438416839224197132> üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 15:26:48.528000+00:00",
                "content": "always appreciate contributors! makes our life much easier haha"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-19 06:15:12.488000+00:00",
                "content": "Used to be an almost full-time maintainer of OSS, so I share the sentiment haha\nIn any case, here's the PR: https://github.com/BoundaryML/baml/pull/968"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 07:17:20.572000+00:00",
                "content": "<@438416839224197132>  how are you liking the Java generator? Is the API interface good enough? Any feedback here helps!"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-27 09:21:30.085000+00:00",
                "content": "Hi Aaron, thanks for following up! The receiving team is quite happy with it, I'll probe for any rough edges and get back to you\n\nTo give a quick update on my use case: I was able to convince the relevant stakeholders that BAML is the way to go and should be adopted for the project. However, since the header forwarding is a blocking issue (all requests need to be proxied through an internal platform that requires fresh OAuth headers and metering headers), this adoption decision is contingent on this issue being resolved by 10th of October since there's a deadline looming.\n<@99252724855496704> if there's been any change in the planned timeline for releasing the client registry over HTTP support, I would be happy to know! \n(I hope this doesn't read as setting an expectation, I know this is open source and I appreciate the amazing work in the project here ‚ù§Ô∏è )\nIf there's any way I can contribute to help get this feature out, I would be happy to"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-27 13:24:18.533000+00:00",
                "content": "If you‚Äôve got cycles, I would totally take you up on that? I can quickly explain what work needs to get done and then share where the code goes. Would you have time for that? I think it‚Äôs 1-2 days of work max! Most likely our team can get to this by your deadline!"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-27 14:11:03.539000+00:00",
                "content": "Sure, let's do that üôÇ feel free to DM me what changes would be needed, also open to a quick call if you think that'd make sense"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-27 14:34:20.394000+00:00",
                "content": "Let‚Äôs do a quick call! If you‚Äôre free at 9 am pst (90 mins from now) can hop on a call then. If you dm me your email, I‚Äôll send out an invite"
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-30 11:05:34.048000+00:00",
                "content": "Okay, the PR should be ready to review now: https://github.com/BoundaryML/baml/pull/986\n\nOne thing that's missing is adding the `__baml_options__` field to each request body schema in `openapi.rs`, I wasn't sure how to do that in a clean way. If you can help me out there, that'd be awesome (the schema definitions for `BamlOptions` are already added)."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 13:17:42.720000+00:00",
                "content": "Awesome. I‚Äôll add that in! I‚Äôll pull and test it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:40:35.045000+00:00",
                "content": "<@503733199130722314> from our team is actually gonna help out with this and help land this to completion! Very excited to get this in by today or tmrw! Looks close to completion"
            },
            {
                "author": "imalsogreg",
                "timestamp": "2024-09-30 19:42:27.440000+00:00",
                "content": "PR looks great! I'm going to focus on running it locally, finishing up the last TODO merging asap."
            },
            {
                "author": "imalsogreg",
                "timestamp": "2024-10-01 16:37:38.172000+00:00",
                "content": "<@438416839224197132> I tested out your PR yesterday and things look promising after implementing that last TODO. If you'd like to try it out, I opened a [PR against your fork](https://github.com/lorenzoh/baml/pull/1)  The description shows an example rust client using `__baml_options__` to select different models.\n\nToday I'd like to test it out using Java, and to see if I can work out why some fields are getting double-wrapped as optional. But I wanted to share the work-in-progress with you before doing that, since you have your Oct 10th internal deadline coming soon."
            },
            {
                "author": "imalsogreg",
                "timestamp": "2024-10-01 18:19:25.552000+00:00",
                "content": "Ok, I've confirmed it works in Java too. Using an openapi generated client, you can do this:\n```\nBamlOptions bamlOptions = new BamlOptions()\n    .clientRegistry(new BamlOptionsClientRegistry()\n                    .clients(Collections.singletonList(\n                        new ClientProperty()\n                            .name(\"OpenAI\")\n                            .provider(\"openai\")\n                            .retryPolicy(null)\n                        .options(new java.util.HashMap<String,Object>() {{\n                            put(\"model\", \"gpt-4o-mini\");\n                            put(\"api_key\", apiKey);\n                        }})\n                    ))\n                    .primary(\"OpenAI\")\n                    );\n// Extract resume example\nString resumeText = \"John Doe\\nSoftware Engineer ...\";\nExtractResumeRequest extractResumeRequest = new ExtractResumeRequest()\n    .resume(resumeText)\n    .bamlOptions(bamlOptions);\n\n```"
            },
            {
                "author": "loohly",
                "timestamp": "2024-10-01 20:30:43.700000+00:00",
                "content": "Thanks for the support,  Greg! I'll do some additional testing tomorrow and then I think it should be good to merge"
            },
            {
                "author": "imalsogreg",
                "timestamp": "2024-10-02 16:38:35.378000+00:00",
                "content": "Happy to help! Hope you don't mind I pulled your changes into a PR and merged it yesterday. Definitely helpful to hear if the API works for you and if it ends up solving your Auth issues, please let me know, we can quickly patch it as needed.\n\nCongrats on being the first contributor to ship a whole feature!  That's  awesome!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 16:48:59.090000+00:00",
                "content": "0.58 includes this change fyi!"
            },
            {
                "author": "loohly",
                "timestamp": "2024-10-03 07:29:17.062000+00:00",
                "content": "I don't mind of course üôÇ awesome to see this released so quickly! Thanks for the support from both of you and I'll definitely give an update after my additional testing (that I haven't gotten to yet üôà )"
            }
        ]
    },
    {
        "thread_id": 1285993548352720971,
        "thread_name": "Using Azure",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 15:59:14.532000+00:00",
                "content": "Guys quick question, how can I use azure-openai? I'm havin troubles when i declare it into the function should i declare the client? When i declare it like:\n\n  client \"azure-openai/gpt4o\"\n\nGives me an error, an also when i call the name on the client that i defined into the clients.baml doesn't work, how should it be?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 16:00:33.435000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 16:00:33.873000+00:00",
                "content": "Ah we have a minor bug in our parser:\n\nthis fails:\n```\nclient MyClientName // foo\n```\n\nbut this will work:\n```\n// foo\nclient MyClientName\n```\n\n(We need to fix trailing comments)"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 22:09:50.097000+00:00",
                "content": "Thank you! I forgot to say!"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 22:09:59.853000+00:00",
                "content": "BTW, i'm moving all my code from instructor to BAML"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-18 22:10:05.892000+00:00",
                "content": "It's really a THING!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:18:21.437000+00:00",
                "content": "!!!!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:18:25.480000+00:00",
                "content": "üòÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:18:30.335000+00:00",
                "content": "So hyped!"
            }
        ]
    },
    {
        "thread_id": 1286089166194610286,
        "thread_name": "parsing questions",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:19:11.602000+00:00",
                "content": "i tried using baml (in my own way) to parse a string (of list of strings):\n\nclass DeDupeResult {\n    duplicates string[]\n}\n\n^^returns nothing\n\nclass DeDupeResult {\n    merged_results string[]\n}\n\n^^ returns the intended list of string (merged_results)\n\ni am wondering how this is working?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:22:55.009000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:22:55.211000+00:00",
                "content": "can you share the prompt?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:26:02.644000+00:00",
                "content": "i'm calling this on an output like the following:\n\n`st = \"['Star Ocean: The Second Story R', 'something']\"\nres = await b.ExtractDeDupe(st)\nres.merged_results[0]`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:26:14.015000+00:00",
                "content": "'Star Ocean: The Second Story R'"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:29:49.097000+00:00",
                "content": "full class:\n\n`class DeDupeResult {\n    merged_results string[]\n}\n\n// Creating a function to extract the Result from a string.\nfunction ExtractDeDupe(graph: string) -> DeDupeResult {\n  client Llama70b\n  prompt #\"\n    Extract from this content:\n    {{ graph }}\n\n    {{ ctx.output_format }}\n  \"#\n}`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:33:55.187000+00:00",
                "content": "I see and that didn't work?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:34:10.994000+00:00",
                "content": "let me try it!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:36:06.724000+00:00",
                "content": "it doesn't work when i use the following class definition:\n`\nclass DeDupeResult {\n    duplicates string[]\n}`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:36:08.555000+00:00",
                "content": "hmm i see it working here:\n\nhttps://www.promptfiddle.com/dudupe-test-SMGZ4"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:36:22.955000+00:00",
                "content": "you may want to try and see the test case in BAML"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:37:08.398000+00:00",
                "content": "then in VSCode, there's a \"Open Playground\" button right over the BAML function:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:37:24.254000+00:00",
                "content": "then you can try running the unit test to see whats going on there"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:41:55.450000+00:00",
                "content": "somehow it looks for graph DS:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:42:19.068000+00:00",
                "content": "swap line 19 from resume -> graph"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:44:26.190000+00:00",
                "content": "somehow i only see resume example:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:45:10.361000+00:00",
                "content": "you can click on either the dropdown in the menu, or click on any line in duplicate_ids.baml to switch to the ExtractDeDupe function"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:46:10.070000+00:00",
                "content": "they are not populating:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:46:38.429000+00:00",
                "content": "want to hop on office hours rq?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 22:46:51.392000+00:00",
                "content": "i can see whats going on"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:47:00.834000+00:00",
                "content": "yes sure"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-18 22:58:31.491000+00:00",
                "content": ""
            }
        ]
    },
    {
        "thread_id": 1286117217267617872,
        "thread_name": "i'm trying to add a field dynamically",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:10:39.499000+00:00",
                "content": "i'm trying to add a field dynamically which would describe another class field but am getting the following error:\n\nCell In[15], line 6, in get_graph(text)\n      4 tb = TypeBuilder()\n      5 tb.SimpleNode.add_property(\"descrpition\", tb.string()).description(\"if possible, provide a description of id\")\n----> 6 res = await b.ExtractGraph(text.content, { \"tb\": tb })\n      7 return res\n\nTypeError: object DynamicGraph can't be used in 'await' expression"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:11:21.168000+00:00",
                "content": ""
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:11:21.470000+00:00",
                "content": "here is the data model(s):\n\n`// Defining a data model.\nclass SimpleNode {\n  id string\n  type string\n  @@dynamic\n}\n\nclass SimpleRelationship {\n    source_node_id string\n    source_node_type string\n    target_node_id string\n    target_node_type string\n    type string\n}\n\nclass DynamicGraph {\n    nodes SimpleNode[]\n    relationships SimpleRelationship[]\n}\n\n// Creating a function to extract the DynamicGraph from a string.\nfunction ExtractGraph(graph: string) -> DynamicGraph {\n  client Llama70b\n  prompt #\"\n    Extract from this content:\n    {{ graph }}\n\n    {{ ctx.output_format }}\n  \"#\n}`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:12:13.756000+00:00",
                "content": "the python code:\n\n`from baml_client.type_builder import TypeBuilder\nfrom baml_client import b\nasync def get_graph(text):\n    tb = TypeBuilder()\n    tb.SimpleNode.add_property(\"descrpition\", tb.string()).description(\"if possible, provide a description of id\")\n    res = await b.ExtractGraph(text.content, { \"tb\": tb })\n    return res`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:23:31.930000+00:00",
                "content": "you likely on't need the await on `res = await b.ExtractGraph` try just:\n\n```\nres = b.ExtractGraph(...)\n```"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:30:59.936000+00:00",
                "content": "oops thanks"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:31:56.358000+00:00",
                "content": "somehow the output is not as i intended it to be:\n\n`DynamicGraph(nodes=[], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:34:35.340000+00:00",
                "content": "without dynamic field:\n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person'), SimpleNode(id='Open AI', type='organization')], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:35:27.685000+00:00",
                "content": "sorry what are you expecting out of it? it'll help me understand"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:37:30.818000+00:00",
                "content": "something like this:\n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person', description='Elon Musk is an entrepreneur'), SimpleNode(id='Open AI', type='organization')], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:38:33.051000+00:00",
                "content": "i see"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:39:38.454000+00:00",
                "content": "or \n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person', description='Elon Musk is an entrepreneur'), SimpleNode(id='Open AI', type='organization', description=none)], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:39:53.024000+00:00",
                "content": "```python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client import b\nasync def get_graph(text):\n    tb = TypeBuilder()\n    tb.SimpleNode.add_property(\"description\", tb.string().optional())\n    res = await b.ExtractGraph(text.content, { \"tb\": tb })\n    return res\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:40:33.170000+00:00",
                "content": "out of curiosity, do you want to set the description in python? or is it doable in BAML?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:40:51.805000+00:00",
                "content": "also do make sure you've save the BAML file fyi!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:41:10.118000+00:00",
                "content": "oh right, thanks!!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:41:35.260000+00:00",
                "content": "BAML would be ideal but having it done in python will work fine"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:42:00.156000+00:00",
                "content": "got it, and you need to modify the description at runtime?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:42:38.083000+00:00",
                "content": "like would this work?\n\n```\nclass SimpleNode {\n  id string\n  type string\n  description string? @description(#\"if possible, provide a description of id\"#)\n}\n```"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:44:55.472000+00:00",
                "content": "that would work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 00:46:49.716000+00:00",
                "content": "oh nice! let me know if that works for you!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:57:03.506000+00:00",
                "content": "for the python example i'm not getting the desired output as description(s) are none:\n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person', description=None), SimpleNode(id='Open AI', type='organization', description=None)], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:57:44.135000+00:00",
                "content": "when i call the llm:\n\n`AIMessage(content='Elon Musk is a business magnate and entrepreneur.', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 53, 'total_tokens': 65}, 'model_name': 'meta-llama/Meta-Llama-3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4bc106e6-2eeb-4571-97ed-915ee36ecd50-0', usage_metadata={'input_tokens': 53, 'output_tokens': 12, 'total_tokens': 65})`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 00:57:56.981000+00:00",
                "content": "i will try the BAML approach now"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:00:22.376000+00:00",
                "content": "same for BAML -- the descriptions are none"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 01:01:05.546000+00:00",
                "content": "have you tried in the playground?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 01:01:08.645000+00:00",
                "content": "what is the output?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:09:02.726000+00:00",
                "content": "they seem to be reurning null:\n\n\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": [\n        {\n          \\\"type\\\": \\\"text\\\",\n          \\\"text\\\": \\\"Extract from this content:\\nHere is the extracted information in the correct format:\\\\n\\\\n**Nodes**\\\\n\\\\n* Elon Musk (person)\\\\n* Open AI (organization)\\\\n\\\\n**Relationships**\\\\n\\\\n* Elon Musk - SUED - Open AI'\\n\\nAnswer in JSON using this schema:\\n{\\n  nodes: [\\n    {\\n      id: string,\\n      type: string,\\n      // Describe id using a sentence\\n      description: string or null,\\n    }\\n  ],\\n  relationships: [\\n    {\\n      source_node_id: string,\\n      source_node_type: string,\\n      target_node_id: string,\\n      target_node_type: string,\\n      type: string,\\n    }\\n  ],\\n}\\\"\n        }\n      ]\n    }\n  ]\n}\"\n{\"id\":\"chatcmpl-WTDt34DuwLWJBjq66nAxff\",\"object\":\"chat.completion\",\"created\":1726708076,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Here is the extracted information in the correct format:\\n\\n```\\n{\\n  \\\"nodes\\\": [\\n    {\\n      \\\"id\\\": \\\"Elon Musk\\\",\\n      \\\"type\\\": \\\"person\\\",\\n      \\\"description\\\": null\\n    },\\n    {\\n      \\\"id\\\": \\\"Open AI\\\",\\n      \\\"type\\\": \\\"organization\\\",\\n      \\\"description\\\": null\\n    }\\n  ],\\n  \\\"relationships\\\": [\\n    {\\n      \\\"source_node_id\\\": \\\"Elon Musk\\\",\\n      \\\"source_node_type\\\": \\\"person\\\",\\n      \\\"target_node_id\\\": \\\"Open AI\\\",\\n      \\\"target_node_type\\\": \\\"organization\\\",\\n      \\\"type\\\": \\\"SUED\\\"\\n    }\\n  ]\\n}\\n```\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":189,\"total_tokens\":321,\"completion_tokens\":132}}%"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:10:38.694000+00:00",
                "content": "sorry"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 01:10:56.991000+00:00",
                "content": "no worries, it looks like the model is choosing to not output a description"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:11:12.523000+00:00",
                "content": "{\"id\":\"chatcmpl-TamyCcnjJxb4oMuhnPeBWa\",\"object\":\"chat.completion\",\"created\":1726708214,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Here is the extracted information in the requested JSON schema:\\n\\n```\\n{\\n  \\\"nodes\\\": [\\n    {\\n      \\\"id\\\": \\\"Elon Musk\\\",\\n      \\\"type\\\": \\\"Person\\\",\\n      \\\"description\\\": \\\"CEO of SpaceX and Tesla\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"OpenAI\\\",\\n      \\\"type\\\": \\\"Organization\\\",\\n      \\\"description\\\": \\\"Artificial intelligence research organization\\\"\\n    }\\n  ],\\n  \\\"relationships\\\": [\\n    {\\n      \\\"source_node_id\\\": \\\"Elon Musk\\\",\\n      \\\"source_node_type\\\": \\\"Person\\\",\\n      \\\"target_node_id\\\": \\\"OpenAI\\\",\\n      \\\"target_node_type\\\": \\\"Organization\\\",\\n      \\\"type\\\": \\\"Sued\\\"\\n    }\\n  ]\\n}\\n```\\n\\nLet me know if you need any further assistance!\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":145,\"total_tokens\":299,\"completion_tokens\":154}}%"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:11:37.384000+00:00",
                "content": "now descriptions ae being populated"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:12:05.404000+00:00",
                "content": "i somehow deleted `default system` earlier"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 01:12:14.399000+00:00",
                "content": "ah üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 01:12:17.821000+00:00",
                "content": "glad its working!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 01:12:22.841000+00:00",
                "content": "Thanks for bearing with it!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:12:36.077000+00:00",
                "content": "good progress!!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-09-19 01:12:42.433000+00:00",
                "content": "thank you!!"
            }
        ]
    },
    {
        "thread_id": 1286213499068088341,
        "thread_name": "CoT",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:33:14.869000+00:00",
                "content": "Anyone had consistent way of imducing CoT before extraction?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:34:11.600000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:34:11.901000+00:00",
                "content": "my template string for CoT:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:35:42.227000+00:00",
                "content": "```\ntemplate_string CoT() #\"\n  Before answering outline some key detials.\n  Example:\n  * ...\n  * ...\n  * ...\n  {\n     .. // Schema\n  }\n\"#\n```\n\nThen i just throw it in the prompt:\n\n```\nprompt #\"\n  {{ ctx.output_format }}\n  {{ CoT() }}\n\n  {{ _.role('user') }}\n  {{ content }}\n\"#\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 06:37:20.097000+00:00",
                "content": "You can say ‚Äúexplain your reasoning and reflect on the reasoning in a few sentences before yoj write out the schema‚Äù"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:37:29.366000+00:00",
                "content": "Cool ye I have variants of this. Is there an easy way to print this as well when extraction is successful?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:37:55.939000+00:00",
                "content": "As in, the reasoning before schema"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 06:38:16.682000+00:00",
                "content": "https://www.promptfiddle.com/BAML-Examples-UM7wr"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 06:38:38.630000+00:00",
                "content": "Ohh you want it in the return value"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:38:41.842000+00:00",
                "content": "sadly not in your python code, but for tests you run in the playground you'll be able to see it"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:39:07.576000+00:00",
                "content": "Ah I see cool"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:39:15.974000+00:00",
                "content": "its tricky to give the text back. We've been playing around with the idea of a `markdown` type, where we could return the actual prefix text in python as well."
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:39:24.436000+00:00",
                "content": "So no way to see full trace?"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:39:43.448000+00:00",
                "content": "Cause I think we get smth when it fails right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:39:51.260000+00:00",
                "content": "so you could could say something like:\n\n```\nfunction Foo() -> markdown<MyType>\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:39:58.869000+00:00",
                "content": "yea on failure we output the full text"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:40:16.654000+00:00",
                "content": "we are still in the middle of processing the baml raw API to determine the right form here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:40:57.826000+00:00",
                "content": "if its useful for you, I can turn on the observability piece for you and then you'll be able to see it.\n\nAlso, i think there's a bug we can patch for tmrw, but BAML_LOG should print out everything to console"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:41:18.446000+00:00",
                "content": "Nah its alright I was curious!"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:42:55.978000+00:00",
                "content": "Although ye will be useful in production and higher volume but that is after we have platform discussions etc."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:43:43.255000+00:00",
                "content": "yea! hope my email described it well, looking forward to the followup conversations üôÇ"
            },
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:44:36.032000+00:00",
                "content": "It did! Will keep you updated"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-19 19:09:20.241000+00:00",
                "content": "just to chime in here, it'd be really valuable to be able to get at the raw text of the response from the model so that we can inspect and improve the CoT from production runs"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-19 19:10:01.090000+00:00",
                "content": "right now we are paying double tokens because we do two calls to the LLM, one to do the CoT so we can see it and then another to do the BAML and it is frustrating"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-20 01:04:56.183000+00:00",
                "content": "do you also want CoT for analytics or for a runtime usecase?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-20 11:16:03.137000+00:00",
                "content": "spot checking for analytics to start, but at scale, that has to get automated so it becomes a runtime thing anyways"
            }
        ]
    },
    {
        "thread_id": 1286213642919874570,
        "thread_name": "Logging",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-19 06:33:49.166000+00:00",
                "content": "Also, what was the way we could view/print that again?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 06:34:47.657000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 06:34:48.025000+00:00",
                "content": "BAML_LOG=info do you use python?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 06:36:14.940000+00:00",
                "content": "You may need to set BOUNDARY_PROJECT_ID=test\nBOUNDARY_SECRET=test\n\n\n(Basiclaly dummy values, due to a bug where we dont log unless these are set)"
            }
        ]
    },
    {
        "thread_id": 1286325461080739921,
        "thread_name": "Retry on parse failure",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 13:58:08.693000+00:00",
                "content": "Hi guys, is there a way to retry in case there is a fail to parse the output? The retry can work with this? or what is your recommendation?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 15:55:33.804000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 15:55:34.429000+00:00",
                "content": "Do you want an lllm to fix the parse failure? Or just call your function agaian?"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:01:30.170000+00:00",
                "content": "fix the parse failure"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:01:48.660000+00:00",
                "content": "I was thinking to sending it back the error over the failure like is done in instructor"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 16:01:56.397000+00:00",
                "content": "We dont currently have a way to do that but it is definitely on our roadmap"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:02:07.750000+00:00",
                "content": "i'm implementing it on python because i was not sure is implemented"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:02:16.830000+00:00",
                "content": "ok oh, perfect!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:02:24.549000+00:00",
                "content": "ah sadly no, we don't do this for a few reasons: but primarily that we wanted very explicit controls when the money printer goes $$$"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:03:10.215000+00:00",
                "content": "what do you mean?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:03:13.747000+00:00",
                "content": "e.g. if you're gonna call an LLM it should be expected. We're thinking of ways to add retries on parsing, as parsing retries are very different than non-parsing retries.\n\n(i.e. maybe we can do something smarter w/ partial information)"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:03:19.625000+00:00",
                "content": "Like to avoid inifinite calls to the llm?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:03:41.073000+00:00",
                "content": "well more so, if the LLM outputed everything but 1 field w/ its parsing error, you may want to do something different"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:03:48.042000+00:00",
                "content": "than if it didn't output any fields"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:04:15.762000+00:00",
                "content": "and we're thinking through what the implications there are.\n\nThat said, we could just do the dumb thing and enable this."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:04:32.665000+00:00",
                "content": "for now python is def the way to go üôÇ"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:08:51.251000+00:00",
                "content": "I think(just take as a personal opinion not want to go over your project), that for new devs and early users even passing the parsing error that comes from the pydantic and retrying(the dumb thing) would be a perfect starter"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:09:43.007000+00:00",
                "content": "yea i think the devX there would be a good idea"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:12:03.476000+00:00",
                "content": "the only call out there is that lets say you have `function ParseResume(doc: image) -> Resume` and the user tosses in a receipt, you likely don't want to retry"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:12:41.981000+00:00",
                "content": "but i'm gonna think about it more and see what the balance is. E.g. maybe if we were able to parse some fields, but not all, then we can retry"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:26:41.379000+00:00",
                "content": "Oh right, but in that case it woudn't be in any case solvable by any library"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-19 16:27:21.526000+00:00",
                "content": "it should be configured as an edge case for the dev, as in case is not a receipt output, {not a receipt} for example"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:28:18.293000+00:00",
                "content": "ooh i like that, ok. You've convinced me i should think about this and write a more formal spec. I will write a frist draft and share with you in about a week"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 16:30:03.988000+00:00",
                "content": "https://tenor.com/bRldp.gif"
            }
        ]
    },
    {
        "thread_id": 1286391398769365025,
        "thread_name": "Dashboard bug",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-09-19 18:20:09.463000+00:00",
                "content": "Have you guys seen this error before? \n\n```\nNo events in the chain\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 19:00:36.119000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 19:00:36.556000+00:00",
                "content": "<@201399017161097216> PTAL i think this is a bug on dashboad"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:01:03.009000+00:00",
                "content": "Ill take a look in about 40min!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-19 19:04:01.689000+00:00",
                "content": "I'm getting this error when using my baml client on a simple function"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-19 19:04:06.923000+00:00",
                "content": "i shouldn't be pushing to the baml dashboard"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-19 19:04:31.051000+00:00",
                "content": "my BAML_LOG is not set"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:04:55.414000+00:00",
                "content": "What BOUNDARY_SECRET?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 20:29:51.113000+00:00",
                "content": "note, we solved this -- it's an issue with api key missing (and our bad errors)"
            }
        ]
    },
    {
        "thread_id": 1286410413210079355,
        "thread_name": "Reduce probability of each token",
        "messages": [
            {
                "author": "kirilligum",
                "timestamp": "2024-09-19 19:35:42.859000+00:00",
                "content": "a follow up question. is it possible to easily identify encoding for a token and reduce the probability of it appearing? for example, i don't wnat certain words to appear in the response"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:51:41.132000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:51:41.716000+00:00",
                "content": "You can do this in your client parameters:\n\n```\nclient<llm> MyClient {\n\n   options {\n      logit_bias { .... }\n```\nfrom https://platform.openai.com/docs/api-reference/chat/create#chat-create-logit_bias"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:52:01.077000+00:00",
                "content": "though you can only do this with individual tokens"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-19 19:52:54.787000+00:00",
                "content": "https://help.openai.com/en/articles/5247780-using-logit-bias-to-alter-token-probability-with-the-openai-api"
            }
        ]
    },
    {
        "thread_id": 1286607816303575102,
        "thread_name": "Error message",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 08:40:07.424000+00:00",
                "content": "Guys is there a way when using azure openai and hitting the rate limits per minute to know what time should i wait from the errors? I'm using client error from the documentation"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 11:35:17.579000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 11:35:23.448000+00:00",
                "content": "It should be included in the actual error message that‚Äôs raised in the exception. Do you see that?"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 14:17:37.075000+00:00",
                "content": "No I'm just trying to do it for resilience"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 14:17:59.925000+00:00",
                "content": "but you know that openai has an ratelimit error but i guess you are doing it differently"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 14:18:50.465000+00:00",
                "content": "Ah. I think we can add something in there where the delay of the retry policy will be based on the rate limit"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 14:19:11.206000+00:00",
                "content": "And we can allow some max timeout or something"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 14:19:39.561000+00:00",
                "content": "Would be amazing!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 14:20:08.370000+00:00",
                "content": "https://docs.boundaryml.com/docs/snippets/clients/retry"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 14:20:18.587000+00:00",
                "content": "You can currently set the delay time yourself"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 14:20:25.624000+00:00",
                "content": "If that would help fyi"
            }
        ]
    },
    {
        "thread_id": 1286843239051300937,
        "thread_name": "How do I use gpt-4o-2024-08-06? \"gpt-4o",
        "messages": [
            {
                "author": "charizard_98",
                "timestamp": "2024-09-21 00:15:36.581000+00:00",
                "content": "How do I use gpt-4o-2024-08-06? \"gpt-4o\" right now defaults to gpt-4o-2024-05-13."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-21 00:16:12.054000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-21 00:16:12.490000+00:00",
                "content": "`client \"openai/gpt-4o-2024-08-06\"` or `model \"gpt-4o-2024-08-06\"`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-21 00:16:37.746000+00:00",
                "content": "https://docs.boundaryml.com/docs/snippets/clients/providers/openai#forwarded-options"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-21 00:18:03.729000+00:00",
                "content": "Thanks!"
            }
        ]
    },
    {
        "thread_id": 1287138848228507729,
        "thread_name": "llm error too long",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-09-21 19:50:15.301000+00:00",
                "content": "Would it be possible to expose the underlying LLM Error upfront. I see LLM error followed by the input text which is thousands of tokens long‚Ä¶ makes it tricky to see what the actual model error was (I assumed rate limiting?)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-21 19:54:23.786000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-21 19:54:24.210000+00:00",
                "content": "yes we can def do that improvement on the next release.\n\nYou can also do this"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-21 19:54:52.064000+00:00",
                "content": "in case you dont want a log with 1 million characters"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-21 19:55:25.280000+00:00",
                "content": "we will truncate the middle of the raw llm prompt and the middle of the raw llm response that is shown with this env var"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-23 16:18:11.228000+00:00",
                "content": "missed this! Need to update the notification settings here"
            }
        ]
    },
    {
        "thread_id": 1287173745928175749,
        "thread_name": "Is it possible to run tests within",
        "messages": [
            {
                "author": "gitzalytics",
                "timestamp": "2024-09-21 22:08:55.561000+00:00",
                "content": "Is it possible to run tests within playground on dynamically created enums?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 02:52:38.070000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 02:52:40.546000+00:00",
                "content": "not yet! But we will add this in the next few weeks"
            }
        ]
    },
    {
        "thread_id": 1287345963178131478,
        "thread_name": "validations",
        "messages": [
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-22 09:33:15.353000+00:00",
                "content": "Hi, I use BAML for vision tasks which mostly involves extracting product information along with their price - is there a way to ensure that the extracted price is valid? I'm assuming that LLM can hallucinate and come up with a number üòì"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:25:41.727000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:25:47.354000+00:00",
                "content": "Hey <@757202652416180235> ! Sadly the only way to do this atm is with code.\n\nHere's a youtube video that shows what i mean: https://www.youtube.com/watch?v=xCpQdHX5iM0&feature=youtu.be\n\nWe'll have more native support in BAML coming up thanks to <@503733199130722314> !\n\nYou can see what that will look like here: <#1265356689796890820>"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-22 13:35:47.434000+00:00",
                "content": "That's so cool! Thank you so much! A lot of good work being done :)"
            }
        ]
    },
    {
        "thread_id": 1287403500413784065,
        "thread_name": "Chome Extensions",
        "messages": [
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:21:53.299000+00:00",
                "content": "Has anyone setup baml with a Chrome Extension yet? I'm trying to get it going with Vite and CRXjs  https://crxjs.dev/vite-plugin <@201399017161097216> <@99252724855496704>"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:29:22.863000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:29:23.807000+00:00",
                "content": "Hey <@323873214336073730> ! I'm actually not sure this will work yet üò¢ We don't ship our WASM-Browser compatible version of BAML atm... We would recommend keeping your BAML code in the backend.\n\nWe made this design choice due to where we assumed people keep their API keys (and openai and anthropic both block calling the api from the browser directly, we Proxy our requests through a backend). That said, if this is really appealing to you, we can help out here and see what it would take to ship our WASM layer (Which we use in our VSCode playground)."
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:30:56.461000+00:00",
                "content": "Makes sense! Yeah, I'm fine hosting an endpoint on a Vercel function for now. I'm just curious if you had an example of how to stream from a vercel api to the chrome extension"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:34:15.315000+00:00",
                "content": "Not specifically, but you can use vercels AI streaming SDK"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:35:22.190000+00:00",
                "content": "Example: https://github.com/BoundaryML/baml-examples/blob/main/nextjs-starter/app/examples/stream-object/page.tsx\n\n(follow `extractResume`)\n\nhttps://docs.boundaryml.com/docs/baml-nextjs/baml-nextjs"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:35:44.327000+00:00",
                "content": "We're gonna work on making a `typescript/nextjs` example that will autogenerate some of this for you fyi!"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:36:14.118000+00:00",
                "content": "Amazing!!"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:36:27.418000+00:00",
                "content": "Thanks <@99252724855496704>, on top of it as usual!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:36:50.949000+00:00",
                "content": "üòÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:37:05.074000+00:00",
                "content": "your wish is my command"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:38:35.684000+00:00",
                "content": "https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExanQxcGJ0OTJybWtqMTIxbmU0ZGhxZ252MHI2M2s0d3hxcmMxMzdkZCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/am0dKpQO1vrOC3DmCb/giphy.gif"
            }
        ]
    },
    {
        "thread_id": 1287411832830689382,
        "thread_name": "chaining functions",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-22 13:54:59.902000+00:00",
                "content": "Guys is there a way to create subfolders that have works like dependencies for a series of calls? for example, 1 -> 2 -> 3 (in an orderly manner?) Or it should be done programmatically?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:56:53.663000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:56:54.453000+00:00",
                "content": "for now we do this programtically"
            }
        ]
    },
    {
        "thread_id": 1287675039700881450,
        "thread_name": "Schema robustness",
        "messages": [
            {
                "author": "gabriel_syme",
                "timestamp": "2024-09-23 07:20:53.310000+00:00",
                "content": "I'm probably repeating myself but what is the best way to add some silly robustness to schema failures? Eg. Assume one output was an invalid enum value, can we return \"None\" or smth like that vs the request failing comlletely and getting no values?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 13:28:02.503000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 13:28:04.841000+00:00",
                "content": "You can make values optional! And we fill auto fill them in"
            }
        ]
    },
    {
        "thread_id": 1287790043896873010,
        "thread_name": "When doing function/tool calling with",
        "messages": [
            {
                "author": "simontam0",
                "timestamp": "2024-09-23 14:57:52.447000+00:00",
                "content": "When doing function/tool calling with BAML if you happen to have more than 1 class that may look similar to another is there a way to better guide the LLM into which function/tool it should call? Would this be in the name of the function, prompt itself, the docstring for the class or maybe an added member in the class to give it a hint?"
            },
            {
                "author": "tdn8",
                "timestamp": "2024-09-23 15:15:30.731000+00:00",
                "content": ""
            },
            {
                "author": "tdn8",
                "timestamp": "2024-09-23 15:15:31.119000+00:00",
                "content": "Until https://github.com/BoundaryML/baml/pull/978 is in (which will allow you to do something like: `action \"Calculate\"` ) what has worked for us is to add a unique field to \"tie-break\" the two classes, or add a discrete enum type which only has that 1 value:\n\n```\nenum CalculateActionType {\n  CALCULATE_ACTION\n}\n\nclass Calculate {\n  action CalculateActionType\n```"
            }
        ]
    },
    {
        "thread_id": 1287815984740175915,
        "thread_name": "Jinja Filters",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-09-23 16:40:57.226000+00:00",
                "content": "Quck question I see that my teemmate uses |tojson() in our baml config but I can't seem to find any docs for it? Also do you support other formats like yaml"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:42:36.148000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:42:36.584000+00:00",
                "content": "We'll add these mini docs directly in our own docs\n\nhttps://docs.boundaryml.com/docs/snippets/prompt-syntax/what-is-jinja#built-in-filters\n\nThey are called filters in jinja"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:42:51.510000+00:00",
                "content": "we plan on adding some more by default, but haven't done it yet"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:43:06.775000+00:00",
                "content": "oops wrong link"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:43:07.569000+00:00",
                "content": "https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:43:20.070000+00:00",
                "content": "so as of now YAML isn't supported"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-23 16:43:33.499000+00:00",
                "content": "oh so that is jinja thing sry I didn't think of that thank you"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 16:44:24.037000+00:00",
                "content": "no worries, we should help provide as much great documentation as possible, so its helpful to us to see these questions!"
            }
        ]
    },
    {
        "thread_id": 1287818844433420391,
        "thread_name": "While you guys are answering questions",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-09-23 16:52:19.030000+00:00",
                "content": "While you guys are answering questions let me ask something, here I pass an argument in the test but it shows up as null in the user prompt (line 2)"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-23 16:58:20.688000+00:00",
                "content": ""
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-23 16:58:21.044000+00:00",
                "content": "Ok found out why it was the wrong argument name"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-23 17:39:46.179000+00:00",
                "content": "this is good feedback for us -- we should probably highlight you have the wrong name in our `args`"
            }
        ]
    },
    {
        "thread_id": 1287818913681248256,
        "thread_name": "Is there a way currently to have @@",
        "messages": [
            {
                "author": "davidyoung",
                "timestamp": "2024-09-23 16:52:35.540000+00:00",
                "content": "Is there a way currently to have @@dynamic in an enum, and fill that enum in a test case?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-23 17:16:58.484000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-23 17:16:59.020000+00:00",
                "content": "dyamic fields for BAML-defined `tests` will be coming later :). Do you find the BAML Playground in VSCode useful to iterate on things?"
            },
            {
                "author": "davidyoung",
                "timestamp": "2024-09-23 19:25:42.498000+00:00",
                "content": "Yes very useful! üôÇ"
            },
            {
                "author": "davidyoung",
                "timestamp": "2024-09-23 19:25:59.624000+00:00",
                "content": "Definitely separates BAML from the rest imo"
            }
        ]
    },
    {
        "thread_id": 1287858098270371891,
        "thread_name": "SAP vs openai",
        "messages": [
            {
                "author": "gggooo",
                "timestamp": "2024-09-23 19:28:17.874000+00:00",
                "content": "Is boundary ml better than openai's structured outputs?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 19:32:59.669000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 19:33:00.301000+00:00",
                "content": "I'll let my bias not show too much here, but yes üôÇ :\n\nHelpful reading: \nWhat is SAP (Schema-aligned parsing): https://www.boundaryml.com/blog/schema-aligned-parsing#sap\nMetrics: https://www.boundaryml.com/blog/sota-function-calling\n\nBut we allow for a few things that openai doesnt like streaming for parallel functino calling. \n\nit turns out that using SAP, we are able to get the same (or better) accuracy while using 15% less tokens in many use cases ."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 19:33:12.538000+00:00",
                "content": "But i'll let the rest of folks who use BAML share more"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-23 19:37:14.870000+00:00",
                "content": "Thank you. But this is a baml vs openai function calling stat, not their recent structured output feature, correct?"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-23 19:37:39.063000+00:00",
                "content": "Or are they essentially the same"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-23 19:40:11.329000+00:00",
                "content": "Oh sorry I didn't read it fully, it is for the structured outputs"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-23 19:40:52.681000+00:00",
                "content": "Ok it was literally in the title, I am dumb"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-23 19:40:53.913000+00:00",
                "content": "Sorry"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-23 20:01:03.114000+00:00",
                "content": "haha no worries. Definitely check out those benchmark results (scroll down to the bottom)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 20:06:02.980000+00:00",
                "content": "üôÇ"
            }
        ]
    },
    {
        "thread_id": 1287862250366963764,
        "thread_name": "retry policy",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-23 19:44:47.811000+00:00",
                "content": "Hi guys, i'm getting one issue with:\n\nretry_policy PolicyToRetry {\n  max_retries 8\n  strategy {\n    type exponential_backoff\n    delay 5000\n    multiplier 2\n  }\n}\n\nstrategy using azure, is there a quick fix?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 20:05:49.849000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 20:05:51.240000+00:00",
                "content": "whats the exact issue btw?"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-24 13:31:59.736000+00:00",
                "content": "Actually don't know, I was hitting that strategy was not a default value when integrating it with a fallback and then having like 100 requests in one minute due to a validation error"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-24 13:32:11.823000+00:00",
                "content": "I just removed the list of fallbacks and work"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-24 13:32:25.839000+00:00",
                "content": "I have to present something and forgot to record the logs"
            }
        ]
    },
    {
        "thread_id": 1288177184137150598,
        "thread_name": "CoT",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-09-24 16:36:13.873000+00:00",
                "content": "If I‚Äôm using chain of thought and asking LLM to explain its thoughts, it looks like I can just add a ‚ÄúCoT string‚Äù output to my output schema to make the parsing easier. More a general question but in this case i should have the CoT output first in the output schema so that it works correctly?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-24 16:40:36.239000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-24 16:40:36.629000+00:00",
                "content": "Here's how i do CoT:\n\n```rust\ntemplate_string CoT(details: string) #\"\n  before answering, note some of the relevant details like {{ details }}.\n  Example:\n  - ...\n  - ...\n  ...\n  \n  {\n    .. // SCHEMA\n  }\n\"#\nfunction Foo(content: string) -> Resume {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Extract this resume.\n  \n    {{ ctx.output_format }}\n     \n    {{ CoT('why this person may be a good hire') }}\n    \n    {{ _.role('user') }}\n    {{ content }} \n  \"#\n}\n```"
            }
        ]
    },
    {
        "thread_id": 1289389757842849886,
        "thread_name": "Template strings",
        "messages": [
            {
                "author": "roeybc",
                "timestamp": "2024-09-28 00:54:33.973000+00:00",
                "content": "Might have missed it in the docs - any way to get a template string as an input? I 'm looking into loading strings that have working baml code (taking into account that all inputs are valid).\nI'd use jinja, but I'm using typescript."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-28 00:56:15.492000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-28 00:56:16.183000+00:00",
                "content": "Hmm not at the moment. Youd just have to add a string input where the string is that baml code"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-28 00:56:27.334000+00:00",
                "content": "What are you trying to do?"
            },
            {
                "author": "roeybc",
                "timestamp": "2024-09-28 01:17:43.858000+00:00",
                "content": "Well, trying to let users write baml code with a constant set of parameters for all users, where they can also write use different prompts on these sets. and then run it üòÑ \nLike - two dev tools (for example lol) where you write code in a completely different way but the sets of parameters is kinda the same."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 03:36:21.177000+00:00",
                "content": "You likely want to use dynamic types!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 03:37:13.205000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/dynamic-types"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 03:37:23.300000+00:00",
                "content": "Then use that to compose the string on your end"
            },
            {
                "author": "roeybc",
                "timestamp": "2024-09-29 03:17:33.509000+00:00",
                "content": "That can be great to the generation (I thought to use a list but this one is WAY better). The thing is that I want the prompt to be dynamic and written by the users, ideally with baml. Something like this: \n```\nsome_prompt = \"\"\"\nExtract the information from this chunk of text:\n    \"{{ user_info }}\"\n     \n    {{ ctx.output_format }}\n\"\"\"\n\nfunction DynamicUserCreator(some_prompt: string, user_info: string) -> User {\n  client GPT4\n  prompt #\"\n    {{some_prompt}}\n  \"#\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-29 17:33:58.342000+00:00",
                "content": "You can do that with template strings, template strings accept parameters. Ill write an example in a bit"
            },
            {
                "author": "roeybc",
                "timestamp": "2024-09-29 20:02:57.913000+00:00",
                "content": "I tried to do something like this: \n```\ntemplate_string PrintPrompt(blob: string) #\"\n  {{blob}}\n\"#\n```\nBut no luck üòÑ"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-29 20:10:36.029000+00:00",
                "content": "here is a working version :)! https://www.promptfiddle.com/New-Project-B057h"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-29 21:44:43.527000+00:00",
                "content": "FYI as of now, we don‚Äôt support you having your users write BAML prompts and then us executing that"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-29 21:45:14.307000+00:00",
                "content": "But I think we should be able to. And I like the idea of us providing UI components that just work for the experience"
            },
            {
                "author": "roeybc",
                "timestamp": "2024-09-29 22:22:07.872000+00:00",
                "content": "Do that! Isn‚Äôt it as simple as running jinja template on top of the given prompt?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 11:54:29.084000+00:00",
                "content": "\"kinda\" is the short answer"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 11:54:55.046000+00:00",
                "content": "but you can do this today if you handle the jinja strings yourself! We just are thinking about how to provide static analysis and other guarantees BAML provides today"
            }
        ]
    },
    {
        "thread_id": 1289712474874052690,
        "thread_name": "Default params",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-09-28 22:16:55.709000+00:00",
                "content": "did support for default parameters get added?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-29 02:13:10.545000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-29 02:13:16.421000+00:00",
                "content": "Not yet!"
            }
        ]
    },
    {
        "thread_id": 1290155795413205086,
        "thread_name": "Input output tokens",
        "messages": [
            {
                "author": "saurabhj80",
                "timestamp": "2024-09-30 03:38:31.562000+00:00",
                "content": "How can I know input and output tokens after I am done calling the baml function?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:20:54.422000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:20:57.189000+00:00",
                "content": "Hey <@701543571265945752> ! We currently do expose that, but have a GitHub issue tracking this. We are thinking of ergonomic designs that can populate this information for you!"
            },
            {
                "author": "saurabhj80",
                "timestamp": "2024-09-30 17:08:23.526000+00:00",
                "content": "got it. so I will need to wait for now"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:21:40.852000+00:00",
                "content": "yep! sorry about that"
            }
        ]
    },
    {
        "thread_id": 1290159919290650674,
        "thread_name": "Inputs rendering with aliases",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 03:54:54.771000+00:00",
                "content": "I'm looking at the symbol tuning example on promptfiddle\n\nIt looks like for the MyClass output type, the parser will convert from the alias (k4) to to the non-alias category (AccountIssue)\n\nHowever, if i wanted to pass that output into a 2nd function, one that has input MyClass and output MyClass, for example a function that validates the output of ClassifyMessageWithSymbol and returns the correct MyClass, would I have to convert the non-alias category (AccountIssue) back into the alias (k4) form for the input of the 2nd function, given that the context only sees the aliases because of the below.\n\nAnswer with any of the categories:\nMyClass\n----\n- k1: Customer wants to refund a product\n- k2: Customer wants to cancel an order\n- k3: Customer needs help with a technical issue unrelated to account creation or login\n- k4: Specifically relates to account-login or account-creation\n- k5: Customer has a question"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:19:38.238000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:19:40.654000+00:00",
                "content": "Oh no! Good catch. We can aim for a fix for this to land tmrw. It shouldn‚Äôt take more than a day."
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 16:23:18.987000+00:00",
                "content": "Also, will ctx.output_format only include the enum definition if the output is a MyClass enum"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 16:24:10.865000+00:00",
                "content": "If a MyClass enum is passed into a function, the enum definition (and associated descriptions) doesn‚Äôt make its way into the context?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:23:03.997000+00:00",
                "content": "yep, for now they dont, but i can see how you'd like to be able to inject that information into the prompt.\n\nIts interesting, as we also would need to inject in schema information to print this out correctly.\n\ne.g.\n\n```\nclass Foo {\n  field MyEnum | int\n}\n```\n\nif you pass in `Foo` into the prompt and `field` is an int, then we shouldn't inject the enum, but if you pass in the enum, then we should likely render its something like:\n\n```\n{ field: MyEnum('SomeValue') }\n```\n\nwhile rendering the values for the enum (so the model can know what you chose it from)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:23:20.195000+00:00",
                "content": "<@201399017161097216> for context"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 20:31:17.963000+00:00",
                "content": "OK for now if I‚Äôm using the same enums in input and output should I just avoid aliases? Though the system tuning is interesting and I would like to use it."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 20:33:35.145000+00:00",
                "content": "<@201399017161097216> what are the odds we can get this in by 1-2 days?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 20:33:58.915000+00:00",
                "content": "let us evaluate and see what the work here is. I think something should be possible as its just a minor change"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 20:34:32.662000+00:00",
                "content": "is there a function to generate that enum description as a string?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 20:34:51.152000+00:00",
                "content": "so I can inject it manually for now"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 21:03:32.475000+00:00",
                "content": "Sadly not, that would be the thing we expose as a part of this something like:\n\n```\n{{ _.print_type(MyType) }}\n```"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-02 20:00:23.962000+00:00",
                "content": "It occurs to me that even normally when you have a class as an input and dump it as a string into the prompt it just renders the json and doesn‚Äôt show your descriptions, that only happens for outputs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 21:49:56.245000+00:00",
                "content": "yes, it's just a plain object at that point instead of a schema. Would you want the descriptions to still be written in?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 21:58:25.970000+00:00",
                "content": "<@201399017161097216> we should spend some good time thinking about this. I think <@152300469094580224> has some really good points. I wonder if we just need a way to render custom types and such"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 21:58:36.960000+00:00",
                "content": "And parts of that type like descriptions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 21:58:59.925000+00:00",
                "content": "We should maybe put a spec up at some point."
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-02 22:00:41.780000+00:00",
                "content": "perhaps optionally"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-02 22:04:06.969000+00:00",
                "content": "also, this is not really an issue but right now if I am using the same class as the output of different functions, I end up splitting the relevant information for each class field between the class @description and my explicit instructions elsewhere in the prompt"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-04 05:08:42.266000+00:00",
                "content": "yeah for now I think having something like this be exposed would be helpful, i'm finding myself having to add in the format of input classes for more context"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:26:19.208000+00:00",
                "content": "good news, I just fixed this -- will patch it in the next release (potentially tomorrow).\n\nso inputs with aliased keys will render with the alias"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-08 14:55:47.278000+00:00",
                "content": "is there an option to print the schema of the input? With description too"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 15:12:31.561000+00:00",
                "content": "Not yet but ill add to our roadmap"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-09 17:08:22.458000+00:00",
                "content": "sorry the inputs being rendered with aliases will take a few more days -- will update this thread when done"
            }
        ]
    },
    {
        "thread_id": 1290297989965021195,
        "thread_name": "have you tried to compare baml (json) to",
        "messages": [
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:03:33.385000+00:00",
                "content": "have you tried to compare baml (json) to code4struct approach where the ouput is a python code?"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:07:09.488000+00:00",
                "content": ""
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:07:09.925000+00:00",
                "content": "codeact https://arxiv.org/pdf/2402.01030 and few others use this output too"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 13:14:58.205000+00:00",
                "content": "We haven‚Äôt actually done anything special yet for snippets like code. For returning strings we just pipe back the raw output. We are looking into adding types like markdown to the language where we can apply interesting corrective techniques. This one looks worth looking into!"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:19:15.264000+00:00",
                "content": "code4struct is a little different."
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:19:33.074000+00:00",
                "content": "it uses python code with classes instead of json"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:19:46.537000+00:00",
                "content": "json has dictionaries, code4struct uses ast tree"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:23:53.682000+00:00",
                "content": "another paper is gollie https://arxiv.org/pdf/2310.03668 a json alternative would be {\"Launcher\":{\"mentions\":str,\"space_company\":str,\"crew\":List[str]}"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:25:20.048000+00:00",
                "content": "i'm trying to decide between using json with baml or python. the main reason for python is that it was a lot more training data and a bit better error feedback"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:26:51.911000+00:00",
                "content": "from gollie paper:\n3.1 INPUT-OUTPUT REPRESENTATION\nWe have adopted a Python code-based representation (Wang et al., 2023b; Li et al., 2023) for both the\ninput and output of the model. This approach not only offers a clear and human-readable structure\nbut also addresses several challenges typically associated with natural language instructions. It\nenables the representation of any information extraction task under a unified format. The inputs\ncan be automatically standardized using Python code formatters such as Black. The output is wellstructured and parsing it is trivial. Furthermore, most current LLMs incorporate code in their pretraining datasets, indicating that these models are already familiar with this representation."
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-09-30 13:27:17.325000+00:00",
                "content": "but, i think you answered my question that you haven't looked into this. let me know if you do"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:17:23.527000+00:00",
                "content": "Sorry about that! I was skimming in the AM and confused it for another paper i thought i read.\n\nThanks for the clarification.\n\nI think this class of techniques is similar to what BAML does in its prompts.\n\nHere they use python to represent the schema, BAML use Type Definitions (closer to gollie).\n\nThats what our helper `{{ ctx.output_format }}` puts out.\n\nyou can read more here: https://www.boundaryml.com/blog/type-definition-prompting-baml\n\nWe currently don't make it super easy to customize this for users, but if code4struct was supported, i could easily imagine a world with something like: `{{ ctx.output_format(mode=\"code4struct\")`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:17:34.613000+00:00",
                "content": "Hope that helps?"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-10-04 21:27:36.212000+00:00",
                "content": "yes. this helps. thank you"
            }
        ]
    },
    {
        "thread_id": 1290324538923548774,
        "thread_name": "generate dynamic objects",
        "messages": [
            {
                "author": "andrewcka",
                "timestamp": "2024-09-30 14:49:03.150000+00:00",
                "content": "Guys, is there a way to generate models on the fly using JSON objects? My goal is to generate fields in different languages to improve translations. I've noticed that when you describe the object name very well, the translation ends up being more accurate, even with simpler models"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:50:08.650000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:50:08.901000+00:00",
                "content": "you may want to do this: https://docs.boundaryml.com/docs/calling-baml/dynamic-types"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:53:27.581000+00:00",
                "content": "so youre saying if you are doing a translation task, describing the object in each particular language does better?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:53:40.400000+00:00",
                "content": "that's pretty interesting"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-30 20:51:14.369000+00:00",
                "content": "Yes, it works really well with alias"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-30 20:51:28.205000+00:00",
                "content": "you asign a descritive alias of the output"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-30 20:51:34.366000+00:00",
                "content": "descriptive"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-30 20:51:54.281000+00:00",
                "content": "instead of a generic name describing the task, and then it aligns better"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-30 20:52:16.241000+00:00",
                "content": "also for complex extractions that normally fail it helps to guide it better"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 21:36:54.343000+00:00",
                "content": "does the dynamic type approach make sense? Would you like a more concrete example?"
            }
        ]
    },
    {
        "thread_id": 1290351410914791506,
        "thread_name": "client graph",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-09-30 16:35:49.932000+00:00",
                "content": "What is Client Graph on promptfiddle supposed to represent?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 18:58:06.405000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 18:58:07.252000+00:00",
                "content": "If you use any of the `retry` logic or `fallback` clients, it will show you the order in which LLMs will be called in the event of failures"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 18:58:10.927000+00:00",
                "content": "and how many times theyre called"
            }
        ]
    },
    {
        "thread_id": 1290538406358814843,
        "thread_name": "Parens highlighting",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-10-01 04:58:53.120000+00:00",
                "content": "I've noticed that within a #\"\"# block, if there are any sections surrounded by parenthesis (sample text), the text after the ) is not highlighted correctly in vscode"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-01 15:58:58.541000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-01 15:58:59.946000+00:00",
                "content": "Thanks for reporting, will add to our next batch of bug fixes"
            }
        ]
    },
    {
        "thread_id": 1290734557997437018,
        "thread_name": "recursive types",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-10-01 17:58:19.315000+00:00",
                "content": "BAML still doesn't support infinite nesting of fields right? like if i had a \n\nclass Element {\n  children Element[]\n}"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-10-01 20:34:25.284000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-10-01 20:34:26.617000+00:00",
                "content": "Sadly, yes, we don‚Äôt currently allow any recursion in type definitions today."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-10-01 20:34:58.370000+00:00",
                "content": "It‚Äôs on our list, but no timeline yet"
            }
        ]
    },
    {
        "thread_id": 1290894828560842815,
        "thread_name": "I asked about how to deal with",
        "messages": [
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 04:35:10.795000+00:00",
                "content": "I asked about how to deal with structured responses in history before but not sure i was clear. I'll try agan. Presently w/o BAML we are asking the LLM to process an array of chat messages, some are from user, AIMessage (from the LLM) and some may be a FunctionMessage (from a tool call).. How should we formulate our prompt function to include/translate these messages to make it usable in BAML and to the LLM, specifically how might I convert/clean the FunctionMessage's  additional_kwargs[\"function_call\"][\"arguments\"] (which is the json output from a tool call). to something usable for BAML? When I was trying it out in the playground just pasting the json into a string didn't work well. Here's an example json string:       'assistant {\"is_outbound_flight_choices\": true, \"indicies_of_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59]}'.\n\nI believe another way to think about it is if I wanted BAML to extract data from a json object how should I do that in the prompt function? Does the JSON have to be \"cleaned\" in some way for BAML? The test cases I was using that worked were very basic json, not the same kind of json output I'm getting from json.dumps."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:57:23.504000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:57:23.916000+00:00",
                "content": "just so I understand, your input is a json blob, and what do you want the output to be? do you want the LLM to choose one of the tools?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:00:17.965000+00:00",
                "content": "you can use BAML to do the tool call itself btw. Imagine a function whose signature is this:\n\n```\nclass FunctionMessage {\n  is_outbound_flight_choices bool\n  indices_of_flights int[]\n}\nfunction CallTool(messages: Message) -> FunctionMessage\n```"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 05:05:13.361000+00:00",
                "content": "yes i saw that and have been playing with that too."
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 05:06:53.515000+00:00",
                "content": "I think I figured out my issue, i had to encapsulate the json in the prompt with #\"json string blob here \"# and thereafter I could extract what I needed"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:07:15.632000+00:00",
                "content": "you can pass the json as a string parameter as well"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 05:08:56.250000+00:00",
                "content": "`class ExtractedJson {\n  ids string[] @description(#\"\n    The id_token_keys extracted from the json.\n  \"#)\n  flight_numbers string[] @description(#\"\n    The flight numbers extracted from the json.\n  \"#)\n}\n\nfunction ExtractFromJson(results: string) -> ExtractedJson {\n  client GPT4o\n  prompt #\"\n\n    Extract the info from this json.\n    ---\n    {{ results }}\n    ---\n    {# special macro to print the output schema. #}\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest TestName {\n  functions [ExtractFromJson]\n  args {\n    results #\"{\"is_outbound_flight_choices\": true, \"indicies_of_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59], \"indicies_of_extracted_unique_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59], \"id_of_extracted_sorted_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59], \"number_of_flights\": 63, \"error_response\": null, \"presentation_message\": \"Flights with the lowest cost fare option that allows for changes were retained.\", \"flight_choices\": [{\"id_token_key\": \"CiAKHgoQZTg5ZGNiNzBlYjZhNTJlMRIICIoDEgExGAEgAQ==\", \"origin\": \"SEA\", \"origin_name\": \"Seattle\\\\u2013Tacoma International Airport\", \"destination\": \"LAX\", ...\n  }\n}`"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 05:09:35.786000+00:00",
                "content": "i shortened the json in the example about but you get the idea"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:09:54.909000+00:00",
                "content": "yeah that makes sense"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:10:19.559000+00:00",
                "content": "although if you know the json structure in advance, can you do this programmatically?"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 05:11:27.689000+00:00",
                "content": "in most cases probably but in some others we may want the llm to look at some of the elements within the json to pull some stuff out"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-02 05:12:30.945000+00:00",
                "content": "in our chat history this json blob is usually included for the user to make a choice. however questions may be asked about those choices. like shopping for items and asking questions about those items"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:35:57.480000+00:00",
                "content": "I see, makes sende"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:36:01.201000+00:00",
                "content": "Sense*"
            }
        ]
    },
    {
        "thread_id": 1290900003455242292,
        "thread_name": "langchain",
        "messages": [
            {
                "author": "flyingaudio",
                "timestamp": "2024-10-02 04:55:44.586000+00:00",
                "content": "Is BAML a LangChain replacement?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:57:55.651000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:57:55.903000+00:00",
                "content": "yep, except for the embeddings side of things -- we don't do any of the embeddings generation"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:58:09.039000+00:00",
                "content": "we focus mostly on the Boundary between programs <> LLMs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:25:25.295000+00:00",
                "content": "<@418848265371648010> were you able to checkout BAML? Any feedback helps us on why or why-not you went with us"
            },
            {
                "author": "flyingaudio",
                "timestamp": "2024-10-13 00:30:02.199000+00:00",
                "content": "I plan on spending some time with it this coming week."
            }
        ]
    },
    {
        "thread_id": 1291058373940346991,
        "thread_name": "Multilabel",
        "messages": [
            {
                "author": "try_another",
                "timestamp": "2024-10-02 15:25:03.052000+00:00",
                "content": "Any tips on multi-label classification with BAML?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 15:33:57.414000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 15:33:58.024000+00:00",
                "content": "You can return an enum array from a function!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 15:34:16.623000+00:00",
                "content": "<@201399017161097216> can share a promptfiddle example when he‚Äôs on! I‚Äôm sadly on my phone"
            },
            {
                "author": "try_another",
                "timestamp": "2024-10-02 15:36:50.699000+00:00",
                "content": "please <@201399017161097216>"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 15:36:58.417000+00:00",
                "content": "working on it!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 15:38:53.996000+00:00",
                "content": "here's a multilabel classification problem!  https://www.promptfiddle.com/New-Project-WDZFV\n\nHow many labels do you have?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 15:40:53.605000+00:00",
                "content": "here's another technique -- adding descriptions to the categories, and renaming them to \"k1\" \"k2\" can also help the LLM focus on the descriptions instead of the names of the classes https://www.promptfiddle.com/New-Project-M6Snf"
            },
            {
                "author": "try_another",
                "timestamp": "2024-10-02 15:49:21.776000+00:00",
                "content": "Love it, thanks! I'll test it on my use case, but looks like it works! The prompt is a bit confusing though: \"Classify the following INPUT into **ONE**\". Output is multilabel though"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 15:50:01.027000+00:00",
                "content": "woops sorry, you can remove that whole line (look at the prompt on the right side for the full view)"
            }
        ]
    },
    {
        "thread_id": 1291060311041769614,
        "thread_name": "Tracing",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-10-02 15:32:44.893000+00:00",
                "content": "any recommendations if I want to track all my LLM calls from baml and store them somewhere (input / output for each LLM api call) and also tagging of calls"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 15:35:31.879000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 15:35:32.649000+00:00",
                "content": "If you‚Äôre interested we‚Äôve got observability for baml available. <@201399017161097216> can set up some time with you today to show you more"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 15:42:06.646000+00:00",
                "content": "feel free to use this link to schedule a time, or let me know what time works for you and I can set you up real quick: https://calendly.com/aaron-vi/30min"
            },
            {
                "author": "f1ddl3r",
                "timestamp": "2024-10-03 07:58:59.210000+00:00",
                "content": "Hopping in here with a related question! Are you planning to support (or already support) allowing users to build custom tracing for the LLM calls?\n\nI really like the approach you're taking with BAML, but I can't really use it since I need a way to push data to my own custom observability platform. The reason is that we contractually can't push customer data to any 3rd party platforms."
            },
            {
                "author": "f1ddl3r",
                "timestamp": "2024-10-03 08:00:33.949000+00:00",
                "content": "But I do understand if this is not in the roadmap, given your biz is around observability part. üôÇ Think users like us would be a minority?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:01:20.993000+00:00",
                "content": "We have a few different ideas there and tehre are existing workaround already! What language are you using atm with BAML?"
            },
            {
                "author": "f1ddl3r",
                "timestamp": "2024-10-03 08:01:34.861000+00:00",
                "content": "Using with TypeScript"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:03:20.904000+00:00",
                "content": "Some of the things we are looking into are:\n* ensuring we only store encrypted data and your browser is actually the thing doing decryption\n* hosting our observability pipeline in your own VPC"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:04:00.953000+00:00",
                "content": "and in terms of Typescript, you can always trace the inputs and outputs of any BAML function similar to any other function you're tracing"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:04:17.323000+00:00",
                "content": "do you specifically want the LLM prompt + response in the trace?"
            },
            {
                "author": "f1ddl3r",
                "timestamp": "2024-10-03 08:05:39.152000+00:00",
                "content": "Looking for the raw prompt especially, but of course response would be cool to have too."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:06:29.850000+00:00",
                "content": "awesome, yea if you think either of the solutions i posted above would make sense for you folks, glad to have a deeper conversation and help set your team up."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:07:45.393000+00:00",
                "content": "we've mostly been building out BAML, but are starting to invest a bit more into the toolchain around it. With some of the new syntax capabilities we're releasing into BAML, I suspect that a vanilla LLM observability will mostly fall short"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:08:03.796000+00:00",
                "content": "but we do plan on adding some escape hatches!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:08:41.103000+00:00",
                "content": "trade off is just ergonomics vs configuration, we're following the rust approach, where we are really conservative atm, and just learning from users, and plan on having escape hatches for pro users"
            },
            {
                "author": "f1ddl3r",
                "timestamp": "2024-10-03 08:15:25.174000+00:00",
                "content": "The thing is that we've built observability features straight into our app where we see the configurations, and user data, what prompts those lead to, and then the outputs. \n\nThat makes it tough for us to use a 3p observability platform since the key piece is seeing our app \"setup\" as well. We could push our own configs to a 3p platform to have all the data there, but the configs are complex with bunch of conditional logic, so visualizing those is tough without proper UI.\n\nSo, in our case, kinda would need a way to access the raw prompts & responses."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:17:27.130000+00:00",
                "content": "got it! id love to learn more and see how we can provide a great ergonomic experience for you folks and your use case specifically"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 08:17:51.521000+00:00",
                "content": "then we can add in the mechanism to get raw prompts + raw responses"
            },
            {
                "author": "f1ddl3r",
                "timestamp": "2024-10-03 08:29:42.861000+00:00",
                "content": "I guess the one approach would be to return additional raw data in addition to the actual response from baml function calls. But that is a breaking change to the current API. Another option could be to have \"_trace\" attribute  containing the raw data that gets always returned in the function response.\n\nBut of course, I'm not best placed to say how it should work since I just discovered Baml a day ago and tried to implement it for a simple test case. I then ran into the issue of not having a any way to push the required raw prompt/response data to our system and that's how I'm here. üôÇ"
            }
        ]
    },
    {
        "thread_id": 1291166712401756182,
        "thread_name": "that didn't seem to stop it -- i",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:35:32.954000+00:00",
                "content": "that didn't seem to stop it -- i probably am not doing it right"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:35:59.815000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:36:00.094000+00:00",
                "content": "can you share a screenshot of where you set it and how you import env vars in the notebook?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:36:33.438000+00:00",
                "content": "youre trying to run this in jupyter right?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:39:40.701000+00:00",
                "content": "yes"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:40:24.550000+00:00",
                "content": "it works now after exporting the variable to bashrc"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:40:54.052000+00:00",
                "content": "for some reason, it didn't work when i set it in bash"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:41:22.652000+00:00",
                "content": "perfect! Sorry about that, our baml_client is very eager to load env vars when you import it, so usually you have to run load_dotenv() _before_ you try and do \"from baml_client import b\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:41:52.091000+00:00",
                "content": "question -- have you figured out how to reload the baml_client in jupyter when you change a .baml file so it picks up your new changes when you re-run a cell?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:42:56.957000+00:00",
                "content": "I restart my notebook.."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:43:54.654000+00:00",
                "content": "let me see if there's a more automatic way -- will let you know!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:44:14.012000+00:00",
                "content": "great!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 22:58:05.034000+00:00",
                "content": "to only restart the baml runtime with your changes, run this:\n\n\nfrom app.baml_client import reset_baml_env_vars\nimport os\n\nreset_baml_env_vars(dict(os.environ))"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 22:59:18.963000+00:00",
                "content": "cool thanks!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 23:19:51.995000+00:00",
                "content": "sorry you must also activate this cell the very first time:\n%load_ext autoreload\n%autoreload 2"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-07 22:54:16.462000+00:00",
                "content": "okay, thanks for the clarification"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:55:09.872000+00:00",
                "content": "i actually added the actual things you need -- it's much simpler:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/python#baml-with-jupyter-notebooks"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:55:32.041000+00:00",
                "content": "it's just that pylance linter in VSCode will still complain if you change your type signatures (even though the types you import did reload)"
            }
        ]
    },
    {
        "thread_id": 1291190274416050258,
        "thread_name": "Baml chaining",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-10-03 00:09:10.576000+00:00",
                "content": "Is it possible to call a BAML function directly from BAML instead of going through TypeScript I can't seem to find it in the docs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-03 00:10:29.380000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-03 00:10:29.916000+00:00",
                "content": "It s not possible to chain functions yet!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-03 00:10:37.526000+00:00",
                "content": "Is that something youd be interested in?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-10-03 00:11:51.419000+00:00",
                "content": "I feel like it is a nice to have, I can still chain thing in TypeScript. This would just make testing the full flow easier."
            }
        ]
    },
    {
        "thread_id": 1291464653741883485,
        "thread_name": "Tokenize",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-10-03 18:19:27.705000+00:00",
                "content": "Does anyone know of a good way to programmatically determine which content should fit in a context window? For example, I know Cursor was doing some interesting stuff with re-ranking the prompt inputs dynamically. Basically trying to prevent the situation where the input exceeds the 128k context window for gpt-4o"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-10-03 20:05:24.803000+00:00",
                "content": ""
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-10-03 20:05:25.767000+00:00",
                "content": "It is pretty easy to tokenize for gpt with tiktoken"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-03 20:33:28.224000+00:00",
                "content": "How long of a context are you running into?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-04 17:28:37.678000+00:00",
                "content": "It‚Äôs not a tokenization problem. Suppose I am 100k over the context window. I want an intelligent way to rank which 100k tokens I should remove from the prompt.\n\nI was wondering if anyone had some good algorithms to do this üôÇ"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-04 17:29:39.004000+00:00",
                "content": "Sometimes I have 250k+ tokens since we deal with massive documents. I get that you can use a basic reranker by chunking the input, but I‚Äôd love to encode domain expertise to be able to say ‚Äúthese chunks should be weighted more than those‚Äù"
            }
        ]
    },
    {
        "thread_id": 1291796561780805835,
        "thread_name": "vscode error",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-10-04 16:18:20.748000+00:00",
                "content": "is this a vscode error \"Function 'baml::Chat' expects 1 arguments, but got 2baml\" when i do {{ _.role(\"user\", cache_control={\"type\": \"ephemeral\"}) }}"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:22:31.983000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:22:32.414000+00:00",
                "content": "Its a yellow swigly line?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-04 16:22:42.444000+00:00",
                "content": "orange"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:22:42.791000+00:00",
                "content": "or like it prevents it from working?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-04 16:22:57.842000+00:00",
                "content": "it runs, i am not sure if the caching is working"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:23:17.388000+00:00",
                "content": "perfect, yea we are working on the jinja static analyzer and it should be updated soon. \n\nWhat version of VSCode Extension are you on?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-04 16:23:21.202000+00:00",
                "content": "but i think that is just vscode thinking it's a syntax error"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:23:27.204000+00:00",
                "content": "i think the latest one doesn't have that bug"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-10-04 16:23:36.120000+00:00",
                "content": "0.58"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:24:29.677000+00:00",
                "content": "odd, ok i'll take a look over the weekend, that our jinja static analyzer being a bit wonky"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:24:43.995000+00:00",
                "content": "one way to confirm is actually to take a look over at the Raw Curl"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:25:00.419000+00:00",
                "content": "and just see if its sending the paramter correctly"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:25:40.896000+00:00",
                "content": "you also need to modify the client to actually enable cache_control as well via: `allowed_role_metadata`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:25:49.406000+00:00",
                "content": "see: https://docs.boundaryml.com/docs/snippets/clients/providers/anthropic"
            }
        ]
    },
    {
        "thread_id": 1292194765420499025,
        "thread_name": "Recursion",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-10-05 18:40:39.894000+00:00",
                "content": "does BAML support recursive types or mutually recursive types?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-06 01:02:37.911000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-06 01:02:40.870000+00:00",
                "content": "Soon! Current eta is around 2 weeks"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-10 23:05:08.064000+00:00",
                "content": "so excited for this, for any UI generation stuff it is super duper key"
            }
        ]
    },
    {
        "thread_id": 1292885897271967754,
        "thread_name": "vllm with images",
        "messages": [
            {
                "author": "xyan4330",
                "timestamp": "2024-10-07 16:26:58.568000+00:00",
                "content": "Hi! I am trying to run some few shot using openbmb/MiniCPM-V-2_6 model served with vLLM. When I run the few-shot example using BAML playground it works well since the raw response includes the actual request to LLM and the few shot example, however the parsed LLM response just shows the first, thus the few shot example parsed, but not the response from the real request. Going back, my problem is that when I use the FastAPI endpoint to send requests using the url of the image or the loaded image in base64 using the BAML Image type as `Image.from_base64(media_type, image_base64)` in order to send the image part of the prompt I get the following error  `ERROR 10-07 14:37:42 serving_chat.py:161] Error in loading multi-modal data: Invalid 'image_url': A valid 'image_url' must start with either 'data:image' or 'http'` I had already set the vllm serve to accept more images, as the BAML playground works just fine. Not sure why this is happening and unfortunately cannot find any relevant information about this to keep trying stuff üò¶\nThank you in advance!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 17:24:54.179000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 17:24:54.478000+00:00",
                "content": "do you send both base64s and urls?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 19:21:15.738000+00:00",
                "content": "you can also try to enable BAML_LOG=debug to see what is being sent in your program"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 19:21:28.566000+00:00",
                "content": "and also inspect the request/responses in the vllm model logs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 19:21:51.998000+00:00",
                "content": "let me know if it fails with both urls AND base64, or just urls"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-07 21:00:20.524000+00:00",
                "content": "It happens with both urls and base64 but only if I am trying to send two images in a single request. If there is only one image there is no problem"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:41:29.372000+00:00",
                "content": "hmm I'll test our multiimage support, but if you can check the logs on the server running the open source model for the raw request (or enable BAML_LOG=debug) it may also help figure out if there's something off!"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-08 07:46:49.568000+00:00",
                "content": "From vllm side I have the following\n```ERROR 10-08 07:40:43 serving_chat.py:161] Error in loading multi-modal data: Invalid 'image_url': A valid 'image_url' must start with either 'data:image' or 'http'.\nINFO:     127.0.0.1:37832 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request```\nI set this env vars in order to have more debugging from vllm\nexport VLLM_LOGGING_LEVEL=DEBUG\nexport VLLM_TRACE_FUNCTION=1\nI am not sure if there is a better way tho\n\nFrom BAML side, I attached you the logs in a file since they are too long.\n\nThank you!"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-08 07:58:48.734000+00:00",
                "content": "In case it helps, this is my endpoint for the url\n```python\n@app.post(\"/get_bill_data_from_url_few_shot/\")\nasync def get_bill_data_from_url_few_shot(\n    url: str = \"https://s1.elespanol.com/2023/05/02/reportajes/760684243_232838530_1024x576.jpg\",\n):\n    media_type = \".jpg\"\n    # url = \"https://i.redd.it/adzt4bz4llfc1.jpeg\"\n    # Load image in base64\n    example_receipt_path = \"./data/few_shot/xuroy_playa_receipt.jpg\"\n    with open(example_receipt_path, \"rb\") as f:\n        example_receipt_base64 = base64.b64encode(f.read())\n\n    example_receipt = Image.from_base64(\n        media_type, example_receipt_base64.decode(\"utf-8\")\n    )\n    # print(\"example_receipt: \", example_receipt)\n    # Load receipt output\n    with open(\"./data/few_shot/xuroy_playa_receipt.json\", \"r\") as f:\n        example_receipt_output = json.dumps(json.load(f))\n    image = Image.from_url(url)\n\n    result = await b.ExtractReceiptFewShot(\n        example_receipt, example_receipt_output, image\n    )\n    result_to_file = json.loads(result.model_dump_json())\n\n    filename = f\"{uuid.uuid4()}.json\"\n    with open(f\"results/{filename}\", \"w\") as f:\n        json.dump(result_to_file, f, indent=4, ensure_ascii=False)\n    return result\n```"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-08 07:58:50.188000+00:00",
                "content": "and this is the BAML function\n```\nfunction ExtractReceiptFewShot(example_receipt: image | string, example_receipt_output: string, user_receipt: image | string) -> Receipt {\n  // see clients.baml\n  client MiniCPMV2_6\n  prompt #\"\n    {# start a system message #}\n    {{ _.role(\"system\") }}\n    Act as a Object Character Detection system, specialized on getting the information within a receipt. The following is an example of what you must do.\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ example_receipt }}\n    ---\n    {{ example_receipt_output }}\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ user_receipt }}\n    ---\n    {# special macro to print the output schema instructions. #}\n    {{ ctx.output_format }}\n  \"#\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 15:13:35.277000+00:00",
                "content": "Thanks ill take a look today"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-08 18:49:06.612000+00:00",
                "content": "Alright, thank you in advance"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 22:25:08.588000+00:00",
                "content": "ahh your media type must be \"image/jpg\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 22:25:17.599000+00:00",
                "content": "that will fix it, sorry for the delay"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 22:25:31.269000+00:00",
                "content": "i was just running your ocde and noticed that bug once i started typing things out"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 14:51:01.108000+00:00",
                "content": "Yes! That solves the issue! Thank you very much and sorry for the delay"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 14:59:50.797000+00:00",
                "content": "Do you have any suggestion on how to create few shot functions in BAML? I was searching for some examples and didn't find anything. Tried to make my own but I have the issue that the model is returning the output response from the first example and not from the actual request, however the raw response contains at the end the response for every example added to the prompt and the request itself"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:47:38.750000+00:00",
                "content": "ah I see, the model returns 2 json blobs right?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:47:41.661000+00:00",
                "content": "in the raw response?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:48:09.725000+00:00",
                "content": "<@99252724855496704>  i think this may be our parser choosing the first json blob (if the model is indeed returning 2)"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 15:53:45.553000+00:00",
                "content": "yes! Thats the case"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 15:55:43.050000+00:00",
                "content": "Here is an example of the raw response\n```json\nUsing the provided images, here is the information extracted from the receipts in the JSON format:\n\nReceipt 1:\n{\n  \"establishment_name\": \"XUROY PLAYA S.L.\",\n  \"date\": \"2019-08-20T15:28:51.000Z\",\n  \"total\": 179.7,\n  \"currency\": \"EUR\",\n  \"items\": [\n    {\n      \"name\": \"Agua grande\",\n      \"price\": 3,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Estrella Galicia sin alcohol\",\n      \"price\": 2.8,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Ca√±a\",\n      \"price\": 14.4,\n      \"quantity\": 6\n    },\n    {\n      \"name\": \"Chipirones fritos\",\n      \"price\": 15.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Croquetas erizo\",\n      \"price\": 12,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Ensalada caprese\",\n      \"price\": 13.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Cap roig a la menorquina\",\n      \"price\": 66,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Arroz negro sepia y gambas\",\n      \"price\": 36,\n      \"quantity\": 2\n    },\n    {\n      \"name\": \"Sepia troceada\",\n      \"price\": 16.5,\n      \"quantity\": 1\n    }\n  ],\n  \"taxes\": 0,\n  \"tip\": 0\n}\n```\n\nReceipt 2:\n```json\n{\n  \"establishment_name\": \"The Living Room\",\n  \"date\": \"2019-08-24T12:40:00.000Z\",\n  \"total\": 52.64,\n  \"currency\": \"USD\",\n  \"items\": [\n    {\n      \"name\": \"Porcini M Burger\",\n      \"price\": 18,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Porcini M Burger\",\n      \"price\": 18,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Side Crispy Brussel Sprouts\",\n      \"price\": 5,\n      \"quantity\": 1\n    }\n  ],\n  \"taxes\": 3.44,\n  \"tip\": 8.2,\n  \"tip_percentage\": 20.0\n}\n```"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 15:56:25.283000+00:00",
                "content": "However, this is the parsed response returned\n```json\n{\n  \"establishment_name\": \"XUROY PLAYA S.L.\",\n  \"date\": \"2019-08-20T15:28:51.000Z\",\n  \"total\": 179.7,\n  \"currency\": \"EUR\",\n  \"items\": [\n    {\n      \"name\": \"Agua grande\",\n      \"price\": 3,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Estrella Galicia sin alcohol\",\n      \"price\": 2.8,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Ca√±a\",\n      \"price\": 14.4,\n      \"quantity\": 6\n    },\n    {\n      \"name\": \"Chipirones fritos\",\n      \"price\": 15.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Croquetas erizo\",\n      \"price\": 12,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Ensalada caprese\",\n      \"price\": 13.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Cap roig a la menorquina\",\n      \"price\": 66,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Arroz negro sepia y gambas\",\n      \"price\": 36,\n      \"quantity\": 2\n    },\n    {\n      \"name\": \"Sepia troceada\",\n      \"price\": 16.5,\n      \"quantity\": 1\n    }\n  ],\n  \"taxes\": 0,\n  \"tip\": 0\n}\n```"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 15:59:36.216000+00:00",
                "content": "I think I find a solution"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 15:59:38.545000+00:00",
                "content": "{{ ctx.output_format[-1] }}"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:00:08.022000+00:00",
                "content": "this returns the last json of the raw response and so it returns the parsed response correcly"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:00:28.183000+00:00",
                "content": "correct me if I am wrong, but for now this works for me in a couple of tests"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:00:38.608000+00:00",
                "content": "Do you want it to return both receipts or one?"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:00:46.922000+00:00",
                "content": "just the last one"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:01:02.010000+00:00",
                "content": "You could also make the type a receoipt array"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:01:12.591000+00:00",
                "content": "And index into the last one"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:03:05.577000+00:00",
                "content": "In order to do that I need to create another class that has as a parameter the array of Receipt? or I could declare it on the Receipt class itself?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:08:57.341000+00:00",
                "content": "you can do it like this:\nReceipt[]"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:09:00.317000+00:00",
                "content": "and it'll just work"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:11:05.268000+00:00",
                "content": "But do you mean on the class declaration or in the return declaration of the BAML function?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:11:58.727000+00:00",
                "content": "the return declaration üôÇ"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 16:12:16.389000+00:00",
                "content": "<@201399017161097216> 's suggestion is likely going to work!\n\ncan you share a bit more of your prompt as well?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 16:12:33.062000+00:00",
                "content": "`{{ ctx.output_format[-1] }}` won't work fyi!"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:13:12.661000+00:00",
                "content": "sure"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:13:42.826000+00:00",
                "content": "function ExtractReceiptFewShot(example_receipt: image | string, example_receipt_output: string, user_receipt: image | string) -> Receipt[] {\n  // see clients.baml\n  client MiniCPMV2_6\n  prompt #\"\n    {# start a system message #}\n    {{ _.role(\"system\") }}\n    Act as a Object Character Detection system, specialized on getting the information within a receipt. The following is an example of what you must do.\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ example_receipt }}\n    ---\n    {{ example_receipt_output }}\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ user_receipt }}\n    ---\n    {# special macro to print the output schema instructions. #}\n    {{ ctx.output_format }}\n  \"#"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:13:51.497000+00:00",
                "content": "This is the prompt for the few shot"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 16:14:54.266000+00:00",
                "content": "a couple of things that may help:\n\nmove:\n\n    {# special macro to print the output schema instructions. #}\n    {{ ctx.output_format }}\n\ninto the system message before the example"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:16:46.926000+00:00",
                "content": "make sense"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:16:53.436000+00:00",
                "content": "thanks"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:33:57.281000+00:00",
                "content": ""
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-10-10 16:34:35.020000+00:00",
                "content": "sorry for the formating of the message, discord converted it as a file bc of the extension limit"
            }
        ]
    },
    {
        "thread_id": 1292980139184885761,
        "thread_name": "LMDeploy",
        "messages": [
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:41:27.591000+00:00",
                "content": "I was gonna serve an mmlm w LMDeploy instead of vLLM, but  having trouble getting structured output and I stumbled on baml"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:42:01.370000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:42:01.709000+00:00",
                "content": "nice -- it should work the same if LMDeploy supports the OpenAI message format. Let me check their docs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:43:08.594000+00:00",
                "content": "based off this:\nhttps://lmdeploy.readthedocs.io/en/latest/llm/api_server.html#integrate-with-openai\n\nyou should be able to use our `openai-generic` client provider:\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/openai-generic"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:43:19.783000+00:00",
                "content": "ah awesome thanks, wasn't entirely sure if that was the only necessity"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:43:28.226000+00:00",
                "content": "ill try it out, I appreciate your fast response"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:43:29.846000+00:00",
                "content": "that's pretty much it!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:43:46.635000+00:00",
                "content": "no worries, let me know if it works, and whether BAML also gave you better responses"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:44:02.150000+00:00",
                "content": "awesome product btw, I hadn't heard of it but I just won a hackathon sponsored by AI tinkerers"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:44:18.488000+00:00",
                "content": "and so it was a funny coincidence as I was looking for better ways sfor structured output"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:44:38.339000+00:00",
                "content": "Nice! we sometimes host AI tinkerers in Seattle at our office. Which city was your hackathon in?"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:44:56.929000+00:00",
                "content": "Austin"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:45:12.789000+00:00",
                "content": "did you just google for better structured outputs and found us?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:46:06.714000+00:00",
                "content": "one thing to check is the `raw curl` checkbox in the VSCode Playground. You can try running that in your terminal and comparing it to what LMDeploy docs say"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:46:27.137000+00:00",
                "content": "yea I either was looking up pydantic alternatives or maybe I saw on YCs site when I was searching for some tools that would help me"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:46:35.393000+00:00",
                "content": "nice"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:46:44.212000+00:00",
                "content": "will do"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:47:51.982000+00:00",
                "content": "what would I be looking for with the raw curl output?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:48:35.373000+00:00",
                "content": "it should look like the curl request for /v1/chat/completions example:\nhttps://lmdeploy.readthedocs.io/en/latest/llm/api_server.html#integrate-with-curl"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 22:48:58.741000+00:00",
                "content": "(you can also just run a test in the playground and see what kind of error you get)"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:56:35.870000+00:00",
                "content": "awesome, thank you very much for your help"
            },
            {
                "author": "nathan9086",
                "timestamp": "2024-10-07 22:58:03.725000+00:00",
                "content": "are you trying to onboard more people to the platform atm? I can help spread the word if so"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 23:58:27.913000+00:00",
                "content": "yes definitely, if we can help you in that regard lmk -- you can definitely tell people that we can meet em 1:1 or answer their questions whenever"
            }
        ]
    },
    {
        "thread_id": 1293002856672006207,
        "thread_name": "parsing issue",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:11:43.862000+00:00",
                "content": "output from CURL:\n\n`{\"id\":\"chatcmpl-MmJJ9RuMoxE8nkA4Mn9wTF\",\"object\":\"chat.completion\",\"created\":1728346255,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The source of this article is CNN.\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":3167,\"total_tokens\":3175,\"completion_tokens\":8}}(base)`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:14:03.488000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:14:03.733000+00:00",
                "content": "does it work if you use an openai model or a diff model?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:14:25.025000+00:00",
                "content": "yes openai's mini model works"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:15:04.038000+00:00",
                "content": "hnm let me try it with ollama"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:15:53.891000+00:00",
                "content": "what open source provider do you use? vllm? ollama?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:16:51.407000+00:00",
                "content": "I think tgi"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:17:10.883000+00:00",
                "content": "they (we) are working on migrating to vllm"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:17:29.628000+00:00",
                "content": "btw, mistral served on vllm works -- \n\n{\"id\":\"cmpl-3793367f8cb84563bdb999961fe64022\",\"object\":\"chat.completion\",\"created\":1728346574,\"model\":\"TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"CNN\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":3854,\"total_tokens\":3857,\"completion_tokens\":3}}(base)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:18:40.466000+00:00",
                "content": "i think i know what the problem is -- can you prompt it to only output the name of the source"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:18:42.250000+00:00",
                "content": "i'll try to find a llama model that's served behind vllm soon"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:19:25.754000+00:00",
                "content": "so it only outputs `CNN`, we may  have a regression in how we find the enums in text"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:20:07.197000+00:00",
                "content": "you're on version 0.59.0 right?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:20:08.784000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:21:07.418000+00:00",
                "content": "yeah so your previous one with meta-llama was saying \"source of this article is CNN\" instead of \"CNN\". You can also try rerunning this one example with Mistral and do the opposite -- tell it to say it in a full sentence"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:23:15.398000+00:00",
                "content": "interesting --"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:23:37.238000+00:00",
                "content": "ok something is wonky with TGI (hugging face's endpoint no?)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:24:01.024000+00:00",
                "content": "copy the raw curl request from the checkbox in the playground and see if you can see the difference in responses"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:26:26.175000+00:00",
                "content": "vllm response:\n\n`{\"id\":\"cmpl-d122d742ff5b4932b9780913853d7cf7\",\"object\":\"chat.completion\",\"created\":1728347063,\"model\":\"TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"This article is from CNN.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":3858,\"total_tokens\":3865,\"completion_tokens\":7}}\n`\ntgi reponse:\n\n`{\"id\":\"chatcmpl-BdfzU5SBWEJFp7SkKsHj4E\",\"object\":\"chat.completion\",\"created\":1728347116,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The source of this article is CNN.\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":3171,\"total_tokens\":3179,\"completion_tokens\":8}}\n`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:27:27.975000+00:00",
                "content": "I'm not sure about the tokenizer but there are about 700 extra tokens for vllm"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:27:54.317000+00:00",
                "content": "ok this is weird -- can you paste the client definition for TGI and vllm?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:30:02.495000+00:00",
                "content": "client<llm> MistralClient {\n  provider openai-generic\n  options {\n    model \"TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ\"\n    max_tokens 2000\n    base_url \"http://seekr-mixtral.artemis-stg.lv-02.k8s/v1\"\n    temperature 0.1\n  }\n}\n\nclient<llm> Llama8b {\n  provider openai-generic\n  options {\n    api_key env.SEEKR_API_KEY\n    model \"meta-llama/Meta-Llama-3-70B-Instruct\"\n    max_tokens 2000\n    base_url \"http://llm-serving-svc-fastchat-api.llm.k8s.prd.int.seekr.com:8001/v1/inference\"\n    temperature 0.1\n    default system\n  }\n}"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:31:22.215000+00:00",
                "content": "your extension is 0.59.0 right?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:32:21.212000+00:00",
                "content": "the baml version is 0.58.0:\n\n[[package]]\nname = \"baml-py\"\nversion = \"0.58.0\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:32:33.769000+00:00",
                "content": "does something show up in the playground when you hover over the \"i\""
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:33:09.816000+00:00",
                "content": "for mistral it says succecced"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:33:14.102000+00:00",
                "content": "does the \"raw prompt\" show up when you use Llama?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:33:42.308000+00:00",
                "content": "maybe just post a screenshot of the test run with llama / and hover over the \"i\""
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:33:45.271000+00:00",
                "content": "for llama it also says succecced"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:33:56.866000+00:00",
                "content": "so it succeeds in the playground but not in python?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:34:33.866000+00:00",
                "content": "I'm not sure"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:35:22.564000+00:00",
                "content": "i can try in a notebook"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:35:37.064000+00:00",
                "content": "i can hop on a quick calll tomorrow morning if that helps. You can post screenshot of the playground test run with llama -- it should give you the parsed output out"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:36:15.300000+00:00",
                "content": "tomorrow morning sounds good"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 00:36:19.864000+00:00",
                "content": "cool"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 00:36:23.428000+00:00",
                "content": "thanks"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:43:24.731000+00:00",
                "content": "following up on this thread:\n\nsomehow the env variable, SEEKR_API_KEY_DEV, is not getting populated\n\n`curl -X POST 'http://llm-serving-svc-fastchat-api.llm.k8s.dev.int.seekr.com:8001/v1/inference/chat/completions' -H \"authorization: Bearer ${SEEKR_API_KEY_DEV}\" -H \"content-type: application/json\" -d \"{\n  \\\"max_tokens\\\": 2000,\n  \\\"model\\\": \\\"mistralai/Mistral-Nemo-Instruct-2407\\\",`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:43:51.908000+00:00",
                "content": "i see the env variable in the shell:\n\n`echo ${SEEKR_API_KEY_DEV}`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 16:43:52.711000+00:00",
                "content": "<@99252724855496704> have you ran into this?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 16:44:10.483000+00:00",
                "content": "oh -- can you make sure you load the env vars before you import baml in your code?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 16:44:29.441000+00:00",
                "content": "so does it work via curl, but not in python?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 16:44:39.094000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/set-env-vars"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:44:46.044000+00:00",
                "content": "i was checking the raw curl variables"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:44:58.529000+00:00",
                "content": "the environment variables have been set a while back though"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 16:45:18.917000+00:00",
                "content": "you should run: \n```\necho ${SEEKR_API_KEY_DEV}\n```\n\nor:\n```\necho $SEEKR_API_KEY_DEV\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 16:45:25.553000+00:00",
                "content": "and see if they output something"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:45:38.871000+00:00",
                "content": "yes"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:46:06.670000+00:00",
                "content": "i was trying to run the unit test"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 16:46:06.819000+00:00",
                "content": "hmm, do you want to hop on a quick discord chat?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:46:09.387000+00:00",
                "content": "yes"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 16:46:10.044000+00:00",
                "content": "may be faster"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-08 16:57:08.425000+00:00",
                "content": "here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 16:58:20.159000+00:00",
                "content": "functino calling / tool use chat bots usually look something like this:\n```typescript\n// myfile.baml\nclass Tool1 {\n  field string\n}\n\nclass Tool2 {\n  field1 string\n}\n\nclass ParallelTool3 {\n  field2 string\n}\n\nclass FinalResult {\n  resume string\n}\n\nfunction DoSomething(query: str, history: string[]) -> Tool1 | Tool2 | ParallelTool3[] | FinalResult {\n  client \"openai/gpt-4o\"\n  prompt #\"\n     some prose...\n     {{ ctx.output_format }} \n     {% if history %} \n     {{ _.role('user') }}\n     {% for h in history %}\n     {{h}}\n     {% endfor %}\n     {% endif %}\n  \"#\n}\n```\n\n```python\ndef resolve(chat: str) -> FinalResult:\n  history = []\n  while True:\n    res = b.DoSomething(chat, history)\n    if isinstance(res, FinalResult):\n       return res\n    if isinstance(res, Tool1):\n       history.append(my_tool1_function(res))\n    if isinstance(res, Tool2):\n       history.append(my_tool2_function(res))    \n    if isinstance(res, list):\n      for tool in res:\n        history.append(my_tool3_function(tool))\n\ndef my_tool1_function(res: Tool1):\n   ...\n```"
            }
        ]
    },
    {
        "thread_id": 1293195411875696671,
        "thread_name": "dynamic types",
        "messages": [
            {
                "author": "goalpha22",
                "timestamp": "2024-10-08 12:56:52.600000+00:00",
                "content": "Hi - I have a usecase where we want to extract specific fields from a document. Our user can define the schema they want, in a JSON format. Any pointers on how to do this with BAML?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:32:28.017000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:32:28.906000+00:00",
                "content": "I think you'll want to use this feature:\nhttps://docs.boundaryml.com/docs/calling-baml/dynamic-types"
            }
        ]
    },
    {
        "thread_id": 1293201832172060767,
        "thread_name": "getting raw json",
        "messages": [
            {
                "author": "aethrvmn_32073",
                "timestamp": "2024-10-08 13:22:23.318000+00:00",
                "content": "Hey, the baml output displays a JSON as the Parsed Response which is then converted into a python object, is there a way to save the JSON or do we have to manipulate the object afterwards?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:33:26.098000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:33:31.875000+00:00",
                "content": "yep! We usually return pydantic objects, so you can use `.model_dump()` or `.dict()`!"
            }
        ]
    },
    {
        "thread_id": 1293334069287059486,
        "thread_name": "class alias",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:07:51.103000+00:00",
                "content": "there's no support for aliasing classes right now right? just fields?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:33:19.259000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:33:24.947000+00:00",
                "content": "currently, there's no way to hoist a class so aliasing would be a no-op. all classes are defined inline"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:40:06.353000+00:00",
                "content": "i wanted to alias two different versions of a class i use in different functions to the same LLM-facing alias cause they mean the same thing to it (but are appropriate in different contexts)"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:40:30.847000+00:00",
                "content": "i need to differentiate them on my end"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:40:32.022000+00:00",
                "content": "could you share an example snippet?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:42:30.561000+00:00",
                "content": "i have a series of UI generator functions that each produce a different output type, like `CreateFormPage() -> FormPage` and `CreateShowPage() -> ShowPage` . `FormPage` and `ShowPage` are different types to me and i care about the differences, but when I am writing prompts, I dont want to have to constantly refer to the two different types that the LLM will later have to generate, I want to alias them both to `Page` and talk about `Page` in the prompt, knowing it might take different values for different calls later"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:42:48.223000+00:00",
                "content": "`FormPage` and `ShowPage` are static also so i dont really want to use `@@dynamic` and lose the other goodness"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:43:25.700000+00:00",
                "content": "leaving them with independent names works but it is a bit annoying, BAML's global namespace means i cant use colliding names for different functions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:43:27.610000+00:00",
                "content": "i see, that makes total sense. I think you can just refer to it as page! and it should work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:43:45.450000+00:00",
                "content": "if you look at the prompt, it doesn't really use the class name for anything"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:43:50.513000+00:00",
                "content": "so i think it'll just work!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:44:06.066000+00:00",
                "content": "the class name is in there though right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:44:11.716000+00:00",
                "content": "nope"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:44:19.597000+00:00",
                "content": "you can check out the prompt previoew"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:44:22.393000+00:00",
                "content": "ah i see, so i probably shoudlnt refer to it at all"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:44:22.715000+00:00",
                "content": "it shouldn't be in there"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:44:26.356000+00:00",
                "content": "yep"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 22:45:05.071000+00:00",
                "content": "yeah havent tried to get the playground going again after the test cases bugged out with the rust recursive error, i will give it another shot"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:45:26.078000+00:00",
                "content": "we are going to support hoisting (similar to what we do with enums):\nhttps://docs.boundaryml.com/docs/snippets/prompt-syntax/output-format#controlling-the-output_format\n\nsee always_hoist_enums"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 22:45:34.350000+00:00",
                "content": "and yea! that bug is fixed now! thanks for spotting that btw"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:32:03.174000+00:00",
                "content": "yeah lmk if you run into the recursive error thing again (and if you have a repro i can go fix it immediately)"
            }
        ]
    },
    {
        "thread_id": 1293516806010568705,
        "thread_name": "bug",
        "messages": [
            {
                "author": "brandburner",
                "timestamp": "2024-10-09 10:13:58.933000+00:00",
                "content": "what am I doing wrong here:\n\n`retry_policy Backoff {\nmax_retries 3\nstrategy {\n  type exponential_backoff\n  delay_ms 600\n  multiplier 1.5\n  max_delay_ms 6000\n}\n\nclient<llm> GPT4o {\n  provider openai\n  retry_policy Backoff\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}`\n\n~/JsonToDocStyler$ baml-cli generate\n[2024-10-09T10:12:33Z ERROR baml_runtime::cli::generate] Error generating clients: Failed to build BAML runtime\n    \n    Caused by:\n        error: Error validating RetryPolicy \"Backoff\": This field declaration is invalid. It is either missing a name or a type.\n          -->  ./baml_src/clients.baml:12\n           | \n        11 | \n        12 | client<llm> GPT4o {\n           | \n        error: Error validating: This line is not a valid field or attribute definition. A valid property may look like: 'myProperty \"some value\"' for example, with no colons.\n          -->  ./baml_src/clients.baml:12\n           | \n        11 | \n        12 | client<llm> GPT4o {\n        13 |   provider openai\n           | \n        error: Property not known: \"provider\". Did you mean one of these: \"options\", \"strategy\", \"max_retries\"?\n          -->  ./baml_src/clients.baml:13\n           | \n        12 | client<llm> GPT4o {\n        13 |   provider openai\n           | \n        error: Property not known: \"retry_policy\". Did you mean one of these: \"strategy\", \"options\", \"max_retries\"?\n          -->  ./baml_src/clients.baml:14\n           | \n        13 |   provider openai\n        14 |   retry_policy Backoff\n           | \n        \n        \n`Traceback (most recent call last):\n  File \"/home/runner/JsonToDocStyler/.pythonlibs/bin/baml-cli\", line 8, in <module>\n    sys.exit(invoke_runtime_cli())\n             ^^^^^^^^^^^^^^^^^^^^\nbaml_py.BamlError: Failed to build BAML runtime`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 13:07:12.649000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 13:07:14.264000+00:00",
                "content": "missed a curly i think!\n\n```\nretry_policy Backoff {\n  max_retries 3\n  strategy {\n    type exponential_backoff\n    delay_ms 600\n    multiplier 1.5\n    max_delay_ms 6000\n  }\n} <---\n\nclient<llm> GPT4o {\n  provider openai\n  retry_policy Backoff\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n```"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-10-09 13:08:03.990000+00:00",
                "content": "doh!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 13:08:29.713000+00:00",
                "content": "kinda wish we could catch taht error for you, but idts üò¢"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 13:08:42.027000+00:00",
                "content": "too hard to make the compiler show that error"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-10-09 13:09:16.300000+00:00",
                "content": "no worries - my rookie mistake üòÑ"
            }
        ]
    },
    {
        "thread_id": 1293713400626413639,
        "thread_name": "release issue",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-09 23:15:10.742000+00:00",
                "content": "Hi, I'm starting to see this issue and I'm not sure what I did wrong:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 23:15:36.552000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 23:15:37.068000+00:00",
                "content": "<@201399017161097216>"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-09 23:15:51.140000+00:00",
                "content": "hm i haven't released 0.60.0 yet"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 23:16:12.081000+00:00",
                "content": "oh, odd, can you help out? Seems like a python environment issue"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-09 23:16:23.527000+00:00",
                "content": "let me see what's going on on 0.59.0 if anything. <@1041386380732923964>  what version of BAML are you on? Do you have time to hop on office hours?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-09 23:16:45.771000+00:00",
                "content": "\"0.59.0\""
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-09 23:16:59.510000+00:00",
                "content": "i'm there"
            }
        ]
    },
    {
        "thread_id": 1293766570694410311,
        "thread_name": "Auto generated baml functions",
        "messages": [
            {
                "author": "demontrius",
                "timestamp": "2024-10-10 02:46:27.474000+00:00",
                "content": "<@99252724855496704>/ <@201399017161097216> is there something in the works for autogenerated baml functions... I have a use case for restructuring unstructured data like LangChain's LLMGraphTransformer and wondering if baml can be used for something like this"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 03:25:43.221000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 03:25:49.328000+00:00",
                "content": "Could you define what you mean by that?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 03:26:08.370000+00:00",
                "content": "Like automatically generate the prompt and return type?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 03:26:58.448000+00:00",
                "content": "one thing that works well for us is:\n\n1.    ‚Å†ask an LLM to generate the schema\n2.    ‚Å†generate against that schema\n\nkinda like this: https://www.youtube.com/watch?v=ZQiEipwZxlw (this is for an image, but it works with raw text as well)\n\nif you don't want to write it yourself, you can just use this API as well: https://docs.boundaryml.com/document-extraction-api/overview/docs/api/extract-data"
            }
        ]
    },
    {
        "thread_id": 1293825328971649036,
        "thread_name": "vim syntax",
        "messages": [
            {
                "author": "kirilligum",
                "timestamp": "2024-10-10 06:39:56.539000+00:00",
                "content": "is there a vim syntax support?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:00:45.754000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:00:47.417000+00:00",
                "content": "Not yet. Do you know what vim uses to render grammars? TreeSitter?"
            },
            {
                "author": "kirilligum",
                "timestamp": "2024-10-10 17:45:17.116000+00:00",
                "content": "yes. i'm using lunarvim (pre-configured vim) https://www.lunarvim.org/docs/features/core-plugins-list it uses https://www.lunarvim.org/docs/features/core-plugins-list by default"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 18:40:09.015000+00:00",
                "content": "we currently don't have this scheduled -- If you wanted to take a stab at a basic tree-sitter grammar to highlight files better you could try porting these: https://github.com/BoundaryML/baml/tree/canary/typescript/vscode-ext/packages/syntaxes\n\nUnfortunately you'd still not get any linting errors / etc until we have proper support for vim"
            }
        ]
    },
    {
        "thread_id": 1294383442028396574,
        "thread_name": "I'm tinkering with asking for responses",
        "messages": [
            {
                "author": "danbecker",
                "timestamp": "2024-10-11 19:37:41.061000+00:00",
                "content": "I'm tinkering with asking for responses as JSON arrays to save the response tokens the LLM otherwise spends repeating back object keys.  It would has a system prompt like\n```\nExtract from this content:\nRespond with a CSV string. The elements and types should be objects with the following fields:\n- company_name: string\n- is_remote: bool\n- city: string\n- job_title: string\n- salary: int\nLeave missing items blank (can have consecutive commas).\n  Don't use quotes around strings and don't give field names. Just values.\n```\n\nIt cuts response tokens in half on the examples I've tried, and the responses make sense just eyeballing them.\n\nI haven't thought much about how to handle arrays in responses... or whether we'd get more errors by relying on ordering instead of explicit keys/names.\n\nDoes baml have a mode for deserializing arrays rather than objects... have others here tinkered with something like this?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-11 19:38:36.456000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-11 19:38:37.143000+00:00",
                "content": "yep! You can do a function that returns arrays.\n\n```\nfunction DoSomething(...) -> MyType[] { .. }\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-11 19:42:17.042000+00:00",
                "content": "let me know if that works!"
            }
        ]
    },
    {
        "thread_id": 1294594116683956244,
        "thread_name": "BAML editor UI component",
        "messages": [
            {
                "author": ".barelyexisting",
                "timestamp": "2024-10-12 09:34:49.813000+00:00",
                "content": "Hey, I am creating a website where i allow the user to input the baml to take the structure. can you guide me on how I can have an editor like the one on promptfiddle"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:03:17.382000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:03:18.561000+00:00",
                "content": "Hey! Out of curiosity are you looking to have someone edit BAML and run it all? Or just define the structure (classes and enums)."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:03:31.529000+00:00",
                "content": "And do you want a single file or multi file?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:04:22.136000+00:00",
                "content": "We don‚Äôt have clear support for this now, but I can point you to a code snippet that does this!"
            },
            {
                "author": ".barelyexisting",
                "timestamp": "2024-10-12 15:47:35.986000+00:00",
                "content": "i just want to define the structure nothing more"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:58:17.942000+00:00",
                "content": "got it, yea let me share some context, the issue will be executing that BAML code. We don't have a good story for this right now as we don't support reflection on BAML types yet from other languages.\n\nWhat I recommend is:\nuse Dynamic types: https://docs.boundaryml.com/docs/calling-baml/dynamic-types\nBut if you are really committed to doing this, I would be down to set up a call with you this upcommign week and showing you how its possible (it technically is!)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:59:07.838000+00:00",
                "content": "https://github.com/aaronvg/extractify/tree/main/myapp"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-12 15:59:11.332000+00:00",
                "content": "This repo does this"
            }
        ]
    }
]