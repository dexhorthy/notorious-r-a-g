[
    {
        "author": "hellovai",
        "content": "BAML editor UI component",
        "timestamp": "2024-10-12 15:03:17.382000+00:00",
        "id": 1294676776219578413,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": ".barelyexisting",
        "content": "Hey, I am creating a website where i allow the user to input the baml to take the structure. can you guide me on how I can have an editor like the one on promptfiddle",
        "timestamp": "2024-10-12 09:34:49.813000+00:00",
        "id": 1294594116683956244,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "This repo does this",
        "timestamp": "2024-10-12 15:59:11.332000+00:00",
        "id": 1294690843705217074,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "https://github.com/aaronvg/extractify/tree/main/myapp",
        "timestamp": "2024-10-12 15:59:07.838000+00:00",
        "id": 1294690829050445886,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "got it, yea let me share some context, the issue will be executing that BAML code. We don't have a good story for this right now as we don't support reflection on BAML types yet from other languages.\n\nWhat I recommend is:\nuse Dynamic types: https://docs.boundaryml.com/docs/calling-baml/dynamic-types\nBut if you are really committed to doing this, I would be down to set up a call with you this upcommign week and showing you how its possible (it technically is!)",
        "timestamp": "2024-10-12 15:58:17.942000+00:00",
        "id": 1294690619771453530,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": ".barelyexisting",
        "content": "i just want to define the structure nothing more",
        "timestamp": "2024-10-12 15:47:35.986000+00:00",
        "id": 1294687927212969994,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "We don’t have clear support for this now, but I can point you to a code snippet that does this!",
        "timestamp": "2024-10-12 15:04:22.136000+00:00",
        "id": 1294677047817277523,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "And do you want a single file or multi file?",
        "timestamp": "2024-10-12 15:03:31.529000+00:00",
        "id": 1294676835556397238,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "Hey! Out of curiosity are you looking to have someone edit BAML and run it all? Or just define the structure (classes and enums).",
        "timestamp": "2024-10-12 15:03:18.561000+00:00",
        "id": 1294676781164396678,
        "parent_id": null,
        "thread_id": 1294594116683956244
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-12 15:03:17.382000+00:00",
        "id": 1294676776219578408,
        "parent_id": 1294594116683956244,
        "thread_id": 1294594116683956244
    },
    {
        "author": "danbecker",
        "content": "I'm tinkering with asking for responses as JSON arrays to save the response tokens the LLM otherwise spends repeating back object keys.  It would has a system prompt like\n```\nExtract from this content:\nRespond with a CSV string. The elements and types should be objects with the following fields:\n- company_name: string\n- is_remote: bool\n- city: string\n- job_title: string\n- salary: int\nLeave missing items blank (can have consecutive commas).\n  Don't use quotes around strings and don't give field names. Just values.\n```\n\nIt cuts response tokens in half on the examples I've tried, and the responses make sense just eyeballing them.\n\nI haven't thought much about how to handle arrays in responses... or whether we'd get more errors by relying on ordering instead of explicit keys/names.\n\nDoes baml have a mode for deserializing arrays rather than objects... have others here tinkered with something like this?",
        "timestamp": "2024-10-11 19:37:41.061000+00:00",
        "id": 1294383442028396574,
        "parent_id": null,
        "thread_id": 1294383442028396574
    },
    {
        "author": "hellovai",
        "content": "let me know if that works!",
        "timestamp": "2024-10-11 19:42:17.042000+00:00",
        "id": 1294384599576608878,
        "parent_id": null,
        "thread_id": 1294383442028396574
    },
    {
        "author": "hellovai",
        "content": "yep! You can do a function that returns arrays.\n\n```\nfunction DoSomething(...) -> MyType[] { .. }\n```",
        "timestamp": "2024-10-11 19:38:37.143000+00:00",
        "id": 1294383677253619773,
        "parent_id": null,
        "thread_id": 1294383442028396574
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-11 19:38:36.456000+00:00",
        "id": 1294383674372128798,
        "parent_id": 1294383442028396574,
        "thread_id": 1294383442028396574
    },
    {
        "author": ".aaronv",
        "content": "vim syntax",
        "timestamp": "2024-10-10 15:00:45.754000+00:00",
        "id": 1293951364514320471,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "kirilligum",
        "content": "is there a vim syntax support?",
        "timestamp": "2024-10-10 06:39:56.539000+00:00",
        "id": 1293825328971649036,
        "parent_id": null,
        "thread_id": 1293825328971649036
    },
    {
        "author": ".aaronv",
        "content": "we currently don't have this scheduled -- If you wanted to take a stab at a basic tree-sitter grammar to highlight files better you could try porting these: https://github.com/BoundaryML/baml/tree/canary/typescript/vscode-ext/packages/syntaxes\n\nUnfortunately you'd still not get any linting errors / etc until we have proper support for vim",
        "timestamp": "2024-10-10 18:40:09.015000+00:00",
        "id": 1294006575232454768,
        "parent_id": null,
        "thread_id": 1293825328971649036
    },
    {
        "author": "kirilligum",
        "content": "yes. i'm using lunarvim (pre-configured vim) https://www.lunarvim.org/docs/features/core-plugins-list it uses https://www.lunarvim.org/docs/features/core-plugins-list by default",
        "timestamp": "2024-10-10 17:45:17.116000+00:00",
        "id": 1293992768007307337,
        "parent_id": null,
        "thread_id": 1293825328971649036
    },
    {
        "author": ".aaronv",
        "content": "Not yet. Do you know what vim uses to render grammars? TreeSitter?",
        "timestamp": "2024-10-10 15:00:47.417000+00:00",
        "id": 1293951371489316884,
        "parent_id": null,
        "thread_id": 1293825328971649036
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-10 15:00:45.754000+00:00",
        "id": 1293951364514320466,
        "parent_id": 1293825328971649036,
        "thread_id": 1293825328971649036
    },
    {
        "author": "hellovai",
        "content": "Auto generated baml functions",
        "timestamp": "2024-10-10 03:25:43.365000+00:00",
        "id": 1293776452017197152,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "demontrius",
        "content": "<@99252724855496704>/ <@201399017161097216> is there something in the works for autogenerated baml functions... I have a use case for restructuring unstructured data like LangChain's LLMGraphTransformer and wondering if baml can be used for something like this",
        "timestamp": "2024-10-10 02:46:27.474000+00:00",
        "id": 1293766570694410311,
        "parent_id": null,
        "thread_id": 1293766570694410311
    },
    {
        "author": "hellovai",
        "content": "one thing that works well for us is:\n\n1.    ⁠ask an LLM to generate the schema\n2.    ⁠generate against that schema\n\nkinda like this: https://www.youtube.com/watch?v=ZQiEipwZxlw (this is for an image, but it works with raw text as well)\n\nif you don't want to write it yourself, you can just use this API as well: https://docs.boundaryml.com/document-extraction-api/overview/docs/api/extract-data",
        "timestamp": "2024-10-10 03:26:58.448000+00:00",
        "id": 1293776766938386492,
        "parent_id": null,
        "thread_id": 1293766570694410311
    },
    {
        "author": "hellovai",
        "content": "Like automatically generate the prompt and return type?",
        "timestamp": "2024-10-10 03:26:08.370000+00:00",
        "id": 1293776556896030810,
        "parent_id": null,
        "thread_id": 1293766570694410311
    },
    {
        "author": "hellovai",
        "content": "Could you define what you mean by that?",
        "timestamp": "2024-10-10 03:25:49.328000+00:00",
        "id": 1293776477027831831,
        "parent_id": null,
        "thread_id": 1293766570694410311
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-10 03:25:43.221000+00:00",
        "id": 1293776451413348402,
        "parent_id": 1293766570694410311,
        "thread_id": 1293766570694410311
    },
    {
        "author": "arindamkhaled4530",
        "content": "Hi, I'm starting to see this issue and I'm not sure what I did wrong:",
        "timestamp": "2024-10-09 23:15:10.742000+00:00",
        "id": 1293713400626413639,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'm there",
        "timestamp": "2024-10-09 23:16:59.510000+00:00",
        "id": 1293713856832733275,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": "arindamkhaled4530",
        "content": "\"0.59.0\"",
        "timestamp": "2024-10-09 23:16:45.771000+00:00",
        "id": 1293713799207190619,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": ".aaronv",
        "content": "let me see what's going on on 0.59.0 if anything. <@1041386380732923964>  what version of BAML are you on? Do you have time to hop on office hours?",
        "timestamp": "2024-10-09 23:16:23.527000+00:00",
        "id": 1293713705908965448,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": "hellovai",
        "content": "oh, odd, can you help out? Seems like a python environment issue",
        "timestamp": "2024-10-09 23:16:12.081000+00:00",
        "id": 1293713657901092905,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": ".aaronv",
        "content": "hm i haven't released 0.60.0 yet",
        "timestamp": "2024-10-09 23:15:51.140000+00:00",
        "id": 1293713570068037754,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216>",
        "timestamp": "2024-10-09 23:15:37.068000+00:00",
        "id": 1293713511045664768,
        "parent_id": null,
        "thread_id": 1293713400626413639
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-09 23:15:36.552000+00:00",
        "id": 1293713508881399868,
        "parent_id": 1293713400626413639,
        "thread_id": 1293713400626413639
    },
    {
        "author": "hellovai",
        "content": "bug",
        "timestamp": "2024-10-09 13:07:12.753000+00:00",
        "id": 1293560400851370004,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "brandburner",
        "content": "what am I doing wrong here:\n\n`retry_policy Backoff {\nmax_retries 3\nstrategy {\n  type exponential_backoff\n  delay_ms 600\n  multiplier 1.5\n  max_delay_ms 6000\n}\n\nclient<llm> GPT4o {\n  provider openai\n  retry_policy Backoff\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}`\n\n~/JsonToDocStyler$ baml-cli generate\n[2024-10-09T10:12:33Z ERROR baml_runtime::cli::generate] Error generating clients: Failed to build BAML runtime\n    \n    Caused by:\n        error: Error validating RetryPolicy \"Backoff\": This field declaration is invalid. It is either missing a name or a type.\n          -->  ./baml_src/clients.baml:12\n           | \n        11 | \n        12 | client<llm> GPT4o {\n           | \n        error: Error validating: This line is not a valid field or attribute definition. A valid property may look like: 'myProperty \"some value\"' for example, with no colons.\n          -->  ./baml_src/clients.baml:12\n           | \n        11 | \n        12 | client<llm> GPT4o {\n        13 |   provider openai\n           | \n        error: Property not known: \"provider\". Did you mean one of these: \"options\", \"strategy\", \"max_retries\"?\n          -->  ./baml_src/clients.baml:13\n           | \n        12 | client<llm> GPT4o {\n        13 |   provider openai\n           | \n        error: Property not known: \"retry_policy\". Did you mean one of these: \"strategy\", \"options\", \"max_retries\"?\n          -->  ./baml_src/clients.baml:14\n           | \n        13 |   provider openai\n        14 |   retry_policy Backoff\n           | \n        \n        \n`Traceback (most recent call last):\n  File \"/home/runner/JsonToDocStyler/.pythonlibs/bin/baml-cli\", line 8, in <module>\n    sys.exit(invoke_runtime_cli())\n             ^^^^^^^^^^^^^^^^^^^^\nbaml_py.BamlError: Failed to build BAML runtime`",
        "timestamp": "2024-10-09 10:13:58.933000+00:00",
        "id": 1293516806010568705,
        "parent_id": null,
        "thread_id": 1293516806010568705
    },
    {
        "author": "brandburner",
        "content": "no worries - my rookie mistake 😄",
        "timestamp": "2024-10-09 13:09:16.300000+00:00",
        "id": 1293560919044915303,
        "parent_id": null,
        "thread_id": 1293516806010568705
    },
    {
        "author": "hellovai",
        "content": "too hard to make the compiler show that error",
        "timestamp": "2024-10-09 13:08:42.027000+00:00",
        "id": 1293560775293665281,
        "parent_id": null,
        "thread_id": 1293516806010568705
    },
    {
        "author": "hellovai",
        "content": "kinda wish we could catch taht error for you, but idts 😢",
        "timestamp": "2024-10-09 13:08:29.713000+00:00",
        "id": 1293560723645136979,
        "parent_id": null,
        "thread_id": 1293516806010568705
    },
    {
        "author": "brandburner",
        "content": "doh!",
        "timestamp": "2024-10-09 13:08:03.990000+00:00",
        "id": 1293560615754797087,
        "parent_id": null,
        "thread_id": 1293516806010568705
    },
    {
        "author": "hellovai",
        "content": "missed a curly i think!\n\n```\nretry_policy Backoff {\n  max_retries 3\n  strategy {\n    type exponential_backoff\n    delay_ms 600\n    multiplier 1.5\n    max_delay_ms 6000\n  }\n} <---\n\nclient<llm> GPT4o {\n  provider openai\n  retry_policy Backoff\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n```",
        "timestamp": "2024-10-09 13:07:14.264000+00:00",
        "id": 1293560407189098506,
        "parent_id": null,
        "thread_id": 1293516806010568705
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-09 13:07:12.649000+00:00",
        "id": 1293560400415035443,
        "parent_id": 1293516806010568705,
        "thread_id": 1293516806010568705
    },
    {
        "author": "hellovai",
        "content": "class alias",
        "timestamp": "2024-10-08 22:33:19.259000+00:00",
        "id": 1293340478837620896,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "airhorns",
        "content": "there's no support for aliasing classes right now right? just fields?",
        "timestamp": "2024-10-08 22:07:51.103000+00:00",
        "id": 1293334069287059486,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": ".aaronv",
        "content": "yeah lmk if you run into the recursive error thing again (and if you have a repro i can go fix it immediately)",
        "timestamp": "2024-10-08 23:32:03.174000+00:00",
        "id": 1293355259208532110,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "and yea! that bug is fixed now! thanks for spotting that btw",
        "timestamp": "2024-10-08 22:45:34.350000+00:00",
        "id": 1293343562032746597,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "we are going to support hoisting (similar to what we do with enums):\nhttps://docs.boundaryml.com/docs/snippets/prompt-syntax/output-format#controlling-the-output_format\n\nsee always_hoist_enums",
        "timestamp": "2024-10-08 22:45:26.078000+00:00",
        "id": 1293343527337590894,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "yeah havent tried to get the playground going again after the test cases bugged out with the rust recursive error, i will give it another shot",
        "timestamp": "2024-10-08 22:45:05.071000+00:00",
        "id": 1293343439227719711,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "yep",
        "timestamp": "2024-10-08 22:44:26.356000+00:00",
        "id": 1293343276845371454,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "it shouldn't be in there",
        "timestamp": "2024-10-08 22:44:22.715000+00:00",
        "id": 1293343261573910619,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "ah i see, so i probably shoudlnt refer to it at all",
        "timestamp": "2024-10-08 22:44:22.393000+00:00",
        "id": 1293343260223213588,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "you can check out the prompt previoew",
        "timestamp": "2024-10-08 22:44:19.597000+00:00",
        "id": 1293343248496066571,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "nope",
        "timestamp": "2024-10-08 22:44:11.716000+00:00",
        "id": 1293343215440887849,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "the class name is in there though right?",
        "timestamp": "2024-10-08 22:44:06.066000+00:00",
        "id": 1293343191742812171,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "so i think it'll just work!",
        "timestamp": "2024-10-08 22:43:50.513000+00:00",
        "id": 1293343126509064256,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "if you look at the prompt, it doesn't really use the class name for anything",
        "timestamp": "2024-10-08 22:43:45.450000+00:00",
        "id": 1293343105273036875,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "i see, that makes total sense. I think you can just refer to it as page! and it should work",
        "timestamp": "2024-10-08 22:43:27.610000+00:00",
        "id": 1293343030446784691,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "leaving them with independent names works but it is a bit annoying, BAML's global namespace means i cant use colliding names for different functions",
        "timestamp": "2024-10-08 22:43:25.700000+00:00",
        "id": 1293343022435795046,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "`FormPage` and `ShowPage` are static also so i dont really want to use `@@dynamic` and lose the other goodness",
        "timestamp": "2024-10-08 22:42:48.223000+00:00",
        "id": 1293342865245601802,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "i have a series of UI generator functions that each produce a different output type, like `CreateFormPage() -> FormPage` and `CreateShowPage() -> ShowPage` . `FormPage` and `ShowPage` are different types to me and i care about the differences, but when I am writing prompts, I dont want to have to constantly refer to the two different types that the LLM will later have to generate, I want to alias them both to `Page` and talk about `Page` in the prompt, knowing it might take different values for different calls later",
        "timestamp": "2024-10-08 22:42:30.561000+00:00",
        "id": 1293342791166066768,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "could you share an example snippet?",
        "timestamp": "2024-10-08 22:40:32.022000+00:00",
        "id": 1293342293977337877,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "i need to differentiate them on my end",
        "timestamp": "2024-10-08 22:40:30.847000+00:00",
        "id": 1293342289049161769,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "airhorns",
        "content": "i wanted to alias two different versions of a class i use in different functions to the same LLM-facing alias cause they mean the same thing to it (but are appropriate in different contexts)",
        "timestamp": "2024-10-08 22:40:06.353000+00:00",
        "id": 1293342186313875456,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "currently, there's no way to hoist a class so aliasing would be a no-op. all classes are defined inline",
        "timestamp": "2024-10-08 22:33:24.947000+00:00",
        "id": 1293340502695088148,
        "parent_id": null,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-08 22:33:19.259000+00:00",
        "id": 1293340478837620891,
        "parent_id": 1293334069287059486,
        "thread_id": 1293334069287059486
    },
    {
        "author": "hellovai",
        "content": "getting raw json",
        "timestamp": "2024-10-08 13:33:26.282000+00:00",
        "id": 1293204612844617856,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "dynamic types",
        "timestamp": "2024-10-08 13:32:28.204000+00:00",
        "id": 1293204369247834257,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "aethrvmn_32073",
        "content": "Hey, the baml output displays a JSON as the Parsed Response which is then converted into a python object, is there a way to save the JSON or do we have to manipulate the object afterwards?",
        "timestamp": "2024-10-08 13:22:23.318000+00:00",
        "id": 1293201832172060767,
        "parent_id": null,
        "thread_id": 1293201832172060767
    },
    {
        "author": "hellovai",
        "content": "yep! We usually return pydantic objects, so you can use `.model_dump()` or `.dict()`!",
        "timestamp": "2024-10-08 13:33:31.875000+00:00",
        "id": 1293204636303364167,
        "parent_id": null,
        "thread_id": 1293201832172060767
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-08 13:33:26.098000+00:00",
        "id": 1293204612072869890,
        "parent_id": 1293201832172060767,
        "thread_id": 1293201832172060767
    },
    {
        "author": "goalpha22",
        "content": "Hi - I have a usecase where we want to extract specific fields from a document. Our user can define the schema they want, in a JSON format. Any pointers on how to do this with BAML?",
        "timestamp": "2024-10-08 12:56:52.600000+00:00",
        "id": 1293195411875696671,
        "parent_id": null,
        "thread_id": 1293195411875696671
    },
    {
        "author": "hellovai",
        "content": "I think you'll want to use this feature:\nhttps://docs.boundaryml.com/docs/calling-baml/dynamic-types",
        "timestamp": "2024-10-08 13:32:28.906000+00:00",
        "id": 1293204372192493568,
        "parent_id": null,
        "thread_id": 1293195411875696671
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-08 13:32:28.017000+00:00",
        "id": 1293204368463495168,
        "parent_id": 1293195411875696671,
        "thread_id": 1293195411875696671
    },
    {
        "author": "arindamkhaled4530",
        "content": "output from CURL:\n\n`{\"id\":\"chatcmpl-MmJJ9RuMoxE8nkA4Mn9wTF\",\"object\":\"chat.completion\",\"created\":1728346255,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The source of this article is CNN.\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":3167,\"total_tokens\":3175,\"completion_tokens\":8}}(base)`",
        "timestamp": "2024-10-08 00:11:43.862000+00:00",
        "id": 1293002856672006207,
        "parent_id": 1293002431847858246,
        "thread_id": 1293002856672006207
    },
    {
        "author": "hellovai",
        "content": "functino calling / tool use chat bots usually look something like this:\n```typescript\n// myfile.baml\nclass Tool1 {\n  field string\n}\n\nclass Tool2 {\n  field1 string\n}\n\nclass ParallelTool3 {\n  field2 string\n}\n\nclass FinalResult {\n  resume string\n}\n\nfunction DoSomething(query: str, history: string[]) -> Tool1 | Tool2 | ParallelTool3[] | FinalResult {\n  client \"openai/gpt-4o\"\n  prompt #\"\n     some prose...\n     {{ ctx.output_format }} \n     {% if history %} \n     {{ _.role('user') }}\n     {% for h in history %}\n     {{h}}\n     {% endfor %}\n     {% endif %}\n  \"#\n}\n```\n\n```python\ndef resolve(chat: str) -> FinalResult:\n  history = []\n  while True:\n    res = b.DoSomething(chat, history)\n    if isinstance(res, FinalResult):\n       return res\n    if isinstance(res, Tool1):\n       history.append(my_tool1_function(res))\n    if isinstance(res, Tool2):\n       history.append(my_tool2_function(res))    \n    if isinstance(res, list):\n      for tool in res:\n        history.append(my_tool3_function(tool))\n\ndef my_tool1_function(res: Tool1):\n   ...\n```",
        "timestamp": "2024-10-08 16:58:20.159000+00:00",
        "id": 1293256177102356531,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "here",
        "timestamp": "2024-10-08 16:57:08.425000+00:00",
        "id": 1293255876227891223,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "hellovai",
        "content": "may be faster",
        "timestamp": "2024-10-08 16:46:10.044000+00:00",
        "id": 1293253114778091520,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes",
        "timestamp": "2024-10-08 16:46:09.387000+00:00",
        "id": 1293253112022175885,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "hellovai",
        "content": "hmm, do you want to hop on a quick discord chat?",
        "timestamp": "2024-10-08 16:46:06.819000+00:00",
        "id": 1293253101251203082,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "i was trying to run the unit test",
        "timestamp": "2024-10-08 16:46:06.670000+00:00",
        "id": 1293253100626509887,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes",
        "timestamp": "2024-10-08 16:45:38.871000+00:00",
        "id": 1293252984028925962,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "hellovai",
        "content": "and see if they output something",
        "timestamp": "2024-10-08 16:45:25.553000+00:00",
        "id": 1293252928169185360,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "hellovai",
        "content": "you should run: \n```\necho ${SEEKR_API_KEY_DEV}\n```\n\nor:\n```\necho $SEEKR_API_KEY_DEV\n```",
        "timestamp": "2024-10-08 16:45:18.917000+00:00",
        "id": 1293252900335915093,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "the environment variables have been set a while back though",
        "timestamp": "2024-10-08 16:44:58.529000+00:00",
        "id": 1293252814822182994,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "i was checking the raw curl variables",
        "timestamp": "2024-10-08 16:44:46.044000+00:00",
        "id": 1293252762456297472,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "hellovai",
        "content": "https://docs.boundaryml.com/docs/calling-baml/set-env-vars",
        "timestamp": "2024-10-08 16:44:39.094000+00:00",
        "id": 1293252733306142761,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "so does it work via curl, but not in python?",
        "timestamp": "2024-10-08 16:44:29.441000+00:00",
        "id": 1293252692818268255,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "oh -- can you make sure you load the env vars before you import baml in your code?",
        "timestamp": "2024-10-08 16:44:10.483000+00:00",
        "id": 1293252613302648934,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "<@99252724855496704> have you ran into this?",
        "timestamp": "2024-10-08 16:43:52.711000+00:00",
        "id": 1293252538761744394,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "i see the env variable in the shell:\n\n`echo ${SEEKR_API_KEY_DEV}`",
        "timestamp": "2024-10-08 16:43:51.908000+00:00",
        "id": 1293252535393583207,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "following up on this thread:\n\nsomehow the env variable, SEEKR_API_KEY_DEV, is not getting populated\n\n`curl -X POST 'http://llm-serving-svc-fastchat-api.llm.k8s.dev.int.seekr.com:8001/v1/inference/chat/completions' -H \"authorization: Bearer ${SEEKR_API_KEY_DEV}\" -H \"content-type: application/json\" -d \"{\n  \\\"max_tokens\\\": 2000,\n  \\\"model\\\": \\\"mistralai/Mistral-Nemo-Instruct-2407\\\",`",
        "timestamp": "2024-10-08 16:43:24.731000+00:00",
        "id": 1293252421404852254,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "thanks",
        "timestamp": "2024-10-08 00:36:23.428000+00:00",
        "id": 1293009062421594213,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "cool",
        "timestamp": "2024-10-08 00:36:19.864000+00:00",
        "id": 1293009047473225799,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "tomorrow morning sounds good",
        "timestamp": "2024-10-08 00:36:15.300000+00:00",
        "id": 1293009028330557441,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "i can hop on a quick calll tomorrow morning if that helps. You can post screenshot of the playground test run with llama -- it should give you the parsed output out",
        "timestamp": "2024-10-08 00:35:37.064000+00:00",
        "id": 1293008867956883477,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "i can try in a notebook",
        "timestamp": "2024-10-08 00:35:22.564000+00:00",
        "id": 1293008807139610705,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "I'm not sure",
        "timestamp": "2024-10-08 00:34:33.866000+00:00",
        "id": 1293008602885390438,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "so it succeeds in the playground but not in python?",
        "timestamp": "2024-10-08 00:33:56.866000+00:00",
        "id": 1293008447696011324,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "for llama it also says succecced",
        "timestamp": "2024-10-08 00:33:45.271000+00:00",
        "id": 1293008399063322625,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "maybe just post a screenshot of the test run with llama / and hover over the \"i\"",
        "timestamp": "2024-10-08 00:33:42.308000+00:00",
        "id": 1293008386635337772,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "does the \"raw prompt\" show up when you use Llama?",
        "timestamp": "2024-10-08 00:33:14.102000+00:00",
        "id": 1293008268330799230,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "for mistral it says succecced",
        "timestamp": "2024-10-08 00:33:09.816000+00:00",
        "id": 1293008250354139157,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "does something show up in the playground when you hover over the \"i\"",
        "timestamp": "2024-10-08 00:32:33.769000+00:00",
        "id": 1293008099162067036,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "the baml version is 0.58.0:\n\n[[package]]\nname = \"baml-py\"\nversion = \"0.58.0\"",
        "timestamp": "2024-10-08 00:32:21.212000+00:00",
        "id": 1293008046494191706,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "your extension is 0.59.0 right?",
        "timestamp": "2024-10-08 00:31:22.215000+00:00",
        "id": 1293007799042707516,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "client<llm> MistralClient {\n  provider openai-generic\n  options {\n    model \"TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ\"\n    max_tokens 2000\n    base_url \"http://seekr-mixtral.artemis-stg.lv-02.k8s/v1\"\n    temperature 0.1\n  }\n}\n\nclient<llm> Llama8b {\n  provider openai-generic\n  options {\n    api_key env.SEEKR_API_KEY\n    model \"meta-llama/Meta-Llama-3-70B-Instruct\"\n    max_tokens 2000\n    base_url \"http://llm-serving-svc-fastchat-api.llm.k8s.prd.int.seekr.com:8001/v1/inference\"\n    temperature 0.1\n    default system\n  }\n}",
        "timestamp": "2024-10-08 00:30:02.495000+00:00",
        "id": 1293007464673054820,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "ok this is weird -- can you paste the client definition for TGI and vllm?",
        "timestamp": "2024-10-08 00:27:54.317000+00:00",
        "id": 1293006927055552686,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "I'm not sure about the tokenizer but there are about 700 extra tokens for vllm",
        "timestamp": "2024-10-08 00:27:27.975000+00:00",
        "id": 1293006816568938498,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "vllm response:\n\n`{\"id\":\"cmpl-d122d742ff5b4932b9780913853d7cf7\",\"object\":\"chat.completion\",\"created\":1728347063,\"model\":\"TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"This article is from CNN.\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":3858,\"total_tokens\":3865,\"completion_tokens\":7}}\n`\ntgi reponse:\n\n`{\"id\":\"chatcmpl-BdfzU5SBWEJFp7SkKsHj4E\",\"object\":\"chat.completion\",\"created\":1728347116,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"The source of this article is CNN.\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":3171,\"total_tokens\":3179,\"completion_tokens\":8}}\n`",
        "timestamp": "2024-10-08 00:26:26.175000+00:00",
        "id": 1293006557361078272,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "copy the raw curl request from the checkbox in the playground and see if you can see the difference in responses",
        "timestamp": "2024-10-08 00:24:01.024000+00:00",
        "id": 1293005948553527306,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "ok something is wonky with TGI (hugging face's endpoint no?)",
        "timestamp": "2024-10-08 00:23:37.238000+00:00",
        "id": 1293005848787947570,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "interesting --",
        "timestamp": "2024-10-08 00:23:15.398000+00:00",
        "id": 1293005757184479363,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "yeah so your previous one with meta-llama was saying \"source of this article is CNN\" instead of \"CNN\". You can also try rerunning this one example with Mistral and do the opposite -- tell it to say it in a full sentence",
        "timestamp": "2024-10-08 00:21:07.418000+00:00",
        "id": 1293005220397453384,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "",
        "timestamp": "2024-10-08 00:20:08.784000+00:00",
        "id": 1293004974468366337,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "you're on version 0.59.0 right?",
        "timestamp": "2024-10-08 00:20:07.197000+00:00",
        "id": 1293004967812009994,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "so it only outputs `CNN`, we may  have a regression in how we find the enums in text",
        "timestamp": "2024-10-08 00:19:25.754000+00:00",
        "id": 1293004793987465227,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'll try to find a llama model that's served behind vllm soon",
        "timestamp": "2024-10-08 00:18:42.250000+00:00",
        "id": 1293004611518726196,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "i think i know what the problem is -- can you prompt it to only output the name of the source",
        "timestamp": "2024-10-08 00:18:40.466000+00:00",
        "id": 1293004604035829773,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "btw, mistral served on vllm works -- \n\n{\"id\":\"cmpl-3793367f8cb84563bdb999961fe64022\",\"object\":\"chat.completion\",\"created\":1728346574,\"model\":\"TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"CNN\"},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":3854,\"total_tokens\":3857,\"completion_tokens\":3}}(base)",
        "timestamp": "2024-10-08 00:17:29.628000+00:00",
        "id": 1293004306919981177,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "they (we) are working on migrating to vllm",
        "timestamp": "2024-10-08 00:17:10.883000+00:00",
        "id": 1293004228297490432,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "I think tgi",
        "timestamp": "2024-10-08 00:16:51.407000+00:00",
        "id": 1293004146609487932,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "what open source provider do you use? vllm? ollama?",
        "timestamp": "2024-10-08 00:15:53.891000+00:00",
        "id": 1293003905369640981,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "hnm let me try it with ollama",
        "timestamp": "2024-10-08 00:15:04.038000+00:00",
        "id": 1293003696271130656,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes openai's mini model works",
        "timestamp": "2024-10-08 00:14:25.025000+00:00",
        "id": 1293003532638879794,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "does it work if you use an openai model or a diff model?",
        "timestamp": "2024-10-08 00:14:03.733000+00:00",
        "id": 1293003443333501059,
        "parent_id": null,
        "thread_id": 1293002856672006207
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-08 00:14:03.488000+00:00",
        "id": 1293003442305896479,
        "parent_id": 1293002856672006207,
        "thread_id": 1293002856672006207
    },
    {
        "author": "arindamkhaled4530",
        "content": "Hi! I'm tried running  unit test(s) from the prompt-shepards model using Llama3.1 70B-instruct but am getting the following error:\nFailed to coerce value: <root>: Expected Source enum value, got String(\"\").",
        "timestamp": "2024-10-08 00:10:02.576000+00:00",
        "id": 1293002431847858246,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "nathan9086",
        "content": "I was gonna serve an mmlm w LMDeploy instead of vLLM, but  having trouble getting structured output and I stumbled on baml",
        "timestamp": "2024-10-07 22:41:27.591000+00:00",
        "id": 1292980139184885761,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "yes definitely, if we can help you in that regard lmk -- you can definitely tell people that we can meet em 1:1 or answer their questions whenever",
        "timestamp": "2024-10-07 23:58:27.913000+00:00",
        "id": 1292999518220062741,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "are you trying to onboard more people to the platform atm? I can help spread the word if so",
        "timestamp": "2024-10-07 22:58:03.725000+00:00",
        "id": 1292984317273964636,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "awesome, thank you very much for your help",
        "timestamp": "2024-10-07 22:56:35.870000+00:00",
        "id": 1292983948783124572,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "(you can also just run a test in the playground and see what kind of error you get)",
        "timestamp": "2024-10-07 22:48:58.741000+00:00",
        "id": 1292982031445393462,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "it should look like the curl request for /v1/chat/completions example:\nhttps://lmdeploy.readthedocs.io/en/latest/llm/api_server.html#integrate-with-curl",
        "timestamp": "2024-10-07 22:48:35.373000+00:00",
        "id": 1292981933432770634,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "what would I be looking for with the raw curl output?",
        "timestamp": "2024-10-07 22:47:51.982000+00:00",
        "id": 1292981751437725798,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "will do",
        "timestamp": "2024-10-07 22:46:44.212000+00:00",
        "id": 1292981467189870633,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "nice",
        "timestamp": "2024-10-07 22:46:35.393000+00:00",
        "id": 1292981430200176803,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "yea I either was looking up pydantic alternatives or maybe I saw on YCs site when I was searching for some tools that would help me",
        "timestamp": "2024-10-07 22:46:27.137000+00:00",
        "id": 1292981395572002816,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "one thing to check is the `raw curl` checkbox in the VSCode Playground. You can try running that in your terminal and comparing it to what LMDeploy docs say",
        "timestamp": "2024-10-07 22:46:06.714000+00:00",
        "id": 1292981309911728253,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "did you just google for better structured outputs and found us?",
        "timestamp": "2024-10-07 22:45:12.789000+00:00",
        "id": 1292981083733889036,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "Austin",
        "timestamp": "2024-10-07 22:44:56.929000+00:00",
        "id": 1292981017212227616,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "Nice! we sometimes host AI tinkerers in Seattle at our office. Which city was your hackathon in?",
        "timestamp": "2024-10-07 22:44:38.339000+00:00",
        "id": 1292980939240247348,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "and so it was a funny coincidence as I was looking for better ways sfor structured output",
        "timestamp": "2024-10-07 22:44:18.488000+00:00",
        "id": 1292980855978856560,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "awesome product btw, I hadn't heard of it but I just won a hackathon sponsored by AI tinkerers",
        "timestamp": "2024-10-07 22:44:02.150000+00:00",
        "id": 1292980787452448838,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "no worries, let me know if it works, and whether BAML also gave you better responses",
        "timestamp": "2024-10-07 22:43:46.635000+00:00",
        "id": 1292980722377687167,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "that's pretty much it!",
        "timestamp": "2024-10-07 22:43:29.846000+00:00",
        "id": 1292980651959648351,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "ill try it out, I appreciate your fast response",
        "timestamp": "2024-10-07 22:43:28.226000+00:00",
        "id": 1292980645164744724,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "ah awesome thanks, wasn't entirely sure if that was the only necessity",
        "timestamp": "2024-10-07 22:43:19.783000+00:00",
        "id": 1292980609752367236,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "based off this:\nhttps://lmdeploy.readthedocs.io/en/latest/llm/api_server.html#integrate-with-openai\n\nyou should be able to use our `openai-generic` client provider:\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/openai-generic",
        "timestamp": "2024-10-07 22:43:08.594000+00:00",
        "id": 1292980562822430740,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "nice -- it should work the same if LMDeploy supports the OpenAI message format. Let me check their docs",
        "timestamp": "2024-10-07 22:42:01.709000+00:00",
        "id": 1292980282286407690,
        "parent_id": null,
        "thread_id": 1292980139184885761
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-07 22:42:01.370000+00:00",
        "id": 1292980280864538624,
        "parent_id": 1292980139184885761,
        "thread_id": 1292980139184885761
    },
    {
        "author": "nathan9086",
        "content": "Any plan to integrate with LMDeploy soon?",
        "timestamp": "2024-10-07 22:40:15.995000+00:00",
        "id": 1292979838889492481,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": ".aaronv",
        "content": "vllm with images",
        "timestamp": "2024-10-07 17:24:54.288000+00:00",
        "id": 1292900475498139699,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "xyan4330",
        "content": "Hi! I am trying to run some few shot using openbmb/MiniCPM-V-2_6 model served with vLLM. When I run the few-shot example using BAML playground it works well since the raw response includes the actual request to LLM and the few shot example, however the parsed LLM response just shows the first, thus the few shot example parsed, but not the response from the real request. Going back, my problem is that when I use the FastAPI endpoint to send requests using the url of the image or the loaded image in base64 using the BAML Image type as `Image.from_base64(media_type, image_base64)` in order to send the image part of the prompt I get the following error  `ERROR 10-07 14:37:42 serving_chat.py:161] Error in loading multi-modal data: Invalid 'image_url': A valid 'image_url' must start with either 'data:image' or 'http'` I had already set the vllm serve to accept more images, as the BAML playground works just fine. Not sure why this is happening and unfortunately cannot find any relevant information about this to keep trying stuff 😦\nThank you in advance!",
        "timestamp": "2024-10-07 16:26:58.568000+00:00",
        "id": 1292885897271967754,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "sorry for the formating of the message, discord converted it as a file bc of the extension limit",
        "timestamp": "2024-10-10 16:34:35.020000+00:00",
        "id": 1293974975367348245,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "",
        "timestamp": "2024-10-10 16:33:57.281000+00:00",
        "id": 1293974817078513755,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "thanks",
        "timestamp": "2024-10-10 16:16:53.436000+00:00",
        "id": 1293970522761203835,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "make sense",
        "timestamp": "2024-10-10 16:16:46.926000+00:00",
        "id": 1293970495456153600,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "hellovai",
        "content": "a couple of things that may help:\n\nmove:\n\n    {# special macro to print the output schema instructions. #}\n    {{ ctx.output_format }}\n\ninto the system message before the example",
        "timestamp": "2024-10-10 16:14:54.266000+00:00",
        "id": 1293970022925991977,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "This is the prompt for the few shot",
        "timestamp": "2024-10-10 16:13:51.497000+00:00",
        "id": 1293969759653859410,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "function ExtractReceiptFewShot(example_receipt: image | string, example_receipt_output: string, user_receipt: image | string) -> Receipt[] {\n  // see clients.baml\n  client MiniCPMV2_6\n  prompt #\"\n    {# start a system message #}\n    {{ _.role(\"system\") }}\n    Act as a Object Character Detection system, specialized on getting the information within a receipt. The following is an example of what you must do.\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ example_receipt }}\n    ---\n    {{ example_receipt_output }}\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ user_receipt }}\n    ---\n    {# special macro to print the output schema instructions. #}\n    {{ ctx.output_format }}\n  \"#",
        "timestamp": "2024-10-10 16:13:42.826000+00:00",
        "id": 1293969723284918272,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "sure",
        "timestamp": "2024-10-10 16:13:12.661000+00:00",
        "id": 1293969596763865219,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "hellovai",
        "content": "`{{ ctx.output_format[-1] }}` won't work fyi!",
        "timestamp": "2024-10-10 16:12:33.062000+00:00",
        "id": 1293969430673490040,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> 's suggestion is likely going to work!\n\ncan you share a bit more of your prompt as well?",
        "timestamp": "2024-10-10 16:12:16.389000+00:00",
        "id": 1293969360741990503,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "the return declaration 🙂",
        "timestamp": "2024-10-10 16:11:58.727000+00:00",
        "id": 1293969286662197269,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "But do you mean on the class declaration or in the return declaration of the BAML function?",
        "timestamp": "2024-10-10 16:11:05.268000+00:00",
        "id": 1293969062438899843,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "and it'll just work",
        "timestamp": "2024-10-10 16:09:00.317000+00:00",
        "id": 1293968538356154410,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "you can do it like this:\nReceipt[]",
        "timestamp": "2024-10-10 16:08:57.341000+00:00",
        "id": 1293968525873909941,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "In order to do that I need to create another class that has as a parameter the array of Receipt? or I could declare it on the Receipt class itself?",
        "timestamp": "2024-10-10 16:03:05.577000+00:00",
        "id": 1293967050469019770,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "And index into the last one",
        "timestamp": "2024-10-10 16:01:12.591000+00:00",
        "id": 1293966576571383970,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "You could also make the type a receoipt array",
        "timestamp": "2024-10-10 16:01:02.010000+00:00",
        "id": 1293966532191326218,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "just the last one",
        "timestamp": "2024-10-10 16:00:46.922000+00:00",
        "id": 1293966468907536394,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "Do you want it to return both receipts or one?",
        "timestamp": "2024-10-10 16:00:38.608000+00:00",
        "id": 1293966434036088914,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "correct me if I am wrong, but for now this works for me in a couple of tests",
        "timestamp": "2024-10-10 16:00:28.183000+00:00",
        "id": 1293966390310469703,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "this returns the last json of the raw response and so it returns the parsed response correcly",
        "timestamp": "2024-10-10 16:00:08.022000+00:00",
        "id": 1293966305749110814,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "{{ ctx.output_format[-1] }}",
        "timestamp": "2024-10-10 15:59:38.545000+00:00",
        "id": 1293966182113611893,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "I think I find a solution",
        "timestamp": "2024-10-10 15:59:36.216000+00:00",
        "id": 1293966172345077900,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "However, this is the parsed response returned\n```json\n{\n  \"establishment_name\": \"XUROY PLAYA S.L.\",\n  \"date\": \"2019-08-20T15:28:51.000Z\",\n  \"total\": 179.7,\n  \"currency\": \"EUR\",\n  \"items\": [\n    {\n      \"name\": \"Agua grande\",\n      \"price\": 3,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Estrella Galicia sin alcohol\",\n      \"price\": 2.8,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Caña\",\n      \"price\": 14.4,\n      \"quantity\": 6\n    },\n    {\n      \"name\": \"Chipirones fritos\",\n      \"price\": 15.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Croquetas erizo\",\n      \"price\": 12,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Ensalada caprese\",\n      \"price\": 13.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Cap roig a la menorquina\",\n      \"price\": 66,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Arroz negro sepia y gambas\",\n      \"price\": 36,\n      \"quantity\": 2\n    },\n    {\n      \"name\": \"Sepia troceada\",\n      \"price\": 16.5,\n      \"quantity\": 1\n    }\n  ],\n  \"taxes\": 0,\n  \"tip\": 0\n}\n```",
        "timestamp": "2024-10-10 15:56:25.283000+00:00",
        "id": 1293965371514032221,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "Here is an example of the raw response\n```json\nUsing the provided images, here is the information extracted from the receipts in the JSON format:\n\nReceipt 1:\n{\n  \"establishment_name\": \"XUROY PLAYA S.L.\",\n  \"date\": \"2019-08-20T15:28:51.000Z\",\n  \"total\": 179.7,\n  \"currency\": \"EUR\",\n  \"items\": [\n    {\n      \"name\": \"Agua grande\",\n      \"price\": 3,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Estrella Galicia sin alcohol\",\n      \"price\": 2.8,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Caña\",\n      \"price\": 14.4,\n      \"quantity\": 6\n    },\n    {\n      \"name\": \"Chipirones fritos\",\n      \"price\": 15.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Croquetas erizo\",\n      \"price\": 12,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Ensalada caprese\",\n      \"price\": 13.5,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Cap roig a la menorquina\",\n      \"price\": 66,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Arroz negro sepia y gambas\",\n      \"price\": 36,\n      \"quantity\": 2\n    },\n    {\n      \"name\": \"Sepia troceada\",\n      \"price\": 16.5,\n      \"quantity\": 1\n    }\n  ],\n  \"taxes\": 0,\n  \"tip\": 0\n}\n```\n\nReceipt 2:\n```json\n{\n  \"establishment_name\": \"The Living Room\",\n  \"date\": \"2019-08-24T12:40:00.000Z\",\n  \"total\": 52.64,\n  \"currency\": \"USD\",\n  \"items\": [\n    {\n      \"name\": \"Porcini M Burger\",\n      \"price\": 18,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Porcini M Burger\",\n      \"price\": 18,\n      \"quantity\": 1\n    },\n    {\n      \"name\": \"Side Crispy Brussel Sprouts\",\n      \"price\": 5,\n      \"quantity\": 1\n    }\n  ],\n  \"taxes\": 3.44,\n  \"tip\": 8.2,\n  \"tip_percentage\": 20.0\n}\n```",
        "timestamp": "2024-10-10 15:55:43.050000+00:00",
        "id": 1293965194376118384,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "yes! Thats the case",
        "timestamp": "2024-10-10 15:53:45.553000+00:00",
        "id": 1293964701557985322,
        "parent_id": 1293963175284441150,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "<@99252724855496704>  i think this may be our parser choosing the first json blob (if the model is indeed returning 2)",
        "timestamp": "2024-10-10 15:48:09.725000+00:00",
        "id": 1293963292993392730,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "in the raw response?",
        "timestamp": "2024-10-10 15:47:41.661000+00:00",
        "id": 1293963175284441150,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "ah I see, the model returns 2 json blobs right?",
        "timestamp": "2024-10-10 15:47:38.750000+00:00",
        "id": 1293963163074564180,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "Do you have any suggestion on how to create few shot functions in BAML? I was searching for some examples and didn't find anything. Tried to make my own but I have the issue that the model is returning the output response from the first example and not from the actual request, however the raw response contains at the end the response for every example added to the prompt and the request itself",
        "timestamp": "2024-10-10 14:59:50.797000+00:00",
        "id": 1293951134007955486,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "Yes! That solves the issue! Thank you very much and sorry for the delay",
        "timestamp": "2024-10-10 14:51:01.108000+00:00",
        "id": 1293948912331264142,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "i was just running your ocde and noticed that bug once i started typing things out",
        "timestamp": "2024-10-08 22:25:31.269000+00:00",
        "id": 1293338515945554074,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "that will fix it, sorry for the delay",
        "timestamp": "2024-10-08 22:25:17.599000+00:00",
        "id": 1293338458609287291,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "ahh your media type must be \"image/jpg\"",
        "timestamp": "2024-10-08 22:25:08.588000+00:00",
        "id": 1293338420814282802,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "Alright, thank you in advance",
        "timestamp": "2024-10-08 18:49:06.612000+00:00",
        "id": 1293284054346633320,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "Thanks ill take a look today",
        "timestamp": "2024-10-08 15:13:35.277000+00:00",
        "id": 1293229816396382218,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "and this is the BAML function\n```\nfunction ExtractReceiptFewShot(example_receipt: image | string, example_receipt_output: string, user_receipt: image | string) -> Receipt {\n  // see clients.baml\n  client MiniCPMV2_6\n  prompt #\"\n    {# start a system message #}\n    {{ _.role(\"system\") }}\n    Act as a Object Character Detection system, specialized on getting the information within a receipt. The following is an example of what you must do.\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ example_receipt }}\n    ---\n    {{ example_receipt_output }}\n\n    {# start a user message #}\n    {{ _.role(\"user\") }}\n\n    Extract info from this receipt:\n    ---\n    {{ user_receipt }}\n    ---\n    {# special macro to print the output schema instructions. #}\n    {{ ctx.output_format }}\n  \"#\n}\n```",
        "timestamp": "2024-10-08 07:58:50.188000+00:00",
        "id": 1293120407603511296,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "In case it helps, this is my endpoint for the url\n```python\n@app.post(\"/get_bill_data_from_url_few_shot/\")\nasync def get_bill_data_from_url_few_shot(\n    url: str = \"https://s1.elespanol.com/2023/05/02/reportajes/760684243_232838530_1024x576.jpg\",\n):\n    media_type = \".jpg\"\n    # url = \"https://i.redd.it/adzt4bz4llfc1.jpeg\"\n    # Load image in base64\n    example_receipt_path = \"./data/few_shot/xuroy_playa_receipt.jpg\"\n    with open(example_receipt_path, \"rb\") as f:\n        example_receipt_base64 = base64.b64encode(f.read())\n\n    example_receipt = Image.from_base64(\n        media_type, example_receipt_base64.decode(\"utf-8\")\n    )\n    # print(\"example_receipt: \", example_receipt)\n    # Load receipt output\n    with open(\"./data/few_shot/xuroy_playa_receipt.json\", \"r\") as f:\n        example_receipt_output = json.dumps(json.load(f))\n    image = Image.from_url(url)\n\n    result = await b.ExtractReceiptFewShot(\n        example_receipt, example_receipt_output, image\n    )\n    result_to_file = json.loads(result.model_dump_json())\n\n    filename = f\"{uuid.uuid4()}.json\"\n    with open(f\"results/{filename}\", \"w\") as f:\n        json.dump(result_to_file, f, indent=4, ensure_ascii=False)\n    return result\n```",
        "timestamp": "2024-10-08 07:58:48.734000+00:00",
        "id": 1293120401504862230,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "From vllm side I have the following\n```ERROR 10-08 07:40:43 serving_chat.py:161] Error in loading multi-modal data: Invalid 'image_url': A valid 'image_url' must start with either 'data:image' or 'http'.\nINFO:     127.0.0.1:37832 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request```\nI set this env vars in order to have more debugging from vllm\nexport VLLM_LOGGING_LEVEL=DEBUG\nexport VLLM_TRACE_FUNCTION=1\nI am not sure if there is a better way tho\n\nFrom BAML side, I attached you the logs in a file since they are too long.\n\nThank you!",
        "timestamp": "2024-10-08 07:46:49.568000+00:00",
        "id": 1293117385103900692,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "hmm I'll test our multiimage support, but if you can check the logs on the server running the open source model for the raw request (or enable BAML_LOG=debug) it may also help figure out if there's something off!",
        "timestamp": "2024-10-07 22:41:29.372000+00:00",
        "id": 1292980146655072296,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": "xyan4330",
        "content": "It happens with both urls and base64 but only if I am trying to send two images in a single request. If there is only one image there is no problem",
        "timestamp": "2024-10-07 21:00:20.524000+00:00",
        "id": 1292954692061757522,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "let me know if it fails with both urls AND base64, or just urls",
        "timestamp": "2024-10-07 19:21:51.998000+00:00",
        "id": 1292929909907525784,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "and also inspect the request/responses in the vllm model logs",
        "timestamp": "2024-10-07 19:21:28.566000+00:00",
        "id": 1292929811626594324,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "you can also try to enable BAML_LOG=debug to see what is being sent in your program",
        "timestamp": "2024-10-07 19:21:15.738000+00:00",
        "id": 1292929757821931644,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "do you send both base64s and urls?",
        "timestamp": "2024-10-07 17:24:54.478000+00:00",
        "id": 1292900476295184524,
        "parent_id": null,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-07 17:24:54.179000+00:00",
        "id": 1292900475041218611,
        "parent_id": 1292885897271967754,
        "thread_id": 1292885897271967754
    },
    {
        "author": ".aaronv",
        "content": "Recursion",
        "timestamp": "2024-10-06 01:02:37.911000+00:00",
        "id": 1292290890550874177,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "airhorns",
        "content": "does BAML support recursive types or mutually recursive types?",
        "timestamp": "2024-10-05 18:40:39.894000+00:00",
        "id": 1292194765420499025,
        "parent_id": null,
        "thread_id": 1292194765420499025
    },
    {
        "author": "airhorns",
        "content": "so excited for this, for any UI generation stuff it is super duper key",
        "timestamp": "2024-10-10 23:05:08.064000+00:00",
        "id": 1294073260677402645,
        "parent_id": null,
        "thread_id": 1292194765420499025
    },
    {
        "author": ".aaronv",
        "content": "Soon! Current eta is around 2 weeks",
        "timestamp": "2024-10-06 01:02:40.870000+00:00",
        "id": 1292290902961688659,
        "parent_id": null,
        "thread_id": 1292194765420499025
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-06 01:02:37.911000+00:00",
        "id": 1292290890550874172,
        "parent_id": 1292194765420499025,
        "thread_id": 1292194765420499025
    },
    {
        "author": "yungweedle",
        "content": "is this a vscode error \"Function 'baml::Chat' expects 1 arguments, but got 2baml\" when i do {{ _.role(\"user\", cache_control={\"type\": \"ephemeral\"}) }}",
        "timestamp": "2024-10-04 16:18:20.748000+00:00",
        "id": 1291796561780805835,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "see: https://docs.boundaryml.com/docs/snippets/clients/providers/anthropic",
        "timestamp": "2024-10-04 16:25:49.406000+00:00",
        "id": 1291798443588718743,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "you also need to modify the client to actually enable cache_control as well via: `allowed_role_metadata`",
        "timestamp": "2024-10-04 16:25:40.896000+00:00",
        "id": 1291798407895187476,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "and just see if its sending the paramter correctly",
        "timestamp": "2024-10-04 16:25:00.419000+00:00",
        "id": 1291798238122348675,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "one way to confirm is actually to take a look over at the Raw Curl",
        "timestamp": "2024-10-04 16:24:43.995000+00:00",
        "id": 1291798169235099702,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "odd, ok i'll take a look over the weekend, that our jinja static analyzer being a bit wonky",
        "timestamp": "2024-10-04 16:24:29.677000+00:00",
        "id": 1291798109180924006,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "yungweedle",
        "content": "0.58",
        "timestamp": "2024-10-04 16:23:36.120000+00:00",
        "id": 1291797884546846751,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "i think the latest one doesn't have that bug",
        "timestamp": "2024-10-04 16:23:27.204000+00:00",
        "id": 1291797847150166067,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "yungweedle",
        "content": "but i think that is just vscode thinking it's a syntax error",
        "timestamp": "2024-10-04 16:23:21.202000+00:00",
        "id": 1291797821975953469,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "perfect, yea we are working on the jinja static analyzer and it should be updated soon. \n\nWhat version of VSCode Extension are you on?",
        "timestamp": "2024-10-04 16:23:17.388000+00:00",
        "id": 1291797805979009178,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "yungweedle",
        "content": "it runs, i am not sure if the caching is working",
        "timestamp": "2024-10-04 16:22:57.842000+00:00",
        "id": 1291797723997278289,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "or like it prevents it from working?",
        "timestamp": "2024-10-04 16:22:42.791000+00:00",
        "id": 1291797660868804688,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "yungweedle",
        "content": "orange",
        "timestamp": "2024-10-04 16:22:42.444000+00:00",
        "id": 1291797659413123166,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "Its a yellow swigly line?",
        "timestamp": "2024-10-04 16:22:32.414000+00:00",
        "id": 1291797617344249897,
        "parent_id": null,
        "thread_id": 1291796561780805835
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-04 16:22:31.983000+00:00",
        "id": 1291797615536504853,
        "parent_id": 1291796561780805835,
        "thread_id": 1291796561780805835
    },
    {
        "author": ".alex4o",
        "content": "Tokenize",
        "timestamp": "2024-10-03 20:05:24.803000+00:00",
        "id": 1291491317343457336,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does anyone know of a good way to programmatically determine which content should fit in a context window? For example, I know Cursor was doing some interesting stuff with re-ranking the prompt inputs dynamically. Basically trying to prevent the situation where the input exceeds the 128k context window for gpt-4o",
        "timestamp": "2024-10-03 18:19:27.705000+00:00",
        "id": 1291464653741883485,
        "parent_id": null,
        "thread_id": 1291464653741883485
    },
    {
        "author": "gabev2037",
        "content": "Sometimes I have 250k+ tokens since we deal with massive documents. I get that you can use a basic reranker by chunking the input, but I’d love to encode domain expertise to be able to say “these chunks should be weighted more than those”",
        "timestamp": "2024-10-04 17:29:39.004000+00:00",
        "id": 1291814506087059516,
        "parent_id": null,
        "thread_id": 1291464653741883485
    },
    {
        "author": "gabev2037",
        "content": "It’s not a tokenization problem. Suppose I am 100k over the context window. I want an intelligent way to rank which 100k tokens I should remove from the prompt.\n\nI was wondering if anyone had some good algorithms to do this 🙂",
        "timestamp": "2024-10-04 17:28:37.678000+00:00",
        "id": 1291814248867172404,
        "parent_id": null,
        "thread_id": 1291464653741883485
    },
    {
        "author": "hellovai",
        "content": "How long of a context are you running into?",
        "timestamp": "2024-10-03 20:33:28.224000+00:00",
        "id": 1291498378123018295,
        "parent_id": null,
        "thread_id": 1291464653741883485
    },
    {
        "author": ".alex4o",
        "content": "It is pretty easy to tokenize for gpt with tiktoken",
        "timestamp": "2024-10-03 20:05:25.767000+00:00",
        "id": 1291491321386762240,
        "parent_id": null,
        "thread_id": 1291464653741883485
    },
    {
        "author": ".alex4o",
        "content": "",
        "timestamp": "2024-10-03 20:05:24.803000+00:00",
        "id": 1291491317343457331,
        "parent_id": 1291464653741883485,
        "thread_id": 1291464653741883485
    },
    {
        "author": ".alex4o",
        "content": "Is it possible to call a BAML function directly from BAML instead of going through TypeScript I can't seem to find it in the docs",
        "timestamp": "2024-10-03 00:09:10.576000+00:00",
        "id": 1291190274416050258,
        "parent_id": null,
        "thread_id": 1291190274416050258
    },
    {
        "author": ".alex4o",
        "content": "I feel like it is a nice to have, I can still chain thing in TypeScript. This would just make testing the full flow easier.",
        "timestamp": "2024-10-03 00:11:51.419000+00:00",
        "id": 1291190949040619561,
        "parent_id": null,
        "thread_id": 1291190274416050258
    },
    {
        "author": ".aaronv",
        "content": "Is that something youd be interested in?",
        "timestamp": "2024-10-03 00:10:37.526000+00:00",
        "id": 1291190639110783020,
        "parent_id": null,
        "thread_id": 1291190274416050258
    },
    {
        "author": ".aaronv",
        "content": "It s not possible to chain functions yet!",
        "timestamp": "2024-10-03 00:10:29.916000+00:00",
        "id": 1291190607191998615,
        "parent_id": null,
        "thread_id": 1291190274416050258
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-03 00:10:29.380000+00:00",
        "id": 1291190604943851572,
        "parent_id": 1291190274416050258,
        "thread_id": 1291190274416050258
    },
    {
        "author": "arindamkhaled4530",
        "content": "that didn't seem to stop it -- i probably am not doing it right",
        "timestamp": "2024-10-02 22:35:32.954000+00:00",
        "id": 1291166712401756182,
        "parent_id": 1291162943513301105,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "it's just that pylance linter in VSCode will still complain if you change your type signatures (even though the types you import did reload)",
        "timestamp": "2024-10-07 22:55:32.041000+00:00",
        "id": 1292983681064894526,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "i actually added the actual things you need -- it's much simpler:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/python#baml-with-jupyter-notebooks",
        "timestamp": "2024-10-07 22:55:09.872000+00:00",
        "id": 1292983588081373195,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "okay, thanks for the clarification",
        "timestamp": "2024-10-07 22:54:16.462000+00:00",
        "id": 1292983364063592501,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "sorry you must also activate this cell the very first time:\n%load_ext autoreload\n%autoreload 2",
        "timestamp": "2024-10-02 23:19:51.995000+00:00",
        "id": 1291177865228062742,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "cool thanks!",
        "timestamp": "2024-10-02 22:59:18.963000+00:00",
        "id": 1291172693516750919,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "to only restart the baml runtime with your changes, run this:\n\n\nfrom app.baml_client import reset_baml_env_vars\nimport os\n\nreset_baml_env_vars(dict(os.environ))",
        "timestamp": "2024-10-02 22:58:05.034000+00:00",
        "id": 1291172383436181504,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "great!",
        "timestamp": "2024-10-02 22:44:14.012000+00:00",
        "id": 1291168897877282858,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "let me see if there's a more automatic way -- will let you know!",
        "timestamp": "2024-10-02 22:43:54.654000+00:00",
        "id": 1291168816683941981,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "I restart my notebook..",
        "timestamp": "2024-10-02 22:42:56.957000+00:00",
        "id": 1291168574685188127,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "question -- have you figured out how to reload the baml_client in jupyter when you change a .baml file so it picks up your new changes when you re-run a cell?",
        "timestamp": "2024-10-02 22:41:52.091000+00:00",
        "id": 1291168302617333862,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "perfect! Sorry about that, our baml_client is very eager to load env vars when you import it, so usually you have to run load_dotenv() _before_ you try and do \"from baml_client import b\"",
        "timestamp": "2024-10-02 22:41:22.652000+00:00",
        "id": 1291168179141345321,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "for some reason, it didn't work when i set it in bash",
        "timestamp": "2024-10-02 22:40:54.052000+00:00",
        "id": 1291168059184250920,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "it works now after exporting the variable to bashrc",
        "timestamp": "2024-10-02 22:40:24.550000+00:00",
        "id": 1291167935444025445,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes",
        "timestamp": "2024-10-02 22:39:40.701000+00:00",
        "id": 1291167751527989298,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "youre trying to run this in jupyter right?",
        "timestamp": "2024-10-02 22:36:33.438000+00:00",
        "id": 1291166966089908275,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "can you share a screenshot of where you set it and how you import env vars in the notebook?",
        "timestamp": "2024-10-02 22:36:00.094000+00:00",
        "id": 1291166826235039795,
        "parent_id": null,
        "thread_id": 1291166712401756182
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-02 22:35:59.815000+00:00",
        "id": 1291166825064960001,
        "parent_id": 1291166712401756182,
        "thread_id": 1291166712401756182
    },
    {
        "author": "arindamkhaled4530",
        "content": "thanks!",
        "timestamp": "2024-10-02 22:20:41.623000+00:00",
        "id": 1291162973888450570,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": ".aaronv",
        "content": "set BAML_LOG=warn",
        "timestamp": "2024-10-02 22:20:34.381000+00:00",
        "id": 1291162943513301105,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "is there a way to stop BAML logging (in the notebook). I upgraded to the version 0.57.1 and am noticing the logs/outputs are being printed:",
        "timestamp": "2024-10-02 22:20:17.642000+00:00",
        "id": 1291162873304846358,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "any recommendations if I want to track all my LLM calls from baml and store them somewhere (input / output for each LLM api call) and also tagging of calls",
        "timestamp": "2024-10-02 15:32:44.893000+00:00",
        "id": 1291060311041769614,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "f1ddl3r",
        "content": "I guess the one approach would be to return additional raw data in addition to the actual response from baml function calls. But that is a breaking change to the current API. Another option could be to have \"_trace\" attribute  containing the raw data that gets always returned in the function response.\n\nBut of course, I'm not best placed to say how it should work since I just discovered Baml a day ago and tried to implement it for a simple test case. I then ran into the issue of not having a any way to push the required raw prompt/response data to our system and that's how I'm here. 🙂",
        "timestamp": "2024-10-03 08:29:42.861000+00:00",
        "id": 1291316238949290045,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "then we can add in the mechanism to get raw prompts + raw responses",
        "timestamp": "2024-10-03 08:17:51.521000+00:00",
        "id": 1291313255373078540,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "got it! id love to learn more and see how we can provide a great ergonomic experience for you folks and your use case specifically",
        "timestamp": "2024-10-03 08:17:27.130000+00:00",
        "id": 1291313153069809665,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "f1ddl3r",
        "content": "The thing is that we've built observability features straight into our app where we see the configurations, and user data, what prompts those lead to, and then the outputs. \n\nThat makes it tough for us to use a 3p observability platform since the key piece is seeing our app \"setup\" as well. We could push our own configs to a 3p platform to have all the data there, but the configs are complex with bunch of conditional logic, so visualizing those is tough without proper UI.\n\nSo, in our case, kinda would need a way to access the raw prompts & responses.",
        "timestamp": "2024-10-03 08:15:25.174000+00:00",
        "id": 1291312641549140031,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "trade off is just ergonomics vs configuration, we're following the rust approach, where we are really conservative atm, and just learning from users, and plan on having escape hatches for pro users",
        "timestamp": "2024-10-03 08:08:41.103000+00:00",
        "id": 1291310946752532483,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "but we do plan on adding some escape hatches!",
        "timestamp": "2024-10-03 08:08:03.796000+00:00",
        "id": 1291310790275760128,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "we've mostly been building out BAML, but are starting to invest a bit more into the toolchain around it. With some of the new syntax capabilities we're releasing into BAML, I suspect that a vanilla LLM observability will mostly fall short",
        "timestamp": "2024-10-03 08:07:45.393000+00:00",
        "id": 1291310713087856680,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "awesome, yea if you think either of the solutions i posted above would make sense for you folks, glad to have a deeper conversation and help set your team up.",
        "timestamp": "2024-10-03 08:06:29.850000+00:00",
        "id": 1291310396237680663,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "f1ddl3r",
        "content": "Looking for the raw prompt especially, but of course response would be cool to have too.",
        "timestamp": "2024-10-03 08:05:39.152000+00:00",
        "id": 1291310183594725438,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "do you specifically want the LLM prompt + response in the trace?",
        "timestamp": "2024-10-03 08:04:17.323000+00:00",
        "id": 1291309840378892339,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "and in terms of Typescript, you can always trace the inputs and outputs of any BAML function similar to any other function you're tracing",
        "timestamp": "2024-10-03 08:04:00.953000+00:00",
        "id": 1291309771718266981,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "Some of the things we are looking into are:\n* ensuring we only store encrypted data and your browser is actually the thing doing decryption\n* hosting our observability pipeline in your own VPC",
        "timestamp": "2024-10-03 08:03:20.904000+00:00",
        "id": 1291309603740581930,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "f1ddl3r",
        "content": "Using with TypeScript",
        "timestamp": "2024-10-03 08:01:34.861000+00:00",
        "id": 1291309158964006993,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "We have a few different ideas there and tehre are existing workaround already! What language are you using atm with BAML?",
        "timestamp": "2024-10-03 08:01:20.993000+00:00",
        "id": 1291309100797394996,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "f1ddl3r",
        "content": "But I do understand if this is not in the roadmap, given your biz is around observability part. 🙂 Think users like us would be a minority?",
        "timestamp": "2024-10-03 08:00:33.949000+00:00",
        "id": 1291308903480430603,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "f1ddl3r",
        "content": "Hopping in here with a related question! Are you planning to support (or already support) allowing users to build custom tracing for the LLM calls?\n\nI really like the approach you're taking with BAML, but I can't really use it since I need a way to push data to my own custom observability platform. The reason is that we contractually can't push customer data to any 3rd party platforms.",
        "timestamp": "2024-10-03 07:58:59.210000+00:00",
        "id": 1291308506116390955,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": ".aaronv",
        "content": "feel free to use this link to schedule a time, or let me know what time works for you and I can set you up real quick: https://calendly.com/aaron-vi/30min",
        "timestamp": "2024-10-02 15:42:06.646000+00:00",
        "id": 1291062667204886670,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "If you’re interested we’ve got observability for baml available. <@201399017161097216> can set up some time with you today to show you more",
        "timestamp": "2024-10-02 15:35:32.649000+00:00",
        "id": 1291061014661435514,
        "parent_id": null,
        "thread_id": 1291060311041769614
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-02 15:35:31.879000+00:00",
        "id": 1291061011431821383,
        "parent_id": 1291060311041769614,
        "thread_id": 1291060311041769614
    },
    {
        "author": "try_another",
        "content": "Any tips on multi-label classification with BAML?",
        "timestamp": "2024-10-02 15:25:03.052000+00:00",
        "id": 1291058373940346991,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": ".aaronv",
        "content": "woops sorry, you can remove that whole line (look at the prompt on the right side for the full view)",
        "timestamp": "2024-10-02 15:50:01.027000+00:00",
        "id": 1291064656902750310,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": "try_another",
        "content": "Love it, thanks! I'll test it on my use case, but looks like it works! The prompt is a bit confusing though: \"Classify the following INPUT into **ONE**\". Output is multilabel though",
        "timestamp": "2024-10-02 15:49:21.776000+00:00",
        "id": 1291064492272259133,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": ".aaronv",
        "content": "here's another technique -- adding descriptions to the categories, and renaming them to \"k1\" \"k2\" can also help the LLM focus on the descriptions instead of the names of the classes https://www.promptfiddle.com/New-Project-M6Snf",
        "timestamp": "2024-10-02 15:40:53.605000+00:00",
        "id": 1291062360848470149,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": ".aaronv",
        "content": "here's a multilabel classification problem!  https://www.promptfiddle.com/New-Project-WDZFV\n\nHow many labels do you have?",
        "timestamp": "2024-10-02 15:38:53.996000+00:00",
        "id": 1291061859171962902,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": ".aaronv",
        "content": "working on it!",
        "timestamp": "2024-10-02 15:36:58.417000+00:00",
        "id": 1291061374398627951,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": "try_another",
        "content": "please <@201399017161097216>",
        "timestamp": "2024-10-02 15:36:50.699000+00:00",
        "id": 1291061342026862642,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> can share a promptfiddle example when he’s on! I’m sadly on my phone",
        "timestamp": "2024-10-02 15:34:16.623000+00:00",
        "id": 1291060695785406554,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": "hellovai",
        "content": "You can return an enum array from a function!",
        "timestamp": "2024-10-02 15:33:58.024000+00:00",
        "id": 1291060617775550464,
        "parent_id": null,
        "thread_id": 1291058373940346991
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-10-02 15:33:57.414000+00:00",
        "id": 1291060615217152122,
        "parent_id": 1291058373940346991,
        "thread_id": 1291058373940346991
    },
    {
        "author": ".aaronv",
        "content": "I asked about how to deal with",
        "timestamp": "2024-10-02 04:57:23.504000+00:00",
        "id": 1290900418347663386,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "flyingaudio",
        "content": "Is BAML a LangChain replacement?",
        "timestamp": "2024-10-02 04:55:44.586000+00:00",
        "id": 1290900003455242292,
        "parent_id": null,
        "thread_id": 1290900003455242292
    },
    {
        "author": ".aaronv",
        "content": "<@418848265371648010> were you able to checkout BAML? Any feedback helps us on why or why-not you went with us",
        "timestamp": "2024-10-08 00:25:25.295000+00:00",
        "id": 1293006302011985932,
        "parent_id": null,
        "thread_id": 1290900003455242292
    },
    {
        "author": ".aaronv",
        "content": "we focus mostly on the Boundary between programs <> LLMs",
        "timestamp": "2024-10-02 04:58:09.039000+00:00",
        "id": 1290900609335033959,
        "parent_id": null,
        "thread_id": 1290900003455242292
    },
    {
        "author": ".aaronv",
        "content": "yep, except for the embeddings side of things -- we don't do any of the embeddings generation",
        "timestamp": "2024-10-02 04:57:55.903000+00:00",
        "id": 1290900554238791712,
        "parent_id": null,
        "thread_id": 1290900003455242292
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-02 04:57:55.651000+00:00",
        "id": 1290900553181696050,
        "parent_id": 1290900003455242292,
        "thread_id": 1290900003455242292
    },
    {
        "author": "simontam0",
        "content": "I asked about how to deal with structured responses in history before but not sure i was clear. I'll try agan. Presently w/o BAML we are asking the LLM to process an array of chat messages, some are from user, AIMessage (from the LLM) and some may be a FunctionMessage (from a tool call).. How should we formulate our prompt function to include/translate these messages to make it usable in BAML and to the LLM, specifically how might I convert/clean the FunctionMessage's  additional_kwargs[\"function_call\"][\"arguments\"] (which is the json output from a tool call). to something usable for BAML? When I was trying it out in the playground just pasting the json into a string didn't work well. Here's an example json string:       'assistant {\"is_outbound_flight_choices\": true, \"indicies_of_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59]}'.\n\nI believe another way to think about it is if I wanted BAML to extract data from a json object how should I do that in the prompt function? Does the JSON have to be \"cleaned\" in some way for BAML? The test cases I was using that worked were very basic json, not the same kind of json output I'm getting from json.dumps.",
        "timestamp": "2024-10-02 04:35:10.795000+00:00",
        "id": 1290894828560842815,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "Sense*",
        "timestamp": "2024-10-02 05:36:01.201000+00:00",
        "id": 1290910139473465414,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "I see, makes sende",
        "timestamp": "2024-10-02 05:35:57.480000+00:00",
        "id": 1290910123866456136,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": "simontam0",
        "content": "in our chat history this json blob is usually included for the user to make a choice. however questions may be asked about those choices. like shopping for items and asking questions about those items",
        "timestamp": "2024-10-02 05:12:30.945000+00:00",
        "id": 1290904224430821460,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": "simontam0",
        "content": "in most cases probably but in some others we may want the llm to look at some of the elements within the json to pull some stuff out",
        "timestamp": "2024-10-02 05:11:27.689000+00:00",
        "id": 1290903959116185648,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "although if you know the json structure in advance, can you do this programmatically?",
        "timestamp": "2024-10-02 05:10:19.559000+00:00",
        "id": 1290903673358123009,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "yeah that makes sense",
        "timestamp": "2024-10-02 05:09:54.909000+00:00",
        "id": 1290903569968398458,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": "simontam0",
        "content": "i shortened the json in the example about but you get the idea",
        "timestamp": "2024-10-02 05:09:35.786000+00:00",
        "id": 1290903489760858153,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": "simontam0",
        "content": "`class ExtractedJson {\n  ids string[] @description(#\"\n    The id_token_keys extracted from the json.\n  \"#)\n  flight_numbers string[] @description(#\"\n    The flight numbers extracted from the json.\n  \"#)\n}\n\nfunction ExtractFromJson(results: string) -> ExtractedJson {\n  client GPT4o\n  prompt #\"\n\n    Extract the info from this json.\n    ---\n    {{ results }}\n    ---\n    {# special macro to print the output schema. #}\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest TestName {\n  functions [ExtractFromJson]\n  args {\n    results #\"{\"is_outbound_flight_choices\": true, \"indicies_of_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59], \"indicies_of_extracted_unique_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59], \"id_of_extracted_sorted_flights\": [1, 6, 11, 16, 21, 26, 30, 35, 40, 44, 49, 54, 59], \"number_of_flights\": 63, \"error_response\": null, \"presentation_message\": \"Flights with the lowest cost fare option that allows for changes were retained.\", \"flight_choices\": [{\"id_token_key\": \"CiAKHgoQZTg5ZGNiNzBlYjZhNTJlMRIICIoDEgExGAEgAQ==\", \"origin\": \"SEA\", \"origin_name\": \"Seattle\\\\u2013Tacoma International Airport\", \"destination\": \"LAX\", ...\n  }\n}`",
        "timestamp": "2024-10-02 05:08:56.250000+00:00",
        "id": 1290903323934720104,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "you can pass the json as a string parameter as well",
        "timestamp": "2024-10-02 05:07:15.632000+00:00",
        "id": 1290902901912240128,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": "simontam0",
        "content": "I think I figured out my issue, i had to encapsulate the json in the prompt with #\"json string blob here \"# and thereafter I could extract what I needed",
        "timestamp": "2024-10-02 05:06:53.515000+00:00",
        "id": 1290902809146818610,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": "simontam0",
        "content": "yes i saw that and have been playing with that too.",
        "timestamp": "2024-10-02 05:05:13.361000+00:00",
        "id": 1290902389070630977,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "you can use BAML to do the tool call itself btw. Imagine a function whose signature is this:\n\n```\nclass FunctionMessage {\n  is_outbound_flight_choices bool\n  indices_of_flights int[]\n}\nfunction CallTool(messages: Message) -> FunctionMessage\n```",
        "timestamp": "2024-10-02 05:00:17.965000+00:00",
        "id": 1290901150089871370,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "just so I understand, your input is a json blob, and what do you want the output to be? do you want the LLM to choose one of the tools?",
        "timestamp": "2024-10-02 04:57:23.916000+00:00",
        "id": 1290900420075716672,
        "parent_id": null,
        "thread_id": 1290894828560842815
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-02 04:57:23.504000+00:00",
        "id": 1290900418347663381,
        "parent_id": 1290894828560842815,
        "thread_id": 1290894828560842815
    },
    {
        "author": "joatmon.pockets",
        "content": "recursive types",
        "timestamp": "2024-10-01 20:34:25.543000+00:00",
        "id": 1290773842805133448,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabev2037",
        "content": "BAML still doesn't support infinite nesting of fields right? like if i had a \n\nclass Element {\n  children Element[]\n}",
        "timestamp": "2024-10-01 17:58:19.315000+00:00",
        "id": 1290734557997437018,
        "parent_id": null,
        "thread_id": 1290734557997437018
    },
    {
        "author": "joatmon.pockets",
        "content": "It’s on our list, but no timeline yet",
        "timestamp": "2024-10-01 20:34:58.370000+00:00",
        "id": 1290773980491284613,
        "parent_id": null,
        "thread_id": 1290734557997437018
    },
    {
        "author": "joatmon.pockets",
        "content": "Sadly, yes, we don’t currently allow any recursion in type definitions today.",
        "timestamp": "2024-10-01 20:34:26.617000+00:00",
        "id": 1290773847309811843,
        "parent_id": null,
        "thread_id": 1290734557997437018
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-10-01 20:34:25.284000+00:00",
        "id": 1290773841718673469,
        "parent_id": 1290734557997437018,
        "thread_id": 1290734557997437018
    },
    {
        "author": ".aaronv",
        "content": "Parens highlighting",
        "timestamp": "2024-10-01 15:58:58.959000+00:00",
        "id": 1290704525287624839,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "I've noticed that within a #\"\"# block, if there are any sections surrounded by parenthesis (sample text), the text after the ) is not highlighted correctly in vscode",
        "timestamp": "2024-10-01 04:58:53.120000+00:00",
        "id": 1290538406358814843,
        "parent_id": null,
        "thread_id": 1290538406358814843
    },
    {
        "author": ".aaronv",
        "content": "Thanks for reporting, will add to our next batch of bug fixes",
        "timestamp": "2024-10-01 15:58:59.946000+00:00",
        "id": 1290704529427267696,
        "parent_id": null,
        "thread_id": 1290538406358814843
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-10-01 15:58:58.541000+00:00",
        "id": 1290704523534532669,
        "parent_id": 1290538406358814843,
        "thread_id": 1290538406358814843
    },
    {
        "author": ".aaronv",
        "content": "client graph",
        "timestamp": "2024-09-30 18:58:06.405000+00:00",
        "id": 1290387215477903509,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "What is Client Graph on promptfiddle supposed to represent?",
        "timestamp": "2024-09-30 16:35:49.932000+00:00",
        "id": 1290351410914791506,
        "parent_id": null,
        "thread_id": 1290351410914791506
    },
    {
        "author": ".aaronv",
        "content": "and how many times theyre called",
        "timestamp": "2024-09-30 18:58:10.927000+00:00",
        "id": 1290387234444546059,
        "parent_id": null,
        "thread_id": 1290351410914791506
    },
    {
        "author": ".aaronv",
        "content": "If you use any of the `retry` logic or `fallback` clients, it will show you the order in which LLMs will be called in the event of failures",
        "timestamp": "2024-09-30 18:58:07.252000+00:00",
        "id": 1290387219030474803,
        "parent_id": null,
        "thread_id": 1290351410914791506
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-30 18:58:06.405000+00:00",
        "id": 1290387215477903504,
        "parent_id": 1290351410914791506,
        "thread_id": 1290351410914791506
    },
    {
        "author": "andrewcka",
        "content": "Guys, is there a way to generate models on the fly using JSON objects? My goal is to generate fields in different languages to improve translations. I've noticed that when you describe the object name very well, the translation ends up being more accurate, even with simpler models",
        "timestamp": "2024-09-30 14:49:03.150000+00:00",
        "id": 1290324538923548774,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": ".aaronv",
        "content": "does the dynamic type approach make sense? Would you like a more concrete example?",
        "timestamp": "2024-09-30 21:36:54.343000+00:00",
        "id": 1290427178546106379,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": "andrewcka",
        "content": "also for complex extractions that normally fail it helps to guide it better",
        "timestamp": "2024-09-30 20:52:16.241000+00:00",
        "id": 1290415945772175436,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": "andrewcka",
        "content": "instead of a generic name describing the task, and then it aligns better",
        "timestamp": "2024-09-30 20:51:54.281000+00:00",
        "id": 1290415853665390732,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": "andrewcka",
        "content": "descriptive",
        "timestamp": "2024-09-30 20:51:34.366000+00:00",
        "id": 1290415770135695381,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": "andrewcka",
        "content": "you asign a descritive alias of the output",
        "timestamp": "2024-09-30 20:51:28.205000+00:00",
        "id": 1290415744294719549,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": "andrewcka",
        "content": "Yes, it works really well with alias",
        "timestamp": "2024-09-30 20:51:14.369000+00:00",
        "id": 1290415686262460436,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": ".aaronv",
        "content": "that's pretty interesting",
        "timestamp": "2024-09-30 14:53:40.400000+00:00",
        "id": 1290325701794201631,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": ".aaronv",
        "content": "so youre saying if you are doing a translation task, describing the object in each particular language does better?",
        "timestamp": "2024-09-30 14:53:27.581000+00:00",
        "id": 1290325648027422733,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": ".aaronv",
        "content": "you may want to do this: https://docs.boundaryml.com/docs/calling-baml/dynamic-types",
        "timestamp": "2024-09-30 14:50:08.901000+00:00",
        "id": 1290324814703104062,
        "parent_id": null,
        "thread_id": 1290324538923548774
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-30 14:50:08.650000+00:00",
        "id": 1290324813650333828,
        "parent_id": 1290324538923548774,
        "thread_id": 1290324538923548774
    },
    {
        "author": "kirilligum",
        "content": "have you tried to compare baml (json) to code4struct approach where the ouput is a python code?",
        "timestamp": "2024-09-30 13:03:33.385000+00:00",
        "id": 1290297989965021195,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "yes. this helps. thank you",
        "timestamp": "2024-10-04 21:27:36.212000+00:00",
        "id": 1291874389037682738,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "hellovai",
        "content": "Hope that helps?",
        "timestamp": "2024-09-30 19:17:34.613000+00:00",
        "id": 1290392115297390665,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "hellovai",
        "content": "Sorry about that! I was skimming in the AM and confused it for another paper i thought i read.\n\nThanks for the clarification.\n\nI think this class of techniques is similar to what BAML does in its prompts.\n\nHere they use python to represent the schema, BAML use Type Definitions (closer to gollie).\n\nThats what our helper `{{ ctx.output_format }}` puts out.\n\nyou can read more here: https://www.boundaryml.com/blog/type-definition-prompting-baml\n\nWe currently don't make it super easy to customize this for users, but if code4struct was supported, i could easily imagine a world with something like: `{{ ctx.output_format(mode=\"code4struct\")`",
        "timestamp": "2024-09-30 19:17:23.527000+00:00",
        "id": 1290392068799205387,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "but, i think you answered my question that you haven't looked into this. let me know if you do",
        "timestamp": "2024-09-30 13:27:17.325000+00:00",
        "id": 1290303962402127966,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "from gollie paper:\n3.1 INPUT-OUTPUT REPRESENTATION\nWe have adopted a Python code-based representation (Wang et al., 2023b; Li et al., 2023) for both the\ninput and output of the model. This approach not only offers a clear and human-readable structure\nbut also addresses several challenges typically associated with natural language instructions. It\nenables the representation of any information extraction task under a unified format. The inputs\ncan be automatically standardized using Python code formatters such as Black. The output is wellstructured and parsing it is trivial. Furthermore, most current LLMs incorporate code in their pretraining datasets, indicating that these models are already familiar with this representation.",
        "timestamp": "2024-09-30 13:26:51.911000+00:00",
        "id": 1290303855807959092,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "i'm trying to decide between using json with baml or python. the main reason for python is that it was a lot more training data and a bit better error feedback",
        "timestamp": "2024-09-30 13:25:20.048000+00:00",
        "id": 1290303470506741862,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "another paper is gollie https://arxiv.org/pdf/2310.03668 a json alternative would be {\"Launcher\":{\"mentions\":str,\"space_company\":str,\"crew\":List[str]}",
        "timestamp": "2024-09-30 13:23:53.682000+00:00",
        "id": 1290303108261609544,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "json has dictionaries, code4struct uses ast tree",
        "timestamp": "2024-09-30 13:19:46.537000+00:00",
        "id": 1290302071660347433,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "it uses python code with classes instead of json",
        "timestamp": "2024-09-30 13:19:33.074000+00:00",
        "id": 1290302015192170548,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "code4struct is a little different.",
        "timestamp": "2024-09-30 13:19:15.264000+00:00",
        "id": 1290301940491747341,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "hellovai",
        "content": "We haven’t actually done anything special yet for snippets like code. For returning strings we just pipe back the raw output. We are looking into adding types like markdown to the language where we can apply interesting corrective techniques. This one looks worth looking into!",
        "timestamp": "2024-09-30 13:14:58.205000+00:00",
        "id": 1290300862308159551,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "codeact https://arxiv.org/pdf/2402.01030 and few others use this output too",
        "timestamp": "2024-09-30 13:07:09.925000+00:00",
        "id": 1290298898199609344,
        "parent_id": null,
        "thread_id": 1290297989965021195
    },
    {
        "author": "kirilligum",
        "content": "",
        "timestamp": "2024-09-30 13:07:09.488000+00:00",
        "id": 1290298896366436466,
        "parent_id": 1290297989965021195,
        "thread_id": 1290297989965021195
    },
    {
        "author": "hellovai",
        "content": "Input output tokens",
        "timestamp": "2024-09-30 05:20:54.422000+00:00",
        "id": 1290181560435212385,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "Inputs rendering with aliases",
        "timestamp": "2024-09-30 05:19:38.238000+00:00",
        "id": 1290181240896491536,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "I'm looking at the symbol tuning example on promptfiddle\n\nIt looks like for the MyClass output type, the parser will convert from the alias (k4) to to the non-alias category (AccountIssue)\n\nHowever, if i wanted to pass that output into a 2nd function, one that has input MyClass and output MyClass, for example a function that validates the output of ClassifyMessageWithSymbol and returns the correct MyClass, would I have to convert the non-alias category (AccountIssue) back into the alias (k4) form for the input of the 2nd function, given that the context only sees the aliases because of the below.\n\nAnswer with any of the categories:\nMyClass\n----\n- k1: Customer wants to refund a product\n- k2: Customer wants to cancel an order\n- k3: Customer needs help with a technical issue unrelated to account creation or login\n- k4: Specifically relates to account-login or account-creation\n- k5: Customer has a question",
        "timestamp": "2024-09-30 03:54:54.771000+00:00",
        "id": 1290159919290650674,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": ".aaronv",
        "content": "sorry the inputs being rendered with aliases will take a few more days -- will update this thread when done",
        "timestamp": "2024-10-09 17:08:22.458000+00:00",
        "id": 1293621091192799397,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": ".aaronv",
        "content": "Not yet but ill add to our roadmap",
        "timestamp": "2024-10-08 15:12:31.561000+00:00",
        "id": 1293229549152112640,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "is there an option to print the schema of the input? With description too",
        "timestamp": "2024-10-08 14:55:47.278000+00:00",
        "id": 1293225336883908619,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": ".aaronv",
        "content": "good news, I just fixed this -- will patch it in the next release (potentially tomorrow).\n\nso inputs with aliased keys will render with the alias",
        "timestamp": "2024-10-08 00:26:19.208000+00:00",
        "id": 1293006528139231352,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "yeah for now I think having something like this be exposed would be helpful, i'm finding myself having to add in the format of input classes for more context",
        "timestamp": "2024-10-04 05:08:42.266000+00:00",
        "id": 1291628041012777003,
        "parent_id": 1290418782103277671,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "also, this is not really an issue but right now if I am using the same class as the output of different functions, I end up splitting the relevant information for each class field between the class @description and my explicit instructions elsewhere in the prompt",
        "timestamp": "2024-10-02 22:04:06.969000+00:00",
        "id": 1291158802007064687,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "perhaps optionally",
        "timestamp": "2024-10-02 22:00:41.780000+00:00",
        "id": 1291157941382156369,
        "parent_id": 1291155233811988645,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "We should maybe put a spec up at some point.",
        "timestamp": "2024-10-02 21:58:59.925000+00:00",
        "id": 1291157514171453471,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "And parts of that type like descriptions",
        "timestamp": "2024-10-02 21:58:36.960000+00:00",
        "id": 1291157417848996002,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> we should spend some good time thinking about this. I think <@152300469094580224> has some really good points. I wonder if we just need a way to render custom types and such",
        "timestamp": "2024-10-02 21:58:25.970000+00:00",
        "id": 1291157371753595012,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": ".aaronv",
        "content": "yes, it's just a plain object at that point instead of a schema. Would you want the descriptions to still be written in?",
        "timestamp": "2024-10-02 21:49:56.245000+00:00",
        "id": 1291155233811988645,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "It occurs to me that even normally when you have a class as an input and dump it as a string into the prompt it just renders the json and doesn’t show your descriptions, that only happens for outputs",
        "timestamp": "2024-10-02 20:00:23.962000+00:00",
        "id": 1291127667659378809,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "Sadly not, that would be the thing we expose as a part of this something like:\n\n```\n{{ _.print_type(MyType) }}\n```",
        "timestamp": "2024-09-30 21:03:32.475000+00:00",
        "id": 1290418782103277671,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "so I can inject it manually for now",
        "timestamp": "2024-09-30 20:34:51.152000+00:00",
        "id": 1290411562351333440,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "is there a function to generate that enum description as a string?",
        "timestamp": "2024-09-30 20:34:32.662000+00:00",
        "id": 1290411484798652487,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "let us evaluate and see what the work here is. I think something should be possible as its just a minor change",
        "timestamp": "2024-09-30 20:33:58.915000+00:00",
        "id": 1290411343253475438,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> what are the odds we can get this in by 1-2 days?",
        "timestamp": "2024-09-30 20:33:35.145000+00:00",
        "id": 1290411243554865223,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "OK for now if I’m using the same enums in input and output should I just avoid aliases? Though the system tuning is interesting and I would like to use it.",
        "timestamp": "2024-09-30 20:31:17.963000+00:00",
        "id": 1290410668171984928,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> for context",
        "timestamp": "2024-09-30 19:23:20.195000+00:00",
        "id": 1290393564773224518,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "yep, for now they dont, but i can see how you'd like to be able to inject that information into the prompt.\n\nIts interesting, as we also would need to inject in schema information to print this out correctly.\n\ne.g.\n\n```\nclass Foo {\n  field MyEnum | int\n}\n```\n\nif you pass in `Foo` into the prompt and `field` is an int, then we shouldn't inject the enum, but if you pass in the enum, then we should likely render its something like:\n\n```\n{ field: MyEnum('SomeValue') }\n```\n\nwhile rendering the values for the enum (so the model can know what you chose it from)",
        "timestamp": "2024-09-30 19:23:03.997000+00:00",
        "id": 1290393496833753217,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "If a MyClass enum is passed into a function, the enum definition (and associated descriptions) doesn’t make its way into the context?",
        "timestamp": "2024-09-30 16:24:10.865000+00:00",
        "id": 1290348478815539200,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "yungweedle",
        "content": "Also, will ctx.output_format only include the enum definition if the output is a MyClass enum",
        "timestamp": "2024-09-30 16:23:18.987000+00:00",
        "id": 1290348261223436438,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "Oh no! Good catch. We can aim for a fix for this to land tmrw. It shouldn’t take more than a day.",
        "timestamp": "2024-09-30 05:19:40.654000+00:00",
        "id": 1290181251030061140,
        "parent_id": null,
        "thread_id": 1290159919290650674
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-30 05:19:38.238000+00:00",
        "id": 1290181240896491531,
        "parent_id": 1290159919290650674,
        "thread_id": 1290159919290650674
    },
    {
        "author": "saurabhj80",
        "content": "How can I know input and output tokens after I am done calling the baml function?",
        "timestamp": "2024-09-30 03:38:31.562000+00:00",
        "id": 1290155795413205086,
        "parent_id": null,
        "thread_id": 1290155795413205086
    },
    {
        "author": "hellovai",
        "content": "yep! sorry about that",
        "timestamp": "2024-09-30 19:21:40.852000+00:00",
        "id": 1290393148098347039,
        "parent_id": null,
        "thread_id": 1290155795413205086
    },
    {
        "author": "saurabhj80",
        "content": "got it. so I will need to wait for now",
        "timestamp": "2024-09-30 17:08:23.526000+00:00",
        "id": 1290359604882051206,
        "parent_id": null,
        "thread_id": 1290155795413205086
    },
    {
        "author": "hellovai",
        "content": "Hey <@701543571265945752> ! We currently do expose that, but have a GitHub issue tracking this. We are thinking of ergonomic designs that can populate this information for you!",
        "timestamp": "2024-09-30 05:20:57.189000+00:00",
        "id": 1290181572040855624,
        "parent_id": null,
        "thread_id": 1290155795413205086
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-30 05:20:54.422000+00:00",
        "id": 1290181560435212380,
        "parent_id": 1290155795413205086,
        "thread_id": 1290155795413205086
    },
    {
        "author": ".aaronv",
        "content": "Default params",
        "timestamp": "2024-09-29 02:13:10.659000+00:00",
        "id": 1289771928923406367,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "did support for default parameters get added?",
        "timestamp": "2024-09-28 22:16:55.709000+00:00",
        "id": 1289712474874052690,
        "parent_id": null,
        "thread_id": 1289712474874052690
    },
    {
        "author": ".aaronv",
        "content": "Not yet!",
        "timestamp": "2024-09-29 02:13:16.421000+00:00",
        "id": 1289771953091117139,
        "parent_id": null,
        "thread_id": 1289712474874052690
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-29 02:13:10.545000+00:00",
        "id": 1289771928445517844,
        "parent_id": 1289712474874052690,
        "thread_id": 1289712474874052690
    },
    {
        "author": "roeybc",
        "content": "Might have missed it in the docs - any way to get a template string as an input? I 'm looking into loading strings that have working baml code (taking into account that all inputs are valid).\nI'd use jinja, but I'm using typescript.",
        "timestamp": "2024-09-28 00:54:33.973000+00:00",
        "id": 1289389757842849886,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "but you can do this today if you handle the jinja strings yourself! We just are thinking about how to provide static analysis and other guarantees BAML provides today",
        "timestamp": "2024-10-02 11:54:55.046000+00:00",
        "id": 1291005492130218034,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "\"kinda\" is the short answer",
        "timestamp": "2024-10-02 11:54:29.084000+00:00",
        "id": 1291005383237959741,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "roeybc",
        "content": "Do that! Isn’t it as simple as running jinja template on top of the given prompt?",
        "timestamp": "2024-09-29 22:22:07.872000+00:00",
        "id": 1290076172046041201,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "But I think we should be able to. And I like the idea of us providing UI components that just work for the experience",
        "timestamp": "2024-09-29 21:45:14.307000+00:00",
        "id": 1290066887681507392,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "FYI as of now, we don’t support you having your users write BAML prompts and then us executing that",
        "timestamp": "2024-09-29 21:44:43.527000+00:00",
        "id": 1290066758580965447,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": ".aaronv",
        "content": "here is a working version :)! https://www.promptfiddle.com/New-Project-B057h",
        "timestamp": "2024-09-29 20:10:36.029000+00:00",
        "id": 1290043071257383026,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "roeybc",
        "content": "I tried to do something like this: \n```\ntemplate_string PrintPrompt(blob: string) #\"\n  {{blob}}\n\"#\n```\nBut no luck 😄",
        "timestamp": "2024-09-29 20:02:57.913000+00:00",
        "id": 1290041149779873884,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": ".aaronv",
        "content": "You can do that with template strings, template strings accept parameters. Ill write an example in a bit",
        "timestamp": "2024-09-29 17:33:58.342000+00:00",
        "id": 1290003654501339177,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "roeybc",
        "content": "That can be great to the generation (I thought to use a list but this one is WAY better). The thing is that I want the prompt to be dynamic and written by the users, ideally with baml. Something like this: \n```\nsome_prompt = \"\"\"\nExtract the information from this chunk of text:\n    \"{{ user_info }}\"\n     \n    {{ ctx.output_format }}\n\"\"\"\n\nfunction DynamicUserCreator(some_prompt: string, user_info: string) -> User {\n  client GPT4\n  prompt #\"\n    {{some_prompt}}\n  \"#\n}\n```",
        "timestamp": "2024-09-29 03:17:33.509000+00:00",
        "id": 1289788130890879027,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "Then use that to compose the string on your end",
        "timestamp": "2024-09-28 03:37:23.300000+00:00",
        "id": 1289430733370425457,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "https://docs.boundaryml.com/docs/calling-baml/dynamic-types",
        "timestamp": "2024-09-28 03:37:13.205000+00:00",
        "id": 1289430691028795493,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "hellovai",
        "content": "You likely want to use dynamic types!",
        "timestamp": "2024-09-28 03:36:21.177000+00:00",
        "id": 1289430472807677952,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": "roeybc",
        "content": "Well, trying to let users write baml code with a constant set of parameters for all users, where they can also write use different prompts on these sets. and then run it 😄 \nLike - two dev tools (for example lol) where you write code in a completely different way but the sets of parameters is kinda the same.",
        "timestamp": "2024-09-28 01:17:43.858000+00:00",
        "id": 1289395587443327027,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": ".aaronv",
        "content": "What are you trying to do?",
        "timestamp": "2024-09-28 00:56:27.334000+00:00",
        "id": 1289390233313611777,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": ".aaronv",
        "content": "Hmm not at the moment. Youd just have to add a string input where the string is that baml code",
        "timestamp": "2024-09-28 00:56:16.183000+00:00",
        "id": 1289390186542665758,
        "parent_id": null,
        "thread_id": 1289389757842849886
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-28 00:56:15.492000+00:00",
        "id": 1289390183644397609,
        "parent_id": 1289389757842849886,
        "thread_id": 1289389757842849886
    },
    {
        "author": "yungweedle",
        "content": "If I’m using chain of thought and asking LLM to explain its thoughts, it looks like I can just add a “CoT string” output to my output schema to make the parsing easier. More a general question but in this case i should have the CoT output first in the output schema so that it works correctly?",
        "timestamp": "2024-09-24 16:36:13.873000+00:00",
        "id": 1288177184137150598,
        "parent_id": null,
        "thread_id": 1288177184137150598
    },
    {
        "author": "hellovai",
        "content": "Here's how i do CoT:\n\n```rust\ntemplate_string CoT(details: string) #\"\n  before answering, note some of the relevant details like {{ details }}.\n  Example:\n  - ...\n  - ...\n  ...\n  \n  {\n    .. // SCHEMA\n  }\n\"#\nfunction Foo(content: string) -> Resume {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Extract this resume.\n  \n    {{ ctx.output_format }}\n     \n    {{ CoT('why this person may be a good hire') }}\n    \n    {{ _.role('user') }}\n    {{ content }} \n  \"#\n}\n```",
        "timestamp": "2024-09-24 16:40:36.629000+00:00",
        "id": 1288178286215561236,
        "parent_id": null,
        "thread_id": 1288177184137150598
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-24 16:40:36.239000+00:00",
        "id": 1288178284579913728,
        "parent_id": 1288177184137150598,
        "thread_id": 1288177184137150598
    },
    {
        "author": "hellovai",
        "content": "retry policy",
        "timestamp": "2024-09-23 20:05:49.975000+00:00",
        "id": 1287867544266346552,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, i'm getting one issue with:\n\nretry_policy PolicyToRetry {\n  max_retries 8\n  strategy {\n    type exponential_backoff\n    delay 5000\n    multiplier 2\n  }\n}\n\nstrategy using azure, is there a quick fix?",
        "timestamp": "2024-09-23 19:44:47.811000+00:00",
        "id": 1287862250366963764,
        "parent_id": null,
        "thread_id": 1287862250366963764
    },
    {
        "author": "andrewcka",
        "content": "I have to present something and forgot to record the logs",
        "timestamp": "2024-09-24 13:32:25.839000+00:00",
        "id": 1288130929210036256,
        "parent_id": null,
        "thread_id": 1287862250366963764
    },
    {
        "author": "andrewcka",
        "content": "I just removed the list of fallbacks and work",
        "timestamp": "2024-09-24 13:32:11.823000+00:00",
        "id": 1288130870422671371,
        "parent_id": null,
        "thread_id": 1287862250366963764
    },
    {
        "author": "andrewcka",
        "content": "Actually don't know, I was hitting that strategy was not a default value when integrating it with a fallback and then having like 100 requests in one minute due to a validation error",
        "timestamp": "2024-09-24 13:31:59.736000+00:00",
        "id": 1288130819725987884,
        "parent_id": null,
        "thread_id": 1287862250366963764
    },
    {
        "author": "hellovai",
        "content": "whats the exact issue btw?",
        "timestamp": "2024-09-23 20:05:51.240000+00:00",
        "id": 1287867549572268125,
        "parent_id": null,
        "thread_id": 1287862250366963764
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-23 20:05:49.849000+00:00",
        "id": 1287867543737991229,
        "parent_id": 1287862250366963764,
        "thread_id": 1287862250366963764
    },
    {
        "author": "gggooo",
        "content": "Is boundary ml better than openai's structured outputs?",
        "timestamp": "2024-09-23 19:28:17.874000+00:00",
        "id": 1287858098270371891,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "hellovai",
        "content": "🙂",
        "timestamp": "2024-09-23 20:06:02.980000+00:00",
        "id": 1287867598813397135,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": ".aaronv",
        "content": "haha no worries. Definitely check out those benchmark results (scroll down to the bottom)",
        "timestamp": "2024-09-23 20:01:03.114000+00:00",
        "id": 1287866341084233869,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "gggooo",
        "content": "Sorry",
        "timestamp": "2024-09-23 19:40:53.913000+00:00",
        "id": 1287861269327515648,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "gggooo",
        "content": "Ok it was literally in the title, I am dumb",
        "timestamp": "2024-09-23 19:40:52.681000+00:00",
        "id": 1287861264160260248,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "gggooo",
        "content": "Oh sorry I didn't read it fully, it is for the structured outputs",
        "timestamp": "2024-09-23 19:40:11.329000+00:00",
        "id": 1287861090717536337,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "gggooo",
        "content": "Or are they essentially the same",
        "timestamp": "2024-09-23 19:37:39.063000+00:00",
        "id": 1287860452067512381,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "gggooo",
        "content": "Thank you. But this is a baml vs openai function calling stat, not their recent structured output feature, correct?",
        "timestamp": "2024-09-23 19:37:14.870000+00:00",
        "id": 1287860350594842625,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "hellovai",
        "content": "But i'll let the rest of folks who use BAML share more",
        "timestamp": "2024-09-23 19:33:12.538000+00:00",
        "id": 1287859334180503614,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "hellovai",
        "content": "I'll let my bias not show too much here, but yes 🙂 :\n\nHelpful reading: \nWhat is SAP (Schema-aligned parsing): https://www.boundaryml.com/blog/schema-aligned-parsing#sap\nMetrics: https://www.boundaryml.com/blog/sota-function-calling\n\nBut we allow for a few things that openai doesnt like streaming for parallel functino calling. \n\nit turns out that using SAP, we are able to get the same (or better) accuracy while using 15% less tokens in many use cases .",
        "timestamp": "2024-09-23 19:33:00.301000+00:00",
        "id": 1287859282854936628,
        "parent_id": null,
        "thread_id": 1287858098270371891
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-23 19:32:59.669000+00:00",
        "id": 1287859280204005459,
        "parent_id": 1287858098270371891,
        "thread_id": 1287858098270371891
    },
    {
        "author": ".aaronv",
        "content": "Is there a way currently to have @@",
        "timestamp": "2024-09-23 17:16:58.484000+00:00",
        "id": 1287825049713119344,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "davidyoung",
        "content": "Ignore, just saw you answered above 🙂",
        "timestamp": "2024-09-23 16:53:40.929000+00:00",
        "id": 1287819187942850682,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "davidyoung",
        "content": "Is there a way currently to have @@dynamic in an enum, and fill that enum in a test case?",
        "timestamp": "2024-09-23 16:52:35.540000+00:00",
        "id": 1287818913681248256,
        "parent_id": null,
        "thread_id": 1287818913681248256
    },
    {
        "author": "davidyoung",
        "content": "Definitely separates BAML from the rest imo",
        "timestamp": "2024-09-23 19:25:59.624000+00:00",
        "id": 1287857518407712983,
        "parent_id": null,
        "thread_id": 1287818913681248256
    },
    {
        "author": "davidyoung",
        "content": "Yes very useful! 🙂",
        "timestamp": "2024-09-23 19:25:42.498000+00:00",
        "id": 1287857446576066571,
        "parent_id": null,
        "thread_id": 1287818913681248256
    },
    {
        "author": ".aaronv",
        "content": "dyamic fields for BAML-defined `tests` will be coming later :). Do you find the BAML Playground in VSCode useful to iterate on things?",
        "timestamp": "2024-09-23 17:16:59.020000+00:00",
        "id": 1287825051961262114,
        "parent_id": null,
        "thread_id": 1287818913681248256
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-23 17:16:58.484000+00:00",
        "id": 1287825049713119338,
        "parent_id": 1287818913681248256,
        "thread_id": 1287818913681248256
    },
    {
        "author": ".alex4o",
        "content": "While you guys are answering questions let me ask something, here I pass an argument in the test but it shows up as null in the user prompt (line 2)",
        "timestamp": "2024-09-23 16:52:19.030000+00:00",
        "id": 1287818844433420391,
        "parent_id": null,
        "thread_id": 1287818844433420391
    },
    {
        "author": ".aaronv",
        "content": "this is good feedback for us -- we should probably highlight you have the wrong name in our `args`",
        "timestamp": "2024-09-23 17:39:46.179000+00:00",
        "id": 1287830786241724521,
        "parent_id": null,
        "thread_id": 1287818844433420391
    },
    {
        "author": ".alex4o",
        "content": "Ok found out why it was the wrong argument name",
        "timestamp": "2024-09-23 16:58:21.044000+00:00",
        "id": 1287820362830315531,
        "parent_id": null,
        "thread_id": 1287818844433420391
    },
    {
        "author": ".alex4o",
        "content": "",
        "timestamp": "2024-09-23 16:58:20.688000+00:00",
        "id": 1287820361336885280,
        "parent_id": 1287818844433420391,
        "thread_id": 1287818844433420391
    },
    {
        "author": ".alex4o",
        "content": "Quck question I see that my teemmate uses |tojson() in our baml config but I can't seem to find any docs for it? Also do you support other formats like yaml",
        "timestamp": "2024-09-23 16:40:57.226000+00:00",
        "id": 1287815984740175915,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "no worries, we should help provide as much great documentation as possible, so its helpful to us to see these questions!",
        "timestamp": "2024-09-23 16:44:24.037000+00:00",
        "id": 1287816852168245460,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": ".alex4o",
        "content": "oh so that is jinja thing sry I didn't think of that thank you",
        "timestamp": "2024-09-23 16:43:33.499000+00:00",
        "id": 1287816640196644935,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "so as of now YAML isn't supported",
        "timestamp": "2024-09-23 16:43:20.070000+00:00",
        "id": 1287816583871332443,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions",
        "timestamp": "2024-09-23 16:43:07.569000+00:00",
        "id": 1287816531438342225,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "oops wrong link",
        "timestamp": "2024-09-23 16:43:06.775000+00:00",
        "id": 1287816528108064892,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "we plan on adding some more by default, but haven't done it yet",
        "timestamp": "2024-09-23 16:42:51.510000+00:00",
        "id": 1287816464082014289,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "We'll add these mini docs directly in our own docs\n\nhttps://docs.boundaryml.com/docs/snippets/prompt-syntax/what-is-jinja#built-in-filters\n\nThey are called filters in jinja",
        "timestamp": "2024-09-23 16:42:36.584000+00:00",
        "id": 1287816401477701632,
        "parent_id": null,
        "thread_id": 1287815984740175915
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-23 16:42:36.148000+00:00",
        "id": 1287816399648981082,
        "parent_id": 1287815984740175915,
        "thread_id": 1287815984740175915
    },
    {
        "author": "tdn8",
        "content": "When doing function/tool calling with",
        "timestamp": "2024-09-23 15:15:30.731000+00:00",
        "id": 1287794482661687301,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "simontam0",
        "content": "When doing function/tool calling with BAML if you happen to have more than 1 class that may look similar to another is there a way to better guide the LLM into which function/tool it should call? Would this be in the name of the function, prompt itself, the docstring for the class or maybe an added member in the class to give it a hint?",
        "timestamp": "2024-09-23 14:57:52.447000+00:00",
        "id": 1287790043896873010,
        "parent_id": null,
        "thread_id": 1287790043896873010
    },
    {
        "author": "tdn8",
        "content": "Until https://github.com/BoundaryML/baml/pull/978 is in (which will allow you to do something like: `action \"Calculate\"` ) what has worked for us is to add a unique field to \"tie-break\" the two classes, or add a discrete enum type which only has that 1 value:\n\n```\nenum CalculateActionType {\n  CALCULATE_ACTION\n}\n\nclass Calculate {\n  action CalculateActionType\n```",
        "timestamp": "2024-09-23 15:15:31.119000+00:00",
        "id": 1287794484289208320,
        "parent_id": null,
        "thread_id": 1287790043896873010
    },
    {
        "author": "tdn8",
        "content": "",
        "timestamp": "2024-09-23 15:15:30.731000+00:00",
        "id": 1287794482661687296,
        "parent_id": 1287790043896873010,
        "thread_id": 1287790043896873010
    },
    {
        "author": "hellovai",
        "content": "Schema robustness",
        "timestamp": "2024-09-23 13:28:02.503000+00:00",
        "id": 1287767436833067054,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I'm probably repeating myself but what is the best way to add some silly robustness to schema failures? Eg. Assume one output was an invalid enum value, can we return \"None\" or smth like that vs the request failing comlletely and getting no values?",
        "timestamp": "2024-09-23 07:20:53.310000+00:00",
        "id": 1287675039700881450,
        "parent_id": null,
        "thread_id": 1287675039700881450
    },
    {
        "author": "hellovai",
        "content": "You can make values optional! And we fill auto fill them in",
        "timestamp": "2024-09-23 13:28:04.841000+00:00",
        "id": 1287767446639607820,
        "parent_id": null,
        "thread_id": 1287675039700881450
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-23 13:28:02.503000+00:00",
        "id": 1287767436833067049,
        "parent_id": 1287675039700881450,
        "thread_id": 1287675039700881450
    },
    {
        "author": "andrewcka",
        "content": "Guys is there a way to create subfolders that have works like dependencies for a series of calls? for example, 1 -> 2 -> 3 (in an orderly manner?) Or it should be done programmatically?",
        "timestamp": "2024-09-22 13:54:59.902000+00:00",
        "id": 1287411832830689382,
        "parent_id": null,
        "thread_id": 1287411832830689382
    },
    {
        "author": "hellovai",
        "content": "for now we do this programtically",
        "timestamp": "2024-09-22 13:56:54.453000+00:00",
        "id": 1287412313292542044,
        "parent_id": null,
        "thread_id": 1287411832830689382
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-22 13:56:53.663000+00:00",
        "id": 1287412309978910742,
        "parent_id": 1287411832830689382,
        "thread_id": 1287411832830689382
    },
    {
        "author": "hellovai",
        "content": "validations",
        "timestamp": "2024-09-22 13:25:41.727000+00:00",
        "id": 1287404458510127119,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "seawatts",
        "content": "Has anyone setup baml with a Chrome Extension yet? I'm trying to get it going with Vite and CRXjs  https://crxjs.dev/vite-plugin <@201399017161097216> <@99252724855496704>",
        "timestamp": "2024-09-22 13:21:53.299000+00:00",
        "id": 1287403500413784065,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "seawatts",
        "content": "https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExanQxcGJ0OTJybWtqMTIxbmU0ZGhxZ252MHI2M2s0d3hxcmMxMzdkZCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/am0dKpQO1vrOC3DmCb/giphy.gif",
        "timestamp": "2024-09-22 13:38:35.684000+00:00",
        "id": 1287407704721326162,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "your wish is my command",
        "timestamp": "2024-09-22 13:37:05.074000+00:00",
        "id": 1287407324675182743,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "😂",
        "timestamp": "2024-09-22 13:36:50.949000+00:00",
        "id": 1287407265430634526,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "seawatts",
        "content": "Thanks <@99252724855496704>, on top of it as usual!",
        "timestamp": "2024-09-22 13:36:27.418000+00:00",
        "id": 1287407166734733384,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "seawatts",
        "content": "Amazing!!",
        "timestamp": "2024-09-22 13:36:14.118000+00:00",
        "id": 1287407110950490143,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "We're gonna work on making a `typescript/nextjs` example that will autogenerate some of this for you fyi!",
        "timestamp": "2024-09-22 13:35:44.327000+00:00",
        "id": 1287406985997844552,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "Example: https://github.com/BoundaryML/baml-examples/blob/main/nextjs-starter/app/examples/stream-object/page.tsx\n\n(follow `extractResume`)\n\nhttps://docs.boundaryml.com/docs/baml-nextjs/baml-nextjs",
        "timestamp": "2024-09-22 13:35:22.190000+00:00",
        "id": 1287406893148409949,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "Not specifically, but you can use vercels AI streaming SDK",
        "timestamp": "2024-09-22 13:34:15.315000+00:00",
        "id": 1287406612654588054,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "seawatts",
        "content": "Makes sense! Yeah, I'm fine hosting an endpoint on a Vercel function for now. I'm just curious if you had an example of how to stream from a vercel api to the chrome extension",
        "timestamp": "2024-09-22 13:30:56.461000+00:00",
        "id": 1287405778600464455,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "Hey <@323873214336073730> ! I'm actually not sure this will work yet 😢 We don't ship our WASM-Browser compatible version of BAML atm... We would recommend keeping your BAML code in the backend.\n\nWe made this design choice due to where we assumed people keep their API keys (and openai and anthropic both block calling the api from the browser directly, we Proxy our requests through a backend). That said, if this is really appealing to you, we can help out here and see what it would take to ship our WASM layer (Which we use in our VSCode playground).",
        "timestamp": "2024-09-22 13:29:23.807000+00:00",
        "id": 1287405389981421598,
        "parent_id": null,
        "thread_id": 1287403500413784065
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-22 13:29:22.863000+00:00",
        "id": 1287405386021998764,
        "parent_id": 1287403500413784065,
        "thread_id": 1287403500413784065
    },
    {
        "author": "philosopherstone",
        "content": "Hi, I use BAML for vision tasks which mostly involves extracting product information along with their price - is there a way to ensure that the extracted price is valid? I'm assuming that LLM can hallucinate and come up with a number 😓",
        "timestamp": "2024-09-22 09:33:15.353000+00:00",
        "id": 1287345963178131478,
        "parent_id": null,
        "thread_id": 1287345963178131478
    },
    {
        "author": "philosopherstone",
        "content": "That's so cool! Thank you so much! A lot of good work being done :)",
        "timestamp": "2024-09-22 13:35:47.434000+00:00",
        "id": 1287406999029420032,
        "parent_id": null,
        "thread_id": 1287345963178131478
    },
    {
        "author": "hellovai",
        "content": "Hey <@757202652416180235> ! Sadly the only way to do this atm is with code.\n\nHere's a youtube video that shows what i mean: https://www.youtube.com/watch?v=xCpQdHX5iM0&feature=youtu.be\n\nWe'll have more native support in BAML coming up thanks to <@503733199130722314> !\n\nYou can see what that will look like here: <#1265356689796890820>",
        "timestamp": "2024-09-22 13:25:47.354000+00:00",
        "id": 1287404482111606916,
        "parent_id": null,
        "thread_id": 1287345963178131478
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-22 13:25:41.727000+00:00",
        "id": 1287404458510127114,
        "parent_id": 1287345963178131478,
        "thread_id": 1287345963178131478
    },
    {
        "author": ".aaronv",
        "content": "Is it possible to run tests within",
        "timestamp": "2024-09-22 02:52:38.309000+00:00",
        "id": 1287245144508076087,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gitzalytics",
        "content": "Is it possible to run tests within playground on dynamically created enums?",
        "timestamp": "2024-09-21 22:08:55.561000+00:00",
        "id": 1287173745928175749,
        "parent_id": null,
        "thread_id": 1287173745928175749
    },
    {
        "author": ".aaronv",
        "content": "not yet! But we will add this in the next few weeks",
        "timestamp": "2024-09-22 02:52:40.546000+00:00",
        "id": 1287245153890992131,
        "parent_id": null,
        "thread_id": 1287173745928175749
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-22 02:52:38.070000+00:00",
        "id": 1287245143505633280,
        "parent_id": 1287173745928175749,
        "thread_id": 1287173745928175749
    },
    {
        "author": "gabev2037",
        "content": "Would it be possible to expose the underlying LLM Error upfront. I see LLM error followed by the input text which is thousands of tokens long… makes it tricky to see what the actual model error was (I assumed rate limiting?)",
        "timestamp": "2024-09-21 19:50:15.301000+00:00",
        "id": 1287138848228507729,
        "parent_id": null,
        "thread_id": 1287138848228507729
    },
    {
        "author": "gabev2037",
        "content": "missed this! Need to update the notification settings here",
        "timestamp": "2024-09-23 16:18:11.228000+00:00",
        "id": 1287810255329431592,
        "parent_id": null,
        "thread_id": 1287138848228507729
    },
    {
        "author": ".aaronv",
        "content": "we will truncate the middle of the raw llm prompt and the middle of the raw llm response that is shown with this env var",
        "timestamp": "2024-09-21 19:55:25.280000+00:00",
        "id": 1287140148374540451,
        "parent_id": null,
        "thread_id": 1287138848228507729
    },
    {
        "author": ".aaronv",
        "content": "in case you dont want a log with 1 million characters",
        "timestamp": "2024-09-21 19:54:52.064000+00:00",
        "id": 1287140009056669696,
        "parent_id": null,
        "thread_id": 1287138848228507729
    },
    {
        "author": ".aaronv",
        "content": "yes we can def do that improvement on the next release.\n\nYou can also do this",
        "timestamp": "2024-09-21 19:54:24.210000+00:00",
        "id": 1287139892228259901,
        "parent_id": null,
        "thread_id": 1287138848228507729
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-21 19:54:23.786000+00:00",
        "id": 1287139890449875033,
        "parent_id": 1287138848228507729,
        "thread_id": 1287138848228507729
    },
    {
        "author": "charizard_98",
        "content": "How do I use gpt-4o-2024-08-06? \"gpt-4o\" right now defaults to gpt-4o-2024-05-13.",
        "timestamp": "2024-09-21 00:15:36.581000+00:00",
        "id": 1286843239051300937,
        "parent_id": null,
        "thread_id": 1286843239051300937
    },
    {
        "author": "charizard_98",
        "content": "Thanks!",
        "timestamp": "2024-09-21 00:18:03.729000+00:00",
        "id": 1286843856234479778,
        "parent_id": null,
        "thread_id": 1286843239051300937
    },
    {
        "author": "joatmon.pockets",
        "content": "https://docs.boundaryml.com/docs/snippets/clients/providers/openai#forwarded-options",
        "timestamp": "2024-09-21 00:16:37.746000+00:00",
        "id": 1286843495595638867,
        "parent_id": null,
        "thread_id": 1286843239051300937
    },
    {
        "author": "joatmon.pockets",
        "content": "`client \"openai/gpt-4o-2024-08-06\"` or `model \"gpt-4o-2024-08-06\"`",
        "timestamp": "2024-09-21 00:16:12.490000+00:00",
        "id": 1286843389664301057,
        "parent_id": null,
        "thread_id": 1286843239051300937
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-21 00:16:12.054000+00:00",
        "id": 1286843387835842640,
        "parent_id": 1286843239051300937,
        "thread_id": 1286843239051300937
    },
    {
        "author": "hellovai",
        "content": "Error message",
        "timestamp": "2024-09-20 11:35:17.688000+00:00",
        "id": 1286651899545653280,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "Guys is there a way when using azure openai and hitting the rate limits per minute to know what time should i wait from the errors? I'm using client error from the documentation",
        "timestamp": "2024-09-20 08:40:07.424000+00:00",
        "id": 1286607816303575102,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "If that would help fyi",
        "timestamp": "2024-09-20 14:20:25.624000+00:00",
        "id": 1286693456441249793,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "You can currently set the delay time yourself",
        "timestamp": "2024-09-20 14:20:18.587000+00:00",
        "id": 1286693426926063690,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "https://docs.boundaryml.com/docs/snippets/clients/retry",
        "timestamp": "2024-09-20 14:20:08.370000+00:00",
        "id": 1286693384072855593,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "andrewcka",
        "content": "Would be amazing!",
        "timestamp": "2024-09-20 14:19:39.561000+00:00",
        "id": 1286693263239020584,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "And we can allow some max timeout or something",
        "timestamp": "2024-09-20 14:19:11.206000+00:00",
        "id": 1286693144309665823,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "Ah. I think we can add something in there where the delay of the retry policy will be based on the rate limit",
        "timestamp": "2024-09-20 14:18:50.465000+00:00",
        "id": 1286693057315737664,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "andrewcka",
        "content": "but you know that openai has an ratelimit error but i guess you are doing it differently",
        "timestamp": "2024-09-20 14:17:59.925000+00:00",
        "id": 1286692845335482378,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "andrewcka",
        "content": "No I'm just trying to do it for resilience",
        "timestamp": "2024-09-20 14:17:37.075000+00:00",
        "id": 1286692749495636070,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "It should be included in the actual error message that’s raised in the exception. Do you see that?",
        "timestamp": "2024-09-20 11:35:23.448000+00:00",
        "id": 1286651923704971274,
        "parent_id": null,
        "thread_id": 1286607816303575102
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-20 11:35:17.579000+00:00",
        "id": 1286651899088601123,
        "parent_id": 1286607816303575102,
        "thread_id": 1286607816303575102
    },
    {
        "author": ".aaronv",
        "content": "Reduce probability of each token",
        "timestamp": "2024-09-19 19:51:41.304000+00:00",
        "id": 1286414433220034685,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "kirilligum",
        "content": "another example of this problem is one of the simple task where all llms fail \n```\nWrite me a sentence without any words that appear in The Bible.\n\n```\nhttps://arxiv.org/html/2405.19616v2#S9.SS2.SSS8 see 9.2.8",
        "timestamp": "2024-09-19 19:50:08.787000+00:00",
        "id": 1286414045175349368,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "kirilligum",
        "content": "i assume that pydantic validation happens after json of the structured response was completed. i filter after the response now already using jq (or python). what i was wondering is since baml works with streaming api (my assumption), it checks every generated token (to make sure json syntax is correct). so it would makes sense to regenerate a wrong token instead of the whole response. if there are multiple wrong tokens in the response, the probability of regecting response goes up",
        "timestamp": "2024-09-19 19:47:56.795000+00:00",
        "id": 1286413491560775723,
        "parent_id": 1286411109456150599,
        "thread_id": null
    },
    {
        "author": "kirilligum",
        "content": "not 100% of the time. i end up removing answers with keywords using jq",
        "timestamp": "2024-09-19 19:39:24.031000+00:00",
        "id": 1286411340872945728,
        "parent_id": 1286410778072842323,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "You can do an after validation with pydantic or a dataclass if the word that appears is on the list, send the error and then try again using the feedback of the error",
        "timestamp": "2024-09-19 19:38:28.857000+00:00",
        "id": 1286411109456150599,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "Prompting doesn’t work for you?",
        "timestamp": "2024-09-19 19:37:09.849000+00:00",
        "id": 1286410778072842323,
        "parent_id": 1286410413210079355,
        "thread_id": null
    },
    {
        "author": "kirilligum",
        "content": "a follow up question. is it possible to easily identify encoding for a token and reduce the probability of it appearing? for example, i don't wnat certain words to appear in the response",
        "timestamp": "2024-09-19 19:35:42.859000+00:00",
        "id": 1286410413210079355,
        "parent_id": null,
        "thread_id": 1286410413210079355
    },
    {
        "author": ".aaronv",
        "content": "https://help.openai.com/en/articles/5247780-using-logit-bias-to-alter-token-probability-with-the-openai-api",
        "timestamp": "2024-09-19 19:52:54.787000+00:00",
        "id": 1286414741429817398,
        "parent_id": null,
        "thread_id": 1286410413210079355
    },
    {
        "author": ".aaronv",
        "content": "though you can only do this with individual tokens",
        "timestamp": "2024-09-19 19:52:01.077000+00:00",
        "id": 1286414516154011769,
        "parent_id": null,
        "thread_id": 1286410413210079355
    },
    {
        "author": ".aaronv",
        "content": "You can do this in your client parameters:\n\n```\nclient<llm> MyClient {\n\n   options {\n      logit_bias { .... }\n```\nfrom https://platform.openai.com/docs/api-reference/chat/create#chat-create-logit_bias",
        "timestamp": "2024-09-19 19:51:41.716000+00:00",
        "id": 1286414434947825766,
        "parent_id": null,
        "thread_id": 1286410413210079355
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-19 19:51:41.132000+00:00",
        "id": 1286414432498487458,
        "parent_id": 1286410413210079355,
        "thread_id": 1286410413210079355
    },
    {
        "author": "kirilligum",
        "content": "is there a way to easily define a python filtering function for each generated token? for example regex",
        "timestamp": "2024-09-19 19:34:48.153000+00:00",
        "id": 1286410183756746762,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "Dashboard bug",
        "timestamp": "2024-09-19 19:00:36.119000+00:00",
        "id": 1286401576902201369,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabev2037",
        "content": "Have you guys seen this error before? \n\n```\nNo events in the chain\n```",
        "timestamp": "2024-09-19 18:20:09.463000+00:00",
        "id": 1286391398769365025,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": ".aaronv",
        "content": "note, we solved this -- it's an issue with api key missing (and our bad errors)",
        "timestamp": "2024-09-19 20:29:51.113000+00:00",
        "id": 1286424037374820365,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": ".aaronv",
        "content": "What BOUNDARY_SECRET?",
        "timestamp": "2024-09-19 19:04:55.414000+00:00",
        "id": 1286402664464257116,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": "gabev2037",
        "content": "my BAML_LOG is not set",
        "timestamp": "2024-09-19 19:04:31.051000+00:00",
        "id": 1286402562278428682,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": "gabev2037",
        "content": "i shouldn't be pushing to the baml dashboard",
        "timestamp": "2024-09-19 19:04:06.923000+00:00",
        "id": 1286402461078257799,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": "gabev2037",
        "content": "I'm getting this error when using my baml client on a simple function",
        "timestamp": "2024-09-19 19:04:01.689000+00:00",
        "id": 1286402439125270568,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": ".aaronv",
        "content": "Ill take a look in about 40min!",
        "timestamp": "2024-09-19 19:01:03.009000+00:00",
        "id": 1286401689687031989,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> PTAL i think this is a bug on dashboad",
        "timestamp": "2024-09-19 19:00:36.556000+00:00",
        "id": 1286401578735243314,
        "parent_id": null,
        "thread_id": 1286391398769365025
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-19 19:00:36.119000+00:00",
        "id": 1286401576902201364,
        "parent_id": 1286391398769365025,
        "thread_id": 1286391398769365025
    },
    {
        "author": ".aaronv",
        "content": "Retry on parse failure",
        "timestamp": "2024-09-19 15:55:33.948000+00:00",
        "id": 1286355011021963338,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "No idea if internally do also the same",
        "timestamp": "2024-09-19 14:15:49.445000+00:00",
        "id": 1286329910197227601,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "I was reading it, but it seems for network error",
        "timestamp": "2024-09-19 14:15:31.506000+00:00",
        "id": 1286329834955477094,
        "parent_id": 1286327153306239091,
        "thread_id": null
    },
    {
        "author": "cat_ethos",
        "content": "maybe this will help https://docs.boundaryml.com/docs/snippets/clients/retry",
        "timestamp": "2024-09-19 14:04:52.151000+00:00",
        "id": 1286327153306239091,
        "parent_id": 1286325461080739921,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, is there a way to retry in case there is a fail to parse the output? The retry can work with this? or what is your recommendation?",
        "timestamp": "2024-09-19 13:58:08.693000+00:00",
        "id": 1286325461080739921,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "https://tenor.com/bRldp.gif",
        "timestamp": "2024-09-19 16:30:03.988000+00:00",
        "id": 1286363693398888489,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "ooh i like that, ok. You've convinced me i should think about this and write a more formal spec. I will write a frist draft and share with you in about a week",
        "timestamp": "2024-09-19 16:28:18.293000+00:00",
        "id": 1286363250081923156,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "it should be configured as an edge case for the dev, as in case is not a receipt output, {not a receipt} for example",
        "timestamp": "2024-09-19 16:27:21.526000+00:00",
        "id": 1286363011984130080,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "Oh right, but in that case it woudn't be in any case solvable by any library",
        "timestamp": "2024-09-19 16:26:41.379000+00:00",
        "id": 1286362843595411575,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "but i'm gonna think about it more and see what the balance is. E.g. maybe if we were able to parse some fields, but not all, then we can retry",
        "timestamp": "2024-09-19 16:12:41.981000+00:00",
        "id": 1286359322904760350,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "the only call out there is that lets say you have `function ParseResume(doc: image) -> Resume` and the user tosses in a receipt, you likely don't want to retry",
        "timestamp": "2024-09-19 16:12:03.476000+00:00",
        "id": 1286359161403216010,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "yea i think the devX there would be a good idea",
        "timestamp": "2024-09-19 16:09:43.007000+00:00",
        "id": 1286358572233396244,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "I think(just take as a personal opinion not want to go over your project), that for new devs and early users even passing the parsing error that comes from the pydantic and retrying(the dumb thing) would be a perfect starter",
        "timestamp": "2024-09-19 16:08:51.251000+00:00",
        "id": 1286358355153125498,
        "parent_id": 1286357054172434502,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "for now python is def the way to go 🙂",
        "timestamp": "2024-09-19 16:04:32.665000+00:00",
        "id": 1286357270564700223,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "and we're thinking through what the implications there are.\n\nThat said, we could just do the dumb thing and enable this.",
        "timestamp": "2024-09-19 16:04:15.762000+00:00",
        "id": 1286357199668510720,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "than if it didn't output any fields",
        "timestamp": "2024-09-19 16:03:48.042000+00:00",
        "id": 1286357083402539040,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "well more so, if the LLM outputed everything but 1 field w/ its parsing error, you may want to do something different",
        "timestamp": "2024-09-19 16:03:41.073000+00:00",
        "id": 1286357054172434502,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "Like to avoid inifinite calls to the llm?",
        "timestamp": "2024-09-19 16:03:19.625000+00:00",
        "id": 1286356964212998226,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "e.g. if you're gonna call an LLM it should be expected. We're thinking of ways to add retries on parsing, as parsing retries are very different than non-parsing retries.\n\n(i.e. maybe we can do something smarter w/ partial information)",
        "timestamp": "2024-09-19 16:03:13.747000+00:00",
        "id": 1286356939558748192,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "what do you mean?",
        "timestamp": "2024-09-19 16:03:10.215000+00:00",
        "id": 1286356924744601652,
        "parent_id": 1286356733207515147,
        "thread_id": 1286325461080739921
    },
    {
        "author": "hellovai",
        "content": "ah sadly no, we don't do this for a few reasons: but primarily that we wanted very explicit controls when the money printer goes $$$",
        "timestamp": "2024-09-19 16:02:24.549000+00:00",
        "id": 1286356733207515147,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "ok oh, perfect!",
        "timestamp": "2024-09-19 16:02:16.830000+00:00",
        "id": 1286356700831678505,
        "parent_id": 1286356615129206808,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "i'm implementing it on python because i was not sure is implemented",
        "timestamp": "2024-09-19 16:02:07.750000+00:00",
        "id": 1286356662747140096,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": ".aaronv",
        "content": "We dont currently have a way to do that but it is definitely on our roadmap",
        "timestamp": "2024-09-19 16:01:56.397000+00:00",
        "id": 1286356615129206808,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "I was thinking to sending it back the error over the failure like is done in instructor",
        "timestamp": "2024-09-19 16:01:48.660000+00:00",
        "id": 1286356582678134827,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": "andrewcka",
        "content": "fix the parse failure",
        "timestamp": "2024-09-19 16:01:30.170000+00:00",
        "id": 1286356505125195817,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": ".aaronv",
        "content": "Do you want an lllm to fix the parse failure? Or just call your function agaian?",
        "timestamp": "2024-09-19 15:55:34.429000+00:00",
        "id": 1286355013039423591,
        "parent_id": null,
        "thread_id": 1286325461080739921
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-19 15:55:33.804000+00:00",
        "id": 1286355010417852456,
        "parent_id": 1286325461080739921,
        "thread_id": 1286325461080739921
    },
    {
        "author": "gabriel_syme",
        "content": "Also, what was the way we could view/print that again?",
        "timestamp": "2024-09-19 06:33:49.166000+00:00",
        "id": 1286213642919874570,
        "parent_id": null,
        "thread_id": 1286213642919874570
    },
    {
        "author": ".aaronv",
        "content": "You may need to set BOUNDARY_PROJECT_ID=test\nBOUNDARY_SECRET=test\n\n\n(Basiclaly dummy values, due to a bug where we dont log unless these are set)",
        "timestamp": "2024-09-19 06:36:14.940000+00:00",
        "id": 1286214254340476951,
        "parent_id": null,
        "thread_id": 1286213642919874570
    },
    {
        "author": ".aaronv",
        "content": "BAML_LOG=info do you use python?",
        "timestamp": "2024-09-19 06:34:48.025000+00:00",
        "id": 1286213889792671797,
        "parent_id": null,
        "thread_id": 1286213642919874570
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-19 06:34:47.657000+00:00",
        "id": 1286213888249036800,
        "parent_id": 1286213642919874570,
        "thread_id": 1286213642919874570
    },
    {
        "author": "gabriel_syme",
        "content": "Anyone had consistent way of imducing CoT before extraction?",
        "timestamp": "2024-09-19 06:33:14.869000+00:00",
        "id": 1286213499068088341,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "airhorns",
        "content": "spot checking for analytics to start, but at scale, that has to get automated so it becomes a runtime thing anyways",
        "timestamp": "2024-09-20 11:16:03.137000+00:00",
        "id": 1286647057007771690,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": ".aaronv",
        "content": "do you also want CoT for analytics or for a runtime usecase?",
        "timestamp": "2024-09-20 01:04:56.183000+00:00",
        "id": 1286493264656072726,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "airhorns",
        "content": "right now we are paying double tokens because we do two calls to the LLM, one to do the CoT so we can see it and then another to do the BAML and it is frustrating",
        "timestamp": "2024-09-19 19:10:01.090000+00:00",
        "id": 1286403946562191511,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "airhorns",
        "content": "just to chime in here, it'd be really valuable to be able to get at the raw text of the response from the model so that we can inspect and improve the CoT from production runs",
        "timestamp": "2024-09-19 19:09:20.241000+00:00",
        "id": 1286403775229333525,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "It did! Will keep you updated",
        "timestamp": "2024-09-19 06:44:36.032000+00:00",
        "id": 1286216356072652813,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "yea! hope my email described it well, looking forward to the followup conversations 🙂",
        "timestamp": "2024-09-19 06:43:43.255000+00:00",
        "id": 1286216134709874688,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "Although ye will be useful in production and higher volume but that is after we have platform discussions etc.",
        "timestamp": "2024-09-19 06:42:55.978000+00:00",
        "id": 1286215936415764490,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "Nah its alright I was curious!",
        "timestamp": "2024-09-19 06:41:18.446000+00:00",
        "id": 1286215527336902749,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "if its useful for you, I can turn on the observability piece for you and then you'll be able to see it.\n\nAlso, i think there's a bug we can patch for tmrw, but BAML_LOG should print out everything to console",
        "timestamp": "2024-09-19 06:40:57.826000+00:00",
        "id": 1286215440850223125,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "we are still in the middle of processing the baml raw API to determine the right form here",
        "timestamp": "2024-09-19 06:40:16.654000+00:00",
        "id": 1286215268162474076,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "yea on failure we output the full text",
        "timestamp": "2024-09-19 06:39:58.869000+00:00",
        "id": 1286215193566773319,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "so you could could say something like:\n\n```\nfunction Foo() -> markdown<MyType>\n```",
        "timestamp": "2024-09-19 06:39:51.260000+00:00",
        "id": 1286215161652314134,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "Cause I think we get smth when it fails right?",
        "timestamp": "2024-09-19 06:39:43.448000+00:00",
        "id": 1286215128886284309,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "So no way to see full trace?",
        "timestamp": "2024-09-19 06:39:24.436000+00:00",
        "id": 1286215049144434699,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "its tricky to give the text back. We've been playing around with the idea of a `markdown` type, where we could return the actual prefix text in python as well.",
        "timestamp": "2024-09-19 06:39:15.974000+00:00",
        "id": 1286215013652238381,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "Ah I see cool",
        "timestamp": "2024-09-19 06:39:07.576000+00:00",
        "id": 1286214978428338206,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "sadly not in your python code, but for tests you run in the playground you'll be able to see it",
        "timestamp": "2024-09-19 06:38:41.842000+00:00",
        "id": 1286214870492254275,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": ".aaronv",
        "content": "Ohh you want it in the return value",
        "timestamp": "2024-09-19 06:38:38.630000+00:00",
        "id": 1286214857020145664,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": ".aaronv",
        "content": "https://www.promptfiddle.com/BAML-Examples-UM7wr",
        "timestamp": "2024-09-19 06:38:16.682000+00:00",
        "id": 1286214764963434577,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "As in, the reasoning before schema",
        "timestamp": "2024-09-19 06:37:55.939000+00:00",
        "id": 1286214677960982571,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "gabriel_syme",
        "content": "Cool ye I have variants of this. Is there an easy way to print this as well when extraction is successful?",
        "timestamp": "2024-09-19 06:37:29.366000+00:00",
        "id": 1286214566505611346,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": ".aaronv",
        "content": "You can say “explain your reasoning and reflect on the reasoning in a few sentences before yoj write out the schema”",
        "timestamp": "2024-09-19 06:37:20.097000+00:00",
        "id": 1286214527628869705,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "```\ntemplate_string CoT() #\"\n  Before answering outline some key detials.\n  Example:\n  * ...\n  * ...\n  * ...\n  {\n     .. // Schema\n  }\n\"#\n```\n\nThen i just throw it in the prompt:\n\n```\nprompt #\"\n  {{ ctx.output_format }}\n  {{ CoT() }}\n\n  {{ _.role('user') }}\n  {{ content }}\n\"#\n```",
        "timestamp": "2024-09-19 06:35:42.227000+00:00",
        "id": 1286214117132210238,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "my template string for CoT:",
        "timestamp": "2024-09-19 06:34:11.901000+00:00",
        "id": 1286213738277371925,
        "parent_id": null,
        "thread_id": 1286213499068088341
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-19 06:34:11.600000+00:00",
        "id": 1286213737015021610,
        "parent_id": 1286213499068088341,
        "thread_id": 1286213499068088341
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'm trying to add a field dynamically which would describe another class field but am getting the following error:\n\nCell In[15], line 6, in get_graph(text)\n      4 tb = TypeBuilder()\n      5 tb.SimpleNode.add_property(\"descrpition\", tb.string()).description(\"if possible, provide a description of id\")\n----> 6 res = await b.ExtractGraph(text.content, { \"tb\": tb })\n      7 return res\n\nTypeError: object DynamicGraph can't be used in 'await' expression",
        "timestamp": "2024-09-19 00:10:39.499000+00:00",
        "id": 1286117217267617872,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "thank you!!",
        "timestamp": "2024-09-19 01:12:42.433000+00:00",
        "id": 1286132832384716902,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "good progress!!",
        "timestamp": "2024-09-19 01:12:36.077000+00:00",
        "id": 1286132805725716532,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "Thanks for bearing with it!",
        "timestamp": "2024-09-19 01:12:22.841000+00:00",
        "id": 1286132750209781821,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "glad its working!",
        "timestamp": "2024-09-19 01:12:17.821000+00:00",
        "id": 1286132729154502657,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "ah 🙂",
        "timestamp": "2024-09-19 01:12:14.399000+00:00",
        "id": 1286132714801729577,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "i somehow deleted `default system` earlier",
        "timestamp": "2024-09-19 01:12:05.404000+00:00",
        "id": 1286132677073698907,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "now descriptions ae being populated",
        "timestamp": "2024-09-19 01:11:37.384000+00:00",
        "id": 1286132559549567067,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "{\"id\":\"chatcmpl-TamyCcnjJxb4oMuhnPeBWa\",\"object\":\"chat.completion\",\"created\":1726708214,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Here is the extracted information in the requested JSON schema:\\n\\n```\\n{\\n  \\\"nodes\\\": [\\n    {\\n      \\\"id\\\": \\\"Elon Musk\\\",\\n      \\\"type\\\": \\\"Person\\\",\\n      \\\"description\\\": \\\"CEO of SpaceX and Tesla\\\"\\n    },\\n    {\\n      \\\"id\\\": \\\"OpenAI\\\",\\n      \\\"type\\\": \\\"Organization\\\",\\n      \\\"description\\\": \\\"Artificial intelligence research organization\\\"\\n    }\\n  ],\\n  \\\"relationships\\\": [\\n    {\\n      \\\"source_node_id\\\": \\\"Elon Musk\\\",\\n      \\\"source_node_type\\\": \\\"Person\\\",\\n      \\\"target_node_id\\\": \\\"OpenAI\\\",\\n      \\\"target_node_type\\\": \\\"Organization\\\",\\n      \\\"type\\\": \\\"Sued\\\"\\n    }\\n  ]\\n}\\n```\\n\\nLet me know if you need any further assistance!\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":145,\"total_tokens\":299,\"completion_tokens\":154}}%",
        "timestamp": "2024-09-19 01:11:12.523000+00:00",
        "id": 1286132455274971198,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "no worries, it looks like the model is choosing to not output a description",
        "timestamp": "2024-09-19 01:10:56.991000+00:00",
        "id": 1286132390129041520,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "sorry",
        "timestamp": "2024-09-19 01:10:38.694000+00:00",
        "id": 1286132313385603185,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "they seem to be reurning null:\n\n\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": [\n        {\n          \\\"type\\\": \\\"text\\\",\n          \\\"text\\\": \\\"Extract from this content:\\nHere is the extracted information in the correct format:\\\\n\\\\n**Nodes**\\\\n\\\\n* Elon Musk (person)\\\\n* Open AI (organization)\\\\n\\\\n**Relationships**\\\\n\\\\n* Elon Musk - SUED - Open AI'\\n\\nAnswer in JSON using this schema:\\n{\\n  nodes: [\\n    {\\n      id: string,\\n      type: string,\\n      // Describe id using a sentence\\n      description: string or null,\\n    }\\n  ],\\n  relationships: [\\n    {\\n      source_node_id: string,\\n      source_node_type: string,\\n      target_node_id: string,\\n      target_node_type: string,\\n      type: string,\\n    }\\n  ],\\n}\\\"\n        }\n      ]\n    }\n  ]\n}\"\n{\"id\":\"chatcmpl-WTDt34DuwLWJBjq66nAxff\",\"object\":\"chat.completion\",\"created\":1726708076,\"model\":\"meta-llama/Meta-Llama-3-70B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Here is the extracted information in the correct format:\\n\\n```\\n{\\n  \\\"nodes\\\": [\\n    {\\n      \\\"id\\\": \\\"Elon Musk\\\",\\n      \\\"type\\\": \\\"person\\\",\\n      \\\"description\\\": null\\n    },\\n    {\\n      \\\"id\\\": \\\"Open AI\\\",\\n      \\\"type\\\": \\\"organization\\\",\\n      \\\"description\\\": null\\n    }\\n  ],\\n  \\\"relationships\\\": [\\n    {\\n      \\\"source_node_id\\\": \\\"Elon Musk\\\",\\n      \\\"source_node_type\\\": \\\"person\\\",\\n      \\\"target_node_id\\\": \\\"Open AI\\\",\\n      \\\"target_node_type\\\": \\\"organization\\\",\\n      \\\"type\\\": \\\"SUED\\\"\\n    }\\n  ]\\n}\\n```\"},\"finish_reason\":\"stop\",\"logprobs\":null}],\"usage\":{\"prompt_tokens\":189,\"total_tokens\":321,\"completion_tokens\":132}}%",
        "timestamp": "2024-09-19 01:09:02.726000+00:00",
        "id": 1286131910866636850,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "what is the output?",
        "timestamp": "2024-09-19 01:01:08.645000+00:00",
        "id": 1286129922427060236,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "have you tried in the playground?",
        "timestamp": "2024-09-19 01:01:05.546000+00:00",
        "id": 1286129909428912128,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "same for BAML -- the descriptions are none",
        "timestamp": "2024-09-19 01:00:22.376000+00:00",
        "id": 1286129728360546356,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "i will try the BAML approach now",
        "timestamp": "2024-09-19 00:57:56.981000+00:00",
        "id": 1286129118529982475,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "when i call the llm:\n\n`AIMessage(content='Elon Musk is a business magnate and entrepreneur.', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 53, 'total_tokens': 65}, 'model_name': 'meta-llama/Meta-Llama-3-70B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4bc106e6-2eeb-4571-97ed-915ee36ecd50-0', usage_metadata={'input_tokens': 53, 'output_tokens': 12, 'total_tokens': 65})`",
        "timestamp": "2024-09-19 00:57:44.135000+00:00",
        "id": 1286129064649691197,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "for the python example i'm not getting the desired output as description(s) are none:\n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person', description=None), SimpleNode(id='Open AI', type='organization', description=None)], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`",
        "timestamp": "2024-09-19 00:57:03.506000+00:00",
        "id": 1286128894239314033,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "oh nice! let me know if that works for you!",
        "timestamp": "2024-09-19 00:46:49.716000+00:00",
        "id": 1286126319817457695,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "that would work",
        "timestamp": "2024-09-19 00:44:55.472000+00:00",
        "id": 1286125840643657749,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "like would this work?\n\n```\nclass SimpleNode {\n  id string\n  type string\n  description string? @description(#\"if possible, provide a description of id\"#)\n}\n```",
        "timestamp": "2024-09-19 00:42:38.083000+00:00",
        "id": 1286125264392425515,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "got it, and you need to modify the description at runtime?",
        "timestamp": "2024-09-19 00:42:00.156000+00:00",
        "id": 1286125105315053630,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "BAML would be ideal but having it done in python will work fine",
        "timestamp": "2024-09-19 00:41:35.260000+00:00",
        "id": 1286125000893403147,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "oh right, thanks!!",
        "timestamp": "2024-09-19 00:41:10.118000+00:00",
        "id": 1286124895440470027,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "also do make sure you've save the BAML file fyi!",
        "timestamp": "2024-09-19 00:40:51.805000+00:00",
        "id": 1286124818629922846,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "out of curiosity, do you want to set the description in python? or is it doable in BAML?",
        "timestamp": "2024-09-19 00:40:33.170000+00:00",
        "id": 1286124740469067776,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "```python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client import b\nasync def get_graph(text):\n    tb = TypeBuilder()\n    tb.SimpleNode.add_property(\"description\", tb.string().optional())\n    res = await b.ExtractGraph(text.content, { \"tb\": tb })\n    return res\n```",
        "timestamp": "2024-09-19 00:39:53.024000+00:00",
        "id": 1286124572084670565,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "or \n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person', description='Elon Musk is an entrepreneur'), SimpleNode(id='Open AI', type='organization', description=none)], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`",
        "timestamp": "2024-09-19 00:39:38.454000+00:00",
        "id": 1286124510973788180,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "i see",
        "timestamp": "2024-09-19 00:38:33.051000+00:00",
        "id": 1286124236653596674,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "something like this:\n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person', description='Elon Musk is an entrepreneur'), SimpleNode(id='Open AI', type='organization')], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`",
        "timestamp": "2024-09-19 00:37:30.818000+00:00",
        "id": 1286123975629340672,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "sorry what are you expecting out of it? it'll help me understand",
        "timestamp": "2024-09-19 00:35:27.685000+00:00",
        "id": 1286123459172110408,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "without dynamic field:\n\n`DynamicGraph(nodes=[SimpleNode(id='Elon Musk', type='person'), SimpleNode(id='Open AI', type='organization')], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`",
        "timestamp": "2024-09-19 00:34:35.340000+00:00",
        "id": 1286123239621398540,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "somehow the output is not as i intended it to be:\n\n`DynamicGraph(nodes=[], relationships=[SimpleRelationship(source_node_id='Elon Musk', source_node_type='person', target_node_id='Open AI', target_node_type='organization', type='SUED')])`",
        "timestamp": "2024-09-19 00:31:56.358000+00:00",
        "id": 1286122572802560030,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "oops thanks",
        "timestamp": "2024-09-19 00:30:59.936000+00:00",
        "id": 1286122336151666762,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "hellovai",
        "content": "you likely on't need the await on `res = await b.ExtractGraph` try just:\n\n```\nres = b.ExtractGraph(...)\n```",
        "timestamp": "2024-09-19 00:23:31.930000+00:00",
        "id": 1286120457078177852,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "the python code:\n\n`from baml_client.type_builder import TypeBuilder\nfrom baml_client import b\nasync def get_graph(text):\n    tb = TypeBuilder()\n    tb.SimpleNode.add_property(\"descrpition\", tb.string()).description(\"if possible, provide a description of id\")\n    res = await b.ExtractGraph(text.content, { \"tb\": tb })\n    return res`",
        "timestamp": "2024-09-19 00:12:13.756000+00:00",
        "id": 1286117612610261002,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "here is the data model(s):\n\n`// Defining a data model.\nclass SimpleNode {\n  id string\n  type string\n  @@dynamic\n}\n\nclass SimpleRelationship {\n    source_node_id string\n    source_node_type string\n    target_node_id string\n    target_node_type string\n    type string\n}\n\nclass DynamicGraph {\n    nodes SimpleNode[]\n    relationships SimpleRelationship[]\n}\n\n// Creating a function to extract the DynamicGraph from a string.\nfunction ExtractGraph(graph: string) -> DynamicGraph {\n  client Llama70b\n  prompt #\"\n    Extract from this content:\n    {{ graph }}\n\n    {{ ctx.output_format }}\n  \"#\n}`",
        "timestamp": "2024-09-19 00:11:21.470000+00:00",
        "id": 1286117393306746921,
        "parent_id": null,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "",
        "timestamp": "2024-09-19 00:11:21.168000+00:00",
        "id": 1286117392040202251,
        "parent_id": 1286117217267617872,
        "thread_id": 1286117217267617872
    },
    {
        "author": "arindamkhaled4530",
        "content": "i tried using baml (in my own way) to parse a string (of list of strings):\n\nclass DeDupeResult {\n    duplicates string[]\n}\n\n^^returns nothing\n\nclass DeDupeResult {\n    merged_results string[]\n}\n\n^^ returns the intended list of string (merged_results)\n\ni am wondering how this is working?",
        "timestamp": "2024-09-18 22:19:11.602000+00:00",
        "id": 1286089166194610286,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "",
        "timestamp": "2024-09-18 22:58:31.491000+00:00",
        "id": 1286099064286613515,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes sure",
        "timestamp": "2024-09-18 22:47:00.834000+00:00",
        "id": 1286096167461191781,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "i can see whats going on",
        "timestamp": "2024-09-18 22:46:51.392000+00:00",
        "id": 1286096127858446367,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "want to hop on office hours rq?",
        "timestamp": "2024-09-18 22:46:38.429000+00:00",
        "id": 1286096073487683596,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "they are not populating:",
        "timestamp": "2024-09-18 22:46:10.070000+00:00",
        "id": 1286095954541412365,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "you can click on either the dropdown in the menu, or click on any line in duplicate_ids.baml to switch to the ExtractDeDupe function",
        "timestamp": "2024-09-18 22:45:10.361000+00:00",
        "id": 1286095704103714870,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "somehow i only see resume example:",
        "timestamp": "2024-09-18 22:44:26.190000+00:00",
        "id": 1286095518837243995,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "swap line 19 from resume -> graph",
        "timestamp": "2024-09-18 22:42:19.068000+00:00",
        "id": 1286094985648930898,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "somehow it looks for graph DS:",
        "timestamp": "2024-09-18 22:41:55.450000+00:00",
        "id": 1286094886587727895,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "then you can try running the unit test to see whats going on there",
        "timestamp": "2024-09-18 22:37:24.254000+00:00",
        "id": 1286093749109260369,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "then in VSCode, there's a \"Open Playground\" button right over the BAML function:",
        "timestamp": "2024-09-18 22:37:08.398000+00:00",
        "id": 1286093682604376154,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "you may want to try and see the test case in BAML",
        "timestamp": "2024-09-18 22:36:22.955000+00:00",
        "id": 1286093492002488383,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "hmm i see it working here:\n\nhttps://www.promptfiddle.com/dudupe-test-SMGZ4",
        "timestamp": "2024-09-18 22:36:08.555000+00:00",
        "id": 1286093431604645971,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "it doesn't work when i use the following class definition:\n`\nclass DeDupeResult {\n    duplicates string[]\n}`",
        "timestamp": "2024-09-18 22:36:06.724000+00:00",
        "id": 1286093423924740167,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "let me try it!",
        "timestamp": "2024-09-18 22:34:10.994000+00:00",
        "id": 1286092938518204438,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "I see and that didn't work?",
        "timestamp": "2024-09-18 22:33:55.187000+00:00",
        "id": 1286092872218841162,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "full class:\n\n`class DeDupeResult {\n    merged_results string[]\n}\n\n// Creating a function to extract the Result from a string.\nfunction ExtractDeDupe(graph: string) -> DeDupeResult {\n  client Llama70b\n  prompt #\"\n    Extract from this content:\n    {{ graph }}\n\n    {{ ctx.output_format }}\n  \"#\n}`",
        "timestamp": "2024-09-18 22:29:49.097000+00:00",
        "id": 1286091840042438667,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "'Star Ocean: The Second Story R'",
        "timestamp": "2024-09-18 22:26:14.015000+00:00",
        "id": 1286090937923014697,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'm calling this on an output like the following:\n\n`st = \"['Star Ocean: The Second Story R', 'something']\"\nres = await b.ExtractDeDupe(st)\nres.merged_results[0]`",
        "timestamp": "2024-09-18 22:26:02.644000+00:00",
        "id": 1286090890229579919,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "can you share the prompt?",
        "timestamp": "2024-09-18 22:22:55.211000+00:00",
        "id": 1286090104078733355,
        "parent_id": null,
        "thread_id": 1286089166194610286
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-18 22:22:55.009000+00:00",
        "id": 1286090103231483945,
        "parent_id": 1286089166194610286,
        "thread_id": 1286089166194610286
    },
    {
        "author": "andrewcka",
        "content": "Guys quick question, how can I use azure-openai? I'm havin troubles when i declare it into the function should i declare the client? When i declare it like:\n\n  client \"azure-openai/gpt4o\"\n\nGives me an error, an also when i call the name on the client that i defined into the clients.baml doesn't work, how should it be?",
        "timestamp": "2024-09-18 15:59:14.532000+00:00",
        "id": 1285993548352720971,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "hellovai",
        "content": "So hyped!",
        "timestamp": "2024-09-18 22:18:30.335000+00:00",
        "id": 1286088993108131946,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "hellovai",
        "content": "😂",
        "timestamp": "2024-09-18 22:18:25.480000+00:00",
        "id": 1286088972744785971,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "hellovai",
        "content": "!!!!",
        "timestamp": "2024-09-18 22:18:21.437000+00:00",
        "id": 1286088955787350027,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "andrewcka",
        "content": "It's really a THING!",
        "timestamp": "2024-09-18 22:10:05.892000+00:00",
        "id": 1286086877320970271,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "andrewcka",
        "content": "BTW, i'm moving all my code from instructor to BAML",
        "timestamp": "2024-09-18 22:09:59.853000+00:00",
        "id": 1286086851991699558,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "andrewcka",
        "content": "Thank you! I forgot to say!",
        "timestamp": "2024-09-18 22:09:50.097000+00:00",
        "id": 1286086811071938571,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "hellovai",
        "content": "Ah we have a minor bug in our parser:\n\nthis fails:\n```\nclient MyClientName // foo\n```\n\nbut this will work:\n```\n// foo\nclient MyClientName\n```\n\n(We need to fix trailing comments)",
        "timestamp": "2024-09-18 16:00:33.873000+00:00",
        "id": 1285993881133125682,
        "parent_id": null,
        "thread_id": 1285993548352720971
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-18 16:00:33.435000+00:00",
        "id": 1285993879295758499,
        "parent_id": 1285993548352720971,
        "thread_id": 1285993548352720971
    },
    {
        "author": "loohly",
        "content": "Hi all! First of all, am having a blast with baml!\nI have a question about serving baml as an API: **Is it possible to have headers from the request to the served baml API be passed through to the LLM provider?**\n\nI have two use cases for this:\n1. The LLM provider is azure-openai \"like\", but requires an OAuth token that has to be refreshed every 12h. Being able to forward a token header from the request to the baml API would mean that the requesting client could take care of having a non-expired token\n2. The client that calls the baml API also wants to add some headers to the LLM provider that let it know who is making the request (required by the provider for metering). \n\nHappy to give more details if needed.",
        "timestamp": "2024-09-18 14:04:19.120000+00:00",
        "id": 1285964626898456708,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "I don't mind of course 🙂 awesome to see this released so quickly! Thanks for the support from both of you and I'll definitely give an update after my additional testing (that I haven't gotten to yet 🙈 )",
        "timestamp": "2024-10-03 07:29:17.062000+00:00",
        "id": 1291301031245778992,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "0.58 includes this change fyi!",
        "timestamp": "2024-10-02 16:48:59.090000+00:00",
        "id": 1291079496614543411,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "imalsogreg",
        "content": "Happy to help! Hope you don't mind I pulled your changes into a PR and merged it yesterday. Definitely helpful to hear if the API works for you and if it ends up solving your Auth issues, please let me know, we can quickly patch it as needed.\n\nCongrats on being the first contributor to ship a whole feature!  That's  awesome!",
        "timestamp": "2024-10-02 16:38:35.378000+00:00",
        "id": 1291076880576942165,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Thanks for the support,  Greg! I'll do some additional testing tomorrow and then I think it should be good to merge",
        "timestamp": "2024-10-01 20:30:43.700000+00:00",
        "id": 1290772912328015882,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "imalsogreg",
        "content": "Ok, I've confirmed it works in Java too. Using an openapi generated client, you can do this:\n```\nBamlOptions bamlOptions = new BamlOptions()\n    .clientRegistry(new BamlOptionsClientRegistry()\n                    .clients(Collections.singletonList(\n                        new ClientProperty()\n                            .name(\"OpenAI\")\n                            .provider(\"openai\")\n                            .retryPolicy(null)\n                        .options(new java.util.HashMap<String,Object>() {{\n                            put(\"model\", \"gpt-4o-mini\");\n                            put(\"api_key\", apiKey);\n                        }})\n                    ))\n                    .primary(\"OpenAI\")\n                    );\n// Extract resume example\nString resumeText = \"John Doe\\nSoftware Engineer ...\";\nExtractResumeRequest extractResumeRequest = new ExtractResumeRequest()\n    .resume(resumeText)\n    .bamlOptions(bamlOptions);\n\n```",
        "timestamp": "2024-10-01 18:19:25.552000+00:00",
        "id": 1290739868980346981,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "imalsogreg",
        "content": "<@438416839224197132> I tested out your PR yesterday and things look promising after implementing that last TODO. If you'd like to try it out, I opened a [PR against your fork](https://github.com/lorenzoh/baml/pull/1)  The description shows an example rust client using `__baml_options__` to select different models.\n\nToday I'd like to test it out using Java, and to see if I can work out why some fields are getting double-wrapped as optional. But I wanted to share the work-in-progress with you before doing that, since you have your Oct 10th internal deadline coming soon.",
        "timestamp": "2024-10-01 16:37:38.172000+00:00",
        "id": 1290714252771983411,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "imalsogreg",
        "content": "PR looks great! I'm going to focus on running it locally, finishing up the last TODO merging asap.",
        "timestamp": "2024-09-30 19:42:27.440000+00:00",
        "id": 1290398376667512924,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "<@503733199130722314> from our team is actually gonna help out with this and help land this to completion! Very excited to get this in by today or tmrw! Looks close to completion",
        "timestamp": "2024-09-30 19:40:35.045000+00:00",
        "id": 1290397905248845887,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "Awesome. I’ll add that in! I’ll pull and test it",
        "timestamp": "2024-09-30 13:17:42.720000+00:00",
        "id": 1290301552333947014,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Okay, the PR should be ready to review now: https://github.com/BoundaryML/baml/pull/986\n\nOne thing that's missing is adding the `__baml_options__` field to each request body schema in `openapi.rs`, I wasn't sure how to do that in a clean way. If you can help me out there, that'd be awesome (the schema definitions for `BamlOptions` are already added).",
        "timestamp": "2024-09-30 11:05:34.048000+00:00",
        "id": 1290268297073524756,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "Let’s do a quick call! If you’re free at 9 am pst (90 mins from now) can hop on a call then. If you dm me your email, I’ll send out an invite",
        "timestamp": "2024-09-27 14:34:20.394000+00:00",
        "id": 1289233672779595776,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Sure, let's do that 🙂 feel free to DM me what changes would be needed, also open to a quick call if you think that'd make sense",
        "timestamp": "2024-09-27 14:11:03.539000+00:00",
        "id": 1289227813945081866,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "If you’ve got cycles, I would totally take you up on that? I can quickly explain what work needs to get done and then share where the code goes. Would you have time for that? I think it’s 1-2 days of work max! Most likely our team can get to this by your deadline!",
        "timestamp": "2024-09-27 13:24:18.533000+00:00",
        "id": 1289216048897200208,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Hi Aaron, thanks for following up! The receiving team is quite happy with it, I'll probe for any rough edges and get back to you\n\nTo give a quick update on my use case: I was able to convince the relevant stakeholders that BAML is the way to go and should be adopted for the project. However, since the header forwarding is a blocking issue (all requests need to be proxied through an internal platform that requires fresh OAuth headers and metering headers), this adoption decision is contingent on this issue being resolved by 10th of October since there's a deadline looming.\n<@99252724855496704> if there's been any change in the planned timeline for releasing the client registry over HTTP support, I would be happy to know! \n(I hope this doesn't read as setting an expectation, I know this is open source and I appreciate the amazing work in the project here ❤️ )\nIf there's any way I can contribute to help get this feature out, I would be happy to",
        "timestamp": "2024-09-27 09:21:30.085000+00:00",
        "id": 1289154944397738015,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": ".aaronv",
        "content": "<@438416839224197132>  how are you liking the Java generator? Is the API interface good enough? Any feedback here helps!",
        "timestamp": "2024-09-27 07:17:20.572000+00:00",
        "id": 1289123698875568171,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Used to be an almost full-time maintainer of OSS, so I share the sentiment haha\nIn any case, here's the PR: https://github.com/BoundaryML/baml/pull/968",
        "timestamp": "2024-09-19 06:15:12.488000+00:00",
        "id": 1286208959232999545,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "always appreciate contributors! makes our life much easier haha",
        "timestamp": "2024-09-18 15:26:48.528000+00:00",
        "id": 1285985386220486709,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "oh awesome, thanks <@438416839224197132> 🙂",
        "timestamp": "2024-09-18 15:26:36.785000+00:00",
        "id": 1285985336966774854,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Never mind that one is correct, however the comment \"This command will be run from within $outputdir\", should be \"This command will be run from within baml_client/outputdir\". I'll find the place that comment is placed and update it.",
        "timestamp": "2024-09-18 15:26:06.659000+00:00",
        "id": 1285985210609041411,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "(otherwise i'll patch it)",
        "timestamp": "2024-09-18 15:19:44.519000+00:00",
        "id": 1285983607797846047,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "its just one line of change",
        "timestamp": "2024-09-18 15:19:36.615000+00:00",
        "id": 1285983574645800960,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "oh nice, if you want, you can actually open up a real pr to fix it 😉 \n\nhttps://github.com/BoundaryML/baml/blob/46322c394dd7a0920874fafab2f61d1f888daa9b/engine/baml-runtime/src/cli/init.rs#L130",
        "timestamp": "2024-09-18 15:19:32.108000+00:00",
        "id": 1285983555742072873,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "That works for me, though I had to change `-i openapi.yaml` to `-i baml_client/openapi.yaml` since that is where the yaml file is generated for me. If this is part of an example that maybe no longer works, I'll happily open up a documentation PR to fix it",
        "timestamp": "2024-09-18 15:02:32.261000+00:00",
        "id": 1285979278193856512,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "> Are you using the generated JavaSDK for the HTTP request?\n\nI was not, since I wasn't aware of this snippet 😮 trying it out now!",
        "timestamp": "2024-09-18 14:53:08.983000+00:00",
        "id": 1285976915634819144,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "> I like how you said build properly\nI wouldn't say that Java is my favorite language either, but in the end there is supposed some consistency in the tech stack 😄",
        "timestamp": "2024-09-18 14:52:20.105000+00:00",
        "id": 1285976710625628243,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "Then you can plug anything you want for `output_dir \"../java\"\n` and it'll dump java files + openapi spec for your specific BAML functions",
        "timestamp": "2024-09-18 14:43:13.995000+00:00",
        "id": 1285974420074270831,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "```\n// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator java_sdk {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"rest/openapi\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../java\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.55.3\"\n\n    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client\n    // This command will be run from within $output_dir\n    on_generate \"npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g java -o . --additional-properties invokerPackage=com.boundaryml.baml_client,modelPackage=com.boundaryml.baml_client.model,apiPackage=com.boundaryml.baml_client.api,java8=true && cd ../baml_client && mvn clean install\"\n}\n```",
        "timestamp": "2024-09-18 14:42:28.100000+00:00",
        "id": 1285974227576553525,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "Are you using the generated JavaSDK for the HTTP request?",
        "timestamp": "2024-09-18 14:39:49.671000+00:00",
        "id": 1285973563077165167,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "I too agree, python is not a proper language XD",
        "timestamp": "2024-09-18 14:39:25.627000+00:00",
        "id": 1285973462229450783,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "I like how you said `build properly` 😂",
        "timestamp": "2024-09-18 14:39:18.768000+00:00",
        "id": 1285973433460461743,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "And since there's no JVM generated client available yet (afaict), the decision to host baml externally and call it through the API",
        "timestamp": "2024-09-18 14:33:45.226000+00:00",
        "id": 1285972034483916820,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Prototyped in Python (which, ofc, has a nice generated client), now to be built properly in Java",
        "timestamp": "2024-09-18 14:33:02.076000+00:00",
        "id": 1285971853499826216,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "btw what language are you using with BAML over HTTP?",
        "timestamp": "2024-09-18 14:32:21.198000+00:00",
        "id": 1285971682045071370,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "we are still working on that 🙂",
        "timestamp": "2024-09-18 14:31:48.068000+00:00",
        "id": 1285971543087644787,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "not yet, we have an internal workitem though! <@711679663746842796>  can you ticket this so we can share this",
        "timestamp": "2024-09-18 14:31:40.063000+00:00",
        "id": 1285971509512372326,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "and on that note, is there an OpenAPI.json spec for the BAML HTTP server available somewhere?\nScouring the repository, I was only able to find the endpoints `/_debug/ping` and `/call/<fn>`",
        "timestamp": "2024-09-18 14:31:35.174000+00:00",
        "id": 1285971489006555166,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Is there a PR/issue open already tracking this?",
        "timestamp": "2024-09-18 14:30:36.419000+00:00",
        "id": 1285971242570223656,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "Makes sense, even better to have it in one request.",
        "timestamp": "2024-09-18 14:30:25.648000+00:00",
        "id": 1285971197393244162,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "so then you'd be able to construct any client as is possible with client registry",
        "timestamp": "2024-09-18 14:30:20.174000+00:00",
        "id": 1285971174433751132,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "it would likely look something like this:\n\n```\ncurl -X POST 'https://yourbamlendpoint.com/call/function' -H \"content-type: application/json\" -d '{\n  param1: ...,\n  __baml_options__: {\n     client_registry: { <info goes here> }\n  }\n}'\n```",
        "timestamp": "2024-09-18 14:29:58.734000+00:00",
        "id": 1285971084507611231,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "That's awesome!\nJust to make sure I understand it correctly, this means there would be (a) endpoints exposed for creating a custom client (which could include the custom headers), (b) a way to specify which client to use in the request to the `/call/*` endpoint (likely as a header, since the body is just the input data itself).\nIs that right?",
        "timestamp": "2024-09-18 14:28:30.708000+00:00",
        "id": 1285970715299938356,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "ah got it yes that would work.\n\n<@711679663746842796> trackign for you to share with <@438416839224197132> when we can ship client registry and typebuilder for BAML over HTTP.\n\nMy thoughts are we are about 2 weeks away from this going live (maybe sooner, but that would be the safe version).",
        "timestamp": "2024-09-18 14:25:08.646000+00:00",
        "id": 1285969867790614609,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "loohly",
        "content": "I am using BAML over HTTP, ie running using `baml-cli`. \nWith the native generated clients this would be quite simple with the `ClientRegistry` I think",
        "timestamp": "2024-09-18 14:24:04.120000+00:00",
        "id": 1285969597148827790,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "so glad you're loving BAML!\n\nQuick question, are you using BAML over HTTP, or native python, or native TS version of BAML?",
        "timestamp": "2024-09-18 14:07:59.113000+00:00",
        "id": 1285965549616238657,
        "parent_id": null,
        "thread_id": 1285964626898456708
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-18 14:07:58.882000+00:00",
        "id": 1285965548647354400,
        "parent_id": 1285964626898456708,
        "thread_id": 1285964626898456708
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, is there a way to use the new commet Opik with BAML?",
        "timestamp": "2024-09-18 13:31:27.349000+00:00",
        "id": 1285956356691460178,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "Then after they send this data to the deployment, i'm hosting it locally and it allows you to test the prompts and use llms as judges easily",
        "timestamp": "2024-09-18 14:27:58.622000+00:00",
        "id": 1285970580721631317,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "Got it, I was reviewing the code, and what Opik does is essentially capture the input prompt and parameters before making the request, and once the response comes back, it captures the output. It doesn't directly intercept or modify the underlying HTTP request itself but wraps the method that handles the request",
        "timestamp": "2024-09-18 14:27:17.561000+00:00",
        "id": 1285970408499052576,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "I see, sorry i meant like the web requests they actually make to track all the content. We could technically add a hook for someone to upload all the requests our way",
        "timestamp": "2024-09-18 13:57:40.944000+00:00",
        "id": 1285962956827267082,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "this is how they do it",
        "timestamp": "2024-09-18 13:57:10.460000+00:00",
        "id": 1285962828968230965,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "def track_openai(openai_client):\n    decorator = OpenaiTrackDecorator()\n    wrapper = decorator.track(\n        type=\"llm\",\n        name=\"chat_completion_create\",\n        generations_aggregator=chunks_aggregator.aggregate,\n    )\n    openai_client.chat.completions.create = wrapper(\n        openai_client.chat.completions.create\n    )\n    return openai_client",
        "timestamp": "2024-09-18 13:57:06.993000+00:00",
        "id": 1285962814426579126,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "opik uses a decorator to wrap the openai client.  It takes an instance of the OpenAI client and replaces it to include the tracking functionality.",
        "timestamp": "2024-09-18 13:56:44.633000+00:00",
        "id": 1285962720642076753,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "if you can share how they do that, then it may be possible to plug that in",
        "timestamp": "2024-09-18 13:38:04.406000+00:00",
        "id": 1285958022069354658,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "hmm it doesn't look they they document how they make the webrequest",
        "timestamp": "2024-09-18 13:37:51.944000+00:00",
        "id": 1285957969799811084,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "Let me try and see how they do this",
        "timestamp": "2024-09-18 13:36:41.683000+00:00",
        "id": 1285957675104079894,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "there's not directly a way to monkey patch,  but i think they do some work to basically forward requests",
        "timestamp": "2024-09-18 13:36:34.357000+00:00",
        "id": 1285957644376342559,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "I see",
        "timestamp": "2024-09-18 13:36:11.621000+00:00",
        "id": 1285957549014646827,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "Is there a way to monkey patch the client?",
        "timestamp": "2024-09-18 13:35:40.770000+00:00",
        "id": 1285957419616440445,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "but since it is handle internally",
        "timestamp": "2024-09-18 13:35:08.250000+00:00",
        "id": 1285957283217412098,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "As you see you would monkey patch openai for it",
        "timestamp": "2024-09-18 13:35:03.773000+00:00",
        "id": 1285957264439644190,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "andrewcka",
        "content": "https://www.comet.com/docs/opik/tracing/integrations/openai/",
        "timestamp": "2024-09-18 13:34:28.516000+00:00",
        "id": 1285957116560937163,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "can you share what that is?",
        "timestamp": "2024-09-18 13:34:16.539000+00:00",
        "id": 1285957066326020150,
        "parent_id": null,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-18 13:34:16.269000+00:00",
        "id": 1285957065193291958,
        "parent_id": 1285956356691460178,
        "thread_id": 1285956356691460178
    },
    {
        "author": "hellovai",
        "content": "Session history in BAML",
        "timestamp": "2024-09-17 14:03:58.775000+00:00",
        "id": 1285602153699872826,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "brandburner",
        "content": "I am LOVING using BAML so far for generating structured metadata from drama transcripts. With Sonnet 3.5 and prompt caching I'm able to process a script scene by scene and extract all the entities, dramatic themes etc. So I'm curious if BAML natively maintains any kind of session history when executing on functions, or whether each call is independent of the previous? For my use case, it would help the consistency of responses if the model could see its previous few responses",
        "timestamp": "2024-09-17 10:30:20.265000+00:00",
        "id": 1285548388972236842,
        "parent_id": null,
        "thread_id": 1285548388972236842
    },
    {
        "author": "hellovai",
        "content": "yep! for that you can just pass it in state as well <@1150651185167085589>  and just update RenderMessage function accoridingly\n\n```\nclass Message {\n  role string\n  content string | History\n}\n\nfunction ProcessScene(script: string, messages: Message[]) -> History {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Given this script.\n    {{ script }}.\n\n    {{ ctx.output_format }}\n\n    {% for m in messages %}\n    {{ RenderMessage(m) }}\n    {% endfor %}\n  \"#\n}\n```",
        "timestamp": "2024-09-18 13:33:20.166000+00:00",
        "id": 1285956829880516629,
        "parent_id": null,
        "thread_id": 1285548388972236842
    },
    {
        "author": "simontam0",
        "content": "Do you have any guidance for how to deal with history that would contain previous structured responses? ex. i ask for a list of things, the content of that is returned as structured output. So the content of that message is technically what? a pydantic model? json?. We need this so that subsequent questions may be asked about the prior content (in this case previous items as structured output)",
        "timestamp": "2024-09-18 01:50:57.735000+00:00",
        "id": 1285780071713345546,
        "parent_id": 1285602162952372224,
        "thread_id": 1285548388972236842
    },
    {
        "author": "brandburner",
        "content": "ahh nice - thanks! I'll see if i can make it work with my main scene processing loop",
        "timestamp": "2024-09-17 14:06:32.573000+00:00",
        "id": 1285602798775570533,
        "parent_id": null,
        "thread_id": 1285548388972236842
    },
    {
        "author": "hellovai",
        "content": "Would this be what you're looking for?",
        "timestamp": "2024-09-17 14:04:29.875000+00:00",
        "id": 1285602284142592095,
        "parent_id": null,
        "thread_id": 1285548388972236842
    },
    {
        "author": "hellovai",
        "content": "Awesome ❤️ \n\nThere's some ways to do history if you'd like:\n\n```rust\nclass Message {\n  role string\n  content string\n}\n\ntemplate_string RenderMessage(m: Message) #\"\n  {{ _.role(m.role) }}\n  {{ m.content }} \n\"#\n\nfunction ProcessScene(script: string, messages: Message[]) -> History {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Given this script.\n    {{ script }}.\n\n    {{ ctx.output_format }}\n\n    {% for m in messages %}\n    {{ RenderMessage(m) }}\n    {% endfor %}\n  \"#\n}\n```",
        "timestamp": "2024-09-17 14:04:00.981000+00:00",
        "id": 1285602162952372224,
        "parent_id": null,
        "thread_id": 1285548388972236842
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-17 14:03:58.775000+00:00",
        "id": 1285602153699872819,
        "parent_id": 1285548388972236842,
        "thread_id": 1285548388972236842
    },
    {
        "author": "sidd065",
        "content": "Hey guys, Is it possible to change the value of the description of a property inside a class via python?\nExample:\n```class Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n  extra_data string @description(\"Description for what extra data to extract, passed as a variable\")\n}```\nHere could I set the description of `extra_data` dynamically using a function call or some other way?",
        "timestamp": "2024-09-17 03:53:45+00:00",
        "id": 1285448584397066414,
        "parent_id": null,
        "thread_id": 1285448584397066414
    },
    {
        "author": "sidd065",
        "content": "awesome, thank you!",
        "timestamp": "2024-09-17 03:56:22.297000+00:00",
        "id": 1285449244148502560,
        "parent_id": null,
        "thread_id": 1285448584397066414
    },
    {
        "author": "joatmon.pockets",
        "content": "Yes it is! https://docs.boundaryml.com/docs/calling-baml/dynamic-types",
        "timestamp": "2024-09-17 03:55:45.168000+00:00",
        "id": 1285449088417927252,
        "parent_id": null,
        "thread_id": 1285448584397066414
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-17 03:55:44.827000+00:00",
        "id": 1285449086987800637,
        "parent_id": 1285448584397066414,
        "thread_id": 1285448584397066414
    },
    {
        "author": "nicarq",
        "content": "if we are using the wasm lib to generate the schemas, how difficult it would be to extend it so it also can run code using the runtime? it seems that it's already doing it for some test cases. I saw that's not quite there right now but wondering how much extra work it could be and if someone like me (external) could be able to do it",
        "timestamp": "2024-09-17 03:50:28.695000+00:00",
        "id": 1285447761034084402,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": ".aaronv",
        "content": "Sounds perfect! Thanks a bunch",
        "timestamp": "2024-09-18 02:57:41.645000+00:00",
        "id": 1285796865329201236,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "i will make it different than the other one so it's also helpful with another stuck, so it will be more barebones using typescript and node without extra frameworks like nextjs",
        "timestamp": "2024-09-18 02:47:13.167000+00:00",
        "id": 1285794229301415957,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "great! i will create the examples in the next couple of days and share them with you. for the ollama one i will create a small pr",
        "timestamp": "2024-09-18 02:46:29.495000+00:00",
        "id": 1285794046127902783,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": ".aaronv",
        "content": "We have baml-examples repo so we might add the ollama one there",
        "timestamp": "2024-09-18 02:29:11.034000+00:00",
        "id": 1285789690506776636,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": ".aaronv",
        "content": "Im down to have more reference examples! We can add disclaimers. And we can link to them if necessary",
        "timestamp": "2024-09-18 02:28:51.600000+00:00",
        "id": 1285789608994541659,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "<@711679663746842796> <@201399017161097216> if you guys think it could be helpful i can create 3 public repos with examples for typescript + ollama (with that specific conf for some models), one for wasm and another one for rust.  it is always good to have more examples but the downside is that it could make people try to use wasm or rust and it is not really intended to be the use case right now.",
        "timestamp": "2024-09-18 02:22:16.735000+00:00",
        "id": 1285787952810692678,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": ".aaronv",
        "content": "gotcha, niice",
        "timestamp": "2024-09-17 17:32:19.289000+00:00",
        "id": 1285654584655745106,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "it didn't work on my mac so i switched to linux",
        "timestamp": "2024-09-17 16:42:09.886000+00:00",
        "id": 1285641962304569477,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "i modified the wasm code, so i had to build it myself",
        "timestamp": "2024-09-17 16:41:55.222000+00:00",
        "id": 1285641900799561839,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": ".aaronv",
        "content": "btw did you build the wasm lib yourself or did you grab the version published from npm?",
        "timestamp": "2024-09-17 15:59:52.440000+00:00",
        "id": 1285631319484797032,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "yeah no prob! i've been warned",
        "timestamp": "2024-09-17 05:05:34.620000+00:00",
        "id": 1285466660253204522,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "joatmon.pockets",
        "content": "fair warning: we have no stability guarantees on the wasm interface right now, and we’ll definitely look into providing them but right now it’s very much “at your own risk”",
        "timestamp": "2024-09-17 05:04:46.084000+00:00",
        "id": 1285466456678600705,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "joatmon.pockets",
        "content": "haha, amazing!",
        "timestamp": "2024-09-17 04:59:57.913000+00:00",
        "id": 1285465248001818646,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "",
        "timestamp": "2024-09-17 04:56:34.022000+00:00",
        "id": 1285464392820985886,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "made it work!",
        "timestamp": "2024-09-17 04:56:23.522000+00:00",
        "id": 1285464348780789792,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "nicarq",
        "content": "it's so much easier to have something crossplatform that can take dynamic (as in on-demand baml schemas) using wasm",
        "timestamp": "2024-09-17 03:55:25.814000+00:00",
        "id": 1285449007241367626,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "joatmon.pockets",
        "content": "Hmm- can you elaborate on your use case?\n\nIt’s entirely doable; the challenge for us is primarily maintaining a stable API contract there and providing a good developer experience. There’s a lot of little details that get annoying and tricky to reason about in the wasm interface today.",
        "timestamp": "2024-09-17 03:54:07.993000+00:00",
        "id": 1285448680836698133,
        "parent_id": null,
        "thread_id": 1285447761034084402
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-17 03:54:07.442000+00:00",
        "id": 1285448678525501470,
        "parent_id": 1285447761034084402,
        "thread_id": 1285447761034084402
    },
    {
        "author": "faizansattar",
        "content": "Hey team! Do you have any recommendations on how to convert a langgraph flow to BAML?",
        "timestamp": "2024-09-16 23:56:52.717000+00:00",
        "id": 1285388973761626122,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "faizansattar",
        "content": "<@99252724855496704> I got something working as well. Will share over DM shortly.",
        "timestamp": "2024-09-17 14:50:14.988000+00:00",
        "id": 1285613797981290576,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "hellovai",
        "content": "ok sorry I passed out last night. I see why this is tricky! But i've got a somewhat working thing.\n\nWhat I'm working on is putting a full example togehter so you can see it",
        "timestamp": "2024-09-17 13:59:31.248000+00:00",
        "id": 1285601031610175540,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "hellovai",
        "content": "ok back! so finally take a look athe code and I'll send you some scripts that do this.",
        "timestamp": "2024-09-17 06:32:28.354000+00:00",
        "id": 1285488528238841908,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "faizansattar",
        "content": "<@99252724855496704> Its a bit unclear to me how best to structure the baml dsl for the Agent. I'll wait to hear back from you pausing for now.",
        "timestamp": "2024-09-17 03:47:29.323000+00:00",
        "id": 1285447008693391372,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "faizansattar",
        "content": "Thinking of just using something like this: https://til.simonwillison.net/llms/python-react-pattern",
        "timestamp": "2024-09-17 01:23:47.321000+00:00",
        "id": 1285410845395914824,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "faizansattar",
        "content": "just trying to figure out the best alternativ ReAct framework",
        "timestamp": "2024-09-17 01:21:02.491000+00:00",
        "id": 1285410154048913429,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "faizansattar",
        "content": "cool - I might also take a stab at it, will keep you posted if I end up coming up with a solution",
        "timestamp": "2024-09-17 01:20:47.382000+00:00",
        "id": 1285410090677178469,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "hellovai",
        "content": "perfect, I'll go ahead and take a look tonight (at an event tonight)",
        "timestamp": "2024-09-17 00:13:59.139000+00:00",
        "id": 1285393278887399425,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "faizansattar",
        "content": "Here you go: https://gist.github.com/fsattar/17dc476650636e7aff7de73a46587cd0\n\nScrubbed away some of our internal stuff but this is roughly what we are trying to do. Its a ReAct agent.\n\nWe want to migrate to BAML so we can stream back a structured UI response. I can add that after you convert it as another tool call.",
        "timestamp": "2024-09-17 00:07:21.451000+00:00",
        "id": 1285391610863030373,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "hellovai",
        "content": "I'll also then make some docs about this 🤣",
        "timestamp": "2024-09-16 23:57:50.390000+00:00",
        "id": 1285389215659593763,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "hellovai",
        "content": "can you post some sample code? I can convert it for you! 🙂",
        "timestamp": "2024-09-16 23:57:24.820000+00:00",
        "id": 1285389108411236424,
        "parent_id": null,
        "thread_id": 1285388973761626122
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-16 23:57:24.498000+00:00",
        "id": 1285389107060670525,
        "parent_id": 1285388973761626122,
        "thread_id": 1285388973761626122
    },
    {
        "author": "demontrius",
        "content": "Hi guys.... Been reading the docs and still not sure how to use BAML on my own. Is there some starter video? Tried following the doc but not sure I am getting the sequence",
        "timestamp": "2024-09-16 21:56:18.538000+00:00",
        "id": 1285358631415447595,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "demontrius",
        "content": "Thanks.... Will get on this in a bit",
        "timestamp": "2024-09-16 23:32:20.127000+00:00",
        "id": 1285382797271367711,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "just make sure you do: `pip install baml-py`",
        "timestamp": "2024-09-16 22:59:05.302000+00:00",
        "id": 1285374430368895106,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "I alos linked the getting started guide FYI",
        "timestamp": "2024-09-16 22:56:48.354000+00:00",
        "id": 1285373855967346800,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "hope its good!",
        "timestamp": "2024-09-16 22:56:38.861000+00:00",
        "id": 1285373816150687804,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "https://www.youtube.com/watch?v=MITj2ukpB-s",
        "timestamp": "2024-09-16 22:56:34.484000+00:00",
        "id": 1285373797792481415,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "demontrius",
        "content": "Much appreciated",
        "timestamp": "2024-09-16 21:58:15.777000+00:00",
        "id": 1285359123151720529,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "awesome, let me record a quick python video and send it to you!",
        "timestamp": "2024-09-16 21:57:56.934000+00:00",
        "id": 1285359044118446163,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "demontrius",
        "content": "Python",
        "timestamp": "2024-09-16 21:57:41.518000+00:00",
        "id": 1285358979459059813,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "Hey <@865664087345725508> appreciate that note. We should make a video for this.\n\nBut what language are you using?",
        "timestamp": "2024-09-16 21:57:06.032000+00:00",
        "id": 1285358830619988018,
        "parent_id": null,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-16 21:57:05.576000+00:00",
        "id": 1285358828707119236,
        "parent_id": 1285358631415447595,
        "thread_id": 1285358631415447595
    },
    {
        "author": "hellovai",
        "content": "Hi guys, I love the project, it's really",
        "timestamp": "2024-09-16 13:36:32.192000+00:00",
        "id": 1285232859564740663,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "andrewcka",
        "content": "Hi guys, I love the project, it's really awesome. I havea question, is therea way to access the raw response from the model?",
        "timestamp": "2024-09-16 08:02:39.429000+00:00",
        "id": 1285148836066754634,
        "parent_id": null,
        "thread_id": 1285148836066754634
    },
    {
        "author": ".aaronv",
        "content": "Thanks for the input! We are planning our roadmap very soon so this feedback is super useful.",
        "timestamp": "2024-09-16 22:06:59.464000+00:00",
        "id": 1285361319653933176,
        "parent_id": null,
        "thread_id": 1285148836066754634
    },
    {
        "author": "andrewcka",
        "content": "(With instructor you can get this overwriting the class, but since Baml is compiled in rust and works really different, is not doable as my understanding)",
        "timestamp": "2024-09-16 21:07:27.316000+00:00",
        "id": 1285346336979554326,
        "parent_id": null,
        "thread_id": 1285148836066754634
    },
    {
        "author": "andrewcka",
        "content": "That's a very good question. My principal use case for this request is fine-tuning. I'm finding BAML really amazing for structuring output and doing prompt engineering that is portable, fast, iterable, and easy to test. I'm trying to fine-tune using the thinking process of the model and its reflection. For this, I would need both the raw output and the structured output.\n\nRegarding your question, I wasn't thinking that deeply, but for example, with Instructor you get the final output (after revalidation and retrying) and also the sum of the total tokens that were expended between iterations. I guess this would fulfill the previous request, and also my purposes since I want the final output.",
        "timestamp": "2024-09-16 21:03:10.013000+00:00",
        "id": 1285345257772290152,
        "parent_id": null,
        "thread_id": 1285148836066754634
    },
    {
        "author": "hellovai",
        "content": "e.g. if we had to try 3 different models to get the response, would you prefer that you only get the final HTTP resposne? Or all 3?",
        "timestamp": "2024-09-16 13:37:11.912000+00:00",
        "id": 1285233026162491505,
        "parent_id": null,
        "thread_id": 1285148836066754634
    },
    {
        "author": "hellovai",
        "content": "Thanks <@619130668567494656> , this is similar to the request we've had about tokens and we're still designing the right interface here. Out of curiosity, whats your use case of this data?\n\nThe reason i ask is that w/ BAML its a bit tricky to just give the raw response due to our offerings of provider stratigies like: `retry_policy` and `fallback` .",
        "timestamp": "2024-09-16 13:36:34.688000+00:00",
        "id": 1285232870033457203,
        "parent_id": null,
        "thread_id": 1285148836066754634
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-16 13:36:31.997000+00:00",
        "id": 1285232858746847233,
        "parent_id": 1285148836066754634,
        "thread_id": 1285148836066754634
    },
    {
        "author": "charizard_98",
        "content": "I'm at a loss here:\n\nIn the playground, I get the intended results under lookup_list.\n\nBut when I call the same function with the same user prompt from my application, I don't get both the items in the list\n\nDo you guys know if there is a way to keep it consistent?",
        "timestamp": "2024-09-14 15:19:42.176000+00:00",
        "id": 1284534046508191894,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "(i can do a smaller write up about why in a bit)",
        "timestamp": "2024-09-24 03:34:40.099000+00:00",
        "id": 1287980497393287229,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "ah got it. I think its likely the prompt sadly. models are pretty inconsistent sadly, even on temperature 0 😦",
        "timestamp": "2024-09-24 03:34:27.003000+00:00",
        "id": 1287980442464555040,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "I don't think there is any problem with the parser. It must be the LLM or my system prompt. I was just confused why I'd get different answers when I call the BAML function from go vs the playground.",
        "timestamp": "2024-09-23 23:41:42.427000+00:00",
        "id": 1287921870787842099,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "I tried switching to 4o from 4o-mini and 4o did much better I think. By parser do you mean when I call from the function from go?",
        "timestamp": "2024-09-23 23:39:12.981000+00:00",
        "id": 1287921243965751377,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "<@410093421420871680> did you find out if this was just happening in the playground or the parser as well?",
        "timestamp": "2024-09-23 23:05:19.382000+00:00",
        "id": 1287912714433466460,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "Hmm but the python code has or doesn’t have this issue?",
        "timestamp": "2024-09-21 05:38:41.091000+00:00",
        "id": 1286924543578869771,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "This is from the playground:",
        "timestamp": "2024-09-20 23:50:02.537000+00:00",
        "id": 1286836804804415563,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "",
        "timestamp": "2024-09-20 23:45:41.564000+00:00",
        "id": 1286835710204313713,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "I don't think the parser is messing up. I am able to see the LLM REPLY and Parsed Response in the logs now.",
        "timestamp": "2024-09-20 23:45:17.556000+00:00",
        "id": 1286835609507336194,
        "parent_id": 1286565409658638337,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Sure! Not sure if you are available in the weekend. If not, we can meet anytime on Monday.",
        "timestamp": "2024-09-20 23:18:11.120000+00:00",
        "id": 1286828787740311573,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "just shoot me over a calendar invite for a time that works for you! vbv@boundaryml.com and I'll be glad to help debug this",
        "timestamp": "2024-09-20 06:22:19.409000+00:00",
        "id": 1286573137734926361,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "I can hop on a quick call and check it out with you if that would be easier",
        "timestamp": "2024-09-20 06:14:40.580000+00:00",
        "id": 1286571213266747485,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "then i think it'll be a bit easier to debug",
        "timestamp": "2024-09-20 06:10:07.406000+00:00",
        "id": 1286570067491819616,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "you can sign up over at: app.boundaryml.com",
        "timestamp": "2024-09-20 06:09:59.573000+00:00",
        "id": 1286570034638098443,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "have you tried our observability dashbaord",
        "timestamp": "2024-09-20 06:09:39.991000+00:00",
        "id": 1286569952505102397,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "i would just look to see if the RAW Response is different than the Parsed Response",
        "timestamp": "2024-09-20 06:05:41.377000+00:00",
        "id": 1286568951685582862,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Is there anything specfic you are looking for in the log?",
        "timestamp": "2024-09-20 06:03:57.258000+00:00",
        "id": 1286568514978840658,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "then run `baml-cli dev",
        "timestamp": "2024-09-20 05:52:05.226000+00:00",
        "id": 1286565528500047872,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "export BAML_LOG=info",
        "timestamp": "2024-09-20 05:51:56.723000+00:00",
        "id": 1286565492835745837,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "I don't think so.",
        "timestamp": "2024-09-20 05:51:45.659000+00:00",
        "id": 1286565446430097420,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "i'm wondering if its the LLM messing up or our parser",
        "timestamp": "2024-09-20 05:51:36.892000+00:00",
        "id": 1286565409658638337,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "do you have BAML_LOG enabled?",
        "timestamp": "2024-09-20 05:51:24.232000+00:00",
        "id": 1286565356558876672,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "This is weird because I changed chairs to Chairs and it is giving me the same results in go and the playground. For some customer names, it gives the same result and for some it changes. Do you know if there is any way to control this?",
        "timestamp": "2024-09-20 05:50:36.857000+00:00",
        "id": 1286565157853466668,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "My VSCode runtime is 0.55.3 as well",
        "timestamp": "2024-09-20 05:39:20.985000+00:00",
        "id": 1286562323040833588,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "",
        "timestamp": "2024-09-20 05:37:34.030000+00:00",
        "id": 1286561874439180318,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "oh in go, its the same thing: can you check what version of `baml-cli` you have?",
        "timestamp": "2024-09-20 05:29:38.412000+00:00",
        "id": 1286559879552827462,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "I'm using go. What do I do then?",
        "timestamp": "2024-09-20 05:28:54.433000+00:00",
        "id": 1286559695091269675,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "in python it'll be `baml-cli --version`",
        "timestamp": "2024-09-20 05:12:29.129000+00:00",
        "id": 1286555562426765313,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-20 05:12:11.620000+00:00",
        "id": 1286555488988696687,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "odd, it works ok for me. Things to try: can you check what version of BAML you have in python vs in the playground",
        "timestamp": "2024-09-20 05:12:06.108000+00:00",
        "id": 1286555465869955105,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Just for more context. \nIn this prompt, \n\nRecord a sale of 10 chairs to Jordan LLC on 10th this month, paid in cash\n\nwhen I change Jordan LLC to Hari, I'm getting the same output in both BAML playground and go. That is why I'm really confused.",
        "timestamp": "2024-09-20 04:28:35.592000+00:00",
        "id": 1286544516571988008,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "https://www.promptfiddle.com/MDA-example---Hari-DbHWI",
        "timestamp": "2024-09-20 04:19:37.858000+00:00",
        "id": 1286542261152383017,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Sure! Thanks.",
        "timestamp": "2024-09-20 04:14:54.287000+00:00",
        "id": 1286541071769272342,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "(There’s a share link)",
        "timestamp": "2024-09-20 04:14:38.220000+00:00",
        "id": 1286541004379263037,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "I’m out at dinner! Let me share some ideas when I’m back. Can you share with me a min repro on prompt fiddle?",
        "timestamp": "2024-09-20 04:14:31.257000+00:00",
        "id": 1286540975174455296,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Hey <@99252724855496704> . I'm still getting the same thing after changing the temperature. Do you have any other ideas why this might happen?",
        "timestamp": "2024-09-20 04:13:42.063000+00:00",
        "id": 1286540768839995473,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "then you'd do the following:\n\n```\nfunction DoSomething(...) -> SomeType {\n   client Foo\n   prompt #\"...\"#\n}\n```",
        "timestamp": "2024-09-14 15:26:45.949000+00:00",
        "id": 1284535823940980767,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "got it! Yep, this is our more explicit client specification",
        "timestamp": "2024-09-14 15:26:20.310000+00:00",
        "id": 1284535716403216406,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Ah okay. I had not used that syntax before. I was just using `client \"openai/gpt-4o\"` till now. Thanks!",
        "timestamp": "2024-09-14 15:25:38.569000+00:00",
        "id": 1284535541328773130,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "```\nclient<llm> Foo {\n  provider openai\n  options {\n    temperature 0.0\n  }\n}\n```",
        "timestamp": "2024-09-14 15:23:48.924000+00:00",
        "id": 1284535081444048946,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "Do you know how I can set the temperature in BAML? I wasn't able to find it in the docs.",
        "timestamp": "2024-09-14 15:22:37.550000+00:00",
        "id": 1284534782079930370,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "we should document that 🙂",
        "timestamp": "2024-09-14 15:21:50.403000+00:00",
        "id": 1284534584331210763,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "charizard_98",
        "content": "That was going to be my next question 😅",
        "timestamp": "2024-09-14 15:21:40.773000+00:00",
        "id": 1284534543940063326,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "that is likely impacting this",
        "timestamp": "2024-09-14 15:21:01.285000+00:00",
        "id": 1284534378315386922,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "can you check what the temperature is? openai sets it to 1 by default, so you'll need to explicitly set it to 0.",
        "timestamp": "2024-09-14 15:20:51.084000+00:00",
        "id": 1284534335529160800,
        "parent_id": null,
        "thread_id": 1284534046508191894
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-14 15:20:50.819000+00:00",
        "id": 1284534334417539142,
        "parent_id": 1284534046508191894,
        "thread_id": 1284534046508191894
    },
    {
        "author": "gabriel_syme",
        "content": "We discussed at some point about conditional logic inside the schemas, is that close?",
        "timestamp": "2024-09-12 23:53:01.550000+00:00",
        "id": 1283938452714553496,
        "parent_id": null,
        "thread_id": 1283938452714553496
    },
    {
        "author": "gabriel_syme",
        "content": "No worries! I just did an in context thing and hope for the best 😀",
        "timestamp": "2024-09-13 00:24:54.874000+00:00",
        "id": 1283946477776932970,
        "parent_id": null,
        "thread_id": 1283938452714553496
    },
    {
        "author": "joatmon.pockets",
        "content": "Happy to help figure out a workaround for you if you need something today though - perhaps [dynamic types](https://docs.boundaryml.com/docs/calling-baml/dynamic-types)?",
        "timestamp": "2024-09-12 23:58:38.296000+00:00",
        "id": 1283939865129517088,
        "parent_id": null,
        "thread_id": 1283938452714553496
    },
    {
        "author": "joatmon.pockets",
        "content": "Still a few weeks out, sadly - we want it as much as you do though!",
        "timestamp": "2024-09-12 23:54:40.045000+00:00",
        "id": 1283938865832398878,
        "parent_id": null,
        "thread_id": 1283938452714553496
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-12 23:54:39.801000+00:00",
        "id": 1283938864808857702,
        "parent_id": 1283938452714553496,
        "thread_id": 1283938452714553496
    },
    {
        "author": "joatmon.pockets",
        "content": "optional list",
        "timestamp": "2024-09-12 04:52:43.506000+00:00",
        "id": 1283651486638804996,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Any way to assign a list of types as optional?",
        "timestamp": "2024-09-12 04:25:03.665000+00:00",
        "id": 1283644524761055294,
        "parent_id": null,
        "thread_id": 1283644524761055294
    },
    {
        "author": "joatmon.pockets",
        "content": "Filed https://github.com/BoundaryML/baml/issues/948, we'll update you if/when we get around to this!",
        "timestamp": "2024-09-12 16:59:17.580000+00:00",
        "id": 1283834333437886566,
        "parent_id": null,
        "thread_id": 1283644524761055294
    },
    {
        "author": "joatmon.pockets",
        "content": "you're not the only one who's asked for this - I expect we'll add this soon!",
        "timestamp": "2024-09-12 16:54:51.971000+00:00",
        "id": 1283833219392737371,
        "parent_id": null,
        "thread_id": 1283644524761055294
    },
    {
        "author": "gabriel_syme",
        "content": "Also, not sure if another pattern is better, like do idk making a list of optional variables smh?",
        "timestamp": "2024-09-12 08:39:42.148000+00:00",
        "id": 1283708607363416118,
        "parent_id": null,
        "thread_id": 1283644524761055294
    },
    {
        "author": "gabriel_syme",
        "content": "Sounds good, would be nice if possible",
        "timestamp": "2024-09-12 08:39:17.331000+00:00",
        "id": 1283708503273246838,
        "parent_id": null,
        "thread_id": 1283644524761055294
    },
    {
        "author": "joatmon.pockets",
        "content": "not currently, but we can make that happen!\n\nthe reason is that in the parser’s current error-correcting behavior, an unset parameter is coerced into an empty list",
        "timestamp": "2024-09-12 04:52:44.193000+00:00",
        "id": 1283651489520287746,
        "parent_id": null,
        "thread_id": 1283644524761055294
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-12 04:52:43.386000+00:00",
        "id": 1283651486135357440,
        "parent_id": 1283644524761055294,
        "thread_id": 1283644524761055294
    },
    {
        "author": "airhorns",
        "content": "how does BAML manage LLM provider rate limits if at all?",
        "timestamp": "2024-09-11 13:50:24.304000+00:00",
        "id": 1283424410367299645,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "My bad, yes you would need the full prompt. We have a festure we will release to give you all the request metadata",
        "timestamp": "2024-09-11 19:50:51.733000+00:00",
        "id": 1283515122379391086,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": "airhorns",
        "content": "how would one get the messages that have already been exchanged in order to do that?",
        "timestamp": "2024-09-11 19:49:59.704000+00:00",
        "id": 1283514904153686077,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "For now the workaround is to just write a \"FixMyPrompt(..)\" baml function that can do this for you. Once we implement this more natively we can help do this with a one-liner or some configuration.",
        "timestamp": "2024-09-11 18:57:35.994000+00:00",
        "id": 1283501718478393436,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "we don't retry with a modified prompt at the moment -- we just retry again with the same request\n\nWe are tracking this since I believe that we can implement this as well, and I agree it can help fix more complex mistakes that perhaps keep popping up even with normal retries. https://github.com/BoundaryML/baml/issues/840",
        "timestamp": "2024-09-11 18:50:40.620000+00:00",
        "id": 1283499976273563791,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": "airhorns",
        "content": "are the retries just \"run the request again\" or is it \"produce a message that explains the schema errors, add it to the list of messages, and run that\"? i've found that the latter works a lot better than the former",
        "timestamp": "2024-09-11 18:15:45.084000+00:00",
        "id": 1283491186958667926,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": "airhorns",
        "content": "yeah, thats what i mean, if it fails to parse. i was under the impression that thats why BAML makes its own HTTP requests -- so that it can implement niceities like retries and whatnot",
        "timestamp": "2024-09-11 18:15:05.842000+00:00",
        "id": 1283491022365528074,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "What do you mean by “requests”? If the http request to the llm api succeeds and the response is valid we always return it. If the http request succeeds but it fails to parse we will retry according to the retry policy or the fallback config",
        "timestamp": "2024-09-11 14:10:55.295000+00:00",
        "id": 1283429573517643838,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": "airhorns",
        "content": "agreed with that for sure -- i more mean that if BAML is doing its own retries internally to manage schema violations, and the first couple requests work but the third is rate limited, that will throw and the message chain is just lost right?",
        "timestamp": "2024-09-11 13:56:25.945000+00:00",
        "id": 1283425927199723603,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "We don’t yet at the moment. Is there a particular implementation you liked from another framework or library? \n\nThe issue is that for many users in serverless contexts or multi-container contexts, per-process rate limiting tends to not be as useful.",
        "timestamp": "2024-09-11 13:53:33.510000+00:00",
        "id": 1283425203954651158,
        "parent_id": null,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-11 13:53:33.197000+00:00",
        "id": 1283425202641834094,
        "parent_id": 1283424410367299645,
        "thread_id": 1283424410367299645
    },
    {
        "author": ".aaronv",
        "content": "Baml raw request",
        "timestamp": "2024-09-11 03:26:41.269000+00:00",
        "id": 1283267446781775918,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "cat_ethos",
        "content": "is there a way to get the token usage info besides logging?",
        "timestamp": "2024-09-11 02:43:51.501000+00:00",
        "id": 1283256668393701419,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "hellovai",
        "content": "👍🏾",
        "timestamp": "2024-09-16 14:42:50.390000+00:00",
        "id": 1285249545336258561,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "analysis for now",
        "timestamp": "2024-09-16 14:42:40.134000+00:00",
        "id": 1285249502319611915,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "hellovai",
        "content": "got it! Is this for real-time decision making? Or more so for analysis btw?",
        "timestamp": "2024-09-16 14:42:01.217000+00:00",
        "id": 1285249339089883137,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "in my case, i want token usage of all the requests. because by looking at the total usage, i can better tweak the what providers and how the retry strategy should be",
        "timestamp": "2024-09-16 14:41:28.416000+00:00",
        "id": 1285249201512513546,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "hellovai",
        "content": "btw <@635507634342068228> question question re: token usage. BAML also offers provider strategies like retry_policy and fallback. In that scenario, how would you want token usage to be filled out? Would you want all of requests made and their token usage? Or would you want only the last successful request and its token usage?",
        "timestamp": "2024-09-16 13:50:12.474000+00:00",
        "id": 1285236300076552223,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "can't think of any now but sure i will share them whenever i have some ideas, so far the openapi server feature is really nice",
        "timestamp": "2024-09-11 03:40:48.287000+00:00",
        "id": 1283270999433019414,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": ".aaronv",
        "content": "Any other features or things youd like to see improved at the moment? Could be anything like the developer experience etc",
        "timestamp": "2024-09-11 03:36:40.632000+00:00",
        "id": 1283269960692404275,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "looking forward to your future roadmap",
        "timestamp": "2024-09-11 03:35:56.197000+00:00",
        "id": 1283269774318505985,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": ".aaronv",
        "content": "Ahh, smart 🙂",
        "timestamp": "2024-09-11 03:35:32.221000+00:00",
        "id": 1283269673755873312,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "yeah now i am getting the raw output and prompt and use the tiktoken to get the token counts",
        "timestamp": "2024-09-11 03:35:14.162000+00:00",
        "id": 1283269598011199510,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": ".aaronv",
        "content": "Gotcha, we are planning or roadmap soon so we will update you on this. Unfortunately that callback doesn’t yet expose the number of tokens though it d be easy to add them there",
        "timestamp": "2024-09-11 03:33:58.066000+00:00",
        "id": 1283269278841442349,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "would like a easier solution in the long term",
        "timestamp": "2024-09-11 03:32:09.730000+00:00",
        "id": 1283268824447062066,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "currently i think we can use the oneventlog callback",
        "timestamp": "2024-09-11 03:31:46.471000+00:00",
        "id": 1283268726891876365,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": "cat_ethos",
        "content": "we are running workloads from clients and would like to make sure fair usage, so need to track how many tokens each clients are using",
        "timestamp": "2024-09-11 03:31:16.409000+00:00",
        "id": 1283268600802578445,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": ".aaronv",
        "content": "Hi there, mind sharing more about your usecase? We’d love to learn what you use this metadata for. Would you say this is a blocker for using BAML longterm or just a desired feature?",
        "timestamp": "2024-09-11 03:26:42.007000+00:00",
        "id": 1283267449877434479,
        "parent_id": null,
        "thread_id": 1283256668393701419
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-11 03:26:41.269000+00:00",
        "id": 1283267446781775913,
        "parent_id": 1283256668393701419,
        "thread_id": 1283256668393701419
    },
    {
        "author": "philosopherstone",
        "content": "hey guys, just a heads up - baml-cli is broken on 0.55.0 version release, but 0.54 works though!",
        "timestamp": "2024-09-10 15:57:42.121000+00:00",
        "id": 1283094057828089907,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "philosopherstone",
        "content": "It works great now! good work :)",
        "timestamp": "2024-09-10 18:19:55.759000+00:00",
        "id": 1283129850500087890,
        "parent_id": 1283129003296817224,
        "thread_id": 1283094057828089907
    },
    {
        "author": "joatmon.pockets",
        "content": "Took longer than anticipated - ran into a hiccup along the way - but fix is out now. Let me know if you still have any issues!",
        "timestamp": "2024-09-10 18:16:33.770000+00:00",
        "id": 1283129003296817224,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "joatmon.pockets",
        "content": "Fix is in and will be out within the hour- thanks for letting us know!",
        "timestamp": "2024-09-10 16:43:19.738000+00:00",
        "id": 1283105540226154606,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "philosopherstone",
        "content": "happy to help :)",
        "timestamp": "2024-09-10 16:15:55.121000+00:00",
        "id": 1283098642202230817,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "joatmon.pockets",
        "content": "ah i see it, thanks",
        "timestamp": "2024-09-10 16:15:23.661000+00:00",
        "id": 1283098510249427100,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "philosopherstone",
        "content": "the exact error that I get",
        "timestamp": "2024-09-10 16:02:27.018000+00:00",
        "id": 1283095252772720701,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "philosopherstone",
        "content": "```\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"C:\\Users\\<my_name>\\.virtualenvs\\grammarcheck-SxcXB2v8\\Scripts\\baml-cli.exe\\__main__.py\", line 4, in <module>\n```",
        "timestamp": "2024-09-10 16:02:20.380000+00:00",
        "id": 1283095224930795594,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "philosopherstone",
        "content": "its a windows pc with intel i5 x86",
        "timestamp": "2024-09-10 16:01:37.081000+00:00",
        "id": 1283095043321761813,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "philosopherstone",
        "content": "Thanks for the quick response! I am using python",
        "timestamp": "2024-09-10 16:00:39.495000+00:00",
        "id": 1283094801788440647,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "joatmon.pockets",
        "content": "Thanks for letting us know, will look asap. Are you using Python or NPM, and is it a macbook arm or something else?",
        "timestamp": "2024-09-10 16:00:05.662000+00:00",
        "id": 1283094659882680416,
        "parent_id": null,
        "thread_id": 1283094057828089907
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-10 16:00:05.271000+00:00",
        "id": 1283094658242838549,
        "parent_id": 1283094057828089907,
        "thread_id": 1283094057828089907
    },
    {
        "author": "joatmon.pockets",
        "content": "<@198831631602024450> re your question:\n\n> Heeey <@711679663746842796> I was playing-around/reading-code about serve function. I think it's pretty useful.\n> \n> As some feedback/questions, why serve needs the BAML files (throught the --from param) in place of being fully generic?\n> Let's say, on every network request to the BAML Rest service, besides:\n> - function (in the route param)\n> - args (in the body)\n> \n> The client (IE, typescript client) could send the BAML files to the endpoint so we could:\n> - Creates a BAML Runtime dynamically\n> - Execute the param parsing pipeline\n> - Execute the function\n> - Collect results\n> - Send response\n> \n> \n> I'm not sure about how inefficient could be stay instantiating a new BAML Runtime on every request but maybe it could be an improve over the first use case or even a completely different use case where we can share/scale the same instance of BAML Http Service throught different set of tools with no need to have them tied 🤔",
        "timestamp": "2024-09-09 19:00:58.370000+00:00",
        "id": 1282777791573786657,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": "hellovai",
        "content": "want to meet sometime this week and discuss in more details",
        "timestamp": "2024-09-09 19:24:43.873000+00:00",
        "id": 1282783770566725653,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": "hellovai",
        "content": "This sounds really interesting! Why don't we grab some time to chat about this later this week.\n\nwe've got a calendly up and running on our web site (book a time with a founder button at the bottom)",
        "timestamp": "2024-09-09 19:24:28.849000+00:00",
        "id": 1282783707551502358,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": "saiko9729",
        "content": "This is my current use case:\n\n- Our team and a community write \"Tools\" using typescript (think about them as functions)\n- Every tool is bundled as a single file\n- Tools are distributed and then executed by our app (multiplatform and client side)\n\nWe created a tool that uses baml but when we tried to bundle it as a single file we found that baml requires some .node files per platform that could be hard to bundle because it means that every tool that use baml will get some additional megabytes.\n\nThen we found the PR about Serve which is really cool because we can have baml as a side-executor, distributed for every platform and working as a HTTP API but is not exactly what we need because the \"baml executor\" is tied to the tools it can run...",
        "timestamp": "2024-09-09 19:15:11.874000+00:00",
        "id": 1282781371428896849,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": ".aaronv",
        "content": "if you want a small example on how to do this clearly in typescript or python, let me know, but this stuff is very different from the `baml serve` feature, which creates a custom server with REST endpoints for each of your baml functions",
        "timestamp": "2024-09-09 19:06:07.800000+00:00",
        "id": 1282779089417277530,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": ".aaronv",
        "content": "Would your users write BAML schemas for you? We've done something like this on TypeScript and Python. It is doable to create a baml runtime for languages where we have SDKs already (Python, Typescript) and execute things that way. The generated code is just a wrapper around that",
        "timestamp": "2024-09-09 19:04:34.131000+00:00",
        "id": 1282778696541012054,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": "joatmon.pockets",
        "content": "Do you have a use case in mind that's driving this request?",
        "timestamp": "2024-09-09 19:04:13.159000+00:00",
        "id": 1282778608577941625,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": "joatmon.pockets",
        "content": "It's an interesting idea!\n\nMy initial reaction is that while we can definitely support this in the future, I don't know if I would want this to be the default: it would impose a lot of burden on the user for packaging their `baml_src/` into their own applications, though, which can get pretty frustrating if, say, you're in an AWS Lambda",
        "timestamp": "2024-09-09 19:03:11.255000+00:00",
        "id": 1282778348933877791,
        "parent_id": null,
        "thread_id": 1282777791573786657
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-09 19:03:10.804000+00:00",
        "id": 1282778347041984512,
        "parent_id": 1282777791573786657,
        "thread_id": 1282777791573786657
    },
    {
        "author": "gabev2037",
        "content": "Follow up questions:\n1. Is https://app.boundaryml.com still the correct dashboard URL? \n2. Running a test case in my BAML Playground is stalling, not sure how to see the what's going on",
        "timestamp": "2024-09-09 14:46:01.290000+00:00",
        "id": 1282713630969958441,
        "parent_id": null,
        "thread_id": 1282713630969958441
    },
    {
        "author": "gabev2037",
        "content": "weird... it was recursively loading for me but now it's good",
        "timestamp": "2024-09-09 14:49:32.621000+00:00",
        "id": 1282714517356150886,
        "parent_id": null,
        "thread_id": 1282713630969958441
    },
    {
        "author": "hellovai",
        "content": "1. yes! is there any issue\n2. click on all tests, then back to test results",
        "timestamp": "2024-09-09 14:48:53.465000+00:00",
        "id": 1282714353123987568,
        "parent_id": null,
        "thread_id": 1282713630969958441
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-09 14:48:52.931000+00:00",
        "id": 1282714350884360254,
        "parent_id": 1282713630969958441,
        "thread_id": 1282713630969958441
    },
    {
        "author": "gabev2037",
        "content": "Hey guys, quick question:\nHow can I modify the azure-openai provider to use the government cloud? All that's needed is to change the endpoint from \n`<base>.openai.azure.com/` to `<base>.openai.azure.us/`",
        "timestamp": "2024-09-09 14:41:44.268000+00:00",
        "id": 1282712552941289543,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "gabev2037",
        "content": "yes!",
        "timestamp": "2024-09-09 16:11:52.999000+00:00",
        "id": 1282735238803619840,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "hellovai",
        "content": "Did you find it out?",
        "timestamp": "2024-09-09 16:05:43.116000+00:00",
        "id": 1282733687401877597,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "gabev2037",
        "content": "Based on the azure playground code, i can see the base URL appears to be the correct one, can share over DM if that's easier",
        "timestamp": "2024-09-09 15:01:18.132000+00:00",
        "id": 1282717476484022303,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "gabev2037",
        "content": "Where would I find the deployment ID? I can see the deployment name though not sure if that's the same in Azure",
        "timestamp": "2024-09-09 15:00:05.810000+00:00",
        "id": 1282717173143441559,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "hellovai",
        "content": "sadly not yet, we're working on that still!",
        "timestamp": "2024-09-09 14:51:58.035000+00:00",
        "id": 1282715127266676757,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "gabev2037",
        "content": "actually i can just make a net new env for base url",
        "timestamp": "2024-09-09 14:50:30.442000+00:00",
        "id": 1282714759875002411,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "gabev2037",
        "content": "Is there a syntax for reading env variables in that base URL?",
        "timestamp": "2024-09-09 14:50:20.803000+00:00",
        "id": 1282714719446368336,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "hellovai",
        "content": "https://docs.boundaryml.com/docs/snippets/clients/providers/azure\n\nI thinik you can just set base_url",
        "timestamp": "2024-09-09 14:48:09.347000+00:00",
        "id": 1282714168079679572,
        "parent_id": null,
        "thread_id": 1282712552941289543
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-09 14:48:08.905000+00:00",
        "id": 1282714166225801311,
        "parent_id": 1282712552941289543,
        "thread_id": 1282712552941289543
    },
    {
        "author": "gabriel_syme",
        "content": "Also, the playground is now gone. Wonder if I did smth to my vscode setup smh",
        "timestamp": "2024-09-09 02:08:40.868000+00:00",
        "id": 1282523040026198089,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "🤦‍♂️",
        "timestamp": "2024-09-09 02:50:37.836000+00:00",
        "id": 1282533596955017237,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Not tabs",
        "timestamp": "2024-09-09 02:50:31.307000+00:00",
        "id": 1282533569570410514,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "I think we require spaces",
        "timestamp": "2024-09-09 02:50:27.373000+00:00",
        "id": 1282533553070149723,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "That will do it",
        "timestamp": "2024-09-09 02:50:18.606000+00:00",
        "id": 1282533516298686534,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Yes",
        "timestamp": "2024-09-09 02:50:15.248000+00:00",
        "id": 1282533502214213662,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Oh damn",
        "timestamp": "2024-09-09 02:50:13.570000+00:00",
        "id": 1282533495176167466,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "It actually works though even with spaces in other files I think",
        "timestamp": "2024-09-09 02:44:43.093000+00:00",
        "id": 1282532109055168584,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "Just noticed code is smh now structured with spaces vs tabs in one file. Wonder if that could do it?",
        "timestamp": "2024-09-09 02:44:23.349000+00:00",
        "id": 1282532026242826292,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "Sure one sec",
        "timestamp": "2024-09-09 02:13:30.568000+00:00",
        "id": 1282524255115808769,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Mind hopping on office hours?",
        "timestamp": "2024-09-09 02:13:20.838000+00:00",
        "id": 1282524214305362014,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "I guess 🙂 there is no other trace",
        "timestamp": "2024-09-09 02:13:17.340000+00:00",
        "id": 1282524199633682482,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "baml_py.BamlError",
        "timestamp": "2024-09-09 02:13:09.772000+00:00",
        "id": 1282524167891189760,
        "parent_id": 1282524006679056519,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Yea. There’s likely some syntax error that is causing BAML not to compile",
        "timestamp": "2024-09-09 02:13:09.100000+00:00",
        "id": 1282524165072752752,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "Weirdly playground says no function available when next to functions",
        "timestamp": "2024-09-09 02:12:46.364000+00:00",
        "id": 1282524069710921819,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Oh! What’s the error it gives?",
        "timestamp": "2024-09-09 02:12:31.336000+00:00",
        "id": 1282524006679056519,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "Cli says error generating clients at invoke_runtjme_cli()",
        "timestamp": "2024-09-09 02:12:19.352000+00:00",
        "id": 1282523956414255205,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "No red files. These were working last time I was in this project",
        "timestamp": "2024-09-09 02:11:55.607000+00:00",
        "id": 1282523856820633653,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "That may help find any syntax errors easier",
        "timestamp": "2024-09-09 02:11:25.501000+00:00",
        "id": 1282523730547052574,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Alternatively, are you able to run baml-cli generate from terminal",
        "timestamp": "2024-09-09 02:11:06.861000+00:00",
        "id": 1282523652364963891,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Also are any baml files red in the side panel? There’s a problems tab you can see.",
        "timestamp": "2024-09-09 02:10:48.258000+00:00",
        "id": 1282523574338453506,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "Tried one, let me try again",
        "timestamp": "2024-09-09 02:10:02.770000+00:00",
        "id": 1282523383548084305,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "Hi gabriel, can you do a reload on vscode?",
        "timestamp": "2024-09-09 02:09:37.868000+00:00",
        "id": 1282523279101268030,
        "parent_id": null,
        "thread_id": 1282523040026198089
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-09 02:09:37.406000+00:00",
        "id": 1282523277163757568,
        "parent_id": 1282523040026198089,
        "thread_id": 1282523040026198089
    },
    {
        "author": "gabriel_syme",
        "content": "I literally added am empty line and saved",
        "timestamp": "2024-09-09 02:07:52.233000+00:00",
        "id": 1282522836036223037,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I somehow can't make a previous successful client compile",
        "timestamp": "2024-09-09 02:07:32.028000+00:00",
        "id": 1282522751290310812,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Any idea what sn empty error linting is?",
        "timestamp": "2024-09-09 02:07:19.474000+00:00",
        "id": 1282522698635022366,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "nazimgirach",
        "content": "",
        "timestamp": "2024-09-07 21:39:38.622000+00:00",
        "id": 1282092946673631293,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "nazimgirach",
        "content": "I can confirm it works with python. Looks like a VS code extension bug.",
        "timestamp": "2024-09-07 22:20:44.935000+00:00",
        "id": 1282103291139854429,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "joatmon.pockets",
        "content": "In the meantime, you can also set `BAML_LOG=info` while running your app to see what I screenshotted above",
        "timestamp": "2024-09-07 21:50:12.132000+00:00",
        "id": 1282095603806900398,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "joatmon.pockets",
        "content": "Filed https://github.com/BoundaryML/baml/issues/928 for us to go and fix this",
        "timestamp": "2024-09-07 21:49:33.831000+00:00",
        "id": 1282095443161124919,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "joatmon.pockets",
        "content": "We _do_ have a rendering bug in the playground here, and I totally get why this would be concerning!",
        "timestamp": "2024-09-07 21:48:15.036000+00:00",
        "id": 1282095112670941245,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "joatmon.pockets",
        "content": "OK, it looks like we do not:",
        "timestamp": "2024-09-07 21:47:56.344000+00:00",
        "id": 1282095034271010880,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "joatmon.pockets",
        "content": "Taking a look...",
        "timestamp": "2024-09-07 21:42:27.295000+00:00",
        "id": 1282093654139338763,
        "parent_id": null,
        "thread_id": 1282092946673631293
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-09-07 21:42:27.168000+00:00",
        "id": 1282093653606662257,
        "parent_id": 1282092946673631293,
        "thread_id": 1282092946673631293
    },
    {
        "author": "nazimgirach",
        "content": "Does the BAML parser remove \\n from the response?",
        "timestamp": "2024-09-07 21:37:07.479000+00:00",
        "id": 1282092312733683734,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": ".aaronv",
        "content": "Pydantic description",
        "timestamp": "2024-09-07 19:45:06.915000+00:00",
        "id": 1282064124645343297,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "Classes in test cases",
        "timestamp": "2024-09-07 19:05:55.058000+00:00",
        "id": 1282054260242251874,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "nazimgirach",
        "content": "I also noticed that the description I add to the BAML code doesn't carry over to the generated Python types in Pydantic.",
        "timestamp": "2024-09-07 18:57:16.005000+00:00",
        "id": 1282052083176181863,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": ".aaronv",
        "content": "(from within a BAML file)",
        "timestamp": "2024-09-07 19:49:26.904000+00:00",
        "id": 1282065215118114998,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": ".aaronv",
        "content": "have you clicked on this button -- \"open playground\" ?",
        "timestamp": "2024-09-07 19:48:55.142000+00:00",
        "id": 1282065081898893352,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": "nazimgirach",
        "content": "Got it, that gives me some peace of mind haha",
        "timestamp": "2024-09-07 19:48:26.369000+00:00",
        "id": 1282064961215926416,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": ".aaronv",
        "content": "All this information is sent to our parser + clients for you -- we don't use pydantic for this. The pydantic class is just used to store the final data",
        "timestamp": "2024-09-07 19:48:06.113000+00:00",
        "id": 1282064876256235572,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": ".aaronv",
        "content": "ah, so when you add descriptions we still add them to the prompt :). If you open up the Playground in VSCode, you'll see the full prompt with the descriptions inlined",
        "timestamp": "2024-09-07 19:47:39.079000+00:00",
        "id": 1282064762867552306,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": "nazimgirach",
        "content": "I noticed that using descriptions with Pydantic improves my results, so I added descriptions to most of the BAML classes I created. However, when I checked the Python types file, I realized that none of the description data was generated in Python.",
        "timestamp": "2024-09-07 19:47:08.697000+00:00",
        "id": 1282064635436208140,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": ".aaronv",
        "content": "Hey Nazim, do you mind explaining what you want to use the descriptions for to understand your usecase?",
        "timestamp": "2024-09-07 19:45:07.314000+00:00",
        "id": 1282064126318874665,
        "parent_id": null,
        "thread_id": 1282052083176181863
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-07 19:45:06.915000+00:00",
        "id": 1282064124645343292,
        "parent_id": 1282052083176181863,
        "thread_id": 1282052083176181863
    },
    {
        "author": "nazimgirach",
        "content": "Hey everyone, how do you add mock data to a class when passing it in a test?",
        "timestamp": "2024-09-07 18:26:32.946000+00:00",
        "id": 1282044352826314875,
        "parent_id": null,
        "thread_id": 1282044352826314875
    },
    {
        "author": "joatmon.pockets",
        "content": "Howdy! Here's one of the examples we have over [in our docs](https://docs.boundaryml.com/docs/snippets/test-cases):\n\n```baml\nclass Message {\n  user string\n  content string\n}\n\nfunction ClassifyMessage(messages: Messages[]) -> Category {\n...\n}\n\ntest Test1 {\n  functions [ClassifyMessage]\n  args {\n    messages [\n      {\n        user \"hey there\"\n        content #\"\n          You can also add a multi-line\n          string with the hashtags\n          Instead of ugly json with \\n\n        \"#\n      }\n    ]\n  }\n}\n```",
        "timestamp": "2024-09-07 20:30:55.105000+00:00",
        "id": 1282075651389657108,
        "parent_id": null,
        "thread_id": 1282044352826314875
    },
    {
        "author": "hellovai",
        "content": "<@711679663746842796> can you share an example",
        "timestamp": "2024-09-07 19:05:55.735000+00:00",
        "id": 1282054263081537678,
        "parent_id": null,
        "thread_id": 1282044352826314875
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-07 19:05:55.058000+00:00",
        "id": 1282054260242251869,
        "parent_id": 1282044352826314875,
        "thread_id": 1282044352826314875
    },
    {
        "author": "hellovai",
        "content": "is there a way to add a description of a",
        "timestamp": "2024-09-07 01:13:24.334000+00:00",
        "id": 1281784353743179899,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "is there a way to add a description of a field in another field:\n\ne.g.\nclass Node {\n  id string \n  type string\n  description string @description(\"if possible, provide a brief description of id\")\n}",
        "timestamp": "2024-09-06 23:31:07.254000+00:00",
        "id": 1281758612963987458,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "arindamkhaled4530",
        "content": "looking forward to it!",
        "timestamp": "2024-09-09 16:45:14.065000+00:00",
        "id": 1282743631882485831,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "hellovai",
        "content": "For now sadly copy and paste",
        "timestamp": "2024-09-07 19:07:16.335000+00:00",
        "id": 1282054601142435940,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "hellovai",
        "content": "Perfect. We’re working on variables. But that’s likely about 2 months away.",
        "timestamp": "2024-09-07 19:06:57.579000+00:00",
        "id": 1282054522474074132,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "arindamkhaled4530",
        "content": "that'd be quite useful",
        "timestamp": "2024-09-07 18:49:27.458000+00:00",
        "id": 1282050117947625604,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes, something like that",
        "timestamp": "2024-09-07 18:07:45.042000+00:00",
        "id": 1282039622053920768,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "hellovai",
        "content": "not yet, we almost will be able to once we release variables.\n\nyou want to do something like this right?\n\n```\nclass Node {\n  id string @description(\"if possible, provide a brief description of id\")\n  type string\n  description string @description(id.description)\n}\n```",
        "timestamp": "2024-09-07 01:13:25.191000+00:00",
        "id": 1281784357337698335,
        "parent_id": null,
        "thread_id": 1281758612963987458
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-07 01:13:24.216000+00:00",
        "id": 1281784353248120872,
        "parent_id": 1281758612963987458,
        "thread_id": 1281758612963987458
    },
    {
        "author": "sudhanshug",
        "content": "does baml support assertion/expectations in tests? I only see test case execution",
        "timestamp": "2024-09-06 21:36:28.016000+00:00",
        "id": 1281729759348523169,
        "parent_id": null,
        "thread_id": 1281729759348523169
    },
    {
        "author": "sudhanshug",
        "content": "alrighty",
        "timestamp": "2024-09-06 21:37:21.205000+00:00",
        "id": 1281729982439358498,
        "parent_id": null,
        "thread_id": 1281729759348523169
    },
    {
        "author": ".aaronv",
        "content": "Not yet! But we are working on it. Those will land in about 4 weeks",
        "timestamp": "2024-09-06 21:37:13.156000+00:00",
        "id": 1281729948679143490,
        "parent_id": null,
        "thread_id": 1281729759348523169
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-06 21:37:12.925000+00:00",
        "id": 1281729947710259330,
        "parent_id": 1281729759348523169,
        "thread_id": 1281729759348523169
    },
    {
        "author": "sudhanshug",
        "content": "can template strings be used across files? like can i have a `message.baml` and use it in `classifier.baml`?",
        "timestamp": "2024-09-05 22:42:44.694000+00:00",
        "id": 1281384050879107085,
        "parent_id": null,
        "thread_id": 1281384050879107085
    },
    {
        "author": "hellovai",
        "content": "yes!",
        "timestamp": "2024-09-05 22:45:18.977000+00:00",
        "id": 1281384697989169234,
        "parent_id": null,
        "thread_id": 1281384050879107085
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-05 22:45:18.626000+00:00",
        "id": 1281384696516968488,
        "parent_id": 1281384050879107085,
        "thread_id": 1281384050879107085
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Instantiating BAML classes in python code",
        "timestamp": "2024-09-05 21:39:28.890000+00:00",
        "id": 1281368130123337748,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": ".aaronv",
        "content": "likewise!",
        "timestamp": "2024-09-06 22:06:25.638000+00:00",
        "id": 1281737299121410058,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "got it - I'll tray thanks and have a great weekend.",
        "timestamp": "2024-09-06 22:06:19.875000+00:00",
        "id": 1281737274949894195,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": ".aaronv",
        "content": "to get around that you can try \"string[] | null\". In general we opted for making arrays non-optional since we'll fill in an empty array if we didnt find anything for that field",
        "timestamp": "2024-09-06 22:05:25.051000+00:00",
        "id": 1281737045001109555,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "<@201399017161097216> quick follow-up question:  I have been using the optional operator '?' on the BAML class fields, though I seem to be having trouble using it on fields that are arrays, e.g.,  preferred_seats string[]?  or preferred_seats string?[]",
        "timestamp": "2024-09-06 22:04:35.286000+00:00",
        "id": 1281736836271575175,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": ".aaronv",
        "content": "feel free to DM the class information, maybe I can try and reproduce -- does the exception happen everytime you instantiate?",
        "timestamp": "2024-09-05 21:43:10.076000+00:00",
        "id": 1281369057844789248,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": ".aaronv",
        "content": "where `Message` was a BAML class, and `Role` was a BAML enum",
        "timestamp": "2024-09-05 21:42:32.586000+00:00",
        "id": 1281368900600070197,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": ".aaronv",
        "content": "yes you should be able to initialize classes like this:",
        "timestamp": "2024-09-05 21:42:10.395000+00:00",
        "id": 1281368807524401214,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Should I be able to do this?",
        "timestamp": "2024-09-05 21:41:20.595000+00:00",
        "id": 1281368598648193034,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "I am trying to track it down - so far it is just an exception that gets trapped.",
        "timestamp": "2024-09-05 21:41:09.234000+00:00",
        "id": 1281368550996709419,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": ".aaronv",
        "content": "what kind of error do you get?",
        "timestamp": "2024-09-05 21:40:01.763000+00:00",
        "id": 1281368268002562099,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "<@201399017161097216> <@99252724855496704> Hey Guys, After getting some of the basics of BAML going in my project, I am pushing my usage a bit further.  Am I able to instantiate the classes I declare in my BAML files outside of the BAML functions (e.g. in my regualr python code)?  I figured out the importing (from baml_client.types import ClassName), but I am getting an exception when I say:\n\nmyClass = ClassName()",
        "timestamp": "2024-09-05 21:39:34.779000+00:00",
        "id": 1281368154823462973,
        "parent_id": null,
        "thread_id": 1281368130123337748
    },
    {
        "author": "ekp",
        "content": "Hi! I just started testing BAML for a new date time parsing feature. Thanks for the great tool!\n\nI’m wondering if there’s any difference in accuracy between streaming vs without (practical or even theoretical).\n1. Will the final parsed response of a streamed function be identical to a function called without streaming?\n2. Once a KV pair is streamed, and assuming the stream isn’t later determined unparseable, is it safe to assume the KV pair is final? Or is it possible for some later tokens to alter previously parsed value in the final response?",
        "timestamp": "2024-09-05 18:57:00.550000+00:00",
        "id": 1281327242621751316,
        "parent_id": null,
        "thread_id": 1281327242621751316
    },
    {
        "author": "hellovai",
        "content": "You can mitigate this difference by making each field optional in the actual class itself",
        "timestamp": "2024-09-05 19:02:58.729000+00:00",
        "id": 1281328744933494926,
        "parent_id": null,
        "thread_id": 1281327242621751316
    },
    {
        "author": "hellovai",
        "content": "does that help clarify the difference?",
        "timestamp": "2024-09-05 19:02:18.945000+00:00",
        "id": 1281328578067173376,
        "parent_id": null,
        "thread_id": 1281327242621751316
    },
    {
        "author": "hellovai",
        "content": "that means if youre return type is actually: \n```ts\nclass Wrapper {\n  foo?: Foo\n}\n```\n\nthen during streaming you could get:\n```ts\n{\n  foo: { car: 0.1 }\n}\n```\n\nbut finally you could get:\n```ts\n{ foo: null }\n```",
        "timestamp": "2024-09-05 19:01:46.685000+00:00",
        "id": 1281328442759188572,
        "parent_id": null,
        "thread_id": 1281327242621751316
    },
    {
        "author": "hellovai",
        "content": "There's always a possibility of it parsing differently in subsequent tokens:\n\nWhen we stream we return a `Partial<T>` but when its complete we return a `T` type.\n\nSo if your type is:\n\n```ts\nclass Foo {\n  bar: int\n  car: float\n}\n```\n\nduring streaming:\n```ts\n{ car: 0.1 }\n```\n\nwould work, but when you get the final response:\n```ts\n<error>\n```\n\nas your return type is a `Foo` type but a `Foo` requires both bar and car. However a `Partial<Foo>` can be just car.\n\nThis is then true recursively",
        "timestamp": "2024-09-05 19:00:29.737000+00:00",
        "id": 1281328120015618089,
        "parent_id": null,
        "thread_id": 1281327242621751316
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-05 19:00:29.377000+00:00",
        "id": 1281328118505799740,
        "parent_id": 1281327242621751316,
        "thread_id": 1281327242621751316
    },
    {
        "author": "hellovai",
        "content": "parsing validation",
        "timestamp": "2024-09-05 13:41:48.497000+00:00",
        "id": 1281247919722332215,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "I'd still like the rest of the schema if one field failed",
        "timestamp": "2024-09-05 11:07:49.166000+00:00",
        "id": 1281209167159230575,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "Is parsing validation going to deal with parsing errors we get now (eg. Null value on non optional params)?",
        "timestamp": "2024-09-05 11:06:58.455000+00:00",
        "id": 1281208954462011393,
        "parent_id": null,
        "thread_id": 1281208954462011393
    },
    {
        "author": "hellovai",
        "content": "yes! so parsing will still work as long as you use @check instead of @assert.\n\n@assert raises ParsingExceptions\n\n@check gives you an error mesmage",
        "timestamp": "2024-09-05 22:43:51.252000+00:00",
        "id": 1281384330043592865,
        "parent_id": null,
        "thread_id": 1281208954462011393
    },
    {
        "author": "gabriel_syme",
        "content": "In new use cases I end up making almost everything optional at first atm 😄",
        "timestamp": "2024-09-05 22:09:23.834000+00:00",
        "id": 1281375658664132753,
        "parent_id": null,
        "thread_id": 1281208954462011393
    },
    {
        "author": "gabriel_syme",
        "content": "Does this allow my parsing to still succeed though or do I need to smh predict where that can happen and make it optional?",
        "timestamp": "2024-09-05 22:09:10.235000+00:00",
        "id": 1281375601625796629,
        "parent_id": null,
        "thread_id": 1281208954462011393
    },
    {
        "author": "hellovai",
        "content": "it can! We have two different kinds of validation:\n\n@assert - will raise an exception and not give any partial values\n@check - will give metadata for a field saying which assertions failed\n\n\n```ts\nclass FooBar {\n  foo int? @check(this != null, \"non_null\")\n}\n```\n\nthis will in python provide metadata such as this:\n\n```python\nres: FooBar\n\nres.foo.value: int | None\nres.foo.checks: [(\"non_null\", bool)]\n```",
        "timestamp": "2024-09-05 13:41:48.837000+00:00",
        "id": 1281247921148268556,
        "parent_id": null,
        "thread_id": 1281208954462011393
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-05 13:41:48.370000+00:00",
        "id": 1281247919189786685,
        "parent_id": 1281208954462011393,
        "thread_id": 1281208954462011393
    },
    {
        "author": "hellovai",
        "content": "openai structured output",
        "timestamp": "2024-09-04 15:47:17.957000+00:00",
        "id": 1280917112701063183,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "Optional arrays",
        "timestamp": "2024-09-04 15:42:34.883000+00:00",
        "id": 1280915925402390573,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "batching requests",
        "timestamp": "2024-09-04 15:41:39.072000+00:00",
        "id": 1280915691314085894,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "What's the best way to do batched runs atm? I can read docs if it's in there, was in flight 😀",
        "timestamp": "2024-09-04 09:36:40.048000+00:00",
        "id": 1280823840150388747,
        "parent_id": null,
        "thread_id": 1280823840150388747
    },
    {
        "author": "hellovai",
        "content": "we recommend doing `asyncio.gather`\n\nthen using semaphores to do max ratelimiting:\n\n```python\nimport asyncio\n\nasync def worker(semaphore, num):\n    async with semaphore:\n        print(f\"Worker {num} starting\")\n        await asyncio.sleep(2)  # Simulate some async work\n        print(f\"Worker {num} finished\")\n\nasync def main():\n    semaphore = asyncio.Semaphore(5)  # Limit to 5 tasks running in parallel\n\n    # Create a list of tasks\n    tasks = [worker(semaphore, i) for i in range(20)]\n\n    # Gather the tasks and run them\n    await asyncio.gather(*tasks)\n\n# Run the main function\nasyncio.run(main())\n```",
        "timestamp": "2024-09-04 15:41:40.069000+00:00",
        "id": 1280915695495942175,
        "parent_id": null,
        "thread_id": 1280823840150388747
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-04 15:41:38.890000+00:00",
        "id": 1280915690550857852,
        "parent_id": 1280823840150388747,
        "thread_id": 1280823840150388747
    },
    {
        "author": "foxicution",
        "content": "Question about https://openai.com/index/introducing-structured-outputs-in-the-api/ and BAML. As far as I understand BAML constructs the prompt, but doesn't use the OpenAI specific \"response_format\" parameter, thus the claim in the article of 100% reliability does not apply to BAML, even when using in conjunction with ChatGPT. Is my assumption correct? Is this a planned feature?",
        "timestamp": "2024-09-04 07:43:55.209000+00:00",
        "id": 1280795466359115776,
        "parent_id": null,
        "thread_id": 1280795466359115776
    },
    {
        "author": "elijas_ai",
        "content": "Thanks <@99252724855496704> \nI've been also following this",
        "timestamp": "2024-09-04 17:26:14.017000+00:00",
        "id": 1280942010341265525,
        "parent_id": null,
        "thread_id": 1280795466359115776
    },
    {
        "author": "hellovai",
        "content": "(That said we aren't against adding support for this officially, but we have seen so far most people havne't had parsing issues with BAML. If that changes we'd prioritize this differently).",
        "timestamp": "2024-09-04 15:47:55.294000+00:00",
        "id": 1280917269303525397,
        "parent_id": null,
        "thread_id": 1280795466359115776
    },
    {
        "author": "hellovai",
        "content": "yep! we don't do what openai does for a few reasons:\n\n1. openai uses a technique called constrained generation which reduces the accuracy of the model. While openai does provide a 100% gurantee of parseability, that does not mean 100% on correctness. \n\n2. openai's approach has many restrictions and doesn't allow for many schemas\n\nTo have a better intuition think of it this way:\n```\nLets start with just a hypothetical model phi420. Phi420 is completely nonsense and produces tokens randomly (its basically a rand(1, num tokens)). In this case, you can use a constrained generation technique like outlines does, and it will technically produce parseable JSON. The JSON still doesn't mean anything useful even if its valid and matches the schema.\n\nParseable != useful\n\nThe implication that the model is able to output something close enough to a schema that we are able to parse gives the confidence that the model is able to understand the task / inputs.\n\nMore practical example: you are parsing a resume object from a OCR'ed PDF. however a user uploads an invoice pdf. Constrained generation will still output a resume, but parsing will correctly raise an exception.\n```\n\nYou can read more here:\nhttps://www.boundaryml.com/blog/sota-function-calling?q=0\n\nYou'll notice that:\n* BAML + gpt-4o-mini we actually outperform Structured output + GPT-4o. \n* GPT-4o + structured output is actually worse than even GPT-4o + BAML\n\nDoes that help?",
        "timestamp": "2024-09-04 15:47:18.711000+00:00",
        "id": 1280917115863306362,
        "parent_id": null,
        "thread_id": 1280795466359115776
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-04 15:47:17.957000+00:00",
        "id": 1280917112701063178,
        "parent_id": 1280795466359115776,
        "thread_id": 1280795466359115776
    },
    {
        "author": "airhorns",
        "content": "why can't array types be optional also?",
        "timestamp": "2024-09-04 03:05:54.018000+00:00",
        "id": 1280725500372848701,
        "parent_id": null,
        "thread_id": 1280725500372848701
    },
    {
        "author": "airhorns",
        "content": "ah that makes sense",
        "timestamp": "2024-09-04 18:43:44.482000+00:00",
        "id": 1280961515805212715,
        "parent_id": null,
        "thread_id": 1280725500372848701
    },
    {
        "author": "hellovai",
        "content": "tehcnically there's a workaround: `string[] | null`\n\nIn practice, during parsing, you'll almost always get an `[]` instead as we can infer that even if the array value isn't set.",
        "timestamp": "2024-09-04 15:42:35.169000+00:00",
        "id": 1280915926602088570,
        "parent_id": null,
        "thread_id": 1280725500372848701
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-04 15:42:34.735000+00:00",
        "id": 1280915924781895772,
        "parent_id": 1280725500372848701,
        "thread_id": 1280725500372848701
    },
    {
        "author": "airhorns",
        "content": "maybe an enum of one element?",
        "timestamp": "2024-09-04 02:47:13.612000+00:00",
        "id": 1280720801049608273,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "airhorns",
        "content": "looking to generate a list of operations as a result and i was hoping for some easy way to determine which among the set of possible operations was selected",
        "timestamp": "2024-09-04 02:44:17.268000+00:00",
        "id": 1280720061409267766,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "airhorns",
        "content": "hey folks -- is there any support for constant/literal types and/or discriminated unions?",
        "timestamp": "2024-09-04 02:43:51.348000+00:00",
        "id": 1280719952692641873,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "hellovai",
        "content": "its two fold. Effectively the LLM is attempting to understand information in your prompt to then produce some tokens that are nice.\n\nideally, you'd like that information to be compressed as much as possible into as few tokens as possible, so the network can quickly come to conclusion on what the best next token in.\n\nA practical example is the same reason we prefer slides with bullet points instead of paragraphs. When its in paragraph form, we need to do work to remove otherwise unnecessary tokens. Transform models benefit from the same context",
        "timestamp": "2024-09-04 21:17:45.942000+00:00",
        "id": 1281000277297729587,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "airhorns",
        "content": "or cause it actually raises output quality",
        "timestamp": "2024-09-04 19:39:31.374000+00:00",
        "id": 1280975553687715913,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "airhorns",
        "content": "is that just to keep the token count low?",
        "timestamp": "2024-09-04 19:39:22.523000+00:00",
        "id": 1280975516563931219,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "hellovai",
        "content": "yep! one thing we recommend in general is don't add a description to every field FYI.\n\nits kinda like when you code, if a field name is self describing, don't add the same thing twice",
        "timestamp": "2024-09-04 18:44:30.325000+00:00",
        "id": 1280961708084428882,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "airhorns",
        "content": "when workin with zod schemas for openai structured outputs i found that adding descriptions to all of them really helped with output quality, so if i am gonna add a description to just about every type i am fine adding a name too",
        "timestamp": "2024-09-04 18:43:32.476000+00:00",
        "id": 1280961465448398949,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "airhorns",
        "content": "havent come up against the need for that yet, i dont having to name everything to be honest",
        "timestamp": "2024-09-04 18:42:55.692000+00:00",
        "id": 1280961311165124681,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "hellovai",
        "content": "<@262014624122011648> would you also like anonymous / unnamed types like Typescript? As we are re-amping our typesystem, i'd actually love to get some thoughts around what you find to be some of the most powerful features here",
        "timestamp": "2024-09-04 15:49:07.346000+00:00",
        "id": 1280917571511783464,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "airhorns",
        "content": "thanks!",
        "timestamp": "2024-09-04 03:05:48.450000+00:00",
        "id": 1280725477019095123,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "hellovai",
        "content": "(On my phone will reply with workaround when I’m at my desk)\nShort answer now, long answer you can do this with unions or types with an enum of one element",
        "timestamp": "2024-09-04 02:47:25.408000+00:00",
        "id": 1280720850525360199,
        "parent_id": null,
        "thread_id": 1280719952692641873
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-04 02:47:24.943000+00:00",
        "id": 1280720848575139882,
        "parent_id": 1280719952692641873,
        "thread_id": 1280719952692641873
    },
    {
        "author": ".aaronv",
        "content": "organization",
        "timestamp": "2024-09-03 22:56:37.153000+00:00",
        "id": 1280662766734147704,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabriel_syme",
        "content": "What is the best way to manage many functions in a single file?\n\nI have a complex extraction that has 16 differ3nt schemas. The final output is a report that contains all that. Is my best wag of doing this a for loop over those? And how do people do this cleanly in code 😀",
        "timestamp": "2024-09-03 22:33:18.774000+00:00",
        "id": 1280656901507383306,
        "parent_id": null,
        "thread_id": 1280656901507383306
    },
    {
        "author": ".aaronv",
        "content": "all types are available in any baml file (everything is globally scoped)",
        "timestamp": "2024-09-03 22:57:11.998000+00:00",
        "id": 1280662912884670531,
        "parent_id": null,
        "thread_id": 1280656901507383306
    },
    {
        "author": ".aaronv",
        "content": "You can add subsets of your schema in other files if needed.\n\nI'm guessing your final output object just has a ton of fields? Or the function returns one out of many other types?",
        "timestamp": "2024-09-03 22:56:37.579000+00:00",
        "id": 1280662768520790117,
        "parent_id": null,
        "thread_id": 1280656901507383306
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-09-03 22:56:37.153000+00:00",
        "id": 1280662766734147699,
        "parent_id": 1280656901507383306,
        "thread_id": 1280656901507383306
    },
    {
        "author": "gabriel_syme",
        "content": "Is it possible to do response prefilling with baml?",
        "timestamp": "2024-09-03 21:36:57.515000+00:00",
        "id": 1280642719479500860,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "hellovai",
        "content": "but i'm not sure if the parser will work in that scenario (it may, but it may not as well).",
        "timestamp": "2024-09-03 21:41:23.862000+00:00",
        "id": 1280643836619653202,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "gabriel_syme",
        "content": "Cool ye lol might be just thay",
        "timestamp": "2024-09-03 21:41:03.159000+00:00",
        "id": 1280643749785112626,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "hellovai",
        "content": "yes, that should work",
        "timestamp": "2024-09-03 21:41:01.083000+00:00",
        "id": 1280643741077475431,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "hellovai",
        "content": "oh, you can try that!",
        "timestamp": "2024-09-03 21:40:52.783000+00:00",
        "id": 1280643706265010260,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "gabriel_syme",
        "content": "Oh wait is it as simple as adding `{` after the ctx.schema?",
        "timestamp": "2024-09-03 21:40:35.995000+00:00",
        "id": 1280643635851038804,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "hellovai",
        "content": "what do you mean by this?",
        "timestamp": "2024-09-03 21:37:19.301000+00:00",
        "id": 1280642810856345642,
        "parent_id": null,
        "thread_id": 1280642719479500860
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-03 21:37:19.085000+00:00",
        "id": 1280642809950507100,
        "parent_id": 1280642719479500860,
        "thread_id": 1280642719479500860
    },
    {
        "author": "demontrius",
        "content": "Hi everyone. So I tried using baml on google colabs but got an error when running the code below:\n!baml-cli generate\n\nError generating clients Traceback (most recent call last): File \"/usr/local/bin/baml-cli\", line 8, in <module> sys.exit(invoke_runtime_cli()) baml_py.BamlError: error: client provider openai-generic not found. Did you mean one of these: `openai`, `azure-openai`, `anthropic`? --> ./baml_src/clients.baml:4 | 3 | client<llm> MyClient { 4 | provider openai-generic |\n\n#/content/baml_src/clients.baml\nclient<llm> MyClient { provider openai-generic options { base_url \"https://api.groq.com/openai/v1\" api_key env.GROQ_API_KEY model \"llama3-70b-8192\" } }",
        "timestamp": "2024-09-03 05:05:47.884000+00:00",
        "id": 1280393285768319038,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "demontrius",
        "content": "Nice demo at the AI hackerspace last Friday 👌",
        "timestamp": "2024-09-12 12:10:04.956000+00:00",
        "id": 1283761551257370655,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "demontrius",
        "content": "I haven't retried this... Will likely try using baml again but in vscode and not colabs when I get the time.",
        "timestamp": "2024-09-12 12:09:41.049000+00:00",
        "id": 1283761450984276072,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "demontrius",
        "content": "Sorry for the late response. Got distracted with other projects",
        "timestamp": "2024-09-12 12:08:45.379000+00:00",
        "id": 1283761217487372350,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "hellovai",
        "content": "did this work?",
        "timestamp": "2024-09-05 13:45:04.038000+00:00",
        "id": 1281248739880730695,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "demontrius",
        "content": "I don't have ollama but I can try with openai",
        "timestamp": "2024-09-03 11:56:50.034000+00:00",
        "id": 1280496726322581558,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "hellovai",
        "content": "Sorry, with groq, can you use “ollama” not openai.",
        "timestamp": "2024-09-03 05:07:47.735000+00:00",
        "id": 1280393788459712593,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "hellovai",
        "content": "We’ll hot fix openai-generic shortly! Seems we missed an edge case in the compiler",
        "timestamp": "2024-09-03 05:07:12.612000+00:00",
        "id": 1280393641143304325,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "hellovai",
        "content": "Lowercase*",
        "timestamp": "2024-09-03 05:06:37.090000+00:00",
        "id": 1280393492153241651,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "hellovai",
        "content": "Hi! Can you try with just OpenAI?",
        "timestamp": "2024-09-03 05:06:31.585000+00:00",
        "id": 1280393469063594096,
        "parent_id": null,
        "thread_id": 1280393285768319038
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-09-03 05:06:30.868000+00:00",
        "id": 1280393466056015892,
        "parent_id": 1280393285768319038,
        "thread_id": 1280393285768319038
    },
    {
        "author": "maweill.",
        "content": "Hi! In Nuxt 3 project I get ```X [ERROR",
        "timestamp": "2024-09-01 14:19:48.712000+00:00",
        "id": 1279807932174831693,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "maweill.",
        "content": "Hi! In Nuxt 3 project I get ```X [ERROR] No loader is configured for \".node\" files: node_modules/.pnpm/@boundaryml+baml-win32-x64-msvc@0.54.0/node_modules/@boundaryml/baml-win32-x64-msvc/baml.win32-x64-msvc.node\n\nnode_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/native.js:96:23:\n      96 │         return require('@boundaryml/baml-win32-x64-msvc')``` \nHow can I solve this?",
        "timestamp": "2024-09-01 14:04:03.648000+00:00",
        "id": 1279803968288985150,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "we'll definitely add it to our roadmap, and sorry about that, our docs should be more specific about Javascript / Browser compatibility and the fact that you do need Node to run it",
        "timestamp": "2024-09-02 16:38:18.893000+00:00",
        "id": 1280205175465709670,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "ah, okay. Would love to get web-assembly SDK in the future. Thanks for help!",
        "timestamp": "2024-09-02 16:37:39.800000+00:00",
        "id": 1280205011497779240,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "the way our playground works is that we use web assembly to call the functions (but the interface to call the functions is more barebones and we dont have types etc)",
        "timestamp": "2024-09-02 16:35:55.802000+00:00",
        "id": 1280204575298551860,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "we currently don't have a frontend-only SDK yet. We may be able to provide you with a web-assembly SDK that you can use to call functions that way, but it may be until the end of this week or the next. The alternative for now is to send API keys and call a function on your backend unfortunately.",
        "timestamp": "2024-09-02 16:35:15.398000+00:00",
        "id": 1280204405832155227,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "the idea is to run all logic only on client-side",
        "timestamp": "2024-09-02 16:32:57.733000+00:00",
        "id": 1280203828423299144,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "do you want it to be all client-side? or would you be open to sending those api keys to your backend?",
        "timestamp": "2024-09-02 16:31:25.909000+00:00",
        "id": 1280203443285266473,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "I am making client-only app with setting API keys via a UI, yep",
        "timestamp": "2024-09-02 16:29:33.213000+00:00",
        "id": 1280202970603978795,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "unless you're letting users put their API keys via a UI on the client side, then we'd need to give you a javascript-sdk",
        "timestamp": "2024-09-02 16:28:52.719000+00:00",
        "id": 1280202800760094773,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "(on the server-side)",
        "timestamp": "2024-09-02 16:28:21.427000+00:00",
        "id": 1280202669511938060,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "since the client also uses your API keys, and those probably are stored somewhere in your backend",
        "timestamp": "2024-09-02 16:28:15.073000+00:00",
        "id": 1280202642861326336,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "here's my nuxt starter:\nhttps://github.com/BoundaryML/baml-examples/tree/main/nuxt-starter\nyou can create a server function:\nhttps://github.com/BoundaryML/baml-examples/blob/main/nuxt-starter/server/extract.get.ts\n\nthat you call in your vue script:\nhttps://github.com/BoundaryML/baml-examples/blob/main/nuxt-starter/app.vue#L3",
        "timestamp": "2024-09-02 16:26:56.188000+00:00",
        "id": 1280202311993393273,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "just a lamda in vue. i am calling it on frontend, yes. It is not possible? Maybe some workaround?",
        "timestamp": "2024-09-02 16:26:11.049000+00:00",
        "id": 1280202122666840135,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "oh wait maybe im reading this incorrectly, is this a react hook?",
        "timestamp": "2024-09-02 16:24:26.465000+00:00",
        "id": 1280201684009746473,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "ok I gotcha, the issue is that you can't call baml functions from your frontend",
        "timestamp": "2024-09-02 16:23:35.077000+00:00",
        "id": 1280201468472725616,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "using `    \"@boundaryml/baml\": \"^0.54.0\",` version",
        "timestamp": "2024-09-02 16:22:44.162000+00:00",
        "id": 1280201254919737374,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "name-chat.baml: ```class Chat {\n    title string\n    icon string\n}\n\nfunction TitleChat(chat_content: string) -> Chat {\n    client Claude\n\n    prompt #\"\n        Given the following chat content, create a short, descriptive title (maximum 5 words) and choose a single appropriate emoji that represents the main theme or topic of the conversation. The title should be clear and informative to simplify user navigation between multiple chats.\n\n        Chat content:\n        ---\n        {{ chat_content }}\n        ---\n\n        {# special macro to print the output instructions. #}\n        {{ ctx.output_format }}\n\n        JSON:\n    \"#\n}```\n\nchatService.ts: ```import { useChatsStore } from \"~/stores/chatsStore\";\nimport { useMessagesStore } from \"~/stores/messagesStore\";\nimport type { ChatState } from \"~/types\";\nimport { b } from \"~/baml_client\";\n\nexport const useChatService = () => {\n  const chatsStore = useChatsStore();\n  const messagesStore = useMessagesStore();\n\n  const createChat = async ({\n    chat,\n    initialMessageContent,\n  }: {\n    chat: Omit<ChatState, \"id\" | \"title\">;\n    initialMessageContent: string;\n  }) => {\n    const bamlChat = await b.TitleChat(initialMessageContent);\n    const newChat = await chatsStore.addChat({\n      ...chat,\n      title: bamlChat.title,\n    });\n    return newChat;\n  };\n\n  const deleteChat = async (id: string) => {\n    await messagesStore.deleteMessagesForChat(id);\n    await chatsStore.deleteChat(id);\n  };\n\n  const renameChat = async (id: string, title: string) => {\n    await chatsStore.updateChat(id, { title });\n  };\n\n  const setActiveChat = (id: string | null) => {\n    chatsStore.setActiveChat(id);\n  };\n\n  return {\n    createChat,\n    deleteChat,\n    renameChat,\n    setActiveChat,\n  };\n};```",
        "timestamp": "2024-09-02 16:21:53.095000+00:00",
        "id": 1280201040729477223,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "how do you call the baml function in your app?",
        "timestamp": "2024-09-02 16:20:36.267000+00:00",
        "id": 1280200718489354261,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "<@201399017161097216> Getting the same error unfortunately ```Uncaught SyntaxError: The requested module '/_nuxt/node_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/index.js?v=ada1cd1b' does not provide an export named 'BamlCtxManager' (at globals.ts:18:10)```\n\nI am trying to run it on dev.\nMy `nuxt.config.ts`: ```// https://nuxt.com/docs/api/configuration/nuxt-config\nexport default defineNuxtConfig({\n  compatibilityDate: \"2024-04-03\",\n  devtools: { enabled: true },\n  modules: [\n    \"@nuxt/ui\",\n    \"@nuxt/eslint\",\n    \"@pinia/nuxt\",\n    \"@formkit/auto-animate\",\n    \"@vueuse/nuxt\",\n    \"@nuxtjs/mdc\",\n  ],\n  plugins: [\"~/plugins/shiki.ts\"],\n  runtimeConfig: {\n    public: {\n      anthropicBaseUrl: process.env.NUXT_ANTHROPIC_BASE_URL,\n      anthropicApiKey: process.env.NUXT_ANTHROPIC_API_KEY,\n    },\n  },\n  ssr: false,\n  mdc: {\n    components: {\n      prose: true,\n    },\n    highlight: false,\n  },\n  vite: {\n    optimizeDeps: {\n      exclude: [\"@boundaryml/baml\"],\n    },\n    build: {\n      target: 'esnext',\n    },\n    ssr: {\n      noExternal: ['@boundaryml/baml'],\n    },\n  },\n});\n```",
        "timestamp": "2024-09-02 16:19:10.349000+00:00",
        "id": 1280200358123016274,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "<@329919568950591489>  Hi! It should work with this config:\n```\n// https://nuxt.com/docs/api/configuration/nuxt-config\nexport default defineNuxtConfig({\n  compatibilityDate: '2024-04-03',\n  devtools: { enabled: true },\n  vite: {\n    optimizeDeps: {\n      exclude: [\"@boundaryml/baml\"],\n    },\n    build: {\n      target: 'esnext',\n    },\n    ssr: {\n      noExternal: ['@boundaryml/baml'],\n    },\n  },\n})\n\n```\n\nLet me know where you're trying to deploy this -- I highly recommend deploying to your server and making sure everything is good there as well (not just in dev server)",
        "timestamp": "2024-09-01 18:24:26.586000+00:00",
        "id": 1279869495640592394,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "Nuxt 3 Error",
        "timestamp": "2024-09-01 14:22:01.934000+00:00",
        "id": 1279808490948268143,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "hellovai",
        "content": "This is an <@201399017161097216> question! Expect a response around PST 11am!",
        "timestamp": "2024-09-01 14:20:34.351000+00:00",
        "id": 1279808123598798941,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "Fixed by adding ```vite: {\n    optimizeDeps: {\n      exclude: [\"@boundaryml/baml\"],\n    },\n  },```\nto `nuxt.config.ts` but now getting this error on page open: ```globals.ts:18 Uncaught SyntaxError: The requested module '/_nuxt/node_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/index.js?v=a5f5513e' does not provide an export named 'BamlCtxManager' (at globals.ts:18:10)```",
        "timestamp": "2024-09-01 14:19:54.268000+00:00",
        "id": 1279807955478515785,
        "parent_id": null,
        "thread_id": 1279803968288985150
    },
    {
        "author": "maweill.",
        "content": "",
        "timestamp": "2024-09-01 14:19:48.712000+00:00",
        "id": 1279807932174831688,
        "parent_id": 1279803968288985150,
        "thread_id": 1279803968288985150
    },
    {
        "author": ".aaronv",
        "content": "Panic",
        "timestamp": "2024-08-30 00:29:40.447000+00:00",
        "id": 1278874245438374016,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "nonetheless, the rest of requests seem to be executing fine.",
        "timestamp": "2024-08-29 23:21:49.330000+00:00",
        "id": 1278857169936187464,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "arindamkhaled4530",
        "content": "i'm wondering what could throw this error:\n\nthread 'tokio-runtime-worker' panicked at baml-lib/jsonish/src/jsonish/parser/multi_json_parser.rs:33:56:\ncalled `Option::unwrap()` on a `None` value\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nProcessing Chunks:   0%|▍                                                                                                                             | 2/625 [02:25<11:34:26, 66.88s/it]\n\n\n*************Error, Request failed: rust future panicked: unknown error",
        "timestamp": "2024-08-29 23:20:43.322000+00:00",
        "id": 1278856893078704169,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "arindamkhaled4530",
        "content": "thanks",
        "timestamp": "2024-09-06 23:23:30.745000+00:00",
        "id": 1281756698226327575,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "arindamkhaled4530",
        "content": "yes",
        "timestamp": "2024-09-06 23:23:28.858000+00:00",
        "id": 1281756690311676016,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "hellovai",
        "content": "<@1041386380732923964> were you able to get the latest?",
        "timestamp": "2024-09-05 23:18:33.140000+00:00",
        "id": 1281393062115020882,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "arindamkhaled4530",
        "content": "okay thanks",
        "timestamp": "2024-09-05 00:58:17.360000+00:00",
        "id": 1281055773887168546,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "hellovai",
        "content": "FYI, we've got a patch that will make it in by 9 pm today that will patch this for anyone unable to update to 0.54+ (some linux users may be impacted)",
        "timestamp": "2024-09-05 00:54:19.586000+00:00",
        "id": 1281054776590729227,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "arindamkhaled4530",
        "content": "0.53.0",
        "timestamp": "2024-09-05 00:35:00.239000+00:00",
        "id": 1281049913936842823,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "hellovai",
        "content": "without any matching opening braces",
        "timestamp": "2024-08-30 01:18:22.273000+00:00",
        "id": 1278886500464857111,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "hellovai",
        "content": "(it used to happen when the LLM produced some garbage text that started with a `]` or `}`",
        "timestamp": "2024-08-30 01:18:10.316000+00:00",
        "id": 1278886450313695317,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": "hellovai",
        "content": "for context, this was fixed in 0.54+. we had removed most `.unwraps` in the pipeline, but had missed one!",
        "timestamp": "2024-08-30 01:17:28.789000+00:00",
        "id": 1278886276136964197,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": ".aaronv",
        "content": "What version of baml are you on?",
        "timestamp": "2024-08-30 00:29:40.947000+00:00",
        "id": 1278874247535525979,
        "parent_id": null,
        "thread_id": 1278856893078704169
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-30 00:29:40.447000+00:00",
        "id": 1278874245438374011,
        "parent_id": 1278856893078704169,
        "thread_id": 1278856893078704169
    },
    {
        "author": "hellovai",
        "content": "Custom transforms",
        "timestamp": "2024-08-29 17:02:37.447000+00:00",
        "id": 1278761741622185988,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "cat_ethos",
        "content": "for optional string field, sometime it returns `null` and another time `N/A`, maybe the parser can coerse those into one single output for better downstream processing",
        "timestamp": "2024-08-29 16:41:58.302000+00:00",
        "id": 1278756544271487119,
        "parent_id": null,
        "thread_id": 1278756544271487119
    },
    {
        "author": "hellovai",
        "content": "That’s a cool idea. I’ll share a doc soon for how we are thinking of generalizing these kinds of transformations in the future so you can control it even at an individual field level",
        "timestamp": "2024-08-29 17:02:38.125000+00:00",
        "id": 1278761744466182144,
        "parent_id": null,
        "thread_id": 1278756544271487119
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-29 17:02:37.335000+00:00",
        "id": 1278761741152419902,
        "parent_id": 1278756544271487119,
        "thread_id": 1278756544271487119
    },
    {
        "author": "hellovai",
        "content": "deno + BAML",
        "timestamp": "2024-08-29 13:25:04.848000+00:00",
        "id": 1278706995053989923,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "cat_ethos",
        "content": "I am trying to install BAML via deno `deno add npm:@boundaryml/baml ` and curiously getting the error `error: Could not find npm package '@boundaryml/baml-win32-arm64-msvc' matching '0.54.0'.` but I am in mac",
        "timestamp": "2024-08-29 11:37:47.744000+00:00",
        "id": 1278679995883192381,
        "parent_id": null,
        "thread_id": 1278679995883192381
    },
    {
        "author": "hellovai",
        "content": "I’ll figure out what it would take for us to officially support deno",
        "timestamp": "2024-08-29 15:56:14.304000+00:00",
        "id": 1278745035109826710,
        "parent_id": null,
        "thread_id": 1278679995883192381
    },
    {
        "author": "hellovai",
        "content": "Zig and rust are so much better",
        "timestamp": "2024-08-29 15:55:55.969000+00:00",
        "id": 1278744958207262833,
        "parent_id": null,
        "thread_id": 1278679995883192381
    },
    {
        "author": "hellovai",
        "content": "Ah got it. As a non TS person myself, I also agree on the state of npm 😅",
        "timestamp": "2024-08-29 15:55:41.297000+00:00",
        "id": 1278744896668307530,
        "parent_id": null,
        "thread_id": 1278679995883192381
    },
    {
        "author": "cat_ethos",
        "content": "i change to Bun runtime after the error and it works....it was for demo purpose and I am not js/ts person so i avoid using npm in the documentation just because i don't want to deal with typescript compiler",
        "timestamp": "2024-08-29 15:49:56.418000+00:00",
        "id": 1278743450140938291,
        "parent_id": null,
        "thread_id": 1278679995883192381
    },
    {
        "author": "hellovai",
        "content": "does it work after that error? i think it should work regardless of that one error. We're wroking on patching it! (also not syet 100% sure if works on the deno runtime)",
        "timestamp": "2024-08-29 13:25:10.422000+00:00",
        "id": 1278707018433298454,
        "parent_id": null,
        "thread_id": 1278679995883192381
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-29 13:25:04.848000+00:00",
        "id": 1278706995053989918,
        "parent_id": 1278679995883192381,
        "thread_id": 1278679995883192381
    },
    {
        "author": "nicarq",
        "content": "hi! is there any way to use BAML from Rust? I saw that the engine is in Rust",
        "timestamp": "2024-08-29 05:51:44.452000+00:00",
        "id": 1278592908324507727,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "Ahh- I didn't realize you two were on the same team, this makes more sense now",
        "timestamp": "2024-09-09 21:49:44.599000+00:00",
        "id": 1282820264056328193,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "thanks <@711679663746842796> ! we are working with <@198831631602024450> with the other approach",
        "timestamp": "2024-09-09 21:49:14.177000+00:00",
        "id": 1282820136457474170,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "<@526965020031188994> BAML 0.55.0 is out, which means you can now try out BAML from Rust!\n\n- [announcement](https://discord.com/channels/1119368998161752075/1119375433666920530/1282818005084016721)\n- [quickstart docs](https://docs.boundaryml.com/docs/get-started/quickstart/openapi)\n- [baml-examples](https://github.com/BoundaryML/baml-examples/tree/main/rust-openapi-starter)\n\nExample code:\n\n```\n    {\n        let image = BamlImage::BamlImageUrl(BamlImageUrl {\n            url: \"https://i.redd.it/adzt4bz4llfc1.jpeg\".to_string(),\n            media_type: None,\n        }.into()).into();\n        let req = ExtractReceiptRequest {\n            receipt: ExtractReceiptRequestReceipt::BamlImage(image).into(),\n        };\n        let resp = b::extract_receipt(&config, req).await?;\n        dbg!(resp);\n    }\n```",
        "timestamp": "2024-09-09 21:47:36+00:00",
        "id": 1282819724673290250,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "saiko9729",
        "content": "For sure, this is something we can develop on our side but idk if this is what we expect... ending creating a custom BAML Client and a custom BAML Server 🤔",
        "timestamp": "2024-09-09 19:00:29.026000+00:00",
        "id": 1282777668495999118,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "(let's start a new 🧵 for this)",
        "timestamp": "2024-09-09 19:00:28.622000+00:00",
        "id": 1282777666801500232,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "saiko9729",
        "content": "Heeey <@711679663746842796> I was playing-around/reading-code about serve function. I think it's pretty useful.\n\nAs some feedback/questions, why serve needs the BAML files (throught the --from param) in place of being fully generic?\nLet's say, on every network request to the BAML Rest service, besides:\n- function (in the route param)\n- args (in the body)\n\nThe client (IE, typescript client) could send the BAML files to the endpoint so we could:\n- Creates a BAML Runtime dynamically\n- Execute the param parsing pipeline\n- Execute the function\n- Collect results\n- Send response\n\n\nI'm not sure about how inefficient could be stay instantiating a new BAML Runtime on every request but maybe it could be an improve over the first use case or even a completely different use case where we can share/scale the same instance of BAML Http Service throught different set of tools with no need to have them tied 🤔",
        "timestamp": "2024-09-09 18:59:20.797000+00:00",
        "id": 1282777382322962534,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "Hi- yes, we haven’t released yet! Will update this thread when we do!",
        "timestamp": "2024-09-09 15:13:48.912000+00:00",
        "id": 1282720625483452447,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "maybe i need to wait for a new version of the npm package? https://www.npmjs.com/package/@boundaryml/baml?activeTab=versions",
        "timestamp": "2024-09-09 05:37:09.415000+00:00",
        "id": 1282575504674914324,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "i have the previous setup working with typescript but I wanted to try to make it work with Rust and the cli server",
        "timestamp": "2024-09-09 05:35:43.837000+00:00",
        "id": 1282575145734770720,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "<@711679663746842796> hey sam. im getting this error creating a new baml project\n\n```\nnpx @boundaryml/baml init \\\n  --client-type rest/openapi --openapi-client-type rust\n\nerror: unexpected argument '--openapi-client-type' found\n\n  tip: a similar argument exists: '--client-type'\n\nUsage: baml-cli init <--dest <DEST>|--client-type <CLIENT_TYPE>>\n\nFor more information, try '--help'.\n```\nhttps://docs.boundaryml.com/docs/get-started/quickstart/openapi#create-a-new-baml-project",
        "timestamp": "2024-09-09 05:35:14.747000+00:00",
        "id": 1282575023722201150,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "Just following up- we're getting closer and closer to the release! Expect it some time this week; we'll update this thread when we do.\n\nTo give you a sense of the current status, we're doing final passes now (e.g. reviewing generated code and making sure that we can preserve backwards compatibility for future releases).\n\nThings you can expect:\n\n- we've implemented OpenAPI codegen that can also run `openapi-generator-cli` for you automatically\n- we've implemented a server with hot reload that will re-generate everything when you edit BAML files\n- we've added not only quickstart docs, but also a `docker-compose` example ([preview](https://boundary-preview-1eabadb4-99c1-45a6-8131-2277b2373fe9.docs.buildwithfern.com/docs/get-started/deploying/openapi))\n- we've published example code for Go, Java, PHP, Rust at https://github.com/BoundaryML/baml-examples",
        "timestamp": "2024-09-03 19:09:40.297000+00:00",
        "id": 1280605653500428461,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "this is what the process model will be - you'll be able to run `baml-cli serve` any way you want: as a subprocess, as a separate Docker container, as a microservice, whichever way you want",
        "timestamp": "2024-08-29 17:02:40.310000+00:00",
        "id": 1278761753630609579,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "cat_ethos",
        "content": "hi I am also interested in using BAML in rust and i am wondering what do you mean by embedding the REST api inside rust. is it spawning a thread to run the external command or simple starting the external api separately, be it within the same machine or not",
        "timestamp": "2024-08-29 16:21:11.871000+00:00",
        "id": 1278751316361089045,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "aha that's great. So the plan is going to be to go that route then. I will keep a close eye on that PR for the baml-cli serve 🙂",
        "timestamp": "2024-08-29 06:24:18.716000+00:00",
        "id": 1278601105101553674,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "Our NPM build targets all the platforms you mentioned, and `baml-cli serve` will be available using the CLI we ship in the NPM package",
        "timestamp": "2024-08-29 06:22:15.669000+00:00",
        "id": 1278600589005295647,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "Oh, if that's the case, then you won't need to pull in a custom build at all",
        "timestamp": "2024-08-29 06:21:46.166000+00:00",
        "id": 1278600465260740660,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "makes sense. we are also already embedding ollama and even node v22 in the shinkai app so communication through the RESTFul api also works for me",
        "timestamp": "2024-08-29 06:20:29.872000+00:00",
        "id": 1278600145260515390,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "You're also welcome to just pull the crate in directly as a dependency (it's all MIT licensed), but the reason we're explicitly supporting the RESTful/OpenAPI route right now is because we care a lot about the DX of our users (aka you) and it's the most effective way for us to scale the surface area of what we support",
        "timestamp": "2024-08-29 06:18:40.365000+00:00",
        "id": 1278599685954605086,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "that's amazing. we tested a bunch of them and we were coming up with potential ideas on how to make them fail less (aka at least work some times)",
        "timestamp": "2024-08-29 06:17:00.998000+00:00",
        "id": 1278599269179195403,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "(we had some entertaining - and surprisingly usable! - results with even phi3. haven't tried with phi3.5 yet.)",
        "timestamp": "2024-08-29 06:15:20.509000+00:00",
        "id": 1278598847697911860,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "we did recently put up some instructions for how you can use promptfiddle with ollama though: https://www.boundaryml.com/blog/ollama-structured-output",
        "timestamp": "2024-08-29 06:14:43.847000+00:00",
        "id": 1278598693926207549,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "thank so much! we will play with it tomorrow",
        "timestamp": "2024-08-29 06:14:19.983000+00:00",
        "id": 1278598593833603094,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "It's still WIP - we're close, but there's still a bit of work we need to do to lock down the APIs - but you can TAL at\n\n- https://github.com/BoundaryML/baml/pull/908\n- `engine/baml-runtime` is the binary target\n- docs/docs/get-started/quickstart/openapi.mdx will have instructions for how to use it",
        "timestamp": "2024-08-29 06:13:59.539000+00:00",
        "id": 1278598508085116994,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "that would be really handy bc it would make embedding BAML into applications much much easier",
        "timestamp": "2024-08-29 06:12:45.465000+00:00",
        "id": 1278598197396377621,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "BAML builds on all of those already! 🙂\n\nWe don't currently ship any binary targets, although I'll be adding one for our internal use as part of the current work - we might be able to commit to supporting it",
        "timestamp": "2024-08-29 06:08:09.807000+00:00",
        "id": 1278597041202790484,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "```\n  - arch: x86_64-unknown-linux-gnu\n            os: ubuntu-22.04\n          - arch: aarch64-apple-darwin\n            os: macos-14\n          - arch: x86_64-pc-windows-msvc\n            os: windows-2022\n```",
        "timestamp": "2024-08-29 06:03:38.950000+00:00",
        "id": 1278595905146191953,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "https://github.com/dcSpark/shinkai-node/blob/main/.github/workflows/build-binaries.yml#L14",
        "timestamp": "2024-08-29 06:03:27.334000+00:00",
        "id": 1278595856425156651,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "joatmon.pockets",
        "content": "What list of platforms are you planning to target?",
        "timestamp": "2024-08-29 06:02:18.550000+00:00",
        "id": 1278595567924023357,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "We are already running local tools (Rust and JS using Node v22 before we were using rquickjs / llrt). A lot of the models that run locally are not thaaat great. I'm super excited about integrating with BAML",
        "timestamp": "2024-08-29 06:01:41.587000+00:00",
        "id": 1278595412890091521,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": "nicarq",
        "content": "I'm trying to have everything embedded in Rust so I can compile it to multiple architectures. For context, my project is Shinkai (https://www.shinkai.com)  Shinkai is a two-click install app that sets up AIs to run locally on your device. It can tap into a decentralized network where AI agents can pay for services from other AI agents or APIs using microtransactions.\n\nDo you know if I could compile the server to different architectures? Currently we are already embedding Ollama and a couple of other tools. I was planning to take a deeper look tomorrow",
        "timestamp": "2024-08-29 05:58:53.100000+00:00",
        "id": 1278594706204397640,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": ".aaronv",
        "content": "No native rust support at the moment. But by monday (or maybe tomorrow) we will have a server you can instantiate using our CLI to call functions via http.\n\nWe can then generate OpenApi clients for any language.\n\nWould that work?",
        "timestamp": "2024-08-29 05:54:23.337000+00:00",
        "id": 1278593574736232509,
        "parent_id": null,
        "thread_id": 1278592908324507727
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-29 05:54:22.987000+00:00",
        "id": 1278593573268492375,
        "parent_id": 1278592908324507727,
        "thread_id": 1278592908324507727
    },
    {
        "author": ".aaronv",
        "content": "Langchain X BAML",
        "timestamp": "2024-08-28 18:54:23.752000+00:00",
        "id": 1278427482038730848,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "mr_zoidberg_",
        "content": "Are there any good examples of Baml and Langchain integration? I have an app with long conversations and a need to sometimes (at certain steps of the conversations) return a specified structured response while maintaining a using textual conversation. Baml looks promising, but I'm lost in trying to figure out how to integrate it for this usecase.",
        "timestamp": "2024-08-28 14:35:14.910000+00:00",
        "id": 1278362265468272691,
        "parent_id": null,
        "thread_id": 1278362265468272691
    },
    {
        "author": ".aaronv",
        "content": "Hello! welcome! Yes, what kind of Langchain things are you using? Their conversation Memory and stuff like that?",
        "timestamp": "2024-08-28 18:54:25.023000+00:00",
        "id": 1278427487369691176,
        "parent_id": null,
        "thread_id": 1278362265468272691
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-28 18:54:23.310000+00:00",
        "id": 1278427480184721479,
        "parent_id": 1278362265468272691,
        "thread_id": 1278362265468272691
    },
    {
        "author": "gleed",
        "content": "OpenAI assistants",
        "timestamp": "2024-08-27 14:18:41.553000+00:00",
        "id": 1277995711161438261,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "hellovai",
        "content": "sadly not, this work hasn't been prioritized for a bit, that said if someone is willing to help make a new provider for `openai-assistant-api`  I'd be happy to walk someone through the code where to do this.\n\nWe were hoping for someone in our community to do this, but they had some other urgent priorities happen and slipped in lower priority for them as well.\n\nif someone is willing to, just reply here and I'll be glad to hop on a meeting and show how the code works.\n\nIf we do it ourselves, we'd likely need to see a large volume of people upvote the assistants API and then prioritize it accordingly.\n\nFor a workaround, we currently recommend:\n1. calling the assistants API in python/ts\n2. passing those values back to a BAML function to get structured data out",
        "timestamp": "2024-09-23 23:04:45.942000+00:00",
        "id": 1287912574175936618,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "huythangphan",
        "content": "Hello <@99252724855496704>, any update on incorporating the Assistants API? Thanks.",
        "timestamp": "2024-09-23 09:38:19.977000+00:00",
        "id": 1287709628729397258,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "hellovai",
        "content": "<@761090952239906828> not yet, we haven't yet found a good way to incorporate the assistants API, but we have built some document processing API's we'll be releasing shortly. Those may help",
        "timestamp": "2024-09-05 22:48:52.356000+00:00",
        "id": 1281385592965431367,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "erikth2355",
        "content": "Hey checking in on this, was there ever an update?",
        "timestamp": "2024-09-04 01:24:17.716000+00:00",
        "id": 1280699930629115950,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "hellovai",
        "content": "Hmm. I think we had a few other requests for assistants api and from what i was speaking to another customer, they mentioned they might implement in BAML. Let me chat with them and see what the conclusion was. Out of curiosity, for your end, how would you see yourself improving the above code with BAML",
        "timestamp": "2024-08-27 15:03:51.473000+00:00",
        "id": 1278007077389734002,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "gleed",
        "content": "Looks like this has been brought up to some extent here <https://discord.com/channels/1119368998161752075/1263509002755510383/1263555015851643047>. I figured assistant api integration wasn't planned but I thought maybe there'd still be some functionality from baml that could help optimize a prompt, for example, even if i perform the LLM call outside of baml.",
        "timestamp": "2024-08-27 14:23:39.447000+00:00",
        "id": 1277996960619561124,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "gleed",
        "content": "I've recently run into a case where the I'd like the LLM to be able to run code to help with answer accuracy, and I found this does exist to an extent with the OpenAI Assistants API. <https://platform.openai.com/docs/assistants/quickstart?lang=curl&context=without-streaming>\n\nFor example, I'd like to ask gpt-4o \"Solve for x: 3x^2 + 5 = 10.\" and be confident the result will be mapped to a pydantic model like is done in BAML, but such a query would be too complex without giving the LLM the power of code interpreter. And I don't think the basic openai client allows use of tools like code interpreter.\n\nI can accomplish this with assistants API with something like this currently:\n\n```py\nfrom openai import OpenAI\nclient = OpenAI()\n\n# Create an assistant\nassistant = client.beta.assistants.create(\n    name=\"Math Solver\",\n    instructions=\"You are a math problem solver. Solve mathematical equations step by step.\",\n    tools=[{\"type\": \"code_interpreter\"}],\n    model=\"gpt-4\"\n)\n\n# Create a thread\nthread = client.beta.threads.create()\n\n# Add a message to the thread\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id,\n    role=\"user\",\n    content=\"Solve for x: 3x^2 + 5 = 10\"\n)\n\n# Run the assistant\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id\n)\n\n# Retrieve the assistant's response\nif run.status == 'completed':\n    messages = client.beta.threads.messages.list(\n        thread_id=thread.id\n    )\n    # The last message in the thread will be the assistant's response\n    assistant_response = messages.data[0].content[0].text.value\n    print(assistant_response)\nelse:\n    print(f\"Run status: {run.status}\")\n```\nIs there a place in this code I could leverage an existing function/class from BAML to be able to map the output to a defined pydantic model? I know the client/function/model logic in BAML is pretty intertwined so I doubt it, but thought I'd check. 🙂",
        "timestamp": "2024-08-27 14:18:43.984000+00:00",
        "id": 1277995721358049350,
        "parent_id": null,
        "thread_id": 1277995711161438261
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "BAML support  AWS Secrets Manager.",
        "timestamp": "2024-08-26 16:34:57.006000+00:00",
        "id": 1277667613631189054,
        "parent_id": null,
        "thread_id": 1277667613631189054
    },
    {
        "author": "hellovai",
        "content": "Hi Jay! the easier way to do this is to use our client registry feature. \n\nThis will let you define clients using runtime values.\ne.g. set env vars for your bedrock client using the secrets manager.\n\nSee docs here: https://docs.boundaryml.com/docs/calling-baml/client-registry\n\nLet me know if that unblocks you! We're still looking for a way to have the entire object be reset with new env vars, but for now this is the current working solution.",
        "timestamp": "2024-08-26 17:40:33.608000+00:00",
        "id": 1277684124936831089,
        "parent_id": null,
        "thread_id": 1277667613631189054
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Hi @Vaibhav @aaronv  Hope you had a good weekend.  We're moving the first bits of our BAML implementation to production in AWS.  We use AWS Secrets Manager and trying to figure out how if BAML will work with it - https://docs.boundaryml.com/docs/calling-baml/set-env-vars - it's not clear to me it does.",
        "timestamp": "2024-08-26 16:34:57.446000+00:00",
        "id": 1277667615476547584,
        "parent_id": null,
        "thread_id": 1277667613631189054
    },
    {
        "author": "huythangphan",
        "content": "Hi <@99252724855496704> , I’d like to inquire if there are any plans for BAML to support JavaScript in the near future. Thank you",
        "timestamp": "2024-08-26 02:25:46.935000+00:00",
        "id": 1277453913544458313,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "Tagging <@711679663746842796> for context who's working on this.",
        "timestamp": "2024-08-26 02:34:24.894000+00:00",
        "id": 1277456086022095040,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "yep! its super lightweight. Glad to hear it'll work for you. Will DM you when its ready",
        "timestamp": "2024-08-26 02:34:13.982000+00:00",
        "id": 1277456040253853734,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "huythangphan",
        "content": "Thank you",
        "timestamp": "2024-08-26 02:33:57.015000+00:00",
        "id": 1277455969089093644,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "huythangphan",
        "content": "I believe the additional microservice won’t be costly, so it should be fine.",
        "timestamp": "2024-08-26 02:33:48.751000+00:00",
        "id": 1277455934427234355,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "awesome. It will add an extra step FYI to your deploy process where you'll have to have a microservice running `baml-cli serve` but we'll be offering that shorlty after as well",
        "timestamp": "2024-08-26 02:32:16.925000+00:00",
        "id": 1277455549281075271,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "huythangphan",
        "content": "That's great to hear! We’re looking to use JavaScript for faster coding and a more efficient review process. However, the new version coming next week is definitely something to look forward to.",
        "timestamp": "2024-08-26 02:31:20.207000+00:00",
        "id": 1277455311388807268,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "(otherwise, also happy to have you set up a tsc command to compile BAML typescript code -> JS code. I suspect this should be just 15 mins of work)",
        "timestamp": "2024-08-26 02:28:25.604000+00:00",
        "id": 1277454579050741833,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "Would that work for you?",
        "timestamp": "2024-08-26 02:27:37.360000+00:00",
        "id": 1277454376700743682,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "Hey John! We're actually releasing a version of BAML this upcoming week that will work as follows:\n\n```\n# write your BAML files\n\n# then run\nbaml-cli serve --port 3000\n```\n\nThat will also generate an `openapi.yaml` file you can use to generate clients to any language. That way until we build a more native JS option, you'll still be able to ping BAML functions locally.",
        "timestamp": "2024-08-26 02:27:33.165000+00:00",
        "id": 1277454359105503332,
        "parent_id": null,
        "thread_id": 1277453913544458313
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-26 02:27:32.616000+00:00",
        "id": 1277454356802834463,
        "parent_id": 1277453913544458313,
        "thread_id": 1277453913544458313
    },
    {
        "author": "bsachs10",
        "content": "Hello! Is here an example of using `map`? <@99252724855496704> you mentioned a tip to me recently about using my own unique keys in my schema to get data back. Not sure how to implement that but very interested! Context: I'm making a tool to classify sales contracts based on whether the contract includes lagnuage offering a specific product. But there are 40-50 products. So I was thinking I could require it give me back an object where each key is the product name and then the property content for each key is whether the product was found, relevant text as proof, etc.",
        "timestamp": "2024-08-25 13:38:58.125000+00:00",
        "id": 1277260938608640081,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "Awesome - thanks as always!",
        "timestamp": "2024-08-25 18:10:44.953000+00:00",
        "id": 1277329334402810040,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "alias will use less tokens! I recommend taking a look at the final prompt to see the differences.\n\nThough if you use alias you may prefer to use no spaces in key names",
        "timestamp": "2024-08-25 18:10:22.375000+00:00",
        "id": 1277329239703818443,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "Is alias better than description?",
        "timestamp": "2024-08-25 18:09:41.996000+00:00",
        "id": 1277329070341881857,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "Duh!",
        "timestamp": "2024-08-25 18:09:34.329000+00:00",
        "id": 1277329038184022117,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "Ha I literally just figured that out. \n\n```\nclass ContractProductMappingResult {\n    Industrial_Consumer ContractProductMappingValue @description( \"Industrial & Consumer\" )\n    Aerospace_Defense ContractProductMappingValue @description( \"Aerospace & Defense\" )\n    Healthcare ContractProductMappingValue @description( \"Healthcare\" )\n    Geotechnology ContractProductMappingValue @description( \"Geotechnology\" )\n}\n```",
        "timestamp": "2024-08-25 18:09:29.692000+00:00",
        "id": 1277329018735034408,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "for now can you make a new object instead?\n\n```\nclass ProductEntitlements {\n    air ContractProductMappingValue @alias( \"Aerospace & Defense\" )\n    health ContractProductMappingValue @alias( \"Healthcare\" )\n    geotech ContractProductMappingValue @alias( \"Geotechnology\" )\n    consumer ContractProductMappingValue @alias( \"Industrial & Consumer\" )\n}\n```\n\nAlternatively you can construct that class dynamically as well",
        "timestamp": "2024-08-25 18:09:14.535000+00:00",
        "id": 1277328955162099816,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "Got it! So in the meantime, is there a programmatic workaround with dynamic types? I think you mentioned something like this before on my last project.",
        "timestamp": "2024-08-25 18:08:12.691000+00:00",
        "id": 1277328695769436244,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "oh yea, we don't have enum support in maps yet! <@711679663746842796> can work on this shortly, i think we should be able to land it in this week (likely ~1 day of work)",
        "timestamp": "2024-08-25 18:07:30.771000+00:00",
        "id": 1277328519944339528,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "I should probably be able to work this out on my own, but embarrassingly, I am a bit stumped here. I figured I would try to specify my actual product names as keys somehow, perhaps as an enum, and pass that to the map definition. But I got `Maps may only have strings as keys`, so obviously my intuition was wrong there. \n\n```\nenum ProductEntitlements {\n    Aerospace_Defense @description( \"Aerospace & Defense\" )\n    Healthcare @description( \"Healthcare\" )\n    Geotechnology @description( \"Geotechnology\" )\n    Industrial_Consumer @description( \"Industrial & Consumer\" )\n}\n\nclass ContractProductMappingValue {\n    confidence float\n    relevantLanguage string\n}\n\nclass ContractProductMappingResult {\n    Products map<ProductEntitlements, ContractProductMappingValue>\n}\n```\n\nMy intention is to get back an object with **every** product listed a a key so that I am assured the LLM gives me its analysis of each. I have 40 or so of those, so ultimately, I'll have to do this with dynamic types I imagine. But for testing, I was trying a few manual entries.",
        "timestamp": "2024-08-25 18:06:28.417000+00:00",
        "id": 1277328258412839037,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "bsachs10",
        "content": "Thanks - I'll give it a try!",
        "timestamp": "2024-08-25 17:44:23.221000+00:00",
        "id": 1277322700137955439,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "we'll add more examples for the docs to show this",
        "timestamp": "2024-08-25 13:41:28.624000+00:00",
        "id": 1277261569847197817,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "seems like a hole in the docs!\n\nhttps://docs.boundaryml.com/docs/snippets/supported-types#map\n\nyou can do this:\n`map<string, ProductInfo>`\n\nThen add a description where you say `@description(\"Key is the name\")`",
        "timestamp": "2024-08-25 13:41:09.163000+00:00",
        "id": 1277261488221847583,
        "parent_id": null,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-25 13:41:08.649000+00:00",
        "id": 1277261486066106451,
        "parent_id": 1277260938608640081,
        "thread_id": 1277260938608640081
    },
    {
        "author": "hellovai",
        "content": "new dynamic enums",
        "timestamp": "2024-08-24 13:14:36.008000+00:00",
        "id": 1276892418179989510,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gleed",
        "content": "Been playing around with BAML a bit, thanks for the project. I submitted that PR the other day about Gemini and might have some others for old docs etc. I might submit later.\n\nI'm trying to build a classifier. I read about enums in the documentation and examples, sounded pretty promising. I need to categorize with strings containing spaces, so the @alias functionality seemed useful as well. \n\nMy question is this, can I add @@dynamic enum fields and set the @alias as well? From what I can tell in the Python source, type builder really only allows you to add the enum value.\n\nOpen to other suggestions as well if there's another way to accomplish this.\n\nEx.\n```py\n#both diseases and categories are parsed from a file and can vary \ncategories=[\n\"Cardiovascular diseases\", \"Immunology\", \"Skin & Soft tissue\"]\n\ndiseases=[\"Melanoma\", \"Coronary artery disease\"]\n\n#call BAML here to categorize diseases\n```\nCurrently, I pass the list of strings in plain text and ask it to categorize, and cross my fingers that the LLM matches the category exactly",
        "timestamp": "2024-08-24 13:00:56.978000+00:00",
        "id": 1276888982919053343,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "gleed",
        "content": "Will give it a shot, thanks a ton",
        "timestamp": "2024-08-24 13:21:02.181000+00:00",
        "id": 1276894037906817077,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "but yea we 100% need to improve the docs around it",
        "timestamp": "2024-08-24 13:20:29.343000+00:00",
        "id": 1276893900174262355,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "if it doesn't, it won't work",
        "timestamp": "2024-08-24 13:20:21.619000+00:00",
        "id": 1276893867777462314,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "gleed",
        "content": "Thanks",
        "timestamp": "2024-08-24 13:20:16.396000+00:00",
        "id": 1276893845870739598,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "It should auto complete 🙂",
        "timestamp": "2024-08-24 13:20:14.758000+00:00",
        "id": 1276893839000342560,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "gleed",
        "content": "Oh perfect, there it is!",
        "timestamp": "2024-08-24 13:20:11.297000+00:00",
        "id": 1276893824483852330,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "the top level enum can also have alias as well i think",
        "timestamp": "2024-08-24 13:20:10.073000+00:00",
        "id": 1276893819350155384,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "those should work as well",
        "timestamp": "2024-08-24 13:19:57.569000+00:00",
        "id": 1276893766904578110,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "I think our docs for typebuilder need some work\n\n```\n  hobbiesEnum = tb.add_enum('Hobbies')\n  hobbiesEnum.add_value('Soccer').alias(..)\n  hobbiesEnum.add_value('Reading').description(..)\n```",
        "timestamp": "2024-08-24 13:19:49.076000+00:00",
        "id": 1276893731282485271,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "gleed",
        "content": "I was looking at type_builder and it didn't seem immediately obvious to me that I could",
        "timestamp": "2024-08-24 13:18:37.333000+00:00",
        "id": 1276893430370406421,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "I think you should be able to at least",
        "timestamp": "2024-08-24 13:18:17.930000+00:00",
        "id": 1276893348988325998,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "oh you can do that as well 🙂",
        "timestamp": "2024-08-24 13:18:06.805000+00:00",
        "id": 1276893302326558750,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "gleed",
        "content": "Yes, but I can't add an alias with a dynamic enum right?",
        "timestamp": "2024-08-24 13:17:57.995000+00:00",
        "id": 1276893265375006832,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "so you can have a value that has no spaces, but then alias it so you can render spaces for the LLM",
        "timestamp": "2024-08-24 13:17:51.378000+00:00",
        "id": 1276893237621297213,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "you can have spaces by using alias fyi!",
        "timestamp": "2024-08-24 13:17:20.399000+00:00",
        "id": 1276893107685687379,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "gleed",
        "content": "Yes I was planning to take that approach but thought I might be limited by the requirement that the classifications have strings with spaces, while the enums can't have spaces",
        "timestamp": "2024-08-24 13:16:52.714000+00:00",
        "id": 1276892991566381087,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "also you can always construct an enum that's dynamic that has 0 values described in BAML, then only add them dynamically",
        "timestamp": "2024-08-24 13:15:33.695000+00:00",
        "id": 1276892660136673423,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "Firstly, thanks for the PR, please do continue to do so! we appreciate it very much.\n\nAnd yess you can do create new dyanmic enums as well!\n\nhttps://docs.boundaryml.com/docs/calling-baml/dynamic-types#creating-new-dynamic-classes-or-enums-not-in-baml\n\nOne thing we're working on is literals as well.",
        "timestamp": "2024-08-24 13:14:36.374000+00:00",
        "id": 1276892419715235862,
        "parent_id": null,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-24 13:14:36.008000+00:00",
        "id": 1276892418179989505,
        "parent_id": 1276888982919053343,
        "thread_id": 1276888982919053343
    },
    {
        "author": "hitzor9",
        "content": "Hi! Recently, I needed to build a small prototype for extracting structured information from images using AI, and I decided to try using BAML. I didn't have much experience working with LLMs before, but with the help of your tool, I was able to build and test the prototype in just a few hours, so a huuuuge shotout for that!\n\nBefore going to production, I have a couple of questions. I'm using the Python SDK with an async client. \n\n1) When an error related to the LLM occurs, a `BamlError` is raised. I see that this error contains a textual representation of the `LLMResponse`, which includes the error code, message, and so on. Is there a way to access this object directly without trying to parse it from the text?\n\n2) As far as I can see in the OpenAI documentation, the server's response includes headers with usage information (`x-ratelimit-limit-requests`, `x-ratelimit-limit-tokens`). Is there any way I can access the raw response object or at least these headers? Or is there another unified way to obtain this information?\"",
        "timestamp": "2024-08-23 16:19:36.998000+00:00",
        "id": 1276576591241285766,
        "parent_id": null,
        "thread_id": 1276576591241285766
    },
    {
        "author": "hellovai",
        "content": "Hi <@591299942820216869>, thanks for giving BAML a try. Very excited that this worked for you, and hope we can continue to learn from you to make it better.\n\n1. parsing exceptions. This is def a major issue on our end, and we're working on releasing exceptions that are objects. We have a spec for this already and I will share the preview docs with you. (note its not yet implemented but should be live by mid september). Until then sadly you'll have to parse the text.\n\n2.As of right now, sadly not. But we are working on a spec for a thing like `b.raw.YourFunction` which will not only return the raw HTTP request, but a bunch of other metadata that BAML was able to collect (including all retry policys etc). This should also come in the mid sept release.\n\nSpeaking of, you may want to take a look at fallbacks and retry_policies to make your app more resiliant.\nhttps://docs.boundaryml.com/docs/snippets/clients/retry",
        "timestamp": "2024-08-23 16:23:50.529000+00:00",
        "id": 1276577654627237949,
        "parent_id": null,
        "thread_id": 1276576591241285766
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-23 16:23:50.168000+00:00",
        "id": 1276577653113225328,
        "parent_id": 1276576591241285766,
        "thread_id": 1276576591241285766
    },
    {
        "author": "hellovai",
        "content": "exceptions",
        "timestamp": "2024-08-23 03:47:32.362000+00:00",
        "id": 1276387324799881242,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "richardclove",
        "content": "won't this call the b.ClassifyMessage one-by-one before asyncio.gather gets to it?",
        "timestamp": "2024-08-23 03:43:38.048000+00:00",
        "id": 1276386342015602748,
        "parent_id": null,
        "thread_id": 1276386342015602748
    },
    {
        "author": "hellovai",
        "content": "and if you've got other questions, will be on for a bit",
        "timestamp": "2024-08-23 03:50:59.429000+00:00",
        "id": 1276388193301823511,
        "parent_id": null,
        "thread_id": 1276386342015602748
    },
    {
        "author": "hellovai",
        "content": "You can read more here: https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#best-practices",
        "timestamp": "2024-08-23 03:50:40.639000+00:00",
        "id": 1276388114490982460,
        "parent_id": null,
        "thread_id": 1276386342015602748
    },
    {
        "author": "hellovai",
        "content": "sorry for the poor docs here! We'll take a note of this and work on improving the docs for this. Must've missed this in the update",
        "timestamp": "2024-08-23 03:50:26.671000+00:00",
        "id": 1276388055904944169,
        "parent_id": null,
        "thread_id": 1276386342015602748
    },
    {
        "author": "hellovai",
        "content": "yes! but you can change your generator to have the `default_client_mode` set to `async`\n\nor you can directly import the. async version of b from:\n\n```python\nfrom baml_client.async_client import b\n```\nor for sync\n```python\nfrom baml_client.sync_client import b\n```\n\n```python\n# this is just shorthand for whichever is set as default_client_mode\nfrom baml_client import b \n```",
        "timestamp": "2024-08-23 03:49:47.798000+00:00",
        "id": 1276387892859764736,
        "parent_id": null,
        "thread_id": 1276386342015602748
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-23 03:49:47.550000+00:00",
        "id": 1276387891819319358,
        "parent_id": 1276386342015602748,
        "thread_id": 1276386342015602748
    },
    {
        "author": "richardclove",
        "content": "How do you do error handling in concurrent functions? Some errors that come up like found multiple words when trying to do classification using enums\n\nBamlError: (3 other previous tries)\nLLM call failed: LLMErrorResponse { client: \"Groq\", model",
        "timestamp": "2024-08-23 03:27:29.265000+00:00",
        "id": 1276382278645452821,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "did that work?",
        "timestamp": "2024-08-23 04:40:55.115000+00:00",
        "id": 1276400758119731293,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "hmm:\n\n```python\nimport asyncio\n\nasync def task1():\n    await asyncio.sleep(1)\n    return \"Task 1 completed\"\n\nasync def task2():\n    await asyncio.sleep(2)\n    raise ValueError(\"Task 2 failed\")\n\nasync def task3():\n    await asyncio.sleep(1)\n    return \"Task 3 completed\"\n\nasync def task4():\n    await asyncio.sleep(2)\n    return \"Task 4 completed\"\n\nasync def task5():\n    await asyncio.sleep(1)\n    raise RuntimeError(\"Task 5 failed\")\n\nasync def main():\n    results = await asyncio.gather(\n        task1(),\n        task2(),\n        task3(),\n        task4(),\n        task5(),\n        return_exceptions=True\n    )\n\n    for i, result in enumerate(results, 1):\n        if isinstance(result, Exception):\n            print(f\"Task {i} failed with exception: {result}\")\n        else:\n            print(f\"Task {i} result: {result}\")\n\nasyncio.run(main())\n```\n\nthis code works for returning exceptions (just ran it)\n\nOne way to quickly test is to put the baml function in one of the tasks and see if that issue fails",
        "timestamp": "2024-08-23 03:57:49.011000+00:00",
        "id": 1276389911213244427,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "hmm, thats odd, let me try and repro rq",
        "timestamp": "2024-08-23 03:55:13.242000+00:00",
        "id": 1276389257870839818,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "richardclove",
        "content": "Yes, I just switched to async right now",
        "timestamp": "2024-08-23 03:54:18.662000+00:00",
        "id": 1276389028945727503,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "are you using the async version btw?",
        "timestamp": "2024-08-23 03:53:14.152000+00:00",
        "id": 1276388758370914324,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "richardclove",
        "content": ".. but, once one of them throw an exception, all of the other ones are no longer running",
        "timestamp": "2024-08-23 03:52:14.525000+00:00",
        "id": 1276388508277145631,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "that will return all exceptions to you",
        "timestamp": "2024-08-23 03:47:41.578000+00:00",
        "id": 1276387363454718016,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "hi <@832768104585101322>, we'll be releasing proper exceptions soon! (likely 1-2 weeks) but for now you just need to wrap it around a try-catch. We're also working on a concept like default value.\n\nIn python you can use `await asyncio.gather(*tasks, return_exceptions=True)`\n\nhttps://docs.python.org/3/library/asyncio-task.html#running-tasks-concurrently",
        "timestamp": "2024-08-23 03:47:32.811000+00:00",
        "id": 1276387326683254815,
        "parent_id": null,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-23 03:47:32.362000+00:00",
        "id": 1276387324799881237,
        "parent_id": 1276382278645452821,
        "thread_id": 1276382278645452821
    },
    {
        "author": "hellovai",
        "content": "BAML + Rust",
        "timestamp": "2024-08-22 13:23:44.024000+00:00",
        "id": 1276169940994359343,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "cat_ethos",
        "content": "is there anyway i can use BAML directly from Rust",
        "timestamp": "2024-08-22 07:26:50.439000+00:00",
        "id": 1276080125909270559,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "hellovai",
        "content": "there's a folder called JSONish in engine you can copy and paste into your repo, but that would mean managing updates is on you!",
        "timestamp": "2024-08-22 15:51:00.561000+00:00",
        "id": 1276207004116979734,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "hellovai",
        "content": "parsing isn't availabe as a module because its deeply coupled to BAML syntax sadly",
        "timestamp": "2024-08-22 15:50:13.285000+00:00",
        "id": 1276206805827194880,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "hellovai",
        "content": "yep! and its a service that you'll be able to run via something like: `baml-cli serve --port 3000`",
        "timestamp": "2024-08-22 15:49:41.693000+00:00",
        "id": 1276206673320743074,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "cat_ethos",
        "content": "by OpenAPI spec do you mean there will be some kind of web service that all language can issue http call to?",
        "timestamp": "2024-08-22 15:02:35.852000+00:00",
        "id": 1276194820884402227,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "cat_ethos",
        "content": "i think i might just need the parsing part in production",
        "timestamp": "2024-08-22 14:48:45.247000+00:00",
        "id": 1276191337074528380,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "hellovai",
        "content": "(if you are really committed, what one team did is clone BAML and use it directly! but we don't provide any guarantees on a stable interface there)",
        "timestamp": "2024-08-22 13:24:54.662000+00:00",
        "id": 1276170237271736444,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "hellovai",
        "content": "We're close to releasing a version of BAML that generates an OpenAPI spec which will then work with every language. As of now, we don't have a rust crate we expose 😢",
        "timestamp": "2024-08-22 13:23:44.377000+00:00",
        "id": 1276169942475214972,
        "parent_id": null,
        "thread_id": 1276080125909270559
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-22 13:23:44.024000+00:00",
        "id": 1276169940994359338,
        "parent_id": 1276080125909270559,
        "thread_id": 1276080125909270559
    },
    {
        "author": "jawnathonjones",
        "content": "Has anyone tried hitting a fine-tuned OpenAI model with BAML?",
        "timestamp": "2024-08-21 19:35:41.138000+00:00",
        "id": 1275901157889413184,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": "jawnathonjones",
        "content": "This is super helpful!",
        "timestamp": "2024-08-21 19:44:05.401000+00:00",
        "id": 1275903272921595986,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": "jawnathonjones",
        "content": "Riiiight ofc ofc. Thanks!",
        "timestamp": "2024-08-21 19:43:52.028000+00:00",
        "id": 1275903216831303722,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": ".aaronv",
        "content": "no, since promptfiddle uses our own API keys in our own org, but if you download the VSCode extension you can setup your own API keys there",
        "timestamp": "2024-08-21 19:43:17.799000+00:00",
        "id": 1275903073264340992,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": "jawnathonjones",
        "content": "Is it supported in prompt fiddle?",
        "timestamp": "2024-08-21 19:42:52.338000+00:00",
        "id": 1275902966473424998,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": "jawnathonjones",
        "content": "<@201399017161097216> I think you might be my new best friend 😂",
        "timestamp": "2024-08-21 19:42:42.221000+00:00",
        "id": 1275902924039651449,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": ".aaronv",
        "content": "Yess, one of our paying customers in the healthcare space is using fine-tuned gpt4 models. All you need to change is the model name to your finetuned model",
        "timestamp": "2024-08-21 19:36:43.079000+00:00",
        "id": 1275901417688928349,
        "parent_id": null,
        "thread_id": 1275901157889413184
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-21 19:36:42.916000+00:00",
        "id": 1275901417005256795,
        "parent_id": 1275901157889413184,
        "thread_id": 1275901157889413184
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "I also have a pydantic-centric multi-agent app that I am developing using langchain and langgraph.  I am trying to understand how BAML would fit into my app.  My agents typically have a prompt but also a pydantic class that are attached to the agent's model via functions.",
        "timestamp": "2024-08-15 20:23:33.989000+00:00",
        "id": 1273738880306380961,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "hellovai",
        "content": "sent a DM with zoom link!",
        "timestamp": "2024-08-20 21:23:33.268000+00:00",
        "id": 1275565916104429630,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "sure",
        "timestamp": "2024-08-20 21:23:10.467000+00:00",
        "id": 1275565820470366400,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "hellovai",
        "content": "hey <@1195491584066724011> this looks rather odd, mind if we hop on a quick call and we can look at some debug logs?",
        "timestamp": "2024-08-20 21:22:58.188000+00:00",
        "id": 1275565768968372359,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "<@99252724855496704> will jump in on this",
        "timestamp": "2024-08-20 21:22:31.146000+00:00",
        "id": 1275565655546138704,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "I call it like this:\n\nresponse = b.Converse(prompt=future_trips_agent_system_prompt, cal_events=None, messages=message_buffer_strs)",
        "timestamp": "2024-08-20 21:21:32.475000+00:00",
        "id": 1275565409462124696,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "is there type-checking or validation happening in the entry to the function?",
        "timestamp": "2024-08-20 21:20:45.070000+00:00",
        "id": 1275565210630881387,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "hmm, neither of these work",
        "timestamp": "2024-08-20 21:19:43.287000+00:00",
        "id": 1275564951494201450,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-20 21:09:40.350000+00:00",
        "id": 1275562422593257564,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "jinja has this \"none\" type thing that all nulls get converted to",
        "timestamp": "2024-08-20 21:09:23.225000+00:00",
        "id": 1275562350765801514,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "or {% if thing != none %}",
        "timestamp": "2024-08-20 21:09:11.297000+00:00",
        "id": 1275562300736278568,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "use this:\n```\n{% if thing %}\n    {{ thing }}\n    {% endif %}\n```",
        "timestamp": "2024-08-20 21:08:53.953000+00:00",
        "id": 1275562227990265856,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "hmm let me check",
        "timestamp": "2024-08-20 21:06:59.148000+00:00",
        "id": 1275561746463199305,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "it must not be correct because I am getting:\n\nbaml_py.BamlError:   Error: cal_events: Expected type String, got `Null`",
        "timestamp": "2024-08-20 21:06:44.118000+00:00",
        "id": 1275561683422679040,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "is my test for null in the jinja correct?",
        "timestamp": "2024-08-20 21:05:43.076000+00:00",
        "id": 1275561427394105364,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "sometimes, it is None, in which case I don't want it included in the prompt",
        "timestamp": "2024-08-20 21:05:11.584000+00:00",
        "id": 1275561295306948799,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "it's a json string",
        "timestamp": "2024-08-20 21:04:48.339000+00:00",
        "id": 1275561197810356357,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "you can make cal_events of type `string?`",
        "timestamp": "2024-08-20 21:03:02.536000+00:00",
        "id": 1275560754040275048,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Hi <@201399017161097216>  - I am up and going with BAML and have replaced one of my pydantic classes (doing one class at a time).  Question:  Sometimes my BAML function takes a parameter that will be None  'cal_events'.  I want to represent the variable conditionally in the prompt.  \n```\nfunction Converse(prompt: string, cal_events: string, messages: string[]) -> FutureTripsConversation {\n  client GPT4o\n  prompt #\"\n    \n    {{ _.role('user') }}\n    {% for m in messages %}\n    - {{ m }}\n    {% endfor %}\n\n    {{ _.role('system') }}\n    {{ prompt }}\n\n    {% if cal_events != null %}\n      Calendar events in JSON format:\n      {{ cal_events }}\n    {% endif %}\n\n    Extract the following data:\n    {{ ctx.output_format }}\n\n  \"#\n}\n```",
        "timestamp": "2024-08-20 21:01:51.903000+00:00",
        "id": 1275560457784004648,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Thanks, <@201399017161097216> .  The BAML part makes perfect sense.  I'll try to integrate into my langchain/langgraph plumbing today.",
        "timestamp": "2024-08-16 15:16:54.592000+00:00",
        "id": 1274024095507546142,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "the response will be of type `MyPydanticClass`",
        "timestamp": "2024-08-16 00:39:42.409000+00:00",
        "id": 1273803340131864688,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "does that make sense?",
        "timestamp": "2024-08-16 00:39:17.669000+00:00",
        "id": 1273803236364517406,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "So here's how you do this with BAML:\n\nBAML function:\n```\nclass MyPydanticClass {\n hello string\n}\n\nfunction MyFunction(messages: string[]) -> MyPydanticClass {\n  client GPT4o // your client def here\n  prompt #\"\n    Extract the following data. < any thing else you wanna add here>\n    \n    {{ ctx.output_format }}\n    \n    {{ _.role('user') }}\n    {% for m in messages %}\n    - {{ m }}\n    {% endfor %}\n  \"#\n}\n```\n\nNote that i added the user role there for your messages and thats not something you currently do, but its something we've seen improve teh accuracy of the model. Similarly, instead of dumping the messages array directly, I unrolled the array into bullet points.\n\nPython code to call this\n```\nfrom baml_client import b\n\nresponse = b.MyFunction(messages: message_buffer)\n```",
        "timestamp": "2024-08-16 00:39:03.963000+00:00",
        "id": 1273803178877653056,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "How would I pass in the message_buffer?  🤔",
        "timestamp": "2024-08-15 23:11:53.855000+00:00",
        "id": 1273781242214482057,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "I guess I could call a BAML function I define inside of my_runnable()",
        "timestamp": "2024-08-15 23:10:20.731000+00:00",
        "id": 1273780851624382514,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "If I'm not mistaken, BAML invokes the model when you call the function you define, e.g., UseTools().",
        "timestamp": "2024-08-15 23:03:08.025000+00:00",
        "id": 1273779036723613789,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "my LCEL chain invokes the model with both the system prompt and the embedded pydantic class",
        "timestamp": "2024-08-15 23:01:22.449000+00:00",
        "id": 1273778593906032763,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "```from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\n    \ndef my_runnable():\n  llm: ChatOpenAI = ChatOpenAI(model=\"gpt-4o\", temperature=0,           api_key=settings.OPENAI_API_KEY)\n    \n  functions = [MyPydanticClass]\n  llm_embedded_with_pydantic_class = llm.bind_functions(functions,   function_call=\"MyPydanticClass\")\n                \n  my_prompt_template = ChatPromptTemplate.from_messages(\n      [\n         (\"system\", my_system_prompt),\n         MessagesPlaceholder(variable_name=\"messages\"),\n      ]\n  )\n    \n  inputs = {\"messages\": message_buffer}\n    \n  # LCEL stuff\n  chain = my_prompt_template | llm_embedded_with_pydantic_class\n  response = chain.invoke(inputs)  # This is the model invocation\n```",
        "timestamp": "2024-08-15 22:56:23.191000+00:00",
        "id": 1273777338726744190,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Thanks, <@201399017161097216> .  As my diagram shows, the langchain runnable (a python function) uses a system prompt + a model that has a pydantic class embedded in it as a function.  Check out this code snippet:",
        "timestamp": "2024-08-15 22:56:12.034000+00:00",
        "id": 1273777291931156553,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "remember all the `class` definitions in .baml files get converted to pydantic models that you can access from your baml responses, and import the type using `from baml_client.types import MyType`",
        "timestamp": "2024-08-15 22:11:43.232000+00:00",
        "id": 1273766098164256822,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "so this is the same thing as AgentA:\n-An LLM that calls a tool  (WeatherAPI), which always gets returned\n\nSo in your code you just call it like:\n```python\nweather_api_params = b.UseTool(\"...\")\ncall_weather_api(weather_api_params.city, weather_api_params.timeOfDay)\n```\n\nIf your Agent A uses more than one tool then you can still have the LLM BAML Fucntion return two different responses instead using a union type (Tool1 | Tool2), you jus thave to check which one got returned:\n```\nres  = b.UseTools(\"...\")\nif (isinstance(res, WeatherApi):\n  // call the weather api with the params\nelse:\n...\n```\n\nIf you need more in depth guidance id be happy to chat on a call if that's easier, or just lmk what details you're looking for",
        "timestamp": "2024-08-15 22:10:20.711000+00:00",
        "id": 1273765752045965396,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "ok one sec let me see what bind_functions does",
        "timestamp": "2024-08-15 22:02:48.689000+00:00",
        "id": 1273763856128409693,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "The good news is I think I try BAML on one of the simpler agents to test it out.",
        "timestamp": "2024-08-15 22:00:14.859000+00:00",
        "id": 1273763210918494260,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "I am trying to wrap my head around how I swap in BAML in place of the Pydantic classes.",
        "timestamp": "2024-08-15 21:59:46.808000+00:00",
        "id": 1273763093264203886,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Given a langchain runnable that has a prompt and a function-enabled LLM (function is a pydantic class)....",
        "timestamp": "2024-08-15 21:57:05.624000+00:00",
        "id": 1273762417209245879,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "The LangGraph part is not important",
        "timestamp": "2024-08-15 21:56:21.936000+00:00",
        "id": 1273762233968754688,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "",
        "timestamp": "2024-08-15 21:55:43.175000+00:00",
        "id": 1273762071393079446,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Here's a simplified diagram of my setup",
        "timestamp": "2024-08-15 21:55:26.854000+00:00",
        "id": 1273762002937970739,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": "noble_fawn_80154_44873",
        "content": "Thanks, <@201399017161097216> .  Agree that \"Agents at the end of the day are just \"LLM functions\" chained together running various prompts in sequence (or in a graph), potentially inside some while-loop.\"",
        "timestamp": "2024-08-15 21:55:04.513000+00:00",
        "id": 1273761909232894024,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "And what are the kinds of features you find useful from Langgraph? Agents at the end of the day are just \"LLM functions\" chained together running various prompts in sequence (or in a graph), potentially inside some while-loop",
        "timestamp": "2024-08-15 20:27:27.321000+00:00",
        "id": 1273739858971459604,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "if you can show me what kind of code you're trying to write I can give you more examples -- do you need an example using langgraph specifically? Or LCEL from langchain?",
        "timestamp": "2024-08-15 20:26:21.344000+00:00",
        "id": 1273739582243999845,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "You can just create specific BAML functions that plug in to the rest of your langchain code.\n\nYou can do any kind of tool calling in those BAML function definitions like this:\nhttps://docs.boundaryml.com/docs/snippets/functions/function-calling#choosing-multiple-tools \n\nFor any types you define in BAML it generates pydantic models for you to leverage elsewhere",
        "timestamp": "2024-08-15 20:25:41.691000+00:00",
        "id": 1273739415927390350,
        "parent_id": null,
        "thread_id": 1273738880306380961
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-15 20:25:41.460000+00:00",
        "id": 1273739414958506004,
        "parent_id": 1273738880306380961,
        "thread_id": 1273738880306380961
    },
    {
        "author": "richardclove",
        "content": "I want to write a preview so our app users can write their own prompts. Is there a way to output the compiled input prompt for Python? I know it showed up on the playground but is there a code version?",
        "timestamp": "2024-08-15 16:52:21.997000+00:00",
        "id": 1273685730119385210,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "hellovai",
        "content": "if you click on it, you should be able to join",
        "timestamp": "2024-08-15 17:00:19.155000+00:00",
        "id": 1273687731465228369,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "hellovai",
        "content": "online on office hours channel! 🙂",
        "timestamp": "2024-08-15 16:59:05.122000+00:00",
        "id": 1273687420948316181,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "richardclove",
        "content": "That'll be great",
        "timestamp": "2024-08-15 16:58:53.354000+00:00",
        "id": 1273687371589881916,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "hellovai",
        "content": "also since this is a more complex usecase, glad to talk in office hours and share more information?",
        "timestamp": "2024-08-15 16:58:42.608000+00:00",
        "id": 1273687326517887078,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "richardclove",
        "content": "I want the user to provide context  (domain knowledge specific to user/org) and i can use it as input/parameter for the LLM function",
        "timestamp": "2024-08-15 16:58:31.784000+00:00",
        "id": 1273687281118478551,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "hellovai",
        "content": "hi! Yes its def possible. what kinds of prompts do you want to support?\n\nDo you mean full BAML files? With the schema and prompt syntax? Or just adding some additional context into prompts you have?",
        "timestamp": "2024-08-15 16:57:10.042000+00:00",
        "id": 1273686938267811942,
        "parent_id": null,
        "thread_id": 1273685730119385210
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-15 16:57:09.764000+00:00",
        "id": 1273686937101926490,
        "parent_id": 1273685730119385210,
        "thread_id": 1273685730119385210
    },
    {
        "author": "kdub03",
        "content": "Was there a sample or example on going from Pydantic -> BAML somewhere? We have a codebase of pydantic model extractions that I'd like to just test with BAML.",
        "timestamp": "2024-08-15 16:26:30.305000+00:00",
        "id": 1273679221851426908,
        "parent_id": null,
        "thread_id": 1273679221851426908
    },
    {
        "author": "hellovai",
        "content": "sadly we don't have a quick way to do this. How complex is your schema?\n\nfor most schemas, I usually just dump some BAML schemas into chat gpt as context then I say:\n\n```\n<baml schemas>\n\ntranslate this pydantic code to the above format\n<pydantic models>\n```",
        "timestamp": "2024-08-15 16:29:20.747000+00:00",
        "id": 1273679936736985210,
        "parent_id": null,
        "thread_id": 1273679221851426908
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-15 16:29:20.288000+00:00",
        "id": 1273679934811799643,
        "parent_id": 1273679221851426908,
        "thread_id": 1273679221851426908
    },
    {
        "author": "richardclove",
        "content": "Is it possible to put test cases in a different file? or better yet.. what do you suggest when we have really long tests or many test cases? What are the best practices here?",
        "timestamp": "2024-08-15 05:58:28.219000+00:00",
        "id": 1273521171727519817,
        "parent_id": null,
        "thread_id": 1273521171727519817
    },
    {
        "author": "richardclove",
        "content": "I understand. It's automatically imported. Thanks",
        "timestamp": "2024-08-15 16:54:57.273000+00:00",
        "id": 1273686381394399383,
        "parent_id": null,
        "thread_id": 1273521171727519817
    },
    {
        "author": ".aaronv",
        "content": "Yeah! You can put tests wherever. Some people create one file per test or others create a directory for all the tests. We dont have an official guideline but splitting things up into different files or a separate long test file can make things more mnageable",
        "timestamp": "2024-08-15 06:00:01.456000+00:00",
        "id": 1273521562791841843,
        "parent_id": null,
        "thread_id": 1273521171727519817
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-15 06:00:01.084000+00:00",
        "id": 1273521561231560816,
        "parent_id": 1273521171727519817,
        "thread_id": 1273521171727519817
    },
    {
        "author": "unsignedint.",
        "content": "Hey folks. Does anyone have an example of a test case that uses/extends a dynamic enum that they could share?",
        "timestamp": "2024-08-14 23:40:54.351000+00:00",
        "id": 1273426154518483043,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "(available in the office hours channel if you'd like to talk through it, or just write things down in this thread)",
        "timestamp": "2024-08-15 00:16:38.477000+00:00",
        "id": 1273435147634606141,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "(nothing official, just free form thoughts would be appriciated)",
        "timestamp": "2024-08-15 00:16:11.249000+00:00",
        "id": 1273435033432100904,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "I agree on that.\n\nif you'd like to chat more about the spec and offer some thoughts, would be very happy to hear your thoughts. We're in the middle of building it and if we have some thoughts that can help shape its direction, would be more than glad.",
        "timestamp": "2024-08-15 00:15:55.901000+00:00",
        "id": 1273434969057788041,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "unsignedint.",
        "content": "Good for now.\n\nThat's an interesting proposal. I'm no PL designer, but I wonder if there is a lot going on, with the predicate loaded into in-line annotations. Looking at the Address example...",
        "timestamp": "2024-08-15 00:12:31.399000+00:00",
        "id": 1273434111314235474,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "Agreed 🙂 we really wish we could do extentions of types trivially in tests but the syntax for it was sadly a bit more nuanced than we expected and requires a full spec similar to <#1265356689796890820>  (hence the delay here).\n\nif it would help, I'd be glad to share some quick and dirty python scripts for easy testing",
        "timestamp": "2024-08-14 23:51:33.650000+00:00",
        "id": 1273428835932704872,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "unsignedint.",
        "content": "> If so, we have an open issue for this (we're hoping to close this by mid sept - theres a few more high prio items atm)\n\nYep, cool.\n\nFor context: we have a junior-engineer playing with prompting and writing test cases in the playground. It's been really useful for experimentation. The goal was to lock in the BAML with playground-driven development, then generate client and move to python when we're ready to build the pipelines.",
        "timestamp": "2024-08-14 23:49:53.215000+00:00",
        "id": 1273428414677778476,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "If so, we have an open issue for this (we're hoping to close this by mid sept - theres a few more high prio items atm)\nhttps://github.com/BoundaryML/baml/issues/711\n\nBut the recommendation for testing dynamic types is to write the tests in a python/ts script for now.",
        "timestamp": "2024-08-14 23:45:43.615000+00:00",
        "id": 1273427367779369090,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "oh do you mean a test case within BAML itself?",
        "timestamp": "2024-08-14 23:44:03.182000+00:00",
        "id": 1273426946532839434,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "`tb.Tools.add_value(new_value)`",
        "timestamp": "2024-08-14 23:43:44.124000+00:00",
        "id": 1273426866598051891,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "https://github.com/BoundaryML/example-massive-categorizer/blob/main/classifier.ipynb\n\nsepcifically:\n\n```python\n@trace\nasync def classify(tool: str, description: str) -> Classification:\n    root_categories = get_best_categories(tool, description)\n    # Filter for all categories which are children of the root categories\n    tb = TypeBuilder()\n    for _, row in df.iterrows():\n        if row[\"Parent\"] in root_categories:\n            tb.Tools.add_value(row['Categories'])\n    selected = await b.Classify(tool, description, count=1, baml_options={ \"tb\": tb })\n    if len(selected) == 0:\n        return None\n    return selected[0]\n```\n\nthe baml defintion:\n```\n// Defining a data model.\nenum Tools {\n  // We'll define these in python\n  @@dynamic\n\n  @@alias(ToolCategory)\n}\n```",
        "timestamp": "2024-08-14 23:43:01.969000+00:00",
        "id": 1273426689786908746,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "sharing a repo! one sec",
        "timestamp": "2024-08-14 23:41:23.293000+00:00",
        "id": 1273426275909894167,
        "parent_id": null,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-14 23:41:23.049000+00:00",
        "id": 1273426274886352998,
        "parent_id": 1273426154518483043,
        "thread_id": 1273426154518483043
    },
    {
        "author": "hellovai",
        "content": "Debugging ClientRegistry",
        "timestamp": "2024-08-14 19:07:12.310000+00:00",
        "id": 1273357275485962365,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": ".alex4o",
        "content": "Also a side note it might be good to support generating imports that include the file extension for typescript because tsc does not want to compile the client",
        "timestamp": "2024-08-14 19:05:28.063000+00:00",
        "id": 1273356838242353204,
        "parent_id": null,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".alex4o",
        "content": "yes, I just wanted to report it",
        "timestamp": "2024-08-14 19:10:54.680000+00:00",
        "id": 1273358208173477948,
        "parent_id": null,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".aaronv",
        "content": "interesting -- so with this you're unblocked right? I'll take a look at our esm compatibility! Thanks for reporting",
        "timestamp": "2024-08-14 19:09:53.726000+00:00",
        "id": 1273357952513740800,
        "parent_id": null,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".alex4o",
        "content": "Yes we did have to modify them because we use esm. \nI fixed it by doing:\n```\n    \"module\": \"Preserve\",\n    \"moduleResolution\": \"Bundler\",\n```",
        "timestamp": "2024-08-14 19:08:21.703000+00:00",
        "id": 1273357566541299812,
        "parent_id": null,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".aaronv",
        "content": "have you tried using a library like https://ui.shadcn.com/docs/components/dialog ? I'm guessing you have to modify those imports as well no?",
        "timestamp": "2024-08-14 19:06:52.322000+00:00",
        "id": 1273357191650345010,
        "parent_id": null,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".aaronv",
        "content": "what are your TS config settings? And what version of TS are you on? Are you also using nextjs?",
        "timestamp": "2024-08-14 19:06:24.168000+00:00",
        "id": 1273357073563910256,
        "parent_id": null,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-14 19:06:23.678000+00:00",
        "id": 1273357071508836412,
        "parent_id": 1273356838242353204,
        "thread_id": 1273356838242353204
    },
    {
        "author": ".alex4o",
        "content": "Hey I am trying to use BAML but I need to provide the api_key during the runtime because I package the client in a typescript library.\n```typescript\n  const clientRegistry = new ClientRegistry();\n\n  clientRegistry.addLlmClient(\"GPT-4o\", \"openai\", {\n    model: \"gpt-4o\",\n    temperature: 0.7,\n    api_key: key,\n  });\n\n  clientRegistry.setPrimary(\"GPT-4o\");\n\n  const res = await b.ExtractResume(\n    \"Mark gonzalez, mark@hello.com. python. 5 years.\",\n    { clientRegistry }\n  );\n\n  return res;\n```\nThis is what I do and I still get an error from open ai saying there is no bearer token",
        "timestamp": "2024-08-14 18:54:32.269000+00:00",
        "id": 1273354087643091035,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "ah 🙂",
        "timestamp": "2024-08-14 19:18:29.062000+00:00",
        "id": 1273360113989845064,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": ".alex4o",
        "content": "i had a typo in the openai key env var",
        "timestamp": "2024-08-14 19:18:24.218000+00:00",
        "id": 1273360093672640604,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "what was the bug btw?",
        "timestamp": "2024-08-14 19:17:34.258000+00:00",
        "id": 1273359884124950598,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "woo! 🙂",
        "timestamp": "2024-08-14 19:17:30.832000+00:00",
        "id": 1273359869755260968,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": ".alex4o",
        "content": "thank you BAML_LOG helped me fix it",
        "timestamp": "2024-08-14 19:17:23.836000+00:00",
        "id": 1273359840412172310,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "also glad to hop on Office hours real quick and see if theres something small being missed",
        "timestamp": "2024-08-14 19:09:33.136000+00:00",
        "id": 1273357866153148556,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "during execution",
        "timestamp": "2024-08-14 19:09:15.325000+00:00",
        "id": 1273357791448399954,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": ".alex4o",
        "content": "should I do it when generating the client or when executing?",
        "timestamp": "2024-08-14 19:09:07.960000+00:00",
        "id": 1273357760557482075,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "Let me take a look at this! can you try running with the following env variable in your shell\n\n```\nBAML_LOG=debug\n```\n\nThat should print out the raw request we are making",
        "timestamp": "2024-08-14 19:07:12.718000+00:00",
        "id": 1273357277197369364,
        "parent_id": null,
        "thread_id": 1273354087643091035
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-14 19:07:12.166000+00:00",
        "id": 1273357274882244711,
        "parent_id": 1273354087643091035,
        "thread_id": 1273354087643091035
    },
    {
        "author": "kdub03",
        "content": "Any thoughts on anthropic caching? https://www.anthropic.com/news/prompt-caching",
        "timestamp": "2024-08-14 18:22:07.256000+00:00",
        "id": 1273345929667411988,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "(this is live on promptfiddle already tho so you can test it there)",
        "timestamp": "2024-08-24 16:26:57.107000+00:00",
        "id": 1276940825057820792,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "FYI, while the PR is merged in, we still have one bug we're tracking to figure out before we can release (unrelated to this). I'll message on announcements when we update and issue a release",
        "timestamp": "2024-08-24 16:17:41.108000+00:00",
        "id": 1276938493028990986,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "We're working on such things and providing the raw HTTP response back. we expect that to be merged in in about 2 weeks",
        "timestamp": "2024-08-24 16:17:04.102000+00:00",
        "id": 1276938337814446110,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "yungweedle",
        "content": "is there a way to access the other items from the response, like anthropic returns number of cache tokens etc",
        "timestamp": "2024-08-24 16:15:27.594000+00:00",
        "id": 1276937933030690908,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "https://github.com/BoundaryML/baml/pull/893\n\nPr is finally out and likely will land today!",
        "timestamp": "2024-08-23 14:45:41.035000+00:00",
        "id": 1276552952298999901,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "but I'm just doing a few small things like fixing the UI so that when you have an openai model it shows you that we ignore cache_policy (BAML already does that, but just making the playground visualize it correctly atm)",
        "timestamp": "2024-08-22 22:01:56.951000+00:00",
        "id": 1276300354182709298,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "(on local build)",
        "timestamp": "2024-08-22 22:01:12.378000+00:00",
        "id": 1276300167229739193,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "(from my machine)",
        "timestamp": "2024-08-22 22:01:01.657000+00:00",
        "id": 1276300122262732952,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "yes! its pretty much working now 🙂",
        "timestamp": "2024-08-22 22:00:56.663000+00:00",
        "id": 1276300101316378757,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "yungweedle",
        "content": "is this one coming out soon 😀",
        "timestamp": "2024-08-22 22:00:34.124000+00:00",
        "id": 1276300006780964927,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "this actually isn't released yet! We were hoping to get it out today, but it should go out in 0.54 (planned for tmrw or day after!)",
        "timestamp": "2024-08-19 20:04:02.587000+00:00",
        "id": 1275183518552555560,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "yungweedle",
        "content": "cache_control parameter needs the update to work?",
        "timestamp": "2024-08-19 17:18:05.462000+00:00",
        "id": 1275141755343343626,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "ah great.  I think in that case the approach here will work for you\n```\n{{ _.role(\"user\") }}\nYour uncached content\n{{ _.role(\"user\", cache_control= {...}) }}\ncached content\n{{ _.role(\"user\") }}\nYour uncached content\n```",
        "timestamp": "2024-08-14 18:29:43.486000+00:00",
        "id": 1273347843234463744,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "kdub03",
        "content": "I haven't tested it yet, but it reads like they do cache tools, which is what I don't want. Since I want to basically cache a long context, then hit it with many structured outputs.",
        "timestamp": "2024-08-14 18:28:25.394000+00:00",
        "id": 1273347515693006980,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "so it should work for all messages in system / your prompt.\n\nThat said we'll be optionally supporting tools API soon and you should see that land in about 2 weeks.",
        "timestamp": "2024-08-14 18:28:18.691000+00:00",
        "id": 1273347487578722445,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "yep! we dont support using tools atm",
        "timestamp": "2024-08-14 18:27:41.881000+00:00",
        "id": 1273347333186392086,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "kdub03",
        "content": "BAML is not using tools for anthropic, correct? \n        |Cache prefixes are created in the following order: tools, system, then messages.",
        "timestamp": "2024-08-14 18:27:25.743000+00:00",
        "id": 1273347265498447904,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "in the future, you'll be able to add arbitrary keys here (not just anthropic)",
        "timestamp": "2024-08-14 18:24:05.835000+00:00",
        "id": 1273346427023527936,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "yes! expect BAML to support this by next monday!\n\n```\n_.role(\"user\", cache_control={...})\n```",
        "timestamp": "2024-08-14 18:22:47.638000+00:00",
        "id": 1273346099041669170,
        "parent_id": null,
        "thread_id": 1273345929667411988
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-14 18:22:47.242000+00:00",
        "id": 1273346097380593755,
        "parent_id": 1273345929667411988,
        "thread_id": 1273345929667411988
    },
    {
        "author": "feres0902",
        "content": "im trying to integrate baml with vllm and after hardcoding the baml pormpt the only thing left is to parse the llm answer, is there a way to do it in python",
        "timestamp": "2024-08-14 07:17:52.913000+00:00",
        "id": 1273178768437153833,
        "parent_id": null,
        "thread_id": 1273178768437153833
    },
    {
        "author": "feres0902",
        "content": "hello <@99252724855496704> hope you doing well, \nis there any news about the parser availability?",
        "timestamp": "2024-08-26 06:57:38.089000+00:00",
        "id": 1277522327483187282,
        "parent_id": null,
        "thread_id": 1273178768437153833
    },
    {
        "author": "feres0902",
        "content": "Yes exactly, i want to use the baml parser only, \ni had 2 usecases: \n1rst predict summary and solution from chat and i was able to do it by using baml extenstion to get the baml prompt ( + small adjustments like adding the <s>[INST] {prompt} [/INST] ) \nthen apply as a function to all chats to get list of prompts that i sent to vllm opeai compatible client and get list of reposnes then to parse those i just load the text to json and use a pydantic class to get thel to the exact object i want, this works welll for this usecase and i alsmost get summaries and  solution for every chat, \nin my second usecase im trying to perform multilabel classfication and in this case the pydantic + json parsing solution stops working alsmost and i want to use Baml parser as its just much better at dealing with raw llm outputs",
        "timestamp": "2024-08-14 08:32:35.881000+00:00",
        "id": 1273197571367763988,
        "parent_id": 1273179281983537162,
        "thread_id": 1273178768437153833
    },
    {
        "author": "hellovai",
        "content": "as of right now, we don't explictly expose the parser, but we should be able to do this for you!",
        "timestamp": "2024-08-14 07:19:55.352000+00:00",
        "id": 1273179281983537162,
        "parent_id": null,
        "thread_id": 1273178768437153833
    },
    {
        "author": "hellovai",
        "content": "hi feres, could you outline what you mean by do it in python? Do you mean use only the parser?",
        "timestamp": "2024-08-14 07:18:52.199000+00:00",
        "id": 1273179017100529738,
        "parent_id": null,
        "thread_id": 1273178768437153833
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-14 07:18:51.740000+00:00",
        "id": 1273179015175340033,
        "parent_id": 1273178768437153833,
        "thread_id": 1273178768437153833
    },
    {
        "author": "dokudoge",
        "content": "Hi, I was looking through the website, and for some reason the demo video isn't playing for me\n\nhttps://docs.boundaryml.com/docs/get-started/what-is-baml\n\nthanks!",
        "timestamp": "2024-08-13 22:10:35.267000+00:00",
        "id": 1273041037367181352,
        "parent_id": null,
        "thread_id": 1273041037367181352
    },
    {
        "author": ".aaronv",
        "content": "thanks for the heads-up!",
        "timestamp": "2024-08-13 22:16:59.719000+00:00",
        "id": 1273042649875480658,
        "parent_id": null,
        "thread_id": 1273041037367181352
    },
    {
        "author": "dokudoge",
        "content": "ya I'm hotspot rn, so maybe that's it, but definitely worth looking into",
        "timestamp": "2024-08-13 22:16:39.544000+00:00",
        "id": 1273042565255659602,
        "parent_id": null,
        "thread_id": 1273041037367181352
    },
    {
        "author": "dokudoge",
        "content": "ya that works, tysm!",
        "timestamp": "2024-08-13 22:16:16.084000+00:00",
        "id": 1273042466857291778,
        "parent_id": null,
        "thread_id": 1273041037367181352
    },
    {
        "author": ".aaronv",
        "content": "does this work? https://boundaryml.wistia.com/medias/5fxpquglde",
        "timestamp": "2024-08-13 22:12:28.331000+00:00",
        "id": 1273041511591841874,
        "parent_id": null,
        "thread_id": 1273041037367181352
    },
    {
        "author": "hellovai",
        "content": "oh no! thanks for flagging. it seems to be working on my end, I'll get you a direct link 🙂",
        "timestamp": "2024-08-13 22:11:46.284000+00:00",
        "id": 1273041335233941607,
        "parent_id": null,
        "thread_id": 1273041037367181352
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-13 22:11:46.013000+00:00",
        "id": 1273041334097412303,
        "parent_id": 1273041037367181352,
        "thread_id": 1273041037367181352
    },
    {
        "author": "mz5910",
        "content": "ClientRegistry parameters",
        "timestamp": "2024-08-12 19:57:01.783000+00:00",
        "id": 1272645038513324084,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Thanks!",
        "timestamp": "2024-08-14 19:12:17.882000+00:00",
        "id": 1273358557148086393,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "yes! you can just change your return type of the function to be a `string` instead of a class and then you can prompt engineer it to return markdown",
        "timestamp": "2024-08-14 19:11:44.546000+00:00",
        "id": 1273358417326768220,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "I tried to manipulate template string but that didn't seem to work.",
        "timestamp": "2024-08-14 19:11:41.861000+00:00",
        "id": 1273358406064935003,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Hi Vaibhav! Thanks for the help again yesterday. I had a question. My output currently looks like this:\n```{\n  \"title\": \"Executive Update on National Security and Defense\",\n  \"sections\": [\n    {\n      \"title\": \"Appropriations Bills\",\n      \"summary\": \"**Appropriations Bills**: The U.S. Congress is considering three critical appropriations bills for fiscal year 2025: the Homeland Security Appropriations Act, the Defense Appropriations Act, and the State Foreign Operations and Related Programs Appropriations Act. These bills are designed to enhance border security, strengthen national defense, and improve the U.S.'s global standing.\"\n    },\n    {\n      \"title\": \"International Defense Cooperation\",\n      \"summary\": \"**Czech Republic**: The U.S. is supporting a major defense acquisition by the Czech Republic to bolster its homeland defense and deter regional threats, enhancing the security of this key NATO ally.\"\n    }\n  ]\n}\n```\n\nIs there a way I can get it to return like this:\n```**Executive Update on National Security and Defense**\n\n1. **Appropriations Bills**: The U.S. Congress is considering three critical appropriations bills for fiscal year 2025: the Homeland Security Appropriations Act, the Defense Appropriations Act, and the State Foreign Operations and Related Programs Appropriations Act. These bills are designed to enhance border security, strengthen national defense, and improve the U.S.'s global standing.\n\n2. **International Defense Cooperation**:\n   - **Czech Republic**: The U.S. is supporting a major defense acquisition by the Czech Republic to bolster its homeland defense and deter regional threats, enhancing the security of this key NATO ally.\n```",
        "timestamp": "2024-08-14 19:10:58.442000+00:00",
        "id": 1273358223952576595,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "on the left side panel, just click on there and it should join",
        "timestamp": "2024-08-13 20:39:24.866000+00:00",
        "id": 1273018092842385490,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Let me know how to join pls?",
        "timestamp": "2024-08-13 20:39:02.593000+00:00",
        "id": 1273017999422394429,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "yea that might be easier 🙂",
        "timestamp": "2024-08-13 20:38:42.837000+00:00",
        "id": 1273017916559720448,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Happy to jump on office hours is easier to solve?",
        "timestamp": "2024-08-13 20:38:35.073000+00:00",
        "id": 1273017883995275317,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "oh, ok! got it.\n\nThat's odd. I'll file a bug and see why thats the case.\n\nCan you share your current baml version so i can include that on the ticket.\nAlong with python version you have?",
        "timestamp": "2024-08-13 20:38:29.283000+00:00",
        "id": 1273017859710124153,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "And when I print raw, I actually do not see the parameters etc like temperature that I passed.",
        "timestamp": "2024-08-13 20:37:27.159000+00:00",
        "id": 1273017599143186597,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "I don't see that in the terminal. I actually print `raw` in `sync_client.py` to get the response you are showing.",
        "timestamp": "2024-08-13 20:36:22.950000+00:00",
        "id": 1273017329831252142,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "(in this case it shows BAML failed to parse, but in your case, it will show the actual parsed response)",
        "timestamp": "2024-08-13 20:34:50.976000+00:00",
        "id": 1273016944064462901,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "oh,, I see. \n\nwhen you run your python function, it will dump it into termianl.\nyou should see something like this:",
        "timestamp": "2024-08-13 20:34:25.199000+00:00",
        "id": 1273016835947630775,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "How do I run the logs?",
        "timestamp": "2024-08-13 20:33:12.736000+00:00",
        "id": 1273016532015779924,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "i can hop on Office hours and take a look!",
        "timestamp": "2024-08-13 20:31:16.177000+00:00",
        "id": 1273016043131899965,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "is that not working when you run the logs?",
        "timestamp": "2024-08-13 20:31:04.527000+00:00",
        "id": 1273015994268385291,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "I have done that..",
        "timestamp": "2024-08-13 20:30:50.808000+00:00",
        "id": 1273015936726859869,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "can you type this in your terminal:\n\n```\nexport BAML_LOG=info\n```",
        "timestamp": "2024-08-13 20:29:38.011000+00:00",
        "id": 1273015631394111508,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "it should dump into your terminal.",
        "timestamp": "2024-08-13 20:29:16.529000+00:00",
        "id": 1273015541291946034,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "```python\n    os.environ['BAML_LOG'] = info\n```",
        "timestamp": "2024-08-13 20:09:31.246000+00:00",
        "id": 1273010569854582784,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "How will I see the output of the log?",
        "timestamp": "2024-08-13 20:07:24.759000+00:00",
        "id": 1273010039329919056,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Ok!",
        "timestamp": "2024-08-13 20:06:00.060000+00:00",
        "id": 1273009684076564613,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "just in your shell environment  (wherever you're running your python code)",
        "timestamp": "2024-08-13 20:05:28.623000+00:00",
        "id": 1273009552220229754,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": ".aaronv",
        "content": "no, that one already shows you the raw output in ther esults",
        "timestamp": "2024-08-13 20:04:56.480000+00:00",
        "id": 1273009417402712066,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "I should set the BAML_LOG variable in the playground?",
        "timestamp": "2024-08-13 20:04:43.579000+00:00",
        "id": 1273009363291996251,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "I am using the client registry so let me set the environment variable.",
        "timestamp": "2024-08-13 20:01:41.013000+00:00",
        "id": 1273008597554429973,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Ok thanks!",
        "timestamp": "2024-08-13 20:01:18.309000+00:00",
        "id": 1273008502327214140,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "yes! but if you are using the client registry, then you won't see it. Then you'll have to dump out the logs yourself with the env var that i said",
        "timestamp": "2024-08-13 20:01:07.025000+00:00",
        "id": 1273008454998429789,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Thanks! I should be seeing the parameters I passed i.e. temperature etc in the raw CURL right?",
        "timestamp": "2024-08-13 20:00:36.849000+00:00",
        "id": 1273008328431239240,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "(the reason is likely thanks to the BAML parser)\n\nThat parser is able to remove anything but the structured response you want.\n\ne.g. the model includes some prefix text prior to your actually structure, the parser will remove the prefix text.\n\nfor more introspection on BAML, you can set the env var: `BAML_LOG=info` and that will dump out more information for you",
        "timestamp": "2024-08-13 19:38:18.339000+00:00",
        "id": 1273002714313523334,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "thats ineteresting. BAML has a quick: \"Raw curl\" button.\n\nAre you saying the Raw CURL is much more verbose than after BAML returns it?",
        "timestamp": "2024-08-13 19:29:48.927000+00:00",
        "id": 1273000577684475904,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Hey. I am following up regarding parameters to the LLM provider. For a prompt and the parameters below, I am getting a very detailed response from directly calling ChatGPT (role is 'assistant'). However, when I make the same call using BAML, I am getting a very concise response. Could you let me know why that may be? Is there something happening with BAML under the hood?\n\n```python\n        \"model\": \"gpt-4o\",\n        \"default_role\": 'assistant',\n        \"temperature\": 0.7,\n        \"max_tokens\": 512,\n        \"top_p\": 1,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0, ```",
        "timestamp": "2024-08-13 19:27:49.918000+00:00",
        "id": 1273000078524682342,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Ok, thanks!",
        "timestamp": "2024-08-12 22:09:46.559000+00:00",
        "id": 1272678445205295136,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "yep! what you're doing in your example is completely ok.\n\nI would just remove teh messages paramter, and it should \"just work\"",
        "timestamp": "2024-08-12 22:08:31.619000+00:00",
        "id": 1272678130884280355,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Sorry, could you clarify more what you mean by passthrough paramaters? Can I set specific values for these as I have done above?",
        "timestamp": "2024-08-12 22:08:04.445000+00:00",
        "id": 1272678016908001330,
        "parent_id": 1272675322872991894,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "in this example, I used teh output format in system prompt and the user's content as user. \n\nThe reason is that the model is trained to understand instructions in the system prompt, so it allows us to take advantage of that training (at least for the model).\n\nAlso, yes, if you set up the `default_role` as `user` it will make the two below exactly the same:\n\n```\nfunction my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ _.role('user') }}\n    {{ ctx.output_format}}\n \n    {{ input }}\n  \"#\n}\n```\n\n\n```\nfunction my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ ctx.output_format}}\n \n    {{ input }}\n  \"#\n}\n```",
        "timestamp": "2024-08-12 22:00:43.058000+00:00",
        "id": 1272676165596876911,
        "parent_id": 1272646105703649310,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "yea, all other options aside from those we mention as `DO NOT USE` are supported as passthrough paramters.",
        "timestamp": "2024-08-12 21:57:22.137000+00:00",
        "id": 1272675322872991894,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Also, I noticed that there is a default_role option so if I set that to 'User' that should be same as setting the role to user as in the code above?",
        "timestamp": "2024-08-12 21:33:15.559000+00:00",
        "id": 1272669255485358151,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Is there a reason why you have put system as the role for output_format and user for input?",
        "timestamp": "2024-08-12 20:35:56.078000+00:00",
        "id": 1272654829256441906,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "The documentation at the end says to refer to OpenAI documentation for other options and that includes temperature etc. But are those currently not supported?",
        "timestamp": "2024-08-12 20:34:56.426000+00:00",
        "id": 1272654579057688728,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "if you want to modify the role for that prompt, you can do dsomething liek this:\n\n```\nfunction my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ _.role('system') }}\n    {{ ctx.output_format}}\n\n    {{ _.role('user') }}\n    {{ input }}\n  \"#\n}\n```",
        "timestamp": "2024-08-12 20:01:16.221000+00:00",
        "id": 1272646105703649310,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Currently, it's this:\n```function my_function(input: string) -> CustomOutput {\n  client GPT4\n  prompt #\"\n    {{ input }}\n\n    {{ ctx.output_format}}\n  \"#\n}```",
        "timestamp": "2024-08-12 20:00:32.820000+00:00",
        "id": 1272645923666792550,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "if you want to tune that, you can actually passs in `_.role('user') ` or any other roles in your prompt directly.\n\nSee for what parameters we support for `options`:\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/openai\n\nSee here for how to add chat roles to the prompt: https://docs.boundaryml.com/docs/snippets/prompt-syntax/roles",
        "timestamp": "2024-08-12 19:59:20.111000+00:00",
        "id": 1272645618703007804,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "the prompt is assembled from the template string you write in BAML",
        "timestamp": "2024-08-12 19:57:58.611000+00:00",
        "id": 1272645276867493928,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "ah, you can, but messages isn't supported!",
        "timestamp": "2024-08-12 19:57:45.357000+00:00",
        "id": 1272645221276057691,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "mz5910",
        "content": "Hi all. I am using the following code in Python to generate a new client:\n```python\ncr.add_llm_client(\nname=\"OpenAIClient\",\nprovider=\"openai\",\noptions={\n\"api_key\": auth,\n\"model\": model_name,\n\"messages\": [{\"role\": \"assistant\"}],\n\"temperature\": temperature,\n\"max_tokens\": max_tokens,\n\"top_p\": top_p,\n\"frequency_penalty\": frequency_penalty,\n\"presence_penalty\": presence_penalty,\n},\n)\n```\n\nThen later on, I am using this client like this:\n```python\nresult = b.my_function(prompt, {\"client_registry\": cr})\n```\nJust wanted to confirm if the role and other parameters will be sent to the GPT call I make?",
        "timestamp": "2024-08-12 19:57:03.014000+00:00",
        "id": 1272645043676643419,
        "parent_id": null,
        "thread_id": 1272645038513324084
    },
    {
        "author": "hellovai",
        "content": "Hi, I am suddenly getting this error,",
        "timestamp": "2024-08-09 17:01:24.951000+00:00",
        "id": 1271513680240185359,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "Hi, I am suddenly getting this error, but inconsistently, when testing out a prompt that previously worked, and it only fails some of the time. I am using temperature 0 so I don't think the output should be changing across different runs.\n\nUnspecified error code: 2\nFailed to parse event: Error(\"missing field `type`\", line: 0, column: 0)\n\nCheck the webview network tab for more details. Command Palette -> Open webview developer tools.",
        "timestamp": "2024-08-09 16:48:14.720000+00:00",
        "id": 1271510365770875005,
        "parent_id": null,
        "thread_id": 1271510365770875005
    },
    {
        "author": "yungweedle",
        "content": "ok, it looks like it is an error on anthropic side: {\"client_error\":false,\"code\":529,\"detail\":\"Overloaded\"}",
        "timestamp": "2024-08-09 20:20:51.423000+00:00",
        "id": 1271563871261556869,
        "parent_id": null,
        "thread_id": 1271510365770875005
    },
    {
        "author": "yungweedle",
        "content": "Claude 3.5",
        "timestamp": "2024-08-09 20:11:12.519000+00:00",
        "id": 1271561443162062981,
        "parent_id": null,
        "thread_id": 1271510365770875005
    },
    {
        "author": "anish.pi",
        "content": "What provider are you using?",
        "timestamp": "2024-08-09 17:55:21.735000+00:00",
        "id": 1271527256295997514,
        "parent_id": null,
        "thread_id": 1271510365770875005
    },
    {
        "author": "hellovai",
        "content": "hi <@152300469094580224> if possible, can you open developer tools in vscode:\nhttps://docs.boundaryml.com/docs/get-started/debugging/vscode-playground#tests-failing-to-run\n\nThen you can look in the network call and see what request we are making.\n\nAlso one surefire way to check is the button:\n\n```\nRaw CURL\n```\n\nthen you can try running that in terminal a few times. if that works, then we have some odd error",
        "timestamp": "2024-08-09 17:01:25.508000+00:00",
        "id": 1271513682576281750,
        "parent_id": null,
        "thread_id": 1271510365770875005
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-09 17:01:24.951000+00:00",
        "id": 1271513680240185354,
        "parent_id": 1271510365770875005,
        "thread_id": 1271510365770875005
    },
    {
        "author": "gabev2037",
        "content": "maybe a naive question: If I set the max_tokens in the client, does the model take this into account when generating a response? IN other words, if i put 150 token maximum, will the model do it's best to constrain the output to 150 tokens or do I also need to prompt it for that",
        "timestamp": "2024-08-07 21:41:19.869000+00:00",
        "id": 1270859347500597288,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "as of right now, likely yes, but soon we'll allow variables in descriptions directly in BAML and that will make life better",
        "timestamp": "2024-08-07 21:48:14.815000+00:00",
        "id": 1270861087910395977,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "gabev2037",
        "content": "i think the `string[]` is interesting if that will impact something",
        "timestamp": "2024-08-07 21:47:40.760000+00:00",
        "id": 1270860945073242236,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "gabev2037",
        "content": "hmmm is that really better than just throwing it into the prompt?",
        "timestamp": "2024-08-07 21:47:31.557000+00:00",
        "id": 1270860906473062513,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "typebuilder to the rescue 😉 \n\nin BAML\n```\nclass Foo {\n  words string[]\n  @@dyanmic\n}\n```\n\nin python:\n```\ntb = TypeBuilder()\ntb.Foo.words.description(f\"at most {count} words\")\n```",
        "timestamp": "2024-08-07 21:47:07.994000+00:00",
        "id": 1270860807642812426,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "gabev2037",
        "content": "that's an interesting idea",
        "timestamp": "2024-08-07 21:46:02.843000+00:00",
        "id": 1270860534379708519,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "and set the description dynamically",
        "timestamp": "2024-08-07 21:45:55.665000+00:00",
        "id": 1270860504272867328,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "you can do `string[]`",
        "timestamp": "2024-08-07 21:45:48.603000+00:00",
        "id": 1270860474652692613,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "gabev2037",
        "content": "it's dynamic, i feed it as a parameter ot the function",
        "timestamp": "2024-08-07 21:45:43.449000+00:00",
        "id": 1270860453035249665,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "how many words do you want",
        "timestamp": "2024-08-07 21:45:26.487000+00:00",
        "id": 1270860381891465310,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "gabev2037",
        "content": "any tips for constraining a model's response to a word count limit? \n\nSo far i've just been doing it with naive prompting and then having a naive checker LLM which then says \"too short\" or \"too long\" and regenerates. I guess baml doesn't natively support something like this?",
        "timestamp": "2024-08-07 21:44:04.459000+00:00",
        "id": 1270860037840961629,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "nope! It won't take this into account. its purely a limitor on the output tokens",
        "timestamp": "2024-08-07 21:43:01.894000+00:00",
        "id": 1270859775424335997,
        "parent_id": null,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-07 21:43:01.445000+00:00",
        "id": 1270859773541093467,
        "parent_id": 1270859347500597288,
        "thread_id": 1270859347500597288
    },
    {
        "author": "hellovai",
        "content": "Benchmark thread",
        "timestamp": "2024-08-07 20:46:26.042000+00:00",
        "id": 1270845532188967071,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "kdub03",
        "content": "Is the \"Strict\" mode comparing the new structured outputs from OpenAI? Is that function calling only? Is there a comparison with structured outputs w/o function calling, since I think that's closer aligned to what BAML is doing?\nhttps://discord.com/channels/1119368998161752075/1119375433666920530/1270775695420817489",
        "timestamp": "2024-08-07 20:35:49.355000+00:00",
        "id": 1270842861730009109,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "looking forward to chatting tmrw",
        "timestamp": "2024-08-07 21:34:32.073000+00:00",
        "id": 1270857637080207470,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "fyi here's my calendly!\n\nhttps://calendly.com/boundary-founders/connect-45",
        "timestamp": "2024-08-07 21:34:24.224000+00:00",
        "id": 1270857604159115294,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "online now if you'd like!",
        "timestamp": "2024-08-07 20:51:04.374000+00:00",
        "id": 1270846699597725819,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "We're usually on most days!",
        "timestamp": "2024-08-07 20:50:54.992000+00:00",
        "id": 1270846660246769674,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "kdub03",
        "content": "I think that makes sense. I'd love to chat though. Do you do office hours every day? I'm working mapping extraction functions to openapi specs, and leaning towards baml, or part of baml as part of the implementation.",
        "timestamp": "2024-08-07 20:50:22.920000+00:00",
        "id": 1270846525727309845,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "(we'll have a more detailed post on thsi later this week or early next!)",
        "timestamp": "2024-08-07 20:49:14.051000+00:00",
        "id": 1270846236869525575,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "glad to explain why over on office hours if you'd like!",
        "timestamp": "2024-08-07 20:49:03.836000+00:00",
        "id": 1270846194024845502,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "mhm",
        "timestamp": "2024-08-07 20:48:47.733000+00:00",
        "id": 1270846126484094977,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "kdub03",
        "content": "so basically hard forcing the response format seemingly has negative impact on the response?",
        "timestamp": "2024-08-07 20:48:39.126000+00:00",
        "id": 1270846090383593572,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "hope that helps!",
        "timestamp": "2024-08-07 20:47:45.780000+00:00",
        "id": 1270845866634121227,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "JSON mode and function calling are almost exactly the same thing, function calling just has a special way to trigger when JSON mode starts.",
        "timestamp": "2024-08-07 20:47:18.636000+00:00",
        "id": 1270845752784195645,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "We'll be running that soon (the benchmark doesn't currently support comparing it w/o function calling). However, you can see what structured does vs non-structured on this post:\n\nSee JSON mode vs Function calling here.\nhttps://www.boundaryml.com/blog/schema-aligned-parsing#json-mode\nhttps://www.boundaryml.com/blog/schema-aligned-parsing#function-calling\n\nTLDR, my gut says that function calling will likely perform better than JSON mode since json more will almsot FORCE the model to produce an output.",
        "timestamp": "2024-08-07 20:46:26.486000+00:00",
        "id": 1270845534051238042,
        "parent_id": null,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-07 20:46:26.042000+00:00",
        "id": 1270845532188967066,
        "parent_id": 1270842861730009109,
        "thread_id": 1270842861730009109
    },
    {
        "author": "hellovai",
        "content": "Dyanmically settings API keys",
        "timestamp": "2024-08-06 19:49:56.172000+00:00",
        "id": 1270468926177939604,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "mz5910",
        "content": "Hey. I am trying to use BAML inside an AWS Lambda function. How do I set the api_key from AWS Secrets Manager i.e modify this part?\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nCurrently, my Lambda's Python code does retrieve OPENAI_SECRET_KEY from AWS Secrets Manager. I am using BAML 0.44.0 with Python 3.10.",
        "timestamp": "2024-08-06 18:39:36.975000+00:00",
        "id": 1270451229583081615,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Thanks! I'll have a look.",
        "timestamp": "2024-08-08 22:45:03.381000+00:00",
        "id": 1271237772338004000,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "if you're in python, all objects are pydantic models. Which has good JSON compat.\n\nSee https://docs.pydantic.dev/latest/concepts/serialization/",
        "timestamp": "2024-08-08 22:44:30.056000+00:00",
        "id": 1271237632562692161,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Thanks again. A question I had was that can I make my BAML object returned JSON serializable?",
        "timestamp": "2024-08-08 22:43:23.919000+00:00",
        "id": 1271237355164008498,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "^ we've also merged a change that brings our compatibility level down to glibc 2.24, which should work with the image you mentioned (it did on my machine), so that'll also be going out in the next release",
        "timestamp": "2024-08-08 22:30:45.694000+00:00",
        "id": 1271234174937989130,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "No worries - thanks for the support!",
        "timestamp": "2024-08-08 22:30:39.330000+00:00",
        "id": 1271234148245307435,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "awesome news -- sorry you had to spent so long fixing this. We'll eventually run integ tests using Amazon Linux 2 on ARM instances and make sure that works for future users",
        "timestamp": "2024-08-08 22:29:47.087000+00:00",
        "id": 1271233929122283592,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "oh, awesome!",
        "timestamp": "2024-08-08 22:29:20.559000+00:00",
        "id": 1271233817855787049,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Hi everyone. Just wanted to give an update that I changed the base image arch to x86_64 and its now working.",
        "timestamp": "2024-08-08 22:29:00.133000+00:00",
        "id": 1271233732183064577,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "> I am trying to change my base image to 86x64 and will see if that works.\nif you're on an arm mac, i don't think that will work, sadly",
        "timestamp": "2024-08-07 23:22:05.941000+00:00",
        "id": 1270884706564575284,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "OK, let's try this:\n\n- download the attached zip and extract it\n- in the directory where you extract it, run `docker build -t baml-py-lambda .` \n- then run `docker run -it baml-py-lambda` and inside resulting bash shell, `BAML_LOG=debug OPENAI_API_KEY=sk..... python app.py`\n\nAnd let me know which step fails for you and how. It works for me, but it sounds like it might fail for you and I'm confused about what would cause that.",
        "timestamp": "2024-08-07 23:21:37.285000+00:00",
        "id": 1270884586372730891,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "I am trying to change my base image to 86x64 and will see if that works.",
        "timestamp": "2024-08-07 23:21:32.314000+00:00",
        "id": 1270884565522710538,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Appreciate it, thanks!",
        "timestamp": "2024-08-07 22:40:47.396000+00:00",
        "id": 1270874310793498669,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "Ah, that makes a lot of sense. Will dig into this and let you know what I come up with.",
        "timestamp": "2024-08-07 22:40:04.755000+00:00",
        "id": 1270874131944046644,
        "parent_id": 1270873084441919511,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "I tried doing this and got this error:\n`ERROR: baml_py-0.53.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl is not a supported wheel on this platform.`",
        "timestamp": "2024-08-07 22:36:43.481000+00:00",
        "id": 1270873287739572245,
        "parent_id": 1270870864531034114,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "This is our base image: `public.ecr.aws/lambda/python:3.10-arm64`",
        "timestamp": "2024-08-07 22:35:55.011000+00:00",
        "id": 1270873084441919511,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Ok. Let me get back to you.",
        "timestamp": "2024-08-07 22:34:18.506000+00:00",
        "id": 1270872679670349935,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "alt. if you can tell us what base image you're using, i can also take a look at that",
        "timestamp": "2024-08-07 22:33:05.539000+00:00",
        "id": 1270872373624832113,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "ok- can you download this and try to `pip install ./baml_py-0.53.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl` in the image?",
        "timestamp": "2024-08-07 22:27:05.743000+00:00",
        "id": 1270870864531034114,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "let me give you a wheel and see if it works for you",
        "timestamp": "2024-08-07 22:25:47.285000+00:00",
        "id": 1270870535454326877,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Is using glibc 2.26 an option?",
        "timestamp": "2024-08-07 22:25:15.145000+00:00",
        "id": 1270870400649400383,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Let me check and get back to you?",
        "timestamp": "2024-08-07 21:48:12.875000+00:00",
        "id": 1270861079773446218,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "Hi <@929667691525050370> , is there any chance you'd be able to use glibc > 2.28+? It looks like we added a core indirect dependency (a low level web sockets library in RUST), that causes this issue.",
        "timestamp": "2024-08-07 21:45:07.111000+00:00",
        "id": 1270860300622630994,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Thank you for your support! Appreciate it.",
        "timestamp": "2024-08-07 17:20:04.284000+00:00",
        "id": 1270793599331860682,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "<@929667691525050370> thanks for your patience, and for highlighting the discrepancy!",
        "timestamp": "2024-08-07 17:18:47.486000+00:00",
        "id": 1270793277217574952,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "Its just a build script thing things 🙂 we should be able to support 2.17 <@711679663746842796> can you take this on?",
        "timestamp": "2024-08-07 17:14:58.088000+00:00",
        "id": 1270792315052621954,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Is it possible to stay at 2.17 which was the version used with BAML 0.44.0?",
        "timestamp": "2024-08-07 17:14:34.413000+00:00",
        "id": 1270792215752347688,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Just verifying and will get back to you. Was there a specific reason to switch to 2.28?",
        "timestamp": "2024-08-07 17:14:06.442000+00:00",
        "id": 1270792098433597506,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "perfect! We found the issue. We should be able to release a version that updates this!\n\nCan you share what version you are?",
        "timestamp": "2024-08-07 17:11:54.165000+00:00",
        "id": 1270791543623516191,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Yes, we use a version less than 2.28.",
        "timestamp": "2024-08-07 17:11:28.569000+00:00",
        "id": 1270791436266377236,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "oh i see. It appears you have glibc version < 2.28!\n\nCan you run:\n\n```\nldd --version\n```",
        "timestamp": "2024-08-07 17:04:01.993000+00:00",
        "id": 1270789563190874172,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "which docker image are you using as well?",
        "timestamp": "2024-08-07 17:03:11.857000+00:00",
        "id": 1270789352905117818,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "I realize that I may need to update my docker image. But wanted to confirm if there was a specific reason to switch to `baml_py-0.53.0-cp38-abi3-manylinux_2_28_aarch64.whl`?",
        "timestamp": "2024-08-07 17:02:56.664000+00:00",
        "id": 1270789289181052938,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "So the last version that was working for me was BAML 0.44.0 which uses this:\n`baml_py-0.44.0-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl `",
        "timestamp": "2024-08-07 17:02:22.246000+00:00",
        "id": 1270789144821366794,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "macOS 14.4.1 (23E224)",
        "timestamp": "2024-08-07 17:01:12.822000+00:00",
        "id": 1270788853636272290,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "whats your OS? Linux arm64?",
        "timestamp": "2024-08-07 17:00:19.690000+00:00",
        "id": 1270788630784249886,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "A follow up question I had was that for BAML 0.53.0, I notice this as the arhictecture:\n`baml_py-0.53.0-cp38-abi3-manylinux_2_28_aarch64.whl`. Is there a reason why you switched to this?",
        "timestamp": "2024-08-07 16:59:11.610000+00:00",
        "id": 1270788345236164739,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Yes, I have done that.",
        "timestamp": "2024-08-07 16:45:03.024000+00:00",
        "id": 1270784786008510514,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "ah! It looks like you are using a different python env here. You may need to do `pip3 install baml-py`",
        "timestamp": "2024-08-07 16:44:48.821000+00:00",
        "id": 1270784726436937810,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Hi Vaibhav. This is the result I got by running the commands in the makefile above. The client has been generated:\n`make check_baml`\n`pip3 show baml-cli`\n`WARNING: Package(s) not found: baml-cli`",
        "timestamp": "2024-08-07 16:44:13.020000+00:00",
        "id": 1270784576276664392,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "and also `python -c 'import baml_py; print(baml_py)'` in your make file after pip show",
        "timestamp": "2024-08-06 22:10:04.686000+00:00",
        "id": 1270504194041843753,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "wait, you mean running:\n`baml-cli generate` causes the same bug? or that after running with the new generated client, you have that issue?\n\ncan you run the `pip show baml-cli` command as a part of your make file?",
        "timestamp": "2024-08-06 22:09:41.684000+00:00",
        "id": 1270504097564594346,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Generating the client again also gave the same error.",
        "timestamp": "2024-08-06 21:57:34.663000+00:00",
        "id": 1270501048217239593,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "we can also do the following:\n\n```\nimport baml_py\nprint(baml_py)\n```\n\nat the very top of your pytest file (before any other imports)",
        "timestamp": "2024-08-06 21:55:15.191000+00:00",
        "id": 1270500463229538418,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "then retry running pytest",
        "timestamp": "2024-08-06 21:53:48.642000+00:00",
        "id": 1270500100216586290,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "one last thing to try is to regen the baml client.\n\n```\nbaml-cli generate\n```",
        "timestamp": "2024-08-06 21:53:27.787000+00:00",
        "id": 1270500012744511539,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "my gut says that somehow the mechanism your make pytest is using is importing the wrong python env (leading to wrong baml_py)",
        "timestamp": "2024-08-06 21:53:07.372000+00:00",
        "id": 1270499927117795380,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Yes it does.",
        "timestamp": "2024-08-06 21:52:54.146000+00:00",
        "id": 1270499871643926601,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "`<module 'baml_py' from '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/baml_py/__init__.py'>`",
        "timestamp": "2024-08-06 21:52:46.714000+00:00",
        "id": 1270499840471601225,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "interesting, so you def have it.\n\n```\npython -c 'import baml_py; print(baml_py)'\n```\n\ndoes that work for you?",
        "timestamp": "2024-08-06 21:51:52.068000+00:00",
        "id": 1270499611269664822,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "`pip3 show baml-py`\n`Name: baml-py`\n`Version: 0.53.0`",
        "timestamp": "2024-08-06 21:50:10.313000+00:00",
        "id": 1270499184478261280,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "is what the version gave.",
        "timestamp": "2024-08-06 21:49:27.781000+00:00",
        "id": 1270499006086254713,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "`baml-runtime 0.53.0`",
        "timestamp": "2024-08-06 21:49:22.655000+00:00",
        "id": 1270498984586379386,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "also can you run the following:\n\n```\npip show baml-py\n```",
        "timestamp": "2024-08-06 21:32:48.913000+00:00",
        "id": 1270494816530337974,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "Ah no worries.\nAre you able to run the tests w/o make?\n\ne.g. directly run `baml-cli --version`",
        "timestamp": "2024-08-06 21:32:29.432000+00:00",
        "id": 1270494734820835438,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Also, this is my organization's code, so I won't be able to share my screen.",
        "timestamp": "2024-08-06 21:31:42.600000+00:00",
        "id": 1270494538393325608,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "I am using makefile to run some pytest tests. So when I do make test, this is the error I get.",
        "timestamp": "2024-08-06 21:31:21.025000+00:00",
        "id": 1270494447901347883,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "We can hop on office hours very quick and screenshare (that may be faster!)",
        "timestamp": "2024-08-06 21:30:07.878000+00:00",
        "id": 1270494141100326944,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "I am using Python 3.10 and BAML 0.53.0 in VS Code.",
        "timestamp": "2024-08-06 21:29:14.007000+00:00",
        "id": 1270493915148976199,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "I just tested our example https://github.com/BoundaryML/baml-examples/tree/main/python-fastapi-starter with 0.53.0 and it seems to be running well -- perhaps it's the python environment?",
        "timestamp": "2024-08-06 21:26:10.148000+00:00",
        "id": 1270493143988441088,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "and how do you run it?",
        "timestamp": "2024-08-06 21:18:28.823000+00:00",
        "id": 1270491209051275385,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": ".aaronv",
        "content": "how do you setup your python environment?",
        "timestamp": "2024-08-06 21:18:21.239000+00:00",
        "id": 1270491177241808997,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "<@201399017161097216> can you help here?",
        "timestamp": "2024-08-06 21:17:33.879000+00:00",
        "id": 1270490978599571531,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Back now. So I upgraded the BAML version to 0.53.0 and also the VS Code Extension version. However, I am faced with this error:\nfunctions/llm_model_execution/baml_client/types.py:16: in <module>\n    import baml_py\nE   ModuleNotFoundError: No module named 'baml_py'",
        "timestamp": "2024-08-06 21:17:14.242000+00:00",
        "id": 1270490896235761837,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "I am just working on something. Will revert.",
        "timestamp": "2024-08-06 20:09:32.529000+00:00",
        "id": 1270473860176806013,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "can you confirm if you aren't able to pip-install? \n\nhttps://pypi.org/project/baml-py/0.53.0/#files",
        "timestamp": "2024-08-06 19:59:08.018000+00:00",
        "id": 1270471240787820605,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "joatmon.pockets",
        "content": "Alternatively, https://docs.aws.amazon.com/secretsmanager/latest/userguide/retrieving-secrets_lambda.html is also an option!",
        "timestamp": "2024-08-06 19:58:23.853000+00:00",
        "id": 1270471055546388675,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Great if you could confirm. I was initially using 0.52.x but downgraded to be compatible with Python 3.10.",
        "timestamp": "2024-08-06 19:58:23.658000+00:00",
        "id": 1270471054728630283,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "(We have local tests for 3.8+ that we use 🙂 so its just an update to the release script if anything)",
        "timestamp": "2024-08-06 19:58:03.293000+00:00",
        "id": 1270470969311625319,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "Oh thats odd, we hsould be supporting 0.53 with 3.8+ I'll double if thats the case. If it isn't, we'll ship and update by end of day.",
        "timestamp": "2024-08-06 19:57:38.504000+00:00",
        "id": 1270470865338765373,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "mz5910",
        "content": "Thanks <@99252724855496704> . My org uses Python 3.10 and the latest version of BAML supported with that is 0.44.0 so I won't be able to upgrade to 0.53. Is there any other work around?",
        "timestamp": "2024-08-06 19:56:36.511000+00:00",
        "id": 1270470605321539626,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "hi <@929667691525050370> ! Yes this is possible, (you'll need to update to the latest version of BAML 0.53)\n\nYou can then modify this code in ptyhon via:\n\n```python\nfrom baml_py import ClientRegistry\nfrom baml_client import b\n\ncr = ClientRegistry()\n    # Creates a new client\ncr.add_llm_client(name='GPT4o', provider='openai', options={\n    \"model\": \"gpt-4o\",\n    \"api_key\": \"SOME KEY FROM AWS\"\n})\n\n\ndef foo():\n  res = b.MyFunction(..., { \"client_registry\": cr })\n```\n\nsee https://docs.boundaryml.com/docs/calling-baml/client-registry",
        "timestamp": "2024-08-06 19:49:56.763000+00:00",
        "id": 1270468928656642059,
        "parent_id": null,
        "thread_id": 1270451229583081615
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-06 19:49:56.172000+00:00",
        "id": 1270468926177939599,
        "parent_id": 1270451229583081615,
        "thread_id": 1270451229583081615
    },
    {
        "author": "gabev2037",
        "content": "Is there a place in the docs where I can see which exceptions to expect from a baml call? Since I have my baml client in the worker, i got an error which automatically retried the worker when ideally i would've detected it were of type `ExceptionTypeA` and instead of retrying, i would do something differently",
        "timestamp": "2024-08-06 16:51:33.268000+00:00",
        "id": 1270424034944749638,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "hellovai",
        "content": "we're doing planning today, so i'll get you a timeline on exceptions!",
        "timestamp": "2024-08-06 18:04:15.434000+00:00",
        "id": 1270442331195183236,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "hellovai",
        "content": "Ah yes, even in that case i think you'd need application level handler. We don't currently have a way to do that in the current retry policies",
        "timestamp": "2024-08-06 18:00:33.914000+00:00",
        "id": 1270441402073088132,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "gabev2037",
        "content": "wondering whether that's still something i can work with, though maybe in this case i still need an application level exception handler",
        "timestamp": "2024-08-06 18:00:04.761000+00:00",
        "id": 1270441279796416514,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "gabev2037",
        "content": "I have a baml function specific retry policy, then i have my celery worker retry policy. A few months back, when I first onboarded to baml, there was a way to configure special retry logic based on the error codes from the LLM client (iirc)",
        "timestamp": "2024-08-06 17:59:42.806000+00:00",
        "id": 1270441187710599249,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "hellovai",
        "content": "What do you mean? by in the retry policies?",
        "timestamp": "2024-08-06 17:51:30.931000+00:00",
        "id": 1270439124637192284,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "gabev2037",
        "content": "Is there something I could use in my baml retry policies? this way i don't need to worry about the client exposing something",
        "timestamp": "2024-08-06 17:37:09.817000+00:00",
        "id": 1270435512863162419,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "joatmon.pockets",
        "content": "Unfortunately we don't expose more nuance about error causes to you, but this is something we've talked about! I've gone ahead and filed https://github.com/BoundaryML/baml/issues/866",
        "timestamp": "2024-08-06 16:58:53.636000+00:00",
        "id": 1270425881982271549,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "gabev2037",
        "content": "Looks like it's just of type `BamlError`?",
        "timestamp": "2024-08-06 16:52:17.152000+00:00",
        "id": 1270424219007717476,
        "parent_id": null,
        "thread_id": 1270424034944749638
    },
    {
        "author": "gabev2037",
        "content": "",
        "timestamp": "2024-08-06 16:52:16.941000+00:00",
        "id": 1270424218122719252,
        "parent_id": 1270424034944749638,
        "thread_id": 1270424034944749638
    },
    {
        "author": "deoxykev",
        "content": "would it be reasonable to have multiple baml_src directories in a repo for organization purposes?",
        "timestamp": "2024-08-05 23:50:14.270000+00:00",
        "id": 1270167012198453259,
        "parent_id": null,
        "thread_id": 1270167012198453259
    },
    {
        "author": "deoxykev",
        "content": "i'll play around with it! thank you",
        "timestamp": "2024-08-05 23:56:18.886000+00:00",
        "id": 1270168541508796581,
        "parent_id": null,
        "thread_id": 1270167012198453259
    },
    {
        "author": ".aaronv",
        "content": "yeah they totally can be! each baml_src can just have a different `output_dir` configuration in the `generator` block you declare. We'll add an integration test on our end to ensure this kind of setup works well",
        "timestamp": "2024-08-05 23:56:00.005000+00:00",
        "id": 1270168462316142622,
        "parent_id": null,
        "thread_id": 1270167012198453259
    },
    {
        "author": "deoxykev",
        "content": "multiple baml clients should be fine, as long as they can be imported separately",
        "timestamp": "2024-08-05 23:52:23.007000+00:00",
        "id": 1270167552160698388,
        "parent_id": null,
        "thread_id": 1270167012198453259
    },
    {
        "author": ".aaronv",
        "content": "the problem is that you end up with multiple baml_clients. If you are ok with each baml_client generated being for some subset of all your prompts I don't see an issue with that.\n\nif not, perhaps we can add a flag to our generator to point to more than 1 BAML_SRC directory to compile all baml files",
        "timestamp": "2024-08-05 23:51:55.261000+00:00",
        "id": 1270167435785273446,
        "parent_id": null,
        "thread_id": 1270167012198453259
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-08-05 23:51:54.957000+00:00",
        "id": 1270167434510467073,
        "parent_id": 1270167012198453259,
        "thread_id": 1270167012198453259
    },
    {
        "author": "gabev2037",
        "content": "If i removed the `BAML_LOG` environment variable, does this mean baml isn't logging anything anymore?",
        "timestamp": "2024-08-05 23:11:09.601000+00:00",
        "id": 1270157177943756851,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "classic gemini",
        "timestamp": "2024-08-06 00:26:26.293000+00:00",
        "id": 1270176122323206235,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "the bug is that gemini api docs are wrong so we expected a field to be there but it wasn't. LMK if it's still failing after this",
        "timestamp": "2024-08-06 00:12:46.500000+00:00",
        "id": 1270172683862147093,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "it's done: 0.53.0",
        "timestamp": "2024-08-06 00:11:49.663000+00:00",
        "id": 1270172445470625822,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "the release is kicked off. ETA is like 25min from now https://github.com/BoundaryML/baml/actions/runs/10257711404\nill follow this release with another one to not print out the whole prompt on those errors (and also fix the dashboard not marking that as an error).",
        "timestamp": "2024-08-05 23:53:45.213000+00:00",
        "id": 1270167896957390910,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "appreciate you",
        "timestamp": "2024-08-05 23:29:27.421000+00:00",
        "id": 1270161782534705183,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "yeah",
        "timestamp": "2024-08-05 23:29:24.106000+00:00",
        "id": 1270161768630718548,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "ok, mind pinging me when it's available?",
        "timestamp": "2024-08-05 23:29:18.872000+00:00",
        "id": 1270161746677727343,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "it's an internal failure of us not serializing the response. I can patch it now and release, but will take 25min",
        "timestamp": "2024-08-05 23:29:09.079000+00:00",
        "id": 1270161705602781215,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "this is urgent",
        "timestamp": "2024-08-05 23:28:46.207000+00:00",
        "id": 1270161609670525050,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "ok, is there something I can do on my end to fix it?",
        "timestamp": "2024-08-05 23:28:44.527000+00:00",
        "id": 1270161602624360550,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "ok we figured it out, we'll patch it (and sorry for this long-ass error, it makes no sense to render the whole input on these)",
        "timestamp": "2024-08-05 23:28:31.720000+00:00",
        "id": 1270161548907778099,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "gemini api",
        "timestamp": "2024-08-05 23:28:02.973000+00:00",
        "id": 1270161428334247996,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "do you use vertex or gemini api?",
        "timestamp": "2024-08-05 23:27:33.226000+00:00",
        "id": 1270161303566024714,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "it also doesn't fail for the previous invocations of the function... just this one specific call",
        "timestamp": "2024-08-05 23:23:22.074000+00:00",
        "id": 1270160250158452756,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "nope",
        "timestamp": "2024-08-05 23:21:59.884000+00:00",
        "id": 1270159905428340808,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "the output",
        "timestamp": "2024-08-05 23:21:31.913000+00:00",
        "id": 1270159788109463672,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "does your schema have a field called \"citations\"?",
        "timestamp": "2024-08-05 23:21:24.606000+00:00",
        "id": 1270159757461688370,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "that's not a field...",
        "timestamp": "2024-08-05 23:21:00.092000+00:00",
        "id": 1270159654642520187,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "in the output",
        "timestamp": "2024-08-05 23:20:39.723000+00:00",
        "id": 1270159569208869006,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "for the client?",
        "timestamp": "2024-08-05 23:16:43.758000+00:00",
        "id": 1270158579499798609,
        "parent_id": 1270158412436603010,
        "thread_id": 1270157177943756851
    },
    {
        "author": "hellovai",
        "content": "lol vscode crashed when i opened that file as well",
        "timestamp": "2024-08-05 23:16:15.235000+00:00",
        "id": 1270158459865927831,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": ".aaronv",
        "content": "theres a missing field `citations`. We're going to truncate these logs",
        "timestamp": "2024-08-05 23:16:03.927000+00:00",
        "id": 1270158412436603010,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "ya so boundary error log is really long, but also i'm not quite sure what the error is",
        "timestamp": "2024-08-05 23:15:10.268000+00:00",
        "id": 1270158187374313482,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "hellovai",
        "content": "or we'll just truncate super long logs",
        "timestamp": "2024-08-05 23:14:54.388000+00:00",
        "id": 1270158120768770159,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "hmm well now i'm thinking there's a different issue\nhttps://app.boundaryml.com/dashboard/projects/proj_4d5d8c28-b9df-435e-8b6e-48f2566cadbb/drilldown?start_time=2024-07-22T23%3A14%3A00.731Z&end_time=2024-08-05T23%3A14%3A01.606Z&eid=74b44362-7ba4-415e-b732-e360c244ee74&s_eid=17aae156-569e-471c-95a9-3eb6c35f2010&test=false&onlyRootEvents=true",
        "timestamp": "2024-08-05 23:14:48.627000+00:00",
        "id": 1270158096605511816,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "hellovai",
        "content": "`BAML_LOG=error`\nShould do the trick.\n\nAlso, I'll patch this in tonight so you can put in `disabled` as well",
        "timestamp": "2024-08-05 23:14:35.436000+00:00",
        "id": 1270158041278451854,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "the boundary log is too large and it's crashing my application",
        "timestamp": "2024-08-05 23:12:56.048000+00:00",
        "id": 1270157624414961695,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "gabev2037",
        "content": "yes",
        "timestamp": "2024-08-05 23:12:47.896000+00:00",
        "id": 1270157590222999695,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "hellovai",
        "content": "no, it will still log WARN / ERROR by default. do you want no logging?",
        "timestamp": "2024-08-05 23:12:21.174000+00:00",
        "id": 1270157478142672998,
        "parent_id": null,
        "thread_id": 1270157177943756851
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-05 23:12:20.889000+00:00",
        "id": 1270157476947300462,
        "parent_id": 1270157177943756851,
        "thread_id": 1270157177943756851
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Maybe missing something super obvious: for tests, is there a way to provide what I expect the output should be? \n\nI have some inputs that I want the function to return `true` for, but I don't see a place for me to say I `expect` `true` as the output",
        "timestamp": "2024-08-05 23:10:47.023000+00:00",
        "id": 1270157083244757045,
        "parent_id": null,
        "thread_id": 1270157083244757045
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha ok",
        "timestamp": "2024-08-05 23:30:04.149000+00:00",
        "id": 1270161936582967327,
        "parent_id": null,
        "thread_id": 1270157083244757045
    },
    {
        "author": "hellovai",
        "content": "ah yes sorry 🙂\n\nthe field validation concept will also be applicable to tests shortly after their release so you will be able to write asserts on the test as well.",
        "timestamp": "2024-08-05 23:19:01.382000+00:00",
        "id": 1270159156736692354,
        "parent_id": null,
        "thread_id": 1270157083244757045
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Hmm, that looks interesting but don't think I'm talking about the same thing -- I mean the ability to write something like:\n\n```\ntest SimilarVendorName1 {\n  functions [VendorIsCorrect]\n  args {\n    existingVendor {\n      name \"Epoxy Tech\"\n    }\n    invoiceVendor {\n      name \"EpoxyTech\"\n    }\n  }\n  expect true\n}\n```",
        "timestamp": "2024-08-05 23:13:50.248000+00:00",
        "id": 1270157851746373654,
        "parent_id": null,
        "thread_id": 1270157083244757045
    },
    {
        "author": "hellovai",
        "content": "we're actively building this atm! check out <#1265356689796890820>",
        "timestamp": "2024-08-05 23:11:35.816000+00:00",
        "id": 1270157287897694371,
        "parent_id": null,
        "thread_id": 1270157083244757045
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-08-05 23:11:35.432000+00:00",
        "id": 1270157286286950442,
        "parent_id": 1270157083244757045,
        "thread_id": 1270157083244757045
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Question about prompting for a true/false response: What's the best way to ask an LLM to answer a question as true or false and have parsing succeed? I did some testing, and found that if I try to use chain-of-thought with such a simple output schema, response coercion fails: https://www.promptfiddle.com/New-Project-GSO_w",
        "timestamp": "2024-08-05 21:52:28.676000+00:00",
        "id": 1270137376949276724,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha I see",
        "timestamp": "2024-08-05 22:06:18.322000+00:00",
        "id": 1270140856736944198,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "hellovai",
        "content": "You can swap `Yes` or `No` to `true` and `false` respectively, but atm you'd still have to swap it to a boolean programtically",
        "timestamp": "2024-08-05 22:06:04.095000+00:00",
        "id": 1270140797064450148,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "hellovai",
        "content": "This is another way to do this",
        "timestamp": "2024-08-05 22:05:26.003000+00:00",
        "id": 1270140637295018106,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "hellovai",
        "content": "https://www.promptfiddle.com/New-Project-8WL2u",
        "timestamp": "2024-08-05 22:05:18.893000+00:00",
        "id": 1270140607473647616,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha makes sense, nw -- I don't need COT right now, was just testing the limits of coercion\n\nMy prompt right now is \"Return true if and only if the following PDF is an invoice.\", but I could see the LLM saying something like \"not true\" in response, so i was testing out different prompts and different distances between `bool` and the LLM's response",
        "timestamp": "2024-08-05 22:04:58.050000+00:00",
        "id": 1270140520051638272,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": ".aaronv",
        "content": "https://github.com/BoundaryML/baml/issues/860",
        "timestamp": "2024-08-05 22:03:55.107000+00:00",
        "id": 1270140256049430530,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": ".aaronv",
        "content": "ah it seems like our coercion isnt powerful enough in this case. If you need COT and a boolean just wrap it around an object for now, sorry we will create an issue for this",
        "timestamp": "2024-08-05 22:03:06.829000+00:00",
        "id": 1270140053556953209,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Prompt engineering for true/false questions",
        "timestamp": "2024-08-05 21:54:07.626000+00:00",
        "id": 1270137791975522427,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "ashwin.a.kumar",
        "content": "The use case is I want to have a very simple function that determines if a PDF is an invoice or not -- curious what the limitations on response coercion are",
        "timestamp": "2024-08-05 21:52:56.660000+00:00",
        "id": 1270137494322679969,
        "parent_id": null,
        "thread_id": 1270137376949276724
    },
    {
        "author": "ashwin.a.kumar",
        "content": "",
        "timestamp": "2024-08-05 21:52:56.053000+00:00",
        "id": 1270137491776737421,
        "parent_id": 1270137376949276724,
        "thread_id": 1270137376949276724
    },
    {
        "author": "kdub03",
        "content": "For the aws-bedrock provider, is there a way to have it use sso, or my authed aws client? \nhttps://docs.boundaryml.com/docs/snippets/clients/providers/aws-bedrock",
        "timestamp": "2024-08-01 01:23:43.470000+00:00",
        "id": 1268378599560577127,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "joatmon.pockets",
        "content": "we can provide instructions for how to provision a narrowly scoped token for playground use, if you think those would help- otherwise we’ll look into how we can make things work as pong as you’re signed into sso",
        "timestamp": "2024-08-01 01:49:45.615000+00:00",
        "id": 1268385151671472203,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "joatmon.pockets",
        "content": "howdy! vaibhav’s response is on the nose- the generated code will work just fine with `aws sso login` but the playground will need a long-lived access token",
        "timestamp": "2024-08-01 01:46:23.512000+00:00",
        "id": 1268384303989919836,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "I did get it to work now restarting the playground w/ the temp creds. I'll try w/ the SSO later. \n\nThanks for the response!",
        "timestamp": "2024-08-01 01:37:16.062000+00:00",
        "id": 1268382007818457088,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "hellovai",
        "content": "fyi, we load default files and env variables that AWS supports\n\nhttps://docs.aws.amazon.com/sdkref/latest/guide/file-location.html\nhttps://docs.aws.amazon.com/sdkref/latest/guide/environment-variables.html\n\nSo. as long some of the default ones are set, i think SSO auth should work.",
        "timestamp": "2024-08-01 01:36:31.615000+00:00",
        "id": 1268381821393965117,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "hellovai",
        "content": "i think so! I think the python / typescript bindings should \"just work\" but if not let me know!\n\nWe were waiting to see how long until someone has an issue like this in the playground, and it seems like its come up 🙂 We'll report the issue and get back with an ETA of how long it will take to solve for the playground shortly.",
        "timestamp": "2024-08-01 01:35:48.818000+00:00",
        "id": 1268381641890467891,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "Oh I could probably use it outside the playground.",
        "timestamp": "2024-08-01 01:33:34.955000+00:00",
        "id": 1268381080428347485,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "I can't use permanent credentials in my environment, so I either get a 1 hour key to set, or SSO.",
        "timestamp": "2024-08-01 01:33:29.402000+00:00",
        "id": 1268381057137381439,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "hellovai",
        "content": "is this in the playground?\n\n(I think as of now, our playground only supports that method)\n\nOur playground is implement in WebAssembly (AKA doesn't have access to the file system - only a web browser).\n\nWe added SSO for Vertex, but i think we should be able to augment this for Bedrock as well.",
        "timestamp": "2024-08-01 01:33:10.293000+00:00",
        "id": 1268380976988295252,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "I have my config file for aws setup with profile \"name\" for various environments. I'm not sure if there's a pass through. Using like boto3, I can specify the profile name, and it will pop sso if needed, or use the cached credentials.",
        "timestamp": "2024-08-01 01:32:40.135000+00:00",
        "id": 1268380850496737381,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "I get this error:\nUnspecified error code: 2\n\"AWS_REGION, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY must be set in the environment\"",
        "timestamp": "2024-08-01 01:31:43.522000+00:00",
        "id": 1268380613044342896,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "hellovai",
        "content": "The library we use def supports sso (https://docs.rs/aws-config/latest/aws_config/index.html)\n\nbut just a mater of making sure we have plumbed it through correctly",
        "timestamp": "2024-08-01 01:30:04.651000+00:00",
        "id": 1268380198349307955,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "hellovai",
        "content": "could you share how your auth'ed client is defined?",
        "timestamp": "2024-08-01 01:29:13.287000+00:00",
        "id": 1268379982913343681,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "hellovai",
        "content": "yes! I think you can do this using env variables! \n\n<@711679663746842796> can you double check if we're missing docs here",
        "timestamp": "2024-08-01 01:26:07.208000+00:00",
        "id": 1268379202441314367,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "client<llm> BedrockClaude {\n  provider aws-bedrock\n  options {\n    AWS_PROFILE \"mgmt\"\n    model_id \"anthropic.claude-3-5-sonnet-20240620-v1:0\n    temperature 0.1\n  }\n}\nis what I was trying.",
        "timestamp": "2024-08-01 01:24:30.852000+00:00",
        "id": 1268378798295093350,
        "parent_id": null,
        "thread_id": 1268378599560577127
    },
    {
        "author": "kdub03",
        "content": "",
        "timestamp": "2024-08-01 01:24:30.714000+00:00",
        "id": 1268378797716017152,
        "parent_id": 1268378599560577127,
        "thread_id": 1268378599560577127
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Typing/output schema question: Is it possible to have a type with a field that I conditionally choose to turn off for some function calls? \n\nI have a BAML type I want to use throughout my code, but I only want one BAML function to try to fill in a particular field, not the other, so I don't want that field to show up in `{{ ctx.output_format }}`  for the first function at all. Is the only way to do this to define a completely new type with one fewer field?",
        "timestamp": "2024-07-31 18:59:33.986000+00:00",
        "id": 1268281923017506910,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "Oh yea. We’d also need that capability as well. I guess both of those together would solve it",
        "timestamp": "2024-08-02 15:53:47.776000+00:00",
        "id": 1268959948155392000,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": ".aaronv",
        "content": "<@99252724855496704> you mean re adding overrides? 😂",
        "timestamp": "2024-08-02 14:41:17.989000+00:00",
        "id": 1268941703826374718,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "and we should be able to solve this for you 🙂",
        "timestamp": "2024-08-02 07:39:30.125000+00:00",
        "id": 1268835554951168002,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "this is a good point on how its annoying",
        "timestamp": "2024-08-02 07:39:21.978000+00:00",
        "id": 1268835520780435466,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "we support the `@skip` keyword for enums already. I think we can support `@skip` for optional fields. that would likely solve this!",
        "timestamp": "2024-08-02 07:39:11.095000+00:00",
        "id": 1268835475133693993,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Sounds good, thanks!",
        "timestamp": "2024-08-01 22:42:29.755000+00:00",
        "id": 1268700412924661760,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": ".aaronv",
        "content": "thanks for the feedback, we'll see what solutions we can give you and come back to you",
        "timestamp": "2024-08-01 22:41:52.718000+00:00",
        "id": 1268700257580224572,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "^which importantly wouldn't have to break type safety, since the only thing that would functionally change is the prompt sent to the LLM -- the return type could be the same type as the predefined one, just with the disabled field always set to null",
        "timestamp": "2024-08-01 22:40:52.896000+00:00",
        "id": 1268700006668566621,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "This *works* but it's a lot more cumbersome than the capability to dynamically turn off a field in a BAML type for one BAML function call would be",
        "timestamp": "2024-08-01 22:39:39.979000+00:00",
        "id": 1268699700832243864,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Circling back to this, just wanted to bump feedback that this is actually a pain point I've hit a second distinct time:\n\nFor the OCR pipeline I'm building, I conditionally want to sometimes parse out the invoice line items in a BAML call and sometimes not, depending on the length of the invoice. The cleanest workaround I can find right now now is defining a new BAML `InvoiceWithoutLineItems` type, and then writing Typescript mappers that convert `InvoiceWithoutLineItems` and `LineItem[]` into the `Invoice` type I'm currently using in business logic",
        "timestamp": "2024-08-01 22:38:45.515000+00:00",
        "id": 1268699472393797663,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "we are gonna be working on that after we are able ship the assert feature",
        "timestamp": "2024-07-31 20:37:22.490000+00:00",
        "id": 1268306537307181160,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "they actually aren't supported yet 🙂",
        "timestamp": "2024-07-31 20:37:02.827000+00:00",
        "id": 1268306454834843692,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Ohh I see, that's better for sure -- FWIW I probably would've figured out `type` and `&`  are supported in BAML had I actually tried making the type myself, but I didn't think BAML supported defining a new type with `type` or using `&`  from reading the docs (I was looking at `|` instead)",
        "timestamp": "2024-07-31 20:20:12.816000+00:00",
        "id": 1268302218541662238,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "I agree that this isn't ideal, the dynamic types approach is better.\n\nI should have implemented a code sample with it:\n\n```rust\nclass LineItem {\n  description string?\n  quantity int?\n  unitPrice float?\n  totalAmount float?\n  name string?\n  currency string?\n}\n\ntype AnotherType = LineItem & {\n  metadata LineItemMetadata?\n}\n```\n\nThen you could use `AnotherType` in the function that needs it",
        "timestamp": "2024-07-31 20:14:43.763000+00:00",
        "id": 1268300838393090110,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Hmm, I'm not sure I see exactly what you mean with union types -- without an actual inheritance/composition system, the best thing I can think of is something like this (with more names xD), which requires the non- \"toggleable\" fields to be another level. deeper:\n\n```\nclass LineItemData {\n  description string?\n  quantity int?\n  unitPrice float?\n  totalAmount float?\n  name string?\n  currency string?\n}\n\nclass LineItemWithoutMetadata {\n  data LineItemData\n}\n\nclass LineItemWithMetadata {\n  data LineItemData\n  metadata LineItemMetadata?\n}\n```\n\nI guess there's nothing strictly incorrect with this approach,  but I'd rather not privilege the `metadata` field in terms of nesting depth compared to others when I'm using the BAML type in my other business logic, or write a mapper to fix the nesting depth because that partially defeats the purpose of having a nice BAML type at all",
        "timestamp": "2024-07-31 19:51:29.521000+00:00",
        "id": 1268294990518554746,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha, that makes sense",
        "timestamp": "2024-07-31 19:41:14.984000+00:00",
        "id": 1268292412963291166,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "that said, it sounds like what you want is some sort of generic composition.\n\nWhere we could know that the types for these two functions are similar but have some key differences. I wonder if unions and more complex type-algebra would make this easier. (The caveat being naming types would harder 🙂 )",
        "timestamp": "2024-07-31 19:30:06.936000+00:00",
        "id": 1268289610967023658,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "only deletion is allowed",
        "timestamp": "2024-07-31 19:29:02.739000+00:00",
        "id": 1268289341705424926,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "as that would break the type-safety types we have in your `LineItem` type in typescript",
        "timestamp": "2024-07-31 19:28:50.804000+00:00",
        "id": 1268289291646406869,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "hellovai",
        "content": "sadly you can't delete stuff dynamically",
        "timestamp": "2024-07-31 19:28:37.197000+00:00",
        "id": 1268289234574508042,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Basically what I'd want to be able to do is have:\n\n```\nclass LineItem {\n  description string?\n  quantity int?\n  unitPrice float?\n  totalAmount float?\n  name string?\n  currency string?\n  metadata LineItemMetadata?\n  @@dynamic\n}\n```\n\nand then do something like\n\n```\ntb.LineItem.deleteProperty('metadata')\n```",
        "timestamp": "2024-07-31 19:11:37.312000+00:00",
        "id": 1268284956866777108,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Is this limited to creating fields, or would I be able to delete fields with this too?",
        "timestamp": "2024-07-31 19:10:03.837000+00:00",
        "id": 1268284564804079668,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha, that should be fine",
        "timestamp": "2024-07-31 19:09:39.571000+00:00",
        "id": 1268284463025098916,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": ".aaronv",
        "content": "you can make that field be dynamic with https://docs.boundaryml.com/docs/calling-baml/dynamic-types\n\nWe don't currently have a way to dynamically turn on or off each field in a BAML file unfortunately. This will need to happen at runtime.",
        "timestamp": "2024-07-31 19:08:40.558000+00:00",
        "id": 1268284215506632834,
        "parent_id": null,
        "thread_id": 1268281923017506910
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-31 19:08:40.295000+00:00",
        "id": 1268284214403530882,
        "parent_id": 1268281923017506910,
        "thread_id": 1268281923017506910
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Best practices w/ BAML question: Do namespaces exist as a concept in BAML, or is every function/type under different files combined to live under a global shared namespace? For context, I'm implementing another feature with BAML and had a name conflict with a similar type in another file -- then I considered importing it from there / making a separate file with shared types to organize them, but I could see that getting messy as I add more things in the future",
        "timestamp": "2024-07-30 21:01:16.732000+00:00",
        "id": 1267950165088534528,
        "parent_id": null,
        "thread_id": 1267950165088534528
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Also talked to Mihir this morning, sounds like he really liked chatting with you -- super excited to see you at Michigan in the fall!",
        "timestamp": "2024-07-30 21:03:26.645000+00:00",
        "id": 1267950709983154237,
        "parent_id": null,
        "thread_id": 1267950165088534528
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Haha appreciate it",
        "timestamp": "2024-07-30 21:03:01.826000+00:00",
        "id": 1267950605884850258,
        "parent_id": null,
        "thread_id": 1267950165088534528
    },
    {
        "author": "hellovai",
        "content": "we're thinking through it and will share a draft with you shortly. Its great to see you ahve this problem tho 😉",
        "timestamp": "2024-07-30 21:02:31.276000+00:00",
        "id": 1267950477748998185,
        "parent_id": null,
        "thread_id": 1267950165088534528
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Nice!",
        "timestamp": "2024-07-30 21:02:23.019000+00:00",
        "id": 1267950443116367932,
        "parent_id": null,
        "thread_id": 1267950165088534528
    },
    {
        "author": "hellovai",
        "content": "we are drafting a spec for a module system! but for now we have a global namespace!",
        "timestamp": "2024-07-30 21:01:56.586000+00:00",
        "id": 1267950332248330261,
        "parent_id": null,
        "thread_id": 1267950165088534528
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-30 21:01:56.291000+00:00",
        "id": 1267950331011010621,
        "parent_id": 1267950165088534528,
        "thread_id": 1267950165088534528
    },
    {
        "author": "bsachs10",
        "content": "Question: Is it possible to run BAML functions client side in NextJS? I know the security concerns about using API keys in browser, but I can mitigate that with a proxy server that uses session-specific authentication rather than API keys. But when I try to run any BAML function from client code (or simply import `b` from `@/baml_client`), I get an immediate error like this one:\n\n```\n./node_modules/@boundaryml/baml/async_context_vars.js:5:1\nModule not found: Can't resolve 'async_hooks'\n\nhttps://nextjs.org/docs/messages/module-not-found\n\nImport trace for requested module:\n./node_modules/@boundaryml/baml/index.js\n./src/baml_client/async_client.ts\n./src/baml_client/index.ts\n```",
        "timestamp": "2024-07-29 15:48:13.698000+00:00",
        "id": 1267508995468427264,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "bsachs10",
        "content": "That makes sense. I use a custom registry so I didn't need to use ENV vars in BAML itself",
        "timestamp": "2024-07-29 15:59:02.952000+00:00",
        "id": 1267511718636945551,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "(The tricky part about browser based is how we need to set up env vars, we do quite a bit of hacking in our current solution)",
        "timestamp": "2024-07-29 15:58:04.101000+00:00",
        "id": 1267511471797964830,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "and then we should know what we can support easily.",
        "timestamp": "2024-07-29 15:57:19.592000+00:00",
        "id": 1267511285113684029,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "thanks for sharing! I'll get back with a response after chatting with the team!",
        "timestamp": "2024-07-29 15:56:51.828000+00:00",
        "id": 1267511168663158887,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "bsachs10",
        "content": "Meaning we could run them client side by authenticating sessions with you as the host? An equally good solution (albeit with vendor lock-in)",
        "timestamp": "2024-07-29 15:56:11.567000+00:00",
        "id": 1267510999796420640,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "Alternatively, if we hosted your baml functions as endpoints for you and provided you with a typesafe interface to them, would that be better?",
        "timestamp": "2024-07-29 15:55:15.441000+00:00",
        "id": 1267510764386910229,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "bsachs10",
        "content": "Useful for me because Netlfiy is our platform of choice and its server actions have relatively short timeout periods compared to Vercel. We also use a proxy server for our own observability, so there's just no need to have streams be repeated through server actions. But may not be important for others!",
        "timestamp": "2024-07-29 15:54:57.489000+00:00",
        "id": 1267510689090637887,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "Interesting! We currently don't support this due to security concerns as you mentioned. \n\nThe `async_hooks` is how we handle observability and such in an automated way. That said, techncially it can run (we use a Wasm version of BAML to execute BAML functions on a browser on promptfiddle.com).\n\nIf this becomes important, we can look into seeing how much work it is to create a generator for a browser (something like `typescript/browser`).",
        "timestamp": "2024-07-29 15:52:10.885000+00:00",
        "id": 1267509990302945404,
        "parent_id": null,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-29 15:52:10.689000+00:00",
        "id": 1267509989480857640,
        "parent_id": 1267508995468427264,
        "thread_id": 1267508995468427264
    },
    {
        "author": "hellovai",
        "content": "custom jinja filters",
        "timestamp": "2024-07-29 15:33:48.453000+00:00",
        "id": 1267505366367993867,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "hellovai",
        "content": "Variables in functions",
        "timestamp": "2024-07-29 15:30:53.973000+00:00",
        "id": 1267504634545569849,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "robert_hoenig",
        "content": "Another question: Is it possible to add variables?\n\nUse case: I'm implementing few-shot learning, and most of my examples are constructed from the same few structured documents. The documents have a type quite similar to the resume example:\n\n```\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n```\n\nI'd like to create a couple `Resume` instances and re-use them across all my BAML prompts. The `template_strings` mechanism suggested in the docs wouldn't be sufficient for this, because each prompt needs to see the Resume instance as a structured object, and not a string.",
        "timestamp": "2024-07-29 13:45:31.857000+00:00",
        "id": 1267478117669142570,
        "parent_id": null,
        "thread_id": 1267478117669142570
    },
    {
        "author": "robert_hoenig",
        "content": "Awesome, sounds great! I guess this particular feature is the most important one for us right now -- for the other's I've been posting, we've found workarounds 🙂",
        "timestamp": "2024-07-29 16:44:41.829000+00:00",
        "id": 1267523206319837184,
        "parent_id": null,
        "thread_id": 1267478117669142570
    },
    {
        "author": "hellovai",
        "content": "oh you're 100% right, we're working on variable support atm 🙂 Just will take a bit to get there.\n\nAs a part of adding in asserts (see <#1265356689796890820> ), we are going to be updating our compiler so that adding things like variables and type aliases will be trivial. \n\nI would expect this to land by end of august!",
        "timestamp": "2024-07-29 16:37:48.272000+00:00",
        "id": 1267521471735926885,
        "parent_id": null,
        "thread_id": 1267478117669142570
    },
    {
        "author": "robert_hoenig",
        "content": "that's indeed what I had in mind for using BAML functions. However, without variable support, the crucial limitation would be that you couldn't use the same set of examples in both the playground and in production: \n\nEither, you add the few-shot resume instances to a test case for FooBar and can see them in the playground. Then, however, there's no way of re-using the few-shot resume instances when invoking FooBar from Python.\n\nOr, you add the few-shot resume instances to your Python code. But then, there's no way to see the full prompt (with the few-shot instances) in the playground.\n\nA third alternative would be to directly write the few-shot instances inside the FooBar prompt. But then, the examples would no longer be typed.\n\nVariables would solve all of this. Do you agree / do the things I say make sense?",
        "timestamp": "2024-07-29 16:35:17.979000+00:00",
        "id": 1267520841361657906,
        "parent_id": null,
        "thread_id": 1267478117669142570
    },
    {
        "author": "hellovai",
        "content": "would that work as a temporary hold?",
        "timestamp": "2024-07-29 15:31:08.141000+00:00",
        "id": 1267504693970731069,
        "parent_id": null,
        "thread_id": 1267478117669142570
    },
    {
        "author": "hellovai",
        "content": "not yet! However, you can achieve this by adding additional paramters to functison\n\n```\nfunction FooBar(param:type, resumes: Resume[]) {\n  prompt #\" {{ resumes|tojson(true) }} \"#\n}\n```",
        "timestamp": "2024-07-29 15:30:54.476000+00:00",
        "id": 1267504636655566899,
        "parent_id": null,
        "thread_id": 1267478117669142570
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-29 15:30:53.973000+00:00",
        "id": 1267504634545569844,
        "parent_id": 1267478117669142570,
        "thread_id": 1267478117669142570
    },
    {
        "author": "robert_hoenig",
        "content": "Is it possible to add custom Jinja filter functions?\n\nUse case: I'm implementing few-shot learning by adding some example outputs, and would like to control their formatting like so: (\"{{ example_output|format_example_output_in_json }}\". I discovered that in this specific instance,  pretty-printed json formatting can be achieved as \"{{ sample_resume_out|tojson(true) }}\", but I'm sure that at some point, I'll have a need for custom filters.",
        "timestamp": "2024-07-29 13:45:20.105000+00:00",
        "id": 1267478068377817261,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "hellovai",
        "content": "the complete list we support is here:\nhttps://docs.rs/minijinja/latest/minijinja/filters/index.html#functions\n\nWe'll be adding these to our docs soon!",
        "timestamp": "2024-07-29 16:44:40.379000+00:00",
        "id": 1267523200237965415,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "hellovai",
        "content": "you may also want to use the `pprint` filter fyi!",
        "timestamp": "2024-07-29 16:44:03.791000+00:00",
        "id": 1267523046776897648,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "robert_hoenig",
        "content": "Not yet! So far our only use case is the conversion of objects to json, which the in-built filter tojson handles well 🙂",
        "timestamp": "2024-07-29 16:43:37.407000+00:00",
        "id": 1267522936114253885,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "hellovai",
        "content": "you would just need to pass in the same strings as paramters to test cases, but yes sadly that custom python code wouldn't run in the playground 😦 \n\nDo you have examples of some of the heavy string lifting you are doing?",
        "timestamp": "2024-07-29 16:38:51.122000+00:00",
        "id": 1267521735348064316,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "robert_hoenig",
        "content": "Thanks! Right, doing heavy string lifting in Python works. The downside is just that those changes would no longer show up in the playground, right?",
        "timestamp": "2024-07-29 16:37:47.251000+00:00",
        "id": 1267521467453669521,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "hellovai",
        "content": "(I think you can also do a lot of manipulation with template strings, howevery they don't support generics yet, so it may require some duplicate code)",
        "timestamp": "2024-07-29 15:34:38.304000+00:00",
        "id": 1267505575458246778,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "hellovai",
        "content": "This is something we are still debating on. In order to property support this we'd need a lot more complex expressions, so as of now, there is no plans to allow this. \n\nAs you come accross some filters you'd like to add, if you can share them, we'd benefit from understanding them and perhaps theres a good pattern we can see.\n\nThe recommended workaround in case of missing this is to just write any necessary logic in the language of your choice and pass in string paramters to baml.",
        "timestamp": "2024-07-29 15:33:48.751000+00:00",
        "id": 1267505367617896528,
        "parent_id": null,
        "thread_id": 1267478068377817261
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-29 15:33:48.341000+00:00",
        "id": 1267505365897973830,
        "parent_id": 1267478068377817261,
        "thread_id": 1267478068377817261
    },
    {
        "author": "etbyrd",
        "content": "Is it possible to ask BAML to _not_ include certain inputs on clients when using the openai API format?\n\nContext: I am testing Fireworks using:\n```client<llm> FireworksLlama31_70b {\n  provider openai\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    api_key env.FIREWORKS_API_KEY\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n  }\n}```\n\n and I get:\n\n```\nRequest failed: {\"error\":{\"object\":\"error\",\"type\":\"invalid_request_error\",\"message\":\"Extra inputs are not permitted, field: 'stream_options'\"}}\n```",
        "timestamp": "2024-07-24 21:44:54.076000+00:00",
        "id": 1265786815831478273,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "no worries at all!",
        "timestamp": "2024-07-24 23:14:12.243000+00:00",
        "id": 1265809289612890237,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "np, will squash this trailing slash bug soon, ty for the patience",
        "timestamp": "2024-07-24 23:10:04.638000+00:00",
        "id": 1265808251081986140,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "thanks again everyone!",
        "timestamp": "2024-07-24 23:09:07.711000+00:00",
        "id": 1265808012312838255,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "It does work when removing the trailing slash 😄",
        "timestamp": "2024-07-24 23:04:21.134000+00:00",
        "id": 1265806810322042942,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "oh interesting - that makes sense.",
        "timestamp": "2024-07-24 23:04:14.051000+00:00",
        "id": 1265806780613787780,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "the playground proxies the request through a localhost vscode server (to get rid of CORS issues), so i think it may be removing the extra \"/\", will take a look. My guess is that curl request doesn't actually execute if you copy it in the terminal",
        "timestamp": "2024-07-24 23:03:14.797000+00:00",
        "id": 1265806532084502700,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "does that curl request run? if you ru it in the terminal",
        "timestamp": "2024-07-24 23:01:50.656000+00:00",
        "id": 1265806179171569796,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "hmm, does it work in your code if you remove the dangling \"/\"?",
        "timestamp": "2024-07-24 23:01:37.331000+00:00",
        "id": 1265806123282206760,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "Thats true - but the raw CURL in the playground has the same and works:\n\n\"curl -X POST 'https://api.fireworks.ai/inference/v1//chat/completions'\"",
        "timestamp": "2024-07-24 23:01:08.595000+00:00",
        "id": 1265806002754949150,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "we'll make it more resilient to this kind of issue",
        "timestamp": "2024-07-24 23:01:08.424000+00:00",
        "id": 1265806002037723139,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "ahh your baseurl has a dangling \"/\"",
        "timestamp": "2024-07-24 23:01:01.388000+00:00",
        "id": 1265805972526469272,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "not sure why it would be inconsistent. There's a /inference/v1//",
        "timestamp": "2024-07-24 23:00:46.597000+00:00",
        "id": 1265805910488387748,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "seems like we don't fix that in our requestbuilder",
        "timestamp": "2024-07-24 23:00:31.972000+00:00",
        "id": 1265805849146949794,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "hmm theres an extra \"/\"",
        "timestamp": "2024-07-24 23:00:23.505000+00:00",
        "id": 1265805813633781832,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "Make sense, thanks. I see this:\n\n```\nbuilt request: Request { method: POST, url: Url { scheme: \"https\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"api.fireworks.ai\")), port: None, path: \"/inference/v1//chat/completions\", query: None, fragment: None }, headers: {\"authorization\": \"Bearer <my_actual_token>\", \"baml-original-url\": \"https://api.fireworks.ai/inference/v1/\", \"content-type\": \"application/json\"} } ```\n\nhmmm, it seems like the correct url",
        "timestamp": "2024-07-24 22:59:49.144000+00:00",
        "id": 1265805669513298043,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "yes- it's not a stable logging format, but it's our internal logging control and will give you a lot of information in `stderr` about what's happening under the hood",
        "timestamp": "2024-07-24 22:53:08.029000+00:00",
        "id": 1265803987114917898,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "interesting - will that make the client be more verbose?",
        "timestamp": "2024-07-24 22:52:23.514000+00:00",
        "id": 1265803800405475402,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "```\n'@boundaryml/baml':\n        specifier: ^0.52.0\n        version: 0.52.1\n```",
        "timestamp": "2024-07-24 22:51:29.449000+00:00",
        "id": 1265803573640560640,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "you can also set `BAML_LOG=debug` in the env to try to get logs out",
        "timestamp": "2024-07-24 22:51:24.200000+00:00",
        "id": 1265803551624396881,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "what version of `@boundaryml/baml` are you on? the latest is 0.52.1",
        "timestamp": "2024-07-24 22:50:38.697000+00:00",
        "id": 1265803360771244113,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "So this works in a local client & playground:\n\n```\nclient<llm> FireworksLlama31_405b {\n   provider ollama\n   options {\n     base_url \"https://api.fireworks.ai/inference/v1/\"\n     model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n     headers {\n         \"Authorization\" env.FIREWORKS_API_KEY\n     }\n   }\n }\n```\n\nBut trying to convert to using ClientRegistry:\n\n```\nconst cr = new ClientRegistry();\ncr.addLlmClient(\"Fireworks_Llama31_405b\", \"ollama\", {\n    base_url: \"https://api.fireworks.ai/inference/v1/\",\n    model: \"accounts/fireworks/models/llama-v3p1-405b-instruct\",\n    headers: {\n        Authorization: \"Bearer \" + FIREWORKS_API_KEY\n    }\n})\n\ncr.setPrimary(\"Fireworks_Llama31_405b\");\n\ntry {\n    result = await b.AnalyzeFileChangeBAML(patch, { clientRegistry: cr });\n} catch (error: any) {\n    console.error(\"Could not use Fireworks, error:\", error);\n}\n```\n\nI get (truncated):\n\n```\n\"Request failed: \", code: Other(404) }\n\n    at BamlAsyncClient.AnalyzeFileChangeBAML (file:///var/task/packages/analyzing/src/analyzeConsumer.mjs:14501:16)\n    at async Runtime.handler (file:///var/task/packages/analyzing/src/analyzeConsumer.mjs:14635:22) {\n  code: 'GenericFailure'\n}\n```\n\nunfortuntely I can't easily check the actual url called in my current environment",
        "timestamp": "2024-07-24 22:49:34.881000+00:00",
        "id": 1265803093107413072,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "Thanks so much guys!",
        "timestamp": "2024-07-24 21:58:00.012000+00:00",
        "id": 1265790112285851722,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "yeah, that is a very good point, and it would be a pain in the ass to debug because I would have 100% forgotten about this converstation by that point...",
        "timestamp": "2024-07-24 21:57:54.863000+00:00",
        "id": 1265790090689511487,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "glad we could get you unblocked!",
        "timestamp": "2024-07-24 21:57:49.991000+00:00",
        "id": 1265790070254731375,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "it's possible that it's a thing they don't realize $underlying-authz-framework supports and that they might break it at any moment",
        "timestamp": "2024-07-24 21:57:18.758000+00:00",
        "id": 1265789939254296636,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "i would stick with whatever the fireworks docs say",
        "timestamp": "2024-07-24 21:56:47.513000+00:00",
        "id": 1265789808203272234,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "don't you love when things just work out? lol",
        "timestamp": "2024-07-24 21:56:47.424000+00:00",
        "id": 1265789807829979249,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "lmao",
        "timestamp": "2024-07-24 21:56:29.652000+00:00",
        "id": 1265789733288673451,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "But surprisingly, this also works:\n\n```\nclient<llm> FireworksLlama31_70b {\n  provider ollama\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n    headers {\n        \"Authorization\" env.FIREWORKS_API_KEY\n    }\n  }\n}\n```\n\nIt doesn't seem to actually need 'bearer' as part of the auth key",
        "timestamp": "2024-07-24 21:56:20.467000+00:00",
        "id": 1265789694764122215,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "Yeah, that should work!",
        "timestamp": "2024-07-24 21:55:59.311000+00:00",
        "id": 1265789606029430817,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "Does this handle what you need to do?",
        "timestamp": "2024-07-24 21:55:02.402000+00:00",
        "id": 1265789367335518260,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "+ with this approach you don't have to redefine all your functions",
        "timestamp": "2024-07-24 21:54:27.112000+00:00",
        "id": 1265789219318534155,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "so you can imagine how that'd work for fireworks",
        "timestamp": "2024-07-24 21:54:03.466000+00:00",
        "id": 1265789120140148849,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "Oh! Very nice",
        "timestamp": "2024-07-24 21:53:51.226000+00:00",
        "id": 1265789068802003086,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "for fireworks:",
        "timestamp": "2024-07-24 21:53:45.716000+00:00",
        "id": 1265789045691256842,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "so you can write TS code like this:\n\n```async function run() {\n    const cr = new ClientRegistry()\n    // Creates a new client\n    cr.addLlmClient({ name: 'MyAmazingClient', provider: 'openai', options: {\n        model: \"gpt-4o\",\n        temperature: 0.7,\n        api_key: \"sk-...\"\n    }})\n    // Sets MyAmazingClient as the primary client\n    cr.setPrimary('MyAmazingClient')\n    // ExtractResume will now use MyAmazingClient as the calling client\n    const res = await b.ExtractResume(\"...\", { clientRegistry: cr })\n}\n```",
        "timestamp": "2024-07-24 21:53:41.572000+00:00",
        "id": 1265789028310192239,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "https://docs.boundaryml.com/docs/calling-baml/client-registry is what you want",
        "timestamp": "2024-07-24 21:53:14.870000+00:00",
        "id": 1265788916313751646,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "I'd still have to export it with the secret, right? I was hoping for something like in TS with `Bearer ${env.FIREWORKS_API_KEY}`",
        "timestamp": "2024-07-24 21:53:05.866000+00:00",
        "id": 1265788878548238458,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "Pulling up the docs, one sec",
        "timestamp": "2024-07-24 21:52:40.869000+00:00",
        "id": 1265788773703225414,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "joatmon.pockets",
        "content": "Actually, there's a better solution than this for your problem",
        "timestamp": "2024-07-24 21:52:36.154000+00:00",
        "id": 1265788753927213137,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "hellovai",
        "content": "i think that should work!",
        "timestamp": "2024-07-24 21:51:47.469000+00:00",
        "id": 1265788549727260753,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "hellovai",
        "content": "you can just do:\n\n```\nclient<llm> FireworksLlama31_70b {\n  provider ollama\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n    headers {\n        \"Authorization\" env.MyKey\n    }\n  }\n}\n```\n\n```\nexport MyKey = \"Bearer <API_KEY>\"\n```",
        "timestamp": "2024-07-24 21:51:36.982000+00:00",
        "id": 1265788505741721685,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "They currently have a very good 3.1 price and I was going to divert some prod traffic to them to see how they handle it if I can 😄",
        "timestamp": "2024-07-24 21:50:34.395000+00:00",
        "id": 1265788243232686203,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "etbyrd",
        "content": "Thanks! Actually, it looks like I can do this:\n\n```\nclient<llm> FireworksLlama31_70b {\n  provider ollama\n  options {\n    base_url \"https://api.fireworks.ai/inference/v1/\"\n    model \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n    headers {\n        \"Authorization\" \"Bearer <API_KEY>\"\n    }\n  }\n}\n```\n\nBut then I actually have to put the API key there - is there way to grab it from env with string interpolation from there?",
        "timestamp": "2024-07-24 21:49:59.269000+00:00",
        "id": 1265788095903699007,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": ".aaronv",
        "content": "seems like someone had a similar issue recently https://github.com/FlowiseAI/Flowise/issues/2866",
        "timestamp": "2024-07-24 21:49:13.693000+00:00",
        "id": 1265787904744226826,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "hellovai",
        "content": "is fireworks high priority for you?",
        "timestamp": "2024-07-24 21:48:45.046000+00:00",
        "id": 1265787784589873172,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "hellovai",
        "content": "oh interesting!! Right now no. Let me chat with the team and find a way to expose this properly! I'll get back to you by EOD!",
        "timestamp": "2024-07-24 21:48:28.227000+00:00",
        "id": 1265787714045743134,
        "parent_id": null,
        "thread_id": 1265786815831478273
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-24 21:48:27.706000+00:00",
        "id": 1265787711860768821,
        "parent_id": 1265786815831478273,
        "thread_id": 1265786815831478273
    },
    {
        "author": "yungweedle",
        "content": "is it possible to set the url for a llm resource, I’m trying to see if baml can work with Martian - I believe Martian matches the OpenAI api",
        "timestamp": "2024-07-24 15:17:04.605000+00:00",
        "id": 1265689216596050021,
        "parent_id": null,
        "thread_id": 1265689216596050021
    },
    {
        "author": "joatmon.pockets",
        "content": "Yeah, you can add arbitrary `client` definitions in `clients.baml` - is that what you're asking about, or are you asking about something else?",
        "timestamp": "2024-07-24 18:25:28.524000+00:00",
        "id": 1265736628668731392,
        "parent_id": null,
        "thread_id": 1265689216596050021
    },
    {
        "author": "yungweedle",
        "content": "can I define a custom resource in clients",
        "timestamp": "2024-07-24 15:23:42.333000+00:00",
        "id": 1265690884788191304,
        "parent_id": null,
        "thread_id": 1265689216596050021
    },
    {
        "author": "hellovai",
        "content": "yes in the playground! There's a raw curl button that will show you the the request we are making",
        "timestamp": "2024-07-24 15:19:23.147000+00:00",
        "id": 1265689797683580978,
        "parent_id": null,
        "thread_id": 1265689216596050021
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-24 15:19:22.649000+00:00",
        "id": 1265689795594817639,
        "parent_id": 1265689216596050021,
        "thread_id": 1265689216596050021
    },
    {
        "author": "energetic_axolotl_22123",
        "content": "I'm having trouble installing baml-py using poetry. It seems to detect 0.51.3/0.52.0 as the latest version, but that version is not actually installable? When I simply run pip install baml-py, I get baml-py v0.46.0. poetry add baml-py@0.46.0 works fine.",
        "timestamp": "2024-07-24 14:40:10.637000+00:00",
        "id": 1265679930541478120,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "energetic_axolotl_22123",
        "content": "Installation working well! Thanks so much!",
        "timestamp": "2024-07-24 20:26:50.447000+00:00",
        "id": 1265767171267756052,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "Can you check now <@1171670007588065293> ? <@711679663746842796> released 0.52.1!",
        "timestamp": "2024-07-24 17:16:04.259000+00:00",
        "id": 1265719162475380759,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "I just DM'ed you!",
        "timestamp": "2024-07-24 14:45:42.115000+00:00",
        "id": 1265681320860717219,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "No worries, let me send a zoom link!",
        "timestamp": "2024-07-24 14:45:29.472000+00:00",
        "id": 1265681267832393772,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "energetic_axolotl_22123",
        "content": "sorry I'm very new to discord, let me try from the iPad",
        "timestamp": "2024-07-24 14:45:08.235000+00:00",
        "id": 1265681178757824653,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "energetic_axolotl_22123",
        "content": "I can try",
        "timestamp": "2024-07-24 14:43:16.032000+00:00",
        "id": 1265680708144468060,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "Might make it faster!",
        "timestamp": "2024-07-24 14:42:54.334000+00:00",
        "id": 1265680617136328777,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "alternatively, would you be open to quickly getting on office hours and seeing if you can screenshare?",
        "timestamp": "2024-07-24 14:42:49.753000+00:00",
        "id": 1265680597922087007,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "```python\nvbv@Vaibhavs-MacBook-Pro-2 super-easy-demo % poetry add baml-py@latest\nUsing version ^0.52.0 for baml-py\n\nUpdating dependencies\nResolving dependencies... (0.5s)\n\nPackage operations: 0 installs, 1 update, 0 removals\n\n  - Updating baml-py (0.51.3 -> 0.52.0)\n```\n\nThis is what i'm see on my macbook pro.\n\nAre you on Arm64?",
        "timestamp": "2024-07-24 14:42:15.637000+00:00",
        "id": 1265680454829477888,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "energetic_axolotl_22123",
        "content": "I tested on WSL first, then logged on a VPS server (Ubuntu) same issue. Python3.11 in both environments.",
        "timestamp": "2024-07-24 14:41:34.767000+00:00",
        "id": 1265680283408007289,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "That's odd! what operating system are you on?",
        "timestamp": "2024-07-24 14:40:54.879000+00:00",
        "id": 1265680116105613404,
        "parent_id": null,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-24 14:40:54.373000+00:00",
        "id": 1265680113983422674,
        "parent_id": 1265679930541478120,
        "thread_id": 1265679930541478120
    },
    {
        "author": "hellovai",
        "content": "Does baml automatically detect whether",
        "timestamp": "2024-07-24 14:33:57.201000+00:00",
        "id": 1265678364237107256,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does baml automatically detect whether the context window limitation is exceeded? e.g. I have a fallback client which is gpt4 then claude sonnet. If i submit input which exceeds the 128k token limit, will baml automatically skip the gpt 4 client or will it try and fail based on my designated retry policy?",
        "timestamp": "2024-07-24 13:35:07.972000+00:00",
        "id": 1265663561577926726,
        "parent_id": null,
        "thread_id": 1265663561577926726
    },
    {
        "author": "gabev2037",
        "content": "ya that's fair, maybe it makes sense for me to keep that logic in application code and use the client registry then",
        "timestamp": "2024-07-24 15:09:11.596000+00:00",
        "id": 1265687232652513324,
        "parent_id": null,
        "thread_id": 1265663561577926726
    },
    {
        "author": "hellovai",
        "content": "no, we'll call each one subsequently, but that does sound like something we could do! (the tokenizer isn't always revealed, so we may need to think a bit more creatively to consider if we run it or not)",
        "timestamp": "2024-07-24 14:33:57.815000+00:00",
        "id": 1265678366812409928,
        "parent_id": null,
        "thread_id": 1265663561577926726
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-24 14:33:57.055000+00:00",
        "id": 1265678363624996976,
        "parent_id": 1265663561577926726,
        "thread_id": 1265663561577926726
    },
    {
        "author": "hellovai",
        "content": "iframe hyperlinks",
        "timestamp": "2024-07-23 05:39:19.353000+00:00",
        "id": 1265181432125460485,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "deoxykev",
        "content": "Can I iframe the hyperlinks to prompt execution traces on the observability platform to internal tools?\n\nI would like to just iframe the prompt structures in grafana with the rest of the execution trace from otel",
        "timestamp": "2024-07-23 01:58:07.346000+00:00",
        "id": 1265125765293281290,
        "parent_id": null,
        "thread_id": 1265125765293281290
    },
    {
        "author": "hellovai",
        "content": "There should be nothing specific that blocks that!\n\nAre you unable to? I can double check with our auth provider to see what would work",
        "timestamp": "2024-07-23 05:39:19.712000+00:00",
        "id": 1265181433631215726,
        "parent_id": null,
        "thread_id": 1265125765293281290
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-23 05:39:19.353000+00:00",
        "id": 1265181432125460480,
        "parent_id": 1265125765293281290,
        "thread_id": 1265125765293281290
    },
    {
        "author": "gintautas_turing",
        "content": "Are you going to support GPT 4o Mini that was released yesterday? Perhaps it is already supported?",
        "timestamp": "2024-07-19 09:35:58.952000+00:00",
        "id": 1263791438097809472,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "https://github.com/BoundaryML/baml/issues/805",
        "timestamp": "2024-07-19 10:56:44.388000+00:00",
        "id": 1263811761329541183,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "I can provide info in that github issue in case you need it, just paste the link here",
        "timestamp": "2024-07-19 10:54:43.033000+00:00",
        "id": 1263811252329644033,
        "parent_id": 1263810982073864253,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "I'll file a github issue regarding the dotenv approach directly in python not working",
        "timestamp": "2024-07-19 10:53:38.599000+00:00",
        "id": 1263810982073864253,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "thats odd! Can you try the following:\n\n```sh\npip install dotenv-cli\ndotenv python my_app.py\n```",
        "timestamp": "2024-07-19 10:53:03.927000+00:00",
        "id": 1263810836648955914,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "Do you have perhaps a project/demo with the env variables, as setting \n\n```\nimport dotenv\ndotenv.load_dotenv()\n```\nbefore importing baml does not seem to work",
        "timestamp": "2024-07-19 10:51:48.633000+00:00",
        "id": 1263810520843030530,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "in VSCode, you can also set up env variables for the playground:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#setting-env-variables",
        "timestamp": "2024-07-19 10:12:29.243000+00:00",
        "id": 1263800624844116020,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "yes! we recommend using something like `dotenv -e .env python your_python.py` or a secrets manager.\n\nYou have some other options here as well:\nhttps://docs.boundaryml.com/docs/calling-baml/set-env-vars",
        "timestamp": "2024-07-19 10:12:02.877000+00:00",
        "id": 1263800514257096797,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "By the way, do you know where I can securely set the env variables for the BAML to pick up?\n\n```\nclient<llm> Claude {\n  provider anthropic\n  options {\n    model \"claude-3-opus-20240229\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n```\n\nANTHROPIC_API_KEY\n\nsetting this key in the .env file in the baml_src does not seem to work",
        "timestamp": "2024-07-19 10:10:31.194000+00:00",
        "id": 1263800129710981170,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "You're such a great help!",
        "timestamp": "2024-07-19 09:40:09.531000+00:00",
        "id": 1263792489102577796,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "Thank you Vaibhav!",
        "timestamp": "2024-07-19 09:40:04.066000+00:00",
        "id": 1263792466180440157,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "yep! `\"gpt-4o-mini\"`\n\neffectively we support any model that you can pass in from here: https://platform.openai.com/docs/models/gpt-4o-mini",
        "timestamp": "2024-07-19 09:38:43.214000+00:00",
        "id": 1263792127062839326,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "but is the model string correct tho? model \"gpt-4.0-mini\"",
        "timestamp": "2024-07-19 09:37:53.611000+00:00",
        "id": 1263791919012777984,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "nailed it!",
        "timestamp": "2024-07-19 09:37:37.271000+00:00",
        "id": 1263791850477719615,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "gintautas_turing",
        "content": "shoulld it be like this?\n\n```\nclient<llm> GPT4Mini {\n  provider openai\n  options {\n    model \"gpt-4.0-mini\"\n    api_key \"my api key\"\n  }\n}\n```",
        "timestamp": "2024-07-19 09:37:30.726000+00:00",
        "id": 1263791823026126930,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "Already supported!\n\njust change the model parameter in your client and it'll \"just work\"",
        "timestamp": "2024-07-19 09:36:40.574000+00:00",
        "id": 1263791612673392733,
        "parent_id": null,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-19 09:36:40.052000+00:00",
        "id": 1263791610483839017,
        "parent_id": 1263791438097809472,
        "thread_id": 1263791438097809472
    },
    {
        "author": "hellovai",
        "content": "BAML vs Outlines vs Instructor",
        "timestamp": "2024-07-18 16:15:02.072000+00:00",
        "id": 1263529474956464176,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "josephsirosh_22062",
        "content": "[Repost from <#1119375594984050779>] Clarification question: I'm comparing Outlines, Instructor, and BAML, and trying to understand the technical nuances that differentiate BAML. Especially over time, as both Outlines and Instructor are also continuously improving their codebases. What is enduringly different about the BAML approach?",
        "timestamp": "2024-07-18 14:53:41.119000+00:00",
        "id": 1263509002755510383,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "hellovai",
        "content": "Thats good to know. That's 100% one of the areas we're lacking. We mostly aim for providing full control to the developer, but we may benefit from providing some higher level building blocks like a vector store search. I'll share this during our bi-weekly roadmap sync and see what we think!\n\ni wouldn't rule it out, but likely a bit out of scope for the next few months. However, we may put together a quick python helper library or something to accompany baml.",
        "timestamp": "2024-07-19 05:55:12.940000+00:00",
        "id": 1263735880296955965,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "josephsirosh_22062",
        "content": "The file_search tool with vector stores is incredibly useful. That is the standout feature for me.",
        "timestamp": "2024-07-18 18:49:01.310000+00:00",
        "id": 1263568227129360520,
        "parent_id": 1263555015851643047,
        "thread_id": 1263509002755510383
    },
    {
        "author": "hellovai",
        "content": "<@1191794836085424149> for some context, we aren't yet ruling out supporting OpenaI Assistant API yet, however one thing we try and do with BAML is support virtually every model / LLM provider.\n\nAssistants API is very, very tied to openai and its hard to deciefer what it would mean to support that for other models. However, if it becomes a trend that others are starting to implement it, then yes, we would definetely support it.\n\nAn example of this is function calling. Originally, it was just openai, but now its looking like all models are starting to support this more natively, that means its a great case for BAML to include offical support for it.\n\nAs a general rule of thumb, we are likely never going to adopt every new capability that everyone adds immediately, we prefer to wait a bit and then see how the API pans out.\n\nTo help us better understand your use case of the assistants API, whats your favorite part of using that interface compared to the chat / completions? (is it the abstractions around RAG / context?)",
        "timestamp": "2024-07-18 17:56:31.496000+00:00",
        "id": 1263555015851643047,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "joatmon.pockets",
        "content": "Are you on the v1 API or the v2?",
        "timestamp": "2024-07-18 17:39:32.007000+00:00",
        "id": 1263550739804848198,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "joatmon.pockets",
        "content": "No support right now, am looking through the API and figuring out how hard it would be for us to add support",
        "timestamp": "2024-07-18 17:38:49.259000+00:00",
        "id": 1263550560507002983,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "josephsirosh_22062",
        "content": "Follow up question: With e.g. OpenAI, can you handle OpenAI Assistant runs (present or in the future)? Or only chat-completions? I'm building with OpenAI assistants.",
        "timestamp": "2024-07-18 17:35:04.964000+00:00",
        "id": 1263549619745460285,
        "parent_id": 1263531637061193770,
        "thread_id": 1263509002755510383
    },
    {
        "author": "joatmon.pockets",
        "content": "In addition to everything Vaibhav said above:\n\n- outlines (and sglang and guidance) takes the constrained output approach, which makes it harder to do chain of thought (you'd have to somehow model that in the output grammar)\n- instructor's selling point is that you can use the underlying SDK directly, and it wraps the underlying SDK with wrapper transforms, which it achieves by (silently!) transforming the prompt for the user\n\nWe wrote about this a bit in https://www.boundaryml.com/blog/structured-output-from-llms",
        "timestamp": "2024-07-18 16:23:37.558000+00:00",
        "id": 1263531637061193770,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "hellovai",
        "content": "And lastly, a big difference in my mind from a technical capability is the algorithmic focus on our team. We hire mostly folks with deep background in algorithm design, which allows us to invent new techniques like our fuzzy parsing. \n\nAnyways, long ramble 🙂 If you would actually be open to it, I'd love to hear in your perspective what you find the differences are as a developer. I'd be happy to hop into the office hours channel.",
        "timestamp": "2024-07-18 16:15:06.387000+00:00",
        "id": 1263529493054750853,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "hellovai",
        "content": "Thats a great question. \n\nFundamentally, its all just code. So there is never really anything that differentiates one peice of software from another. For example, one could say you can't have fast code in python, but technically, you can write a library in C++ with bindings to python (e.g. numpy), and make python pretty fast. \n\nGiven that disclaimer, I think of the goal of Instructor / Outlines very different than that of BAML. All of them have a shared goal of providing an interface to communicate with LLMs.\n\nBAML has two additional explicit goals built into its design: provide a great developer experience and complete transparency.\n\n- Providing complete transparency\n\n1. This includes our tools which **visualize the prompt** PRIOR to execution\n2. The ability to **see the actual CURL request** we are building to the LLM PRIOR to us making it\n3. (soon!) The ability to see the exact order of how different LLMs are called given complex retry policies, fallbacks.\n4. **Complete control over the prompt** (BAML encodes no special text or prefix)\n\n- Providing a great developer experience\n\n1. Includes tools like the VSCode playground where you can **run tests easily** and visualize almost every detail about the function are you working on. \n2. **Special prompt syntax** like `@alias` for Enum Values which is not really doable in python, or even comments in prompts. \n3. A much more **powerful type library**. For example: we are able toreturn a list of objects without wrapping them in another object.\n4. **BAML's stability across languages**. By nature of implementing BAML in rust and providing a thin interface to Python / Typescript / Ruby / etc, we have almost no bugs that are only in one language. However, Instructor / Outlines would basically have to be re-written every time they need to support another language which means there will be more drift and bugs.",
        "timestamp": "2024-07-18 16:15:02.474000+00:00",
        "id": 1263529476642308196,
        "parent_id": null,
        "thread_id": 1263509002755510383
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-18 16:15:02.072000+00:00",
        "id": 1263529474956464171,
        "parent_id": 1263509002755510383,
        "thread_id": 1263509002755510383
    },
    {
        "author": "gabev2037",
        "content": "for some LLM calls, where I need to generate a lot of output, I'm running up against the bounds of the 4096 output token maximum. Does anyone have any good prompt engineering tips so that I can somehow recognize when the bounds have been reached to then pass this output as input to a subsequent continuation LLM call?",
        "timestamp": "2024-07-17 22:55:04.952000+00:00",
        "id": 1263267762466328638,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "joatmon.pockets",
        "content": "fyi there's also `gpt-3.5-turbo-16k-0613`",
        "timestamp": "2024-07-17 23:43:11.710000+00:00",
        "id": 1263279870406955049,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "hellovai",
        "content": "https://docs.boundaryml.com/docs/calling-baml/client-registry\n\n```python\nfrom baml_py import ClientRegistry\nasync def run():\n    cr = ClientRegistry()\n    # Sets MyAmazingClient as the primary client (MyAmazingClient defined in BAML)\n    cr.set_primary('MyAmazingClient')\n    # ExtractResume will now use MyAmazingClient as the calling client\n    res = await b.ExtractResume(\"...\", { \"client_registry\": cr })\n```",
        "timestamp": "2024-07-17 23:37:35.027000+00:00",
        "id": 1263278458255966269,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "joatmon.pockets",
        "content": "there are 32k ones in azure https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability",
        "timestamp": "2024-07-17 23:36:38.651000+00:00",
        "id": 1263278221798146110,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "joatmon.pockets",
        "content": "https://www.promptfiddle.com/GPT4-32k-tokens-kdE9P\n\n```\nclient<llm> GPT4 {\n  // Use one of the following: https://docs.boundaryml.com/docs/snippets/clients/providers/openai\n  provider openai\n  // You can pass in any parameters from the OpenAI Python documentation into the options block.\n  options {\n    model gpt-4-32k-0613\n    max_tokens 8192\n    api_key env.OPENAI_API_KEY\n  }\n} \n```\n\nbut it is *expensive*",
        "timestamp": "2024-07-17 23:32:30.138000+00:00",
        "id": 1263277179458814064,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "let's chat",
        "timestamp": "2024-07-17 23:15:48.571000+00:00",
        "id": 1263272978582470736,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "hellovai",
        "content": "curious, would you rather have us raise an error when max_tokens is hit? (also can talk on discord really quick",
        "timestamp": "2024-07-17 23:15:42.542000+00:00",
        "id": 1263272953295015991,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "so i think i get the n-1 first entries parsed",
        "timestamp": "2024-07-17 23:15:42.414000+00:00",
        "id": 1263272952758145121,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "because my parsed response is basically a list",
        "timestamp": "2024-07-17 23:15:31.116000+00:00",
        "id": 1263272905370767461,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "correct",
        "timestamp": "2024-07-17 23:15:24.822000+00:00",
        "id": 1263272878971949157,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "hellovai",
        "content": "Oh see, basically we're parsing it for you, but programatically you have no signal about if we ran into max_tokens or not",
        "timestamp": "2024-07-17 23:15:16.245000+00:00",
        "id": 1263272842997399612,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "manually inspecting the raw LLM response it's clear that it was cut part way through",
        "timestamp": "2024-07-17 23:14:50.451000+00:00",
        "id": 1263272734809653278,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "i hear you. I mean idrc what the solution is, though my problem is I don't know whether the LLM gave me my complete list or not 😦",
        "timestamp": "2024-07-17 23:14:31.341000+00:00",
        "id": 1263272654656241684,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "hellovai",
        "content": "But that won't land in the next two weeks. The max_tokens is more likely to be done.",
        "timestamp": "2024-07-17 23:14:18.508000+00:00",
        "id": 1263272600831000759,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "hellovai",
        "content": "sadly not really. only in the dashboard. \n\nI can see if we can release this within the next few days?\n\nreason why its not very visible:\n- we only expose the parsed data type and we'd have to change the function's callstack interface to patch this. We are working on a more transparent interface that is a bit more cumbersome to call, but gives you the raw openai response we made.",
        "timestamp": "2024-07-17 23:13:50.471000+00:00",
        "id": 1263272483235303454,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "does that mean there's no way to see the stop reason for my LLM call today?",
        "timestamp": "2024-07-17 23:09:09.942000+00:00",
        "id": 1263271306611392513,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "hellovai",
        "content": "^we are thinking of ways to do taht actually! \n\nAdding a trigger into BAML that auto calls the llm when a parameter is explictly set the client.\n\nsomething like:\n\n```rust\nclient<llm> SomeClient {\n  provider openai\n  options {\n     max_tokens_behaviour 'continue' | 'stop'\n  }\n}\n```",
        "timestamp": "2024-07-17 23:07:52.804000+00:00",
        "id": 1263270983070908528,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "Alternatively, is there a way to tell in Baml when the parsed response was cut short due to max tokens being reached? This could be an indicator for me that I need to pump the output back into a continuation call",
        "timestamp": "2024-07-17 22:59:30.239000+00:00",
        "id": 1263268875160653854,
        "parent_id": null,
        "thread_id": 1263267762466328638
    },
    {
        "author": "gabev2037",
        "content": "",
        "timestamp": "2024-07-17 22:59:30.045000+00:00",
        "id": 1263268874346958879,
        "parent_id": 1263267762466328638,
        "thread_id": 1263267762466328638
    },
    {
        "author": "chocobeery",
        "content": "Hi, when I access a provided share link, I only see the default template. Is this the intended display or should it be showing someone else code?",
        "timestamp": "2024-07-17 15:16:39.678000+00:00",
        "id": 1263152396985696329,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "chocobeery",
        "content": "ic! tks for the fast response",
        "timestamp": "2024-07-18 01:25:49.106000+00:00",
        "id": 1263305696397754378,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "hellovai",
        "content": "in that specific links case the difference between that an the default tempalte is this bit i believe",
        "timestamp": "2024-07-17 15:39:01.291000+00:00",
        "id": 1263158024118468668,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-17 15:38:01.376000+00:00",
        "id": 1263157772816486505,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "hellovai",
        "content": "ah yes, i think it works, you just need to switch files to main.baml",
        "timestamp": "2024-07-17 15:37:59.578000+00:00",
        "id": 1263157765275258993,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "chocobeery",
        "content": "here you go, https://www.promptfiddle.com/gpt4o-maxes-at-4096-output-tokens-pkc3i",
        "timestamp": "2024-07-17 15:29:39.590000+00:00",
        "id": 1263155668173455522,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "hellovai",
        "content": "can you also post the link you are trying to access which isn't working?",
        "timestamp": "2024-07-17 15:24:18.484000+00:00",
        "id": 1263154321357537342,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "hellovai",
        "content": "you will need to switch files ovrer to the main.baml file! We will patch that tho!",
        "timestamp": "2024-07-17 15:23:50.068000+00:00",
        "id": 1263154202172063876,
        "parent_id": null,
        "thread_id": 1263152396985696329
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-17 15:23:49.678000+00:00",
        "id": 1263154200536285194,
        "parent_id": 1263152396985696329,
        "thread_id": 1263152396985696329
    },
    {
        "author": "robert_hoenig",
        "content": "How are the query costs in the observability platform calculated? In my case, I'm using the Huggingface Serverless Inference API Pro as a client, which has a fixed cost of $10 / month (but costs per query show up regardless).",
        "timestamp": "2024-07-17 08:44:34.037000+00:00",
        "id": 1263053723295416320,
        "parent_id": null,
        "thread_id": 1263053723295416320
    },
    {
        "author": "hellovai",
        "content": "we've actually hard coded it atm for certain models (we assume gpt-3.5 in the case of not one of the ones we support i beleive)! We're working on adding pricing to the baml client files actually! So its easier for users to control it.",
        "timestamp": "2024-07-17 08:46:55.344000+00:00",
        "id": 1263054315979804722,
        "parent_id": null,
        "thread_id": 1263053723295416320
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-17 08:46:55.070000+00:00",
        "id": 1263054314830823424,
        "parent_id": 1263053723295416320,
        "thread_id": 1263053723295416320
    },
    {
        "author": "yungweedle",
        "content": "perhaps this is too specific but if i wanted to use baml with something like windmill, i would either have to manually deploy the baml generated python files or figure out a way for the worker to have called baml-cli generate",
        "timestamp": "2024-07-17 02:12:41.414000+00:00",
        "id": 1262955104206590034,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": "deoxykev",
        "content": "I’m using temporal for workflow management with similar use case including QA like you\n\nFits well with BAML I think",
        "timestamp": "2024-07-17 04:34:27.872000+00:00",
        "id": 1262990782877663262,
        "parent_id": 1262970526322921634,
        "thread_id": 1262955104206590034
    },
    {
        "author": "yungweedle",
        "content": "separately, are there any other workflow tools that you like other than windmill? i was just trying to set up a simple pipeline with user QA / inputs at various points",
        "timestamp": "2024-07-17 03:13:58.333000+00:00",
        "id": 1262970526322921634,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": "yungweedle",
        "content": "thanks, given instructor doesn't fuzzy parse the LLM outputs i would have to use temperature > 0 for the retries to work? tbh i don't know what % of time this is even an issue but i have rather large prompt contexts / outputs so i don't want a stray comma or quote to break instructor",
        "timestamp": "2024-07-17 03:12:08.516000+00:00",
        "id": 1262970065716777010,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": "hellovai",
        "content": "Alternatively you host a python server with fly.io or something that hosts baml endpoints that you then call from windmill",
        "timestamp": "2024-07-17 02:43:42.860000+00:00",
        "id": 1262962911677124700,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": "hellovai",
        "content": "Agreed. This is not really a well supported path! (Ps there is a way if you are incredibly committed to do it without baml-cli generate, but we don’t have great docs for it. But it is technicallly possible.)\nIf you’re interested I can post a Jupyter notebook example when I’m home! But you wouldnt have the same level of autocomplete capabilities that baml-cli generate offers. Hence why instructor may be a better call.",
        "timestamp": "2024-07-17 02:42:49.922000+00:00",
        "id": 1262962689639190579,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": "deoxykev",
        "content": "I would probably just use instructor to keep things simple in windmill tbh",
        "timestamp": "2024-07-17 02:24:55.717000+00:00",
        "id": 1262958184096858204,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": "deoxykev",
        "content": "Too complicated with windmill, you have to run your baml-cli in a docker container then run everything inside",
        "timestamp": "2024-07-17 02:23:37.087000+00:00",
        "id": 1262957854298738819,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": ".aaronv",
        "content": "Yes you would need to include the generated files or do the recommended way which is to generate them at build time",
        "timestamp": "2024-07-17 02:14:36.722000+00:00",
        "id": 1262955587843653653,
        "parent_id": null,
        "thread_id": 1262955104206590034
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-17 02:14:36.321000+00:00",
        "id": 1262955586161475665,
        "parent_id": 1262955104206590034,
        "thread_id": 1262955104206590034
    },
    {
        "author": "hellovai",
        "content": "Restricting BAML output",
        "timestamp": "2024-07-16 13:08:02.830000+00:00",
        "id": 1262757642313400350,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "hellovai",
        "content": "It’s a tricky problem as technically the LLM could eventually dump out a valid json after some amount of I I am:…  one way I could imagine is you calling the stream based function and then if you don’t get a parseabke response within the first 100, then it’s not a good request. (Almost like a timeout?)",
        "timestamp": "2024-07-17 15:36:31.594000+00:00",
        "id": 1263157396243611750,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "hellovai",
        "content": "How often does this happen?\nOne idea I have is perhaps a timeout?",
        "timestamp": "2024-07-17 15:31:41.889000+00:00",
        "id": 1263156181132640289,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "hellovai",
        "content": "Oh that’s an interesting scenario. How did you handle this in the past?",
        "timestamp": "2024-07-17 15:30:10.269000+00:00",
        "id": 1263155796850507847,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "sduck.is",
        "content": "I'm trying to use it to refine roleplay responses. And this is good and bad response examples.\n\n**Good**\n```\n{\n  \"actor\": \"Harumi\",\n  \"why\": \"to respond to Ken\",\n  \"actions\": [\n    {\n      \"move\": \"She closes her book and puts it on the table\"\n    },\n    {\n      \"emotion\": \"pleasant surprise\",\n      \"speak\": \"Welcome to the club, Ken. We don't have any members yet, so we can start from scratch. What makes you interested in literature?\"\n    }\n  ]\n}\n```\n\n**Bad**\n```\nI I am: I: Harumi: I am: ... (ongoing like this)\n```",
        "timestamp": "2024-07-17 10:46:50.249000+00:00",
        "id": 1263084493598625802,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "sduck.is",
        "content": "Since `max_tokens` is set up, streaming doesn't go on indefinitely in Giverish case.",
        "timestamp": "2024-07-17 10:42:52.076000+00:00",
        "id": 1263083494628786257,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "sduck.is",
        "content": "I'm serving the model using vLLM, connecting with vLLM OpenAI Compatible API.",
        "timestamp": "2024-07-17 10:40:10.483000+00:00",
        "id": 1263082816858624114,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "hellovai",
        "content": "Also, it would be helpful to see an example of a good vs bad output. You may be able to abort for example and kill the network request as well.",
        "timestamp": "2024-07-16 13:08:58.895000+00:00",
        "id": 1262757877467058197,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "hellovai",
        "content": "<@222588854308175872> \n> I'm using BAML with llama-3-8b-instruct model. It works well overall, but sometimes it comes out as Giverish. I need a way to control this because the response lasts indefinitely.\n\nCould you further clarify? Also how are you calling llama-3? Ollama provider?\n\nCould you consider putting a max-token limit on the output?",
        "timestamp": "2024-07-16 13:08:05.960000+00:00",
        "id": 1262757655441571850,
        "parent_id": null,
        "thread_id": 1262757642313400350
    },
    {
        "author": "neuralcorrelate",
        "content": "when I save my baml file in vs code, it is autogenerating a baml_client directory in the parent directory,  and not the directory that contains baml_src. How can I fix this?",
        "timestamp": "2024-07-15 15:49:32.243000+00:00",
        "id": 1262435894791766016,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": ".aaronv",
        "content": "Version 0.51.0 of VSCode (update baml-py or the TS dependency as well to that version)",
        "timestamp": "2024-07-19 23:28:24.221000+00:00",
        "id": 1264000923739619338,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": ".aaronv",
        "content": "It's ready, see https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#vscode-generator-settings !",
        "timestamp": "2024-07-19 23:28:03.820000+00:00",
        "id": 1264000838171627694,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "hellovai",
        "content": "FYI we sadly couldn’t land it in today, but it should come in tmrw! We’re just doing some final testing. \n\nhttps://github.com/BoundaryML/baml/pull/791",
        "timestamp": "2024-07-16 07:27:33.025000+00:00",
        "id": 1262671953500831866,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "hellovai",
        "content": "Oh! Thats great to know. We'll also file that bug, but for now disabling it seems like the easiest fix.",
        "timestamp": "2024-07-15 16:45:49.142000+00:00",
        "id": 1262450058532618250,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "neuralcorrelate",
        "content": "it works correctly with baml-cli generate, but not when it is autogenerated on saves",
        "timestamp": "2024-07-15 16:44:56.519000+00:00",
        "id": 1262449837815631933,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "neuralcorrelate",
        "content": "if this is my directory structure ./parent_dir/package/baml_src I am finding that baml_client is being generated here ./parent_dir/baml_client instead of ./parent/package/baml_client",
        "timestamp": "2024-07-15 16:44:43.993000+00:00",
        "id": 1262449785277775922,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "hellovai",
        "content": "Out of curiosity, can you share why you don't want it to auto-generate? Is it beacuse you are just testing `baml` out for now or just you don't like auto-generate and prefer to have more control over when it runs?",
        "timestamp": "2024-07-15 16:18:49.312000+00:00",
        "id": 1262443264473305088,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "hellovai",
        "content": "we're actually working on adding the capability to disable auto-generate. Should land either today or tmrw! <@201399017161097216> in on check of this.\n\n`baml_client` should actually always be generated outside of the `baml_src` folder as its a binding from baml -> language of your choice, so we opt to not include it as a part of the `baml` project.\n\nthat said, you can modify the `generator` block that was auto-generated for you to change the path where `baml_client` is generated. Or if you don't want any bindings, you can just remove that block all together.\n\nIf the block is not present that should disable auto-generate as well.\n\nThen you can see the `baml-cli generate --help` for instructions on how to generate if the block is not present.",
        "timestamp": "2024-07-15 16:17:53.688000+00:00",
        "id": 1262443031169204285,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "neuralcorrelate",
        "content": "or better yet, can I disable the autogenerate on save in vscode?",
        "timestamp": "2024-07-15 15:54:53.181000+00:00",
        "id": 1262437240903045295,
        "parent_id": null,
        "thread_id": 1262435894791766016
    },
    {
        "author": "neuralcorrelate",
        "content": "",
        "timestamp": "2024-07-15 15:54:52.717000+00:00",
        "id": 1262437238957146152,
        "parent_id": 1262435894791766016,
        "thread_id": 1262435894791766016
    },
    {
        "author": "kdub03",
        "content": "Is there thought/work towards how this could work with evaluations? I'm finding that defining my models in baml is more challenging to iterate my ground truth -> schema mapping, since the schema is abstracted and volatile.",
        "timestamp": "2024-07-15 14:33:40.167000+00:00",
        "id": 1262416802000928999,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "got it! thats very helpful. thanks for sharing. If it understand correctly the issue boils down to:\n1. Due to LLMs, you end up. changing the datamodel quite a lot\n2. the requires you to modify the output quite a lot to get the final schema you want.\n\nI think we have a couple of things in store that will help with this (evaluations in BAML is one, but i think its insufficient to address all your needs here, you likely need something like the `@compute` and `@no_binding` idea we briefly touch on in the spec i listed).\n\nHere's my suggestion to help unblock you wihtout any new features:\n\nI would write a simple test suite in python like the following:\n\n```python\n# test_my_type.py\n\nfrom baml_client import b\nfrom baml_client.types import MyLLMResponseType\n\nfrom app.somewhere import MyGTType\n\n# Update this function every time you modify MyLLMResponseType\ndef validate(res: MyLLMResponseType, expected: MyGTType):\n  assert res.foo == expected.bar\n\nasync def test_my_function():\n  params, expected = load_from_file(\"./mydata.json\")\n  res = await b.MyFunction(**params)\n  validate(res, expected)\n```",
        "timestamp": "2024-07-15 15:00:22.517000+00:00",
        "id": 1262423522744074240,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "kdub03",
        "content": "Python:\n\nI'm working with commercial insurance data, so I represent different types of coverages, limits, endorsements as various classes. I find I need to iterate on things being like, 4 booleans, vs an enum. Or how detailed an endorsement extraction is. As I iterate on those, I would typically make changes to my GT to reflect what I want.\nWith the abstraction from baml, its harder to \"sync\" my GT data to the schema I need to compare against.",
        "timestamp": "2024-07-15 14:52:22.045000+00:00",
        "id": 1262421507498446919,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "and which binding are you using? TS or Python or ruby?",
        "timestamp": "2024-07-15 14:48:31.443000+00:00",
        "id": 1262420540283424928,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "curious, can you share with me a code sample of how the two end up diverting?",
        "timestamp": "2024-07-15 14:48:13.463000+00:00",
        "id": 1262420464869834792,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "that makes sense. whats hard is not having access to some map based operator to help you do transformations?",
        "timestamp": "2024-07-15 14:47:23.138000+00:00",
        "id": 1262420253791748137,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "kdub03",
        "content": "Does my problem space make sense? So I have 10x ground truth examples. I think my ideal workflow would be to iterate on extraction schemas, and test them against my GT data. The challenge is that I have to continually adapt my GT data to match my extraction schema, since the two don't necessarily align, and any changes I make to extraction would require changes on each GT case.",
        "timestamp": "2024-07-15 14:46:21.353000+00:00",
        "id": 1262419994646679673,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "What would be very helpful here is if you could share a few examples of ground turth you’d like to assert!",
        "timestamp": "2024-07-15 14:44:51.378000+00:00",
        "id": 1262419617264046080,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "Sorry misread. That feature is also coming up shortly! We have a spec on it that we will post for review and then implement shortly! It will be something akin to adding a the assert keyword in a test case",
        "timestamp": "2024-07-15 14:44:02.563000+00:00",
        "id": 1262419412519227527,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "Oh wait. You mean asserts in test cases!",
        "timestamp": "2024-07-15 14:42:41.337000+00:00",
        "id": 1262419071832686705,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "Your timing is fortuitous!  I just started my draft of this feature!\n\nhttps://github.com/orgs/BoundaryML/discussions/786",
        "timestamp": "2024-07-15 14:42:05.756000+00:00",
        "id": 1262418922595156081,
        "parent_id": null,
        "thread_id": 1262416802000928999
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-15 14:42:05.297000+00:00",
        "id": 1262418920669974580,
        "parent_id": 1262416802000928999,
        "thread_id": 1262416802000928999
    },
    {
        "author": ".aaronv",
        "content": "Formatting",
        "timestamp": "2024-07-14 07:04:14.369000+00:00",
        "id": 1261941311380914258,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "etbyrd",
        "content": "Are there any plans to support auto-formatting in VS Code?",
        "timestamp": "2024-07-14 05:20:26.760000+00:00",
        "id": 1261915190895579197,
        "parent_id": null,
        "thread_id": 1261915190895579197
    },
    {
        "author": "etbyrd",
        "content": "Awesome, thanks!",
        "timestamp": "2024-07-15 00:35:21.877000+00:00",
        "id": 1262205835682119811,
        "parent_id": null,
        "thread_id": 1261915190895579197
    },
    {
        "author": ".aaronv",
        "content": "Yes! It is on our roadmap! But we dont have an exact date just yet",
        "timestamp": "2024-07-14 07:04:15.023000+00:00",
        "id": 1261941314123989065,
        "parent_id": null,
        "thread_id": 1261915190895579197
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-14 07:04:14.248000+00:00",
        "id": 1261941310873407568,
        "parent_id": 1261915190895579197,
        "thread_id": 1261915190895579197
    },
    {
        "author": "joatmon.pockets",
        "content": "100% cpu usage",
        "timestamp": "2024-07-12 16:04:03.821000+00:00",
        "id": 1261352386858061860,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabev2037",
        "content": "Does baml offer some sort of timeout on clients? in development i noticed a boundary function led to 100% CPU usage for my worker and it's clearly stalled. Though I'm not seeing any options to set timeouts (which I would think can be irrespective of provider since it's controlled by the baml client)\n\nhttps://docs.boundaryml.com/docs/snippets/clients/overview",
        "timestamp": "2024-07-12 15:39:17.534000+00:00",
        "id": 1261346152918290508,
        "parent_id": null,
        "thread_id": 1261346152918290508
    },
    {
        "author": "hellovai",
        "content": "Answered in <#1262757642313400350>",
        "timestamp": "2024-07-16 13:09:24.567000+00:00",
        "id": 1262757985143488532,
        "parent_id": null,
        "thread_id": 1261346152918290508
    },
    {
        "author": "sduck.is",
        "content": "Anyway, it's a great library. Thank you for the hard work of the baml team. 👍",
        "timestamp": "2024-07-16 08:15:37.271000+00:00",
        "id": 1262684050905370704,
        "parent_id": null,
        "thread_id": 1261346152918290508
    },
    {
        "author": "sduck.is",
        "content": "I'm using BAML with `llama-3-8b-instruct` model. It works well overall, but sometimes it comes out as Giverish. I need a way to control this because the response lasts indefinitely.",
        "timestamp": "2024-07-16 08:15:02.300000+00:00",
        "id": 1262683904226230312,
        "parent_id": null,
        "thread_id": 1261346152918290508
    },
    {
        "author": "hellovai",
        "content": "<@711679663746842796> can you add a feature request for adding timeouts?",
        "timestamp": "2024-07-12 17:21:05.822000+00:00",
        "id": 1261371772935213239,
        "parent_id": null,
        "thread_id": 1261346152918290508
    },
    {
        "author": "joatmon.pockets",
        "content": "this is almost definitely the tracing bug you’re hitting, unfortunately- you can use atexit.unregister(flush) to disable this",
        "timestamp": "2024-07-12 16:04:04.107000+00:00",
        "id": 1261352388057497694,
        "parent_id": null,
        "thread_id": 1261346152918290508
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-07-12 16:04:03.821000+00:00",
        "id": 1261352386858061855,
        "parent_id": 1261346152918290508,
        "thread_id": 1261346152918290508
    },
    {
        "author": "gabev2037",
        "content": "Where can I find the correct azure api_version I should be using?",
        "timestamp": "2024-07-11 23:05:05.449000+00:00",
        "id": 1261095953939693658,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "deoxykev",
        "content": "Here’s our working azure config\n\nclient<llm> AzureGPT4o {\n  provider azure-openai\n  options {\n    temperature 0.0\n    seed 42\n    base_url \"https://mycustomdomain.openai.azure.com/openai/deployments/gpt-4o\"\n    api_version \"2024-02-01\"\n    api_key env.AZURE_OPENAI_API_KEY\n  }\n}",
        "timestamp": "2024-07-12 01:40:58.431000+00:00",
        "id": 1261135183189512323,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "gabev2037",
        "content": "you're good, p sure the link i dropped is the one",
        "timestamp": "2024-07-11 23:11:18.787000+00:00",
        "id": 1261097519832629258,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "joatmon.pockets",
        "content": "(we're setting up for an event right now, hence why we're a bit slow to respond right now)",
        "timestamp": "2024-07-11 23:11:07.542000+00:00",
        "id": 1261097472667811850,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "joatmon.pockets",
        "content": "none of us know where in the azure portal it is",
        "timestamp": "2024-07-11 23:10:42.837000+00:00",
        "id": 1261097369047666748,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "gabev2037",
        "content": "I found this? https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation",
        "timestamp": "2024-07-11 23:09:35.798000+00:00",
        "id": 1261097087865589814,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "joatmon.pockets",
        "content": "lemme check",
        "timestamp": "2024-07-11 23:08:54.091000+00:00",
        "id": 1261096912933879838,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "joatmon.pockets",
        "content": "ahh",
        "timestamp": "2024-07-11 23:08:20.403000+00:00",
        "id": 1261096771635904633,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "gabev2037",
        "content": "to be clear: I'm trying to figure out what the right api_version is in azure haha, was wondering if someone knew where in the azure portal I could find that",
        "timestamp": "2024-07-11 23:08:04.018000+00:00",
        "id": 1261096702912237689,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "joatmon.pockets",
        "content": "https://docs.boundaryml.com/docs/snippets/clients/providers/azure",
        "timestamp": "2024-07-11 23:07:25.006000+00:00",
        "id": 1261096539284177027,
        "parent_id": null,
        "thread_id": 1261095953939693658
    },
    {
        "author": "joatmon.pockets",
        "content": "",
        "timestamp": "2024-07-11 23:07:24.742000+00:00",
        "id": 1261096538176749618,
        "parent_id": 1261095953939693658,
        "thread_id": 1261095953939693658
    },
    {
        "author": "gabev2037",
        "content": "Did you guys say something about rendering `\\n` characters not actually rendering newlines?",
        "timestamp": "2024-07-11 20:31:45.063000+00:00",
        "id": 1261057364723761152,
        "parent_id": null,
        "thread_id": 1261057364723761152
    },
    {
        "author": "gabev2037",
        "content": "But is this for test cases or for the actual function ?",
        "timestamp": "2024-07-11 20:34:08.427000+00:00",
        "id": 1261057966036095098,
        "parent_id": null,
        "thread_id": 1261057364723761152
    },
    {
        "author": "hellovai",
        "content": "I would recommend block strings isntead `#\"..\"#` with actual new lines. \n\nWe're working on patching a bug in our quoted string `\"...\"` so it does correct escaping!",
        "timestamp": "2024-07-11 20:32:42.970000+00:00",
        "id": 1261057607603589341,
        "parent_id": null,
        "thread_id": 1261057364723761152
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-11 20:32:42.630000+00:00",
        "id": 1261057606177525850,
        "parent_id": 1261057364723761152,
        "thread_id": 1261057364723761152
    },
    {
        "author": "deoxykev",
        "content": "will the version after 0.49.0 be released today? I want to try out the dynamic clients",
        "timestamp": "2024-07-11 17:44:05.702000+00:00",
        "id": 1261015172705812510,
        "parent_id": null,
        "thread_id": 1261015172705812510
    },
    {
        "author": "hellovai",
        "content": "version 0.50 is out which now supports ClientRegistry. \n\nhttps://docs.boundaryml.com/docs/calling-baml/client-registry",
        "timestamp": "2024-07-11 19:19:41.056000+00:00",
        "id": 1261039228523909171,
        "parent_id": null,
        "thread_id": 1261015172705812510
    },
    {
        "author": "hellovai",
        "content": "yes! we're working on the release this AM.",
        "timestamp": "2024-07-11 17:44:36.400000+00:00",
        "id": 1261015301462560798,
        "parent_id": null,
        "thread_id": 1261015172705812510
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-11 17:44:36.042000+00:00",
        "id": 1261015299960999986,
        "parent_id": 1261015172705812510,
        "thread_id": 1261015172705812510
    },
    {
        "author": "gabev2037",
        "content": "Anyone know whether Claude 3.5 Sonnet is actually better than Opus in every regard?",
        "timestamp": "2024-07-11 15:24:53.316000+00:00",
        "id": 1260980140259938344,
        "parent_id": null,
        "thread_id": 1260980140259938344
    },
    {
        "author": "joatmon.pockets",
        "content": "https://chat.lmsys.org/?leaderboard as well",
        "timestamp": "2024-07-11 17:26:12.109000+00:00",
        "id": 1261010669730402365,
        "parent_id": null,
        "thread_id": 1260980140259938344
    },
    {
        "author": ".aaronv",
        "content": "someone also did this recently: https://vlmsareblind.github.io/ but it's not peer reviewed fyi (and their prompts may be bad)",
        "timestamp": "2024-07-11 17:12:08.718000+00:00",
        "id": 1261007132292153385,
        "parent_id": null,
        "thread_id": 1260980140259938344
    },
    {
        "author": "hellovai",
        "content": "Here's some benchmarks:\nhttps://gorilla.cs.berkeley.edu/leaderboard.html\n\nhttps://klu.ai/glossary/mmlu-eval\n\nIt seems like yes from these cases fyi!",
        "timestamp": "2024-07-11 15:27:19.264000+00:00",
        "id": 1260980752409952337,
        "parent_id": null,
        "thread_id": 1260980140259938344
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-11 15:27:18.958000+00:00",
        "id": 1260980751126630420,
        "parent_id": 1260980140259938344,
        "thread_id": 1260980140259938344
    },
    {
        "author": "hellovai",
        "content": "VLLM support",
        "timestamp": "2024-07-11 14:15:51.374000+00:00",
        "id": 1260962767695777917,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "feres0902",
        "content": "i want to change code and add a vllm provider how could i do it im new here",
        "timestamp": "2024-07-11 09:26:27.994000+00:00",
        "id": 1260889940401721474,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "hellovai",
        "content": "awesome, let me chat with the team tmrw and figure out what this would take for us to expose for you!",
        "timestamp": "2024-08-14 07:27:42.880000+00:00",
        "id": 1273181242937966613,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "feres0902",
        "content": "Yes agree on the logic; it should send a batch of prompts and then loop to parse (using baml parser) \n, i do vllm inference this way:\nclient = OpenAI(\n                    base_url=\"http://localhost:8000/v1\",\n                    api_key=\"token-abc123\",\n                )\n\ncompletion = client.completions.create(\n        model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n        prompt=list_of_prompts,\n        max_tokens= 1000 # chane this if you notice that the model output is not complete\n        )",
        "timestamp": "2024-08-14 07:21:08.759000+00:00",
        "id": 1273179589874810976,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "hellovai",
        "content": "native batch support will be tricky as openai's batching inference is quite different than how VLLM does it.",
        "timestamp": "2024-08-13 14:24:27.994000+00:00",
        "id": 1272923734121910347,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "hellovai",
        "content": "perfect. i think batching is quite interesting of a concept, i wonder if we can support this more natively. \n\nIt doesn't look like their native sdk supports batched inference via the openai compatiable one:\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n\nWe would likely need to create a new instance of batch that is supported here. \nI wonder if it may be easier for these kinds of use cases for us to have an API in our SDK that does something like the following:\n\n```python\nfrom vllm import LLM, SamplingParams\n\nprompts = b.parts.prompt.MyFunction(...)\n# you call VLLM directly\noutputs = llm.generate(prompts, sampling_params)\n# you parse it using baml\nfor output in outputs:\n   parsed = b.parts.parse.MyFunction(output)\n```",
        "timestamp": "2024-08-13 14:22:37.468000+00:00",
        "id": 1272923270542393354,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "feres0902",
        "content": "sorry for late replay, \nThank you so much, yes i did make it work like this, the only issue now is how to send a batch requests so vlmm deal with them at very short time as the call now are made throw baml python code, the vllm openai compatible client also accespt multiple requests at the same time and the response time is very short",
        "timestamp": "2024-08-13 14:15:05.653000+00:00",
        "id": 1272921375492804630,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "hellovai",
        "content": "Hi Feres, to add VLLM support we can do it a few ways:\n\n1. add official provider in rust (if you are familiar with it, I can point you to the code)\n2. Use VLLM's openai compatibility and it should work out of the box with BAML\n\n\nHere's how you can do it for 2:\nhttps://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n\nUsing other openai compatible providers:\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/other\n\n```bash\npython -m vllm.entrypoints.openai.api_server --model NousResearch/Meta-Llama-3-8B-Instruct --dtype auto --api-key $VLLM_API_KEY\n```\n\n```rust\nclient<llm> MyVLLM {\n  provider openai\n  options {\n    base_url \"http://localhost:8000/v1\"\n    api_key env.VLLM_API_KEY\n    model \"NousResearch/Meta-Llama-3-8B-Instruct\"\n  }\n}\n```",
        "timestamp": "2024-07-11 14:15:51.726000+00:00",
        "id": 1260962769172168784,
        "parent_id": null,
        "thread_id": 1260889940401721474
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-11 14:15:51.208000+00:00",
        "id": 1260962766999519234,
        "parent_id": 1260889940401721474,
        "thread_id": 1260889940401721474
    },
    {
        "author": "gabev2037",
        "content": "I am processing a PDF with unstructured. At some points, I recognize that a Table is present. Unfortunately, Unstructured doesn't do a great job with preserving the format of the table/form. I have found that if I can pipe that table somehow to GPT4o, it does a really great job of outputting the table in markdown. Is there a clean way to accomplish something like this in Baml? I know the page number in the PDF where the table lives",
        "timestamp": "2024-07-10 19:39:05.808000+00:00",
        "id": 1260681725982544035,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "so its helpful for us too 🙂",
        "timestamp": "2024-07-10 22:45:44.073000+00:00",
        "id": 1260728694910091287,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "not an issue at all, i gotta update the docs on testing images now cause it took me more than 5 mins to find it when you asked",
        "timestamp": "2024-07-10 22:45:39.177000+00:00",
        "id": 1260728674374909992,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "I appreciate you guys having patience lol",
        "timestamp": "2024-07-10 22:45:06.868000+00:00",
        "id": 1260728538861146122,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "😉",
        "timestamp": "2024-07-10 22:44:55.670000+00:00",
        "id": 1260728491893199011,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "`image/jpeg` lol",
        "timestamp": "2024-07-10 22:44:49.985000+00:00",
        "id": 1260728468048707594,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "🙂",
        "timestamp": "2024-07-10 22:44:46.345000+00:00",
        "id": 1260728452781310133,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "fixed it",
        "timestamp": "2024-07-10 22:44:42.941000+00:00",
        "id": 1260728438504034315,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "so it is almost def the fact that claude may need a different serialization, is it png or jpeg?",
        "timestamp": "2024-07-10 22:44:07.225000+00:00",
        "id": 1260728288700272781,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "that seems to be a message directly from claude's servers",
        "timestamp": "2024-07-10 22:43:10.473000+00:00",
        "id": 1260728050664865853,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "^ from claude 35",
        "timestamp": "2024-07-10 22:42:49.109000+00:00",
        "id": 1260727961057755228,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "in curl*",
        "timestamp": "2024-07-10 22:42:42.408000+00:00",
        "id": 1260727932951728230,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "Looking at boundary dashboard, this is the return value:\n```\nRequest failed: {\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"messages.0.content.1.image.source.base64.data: The image was specified using the image/png media type, but does not appear to be a valid png image\"}}\n```",
        "timestamp": "2024-07-10 22:42:39.730000+00:00",
        "id": 1260727921719513108,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "easy thing to do is click on `raw curl` toggle, try the request in browser",
        "timestamp": "2024-07-10 22:42:34.856000+00:00",
        "id": 1260727901276602439,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "does this mean sonnet isn't supported?",
        "timestamp": "2024-07-10 22:42:00.470000+00:00",
        "id": 1260727757051134012,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "I have a fallback which starts with claude 3.5. sonnet and then gpt4o, seems like the test is defaulting to gpt 4o",
        "timestamp": "2024-07-10 22:41:56.565000+00:00",
        "id": 1260727740672245842,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "ty",
        "timestamp": "2024-07-10 22:33:18.427000+00:00",
        "id": 1260725567443959859,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "legendary",
        "timestamp": "2024-07-10 22:33:17.546000+00:00",
        "id": 1260725563748909160,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "ah yes!  \n\nhttps://docs.boundaryml.com/docs/snippets/test-cases#images",
        "timestamp": "2024-07-10 22:33:05.490000+00:00",
        "id": 1260725513182380102,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "can I write test cases in my Baml file for functions that accept images as inputs",
        "timestamp": "2024-07-10 22:32:09.596000+00:00",
        "id": 1260725278745952267,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "what do you mean?",
        "timestamp": "2024-07-10 22:31:54.434000+00:00",
        "id": 1260725215151915111,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "that would be great Lol",
        "timestamp": "2024-07-10 22:31:37.672000+00:00",
        "id": 1260725144846860371,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "Would baml support test cases if i input a base64 string?",
        "timestamp": "2024-07-10 22:31:30.586000+00:00",
        "id": 1260725115126022174,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "thnaks!",
        "timestamp": "2024-07-10 22:27:32.561000+00:00",
        "id": 1260724116776947762,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "I'm not sure what Openai/Anthropic explicitly support, but the copmlete list is here:\n\nhttps://www.iana.org/assignments/media-types/media-types.xhtml#image",
        "timestamp": "2024-07-10 22:26:57.469000+00:00",
        "id": 1260723969590562947,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "dumb question but what are the options then?",
        "timestamp": "2024-07-10 22:25:59.327000+00:00",
        "id": 1260723725725208726,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "it should support any media type! We just add it into the CURL request as the model requests it",
        "timestamp": "2024-07-10 22:24:47.743000+00:00",
        "id": 1260723425480020079,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "i believe it's default saved as jpg",
        "timestamp": "2024-07-10 22:24:12.816000+00:00",
        "id": 1260723278985695272,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "```python\nfrom baml_py import Image\nfrom baml_client import b\n\nasync def test_image_input():\n  # from URL\n  res = await b.TestImageInput(\n      img=Image.from_url(\n          \"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n      )\n  )\n\n  # Base64 image\n  image_b64 = \"iVBORw0K....\"\n  res = await b.TestImageInput(\n    img=Image.from_base64(\"image/png\", image_b64)\n  )\n```\n\nDoes it have to be `image/png` or what are the options for the base64?",
        "timestamp": "2024-07-10 22:24:08.934000+00:00",
        "id": 1260723262703272116,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "ok, let me see how well the base64 approach works",
        "timestamp": "2024-07-10 20:48:34.955000+00:00",
        "id": 1260699212652216431,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "But only for playground. In the actual code youll likely always need base64 or image url",
        "timestamp": "2024-07-10 20:47:31.977000+00:00",
        "id": 1260698948503470090,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "We dont support loading from a file yet :(, only base64 and image url. We have file uris in our roadmap",
        "timestamp": "2024-07-10 20:46:51.708000+00:00",
        "id": 1260698779603042337,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "Maybe a silly question but in the docs I only see balml Images loaded from URL or base64, is there a way to do it when i have a saved .jpg file?",
        "timestamp": "2024-07-10 20:46:00.752000+00:00",
        "id": 1260698565878222919,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "oh interesting, yea makes sense!",
        "timestamp": "2024-07-10 20:34:49.234000+00:00",
        "id": 1260695749327327232,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "in a temporary file location",
        "timestamp": "2024-07-10 20:34:42.035000+00:00",
        "id": 1260695719132790846,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "yep",
        "timestamp": "2024-07-10 20:34:35.973000+00:00",
        "id": 1260695693706662060,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "wait do you save to disk?",
        "timestamp": "2024-07-10 20:34:17.453000+00:00",
        "id": 1260695616028147833,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "whats that lib?",
        "timestamp": "2024-07-10 20:34:09.439000+00:00",
        "id": 1260695582415257711,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "no, in the instructured library there is a way to specify saving tables as images",
        "timestamp": "2024-07-10 20:33:58.500000+00:00",
        "id": 1260695536533635166,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "or something else?",
        "timestamp": "2024-07-10 20:33:34.733000+00:00",
        "id": 1260695436847616050,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "btw did you end up using the codesnippet I had for converting to baml Image?",
        "timestamp": "2024-07-10 20:33:30.429000+00:00",
        "id": 1260695418795196486,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "lol",
        "timestamp": "2024-07-10 20:32:45.705000+00:00",
        "id": 1260695231209406474,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "but idk man sometimes when you're talking to customers, certain data privacy things are ok and others aren't",
        "timestamp": "2024-07-10 20:32:39.621000+00:00",
        "id": 1260695205691265034,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "or do you mean local for everything but that?",
        "timestamp": "2024-07-10 20:32:29.305000+00:00",
        "id": 1260695162422825061,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "correct",
        "timestamp": "2024-07-10 20:32:24.381000+00:00",
        "id": 1260695141769805935,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "wait gpt4o isn't local tho",
        "timestamp": "2024-07-10 20:32:21.178000+00:00",
        "id": 1260695128335716472,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "plus the local-inference is huge",
        "timestamp": "2024-07-10 20:32:05.432000+00:00",
        "id": 1260695062292201503,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "it is very good, though honestly this combo is just as good and (i imagine) a lot cheaper",
        "timestamp": "2024-07-10 20:31:55.187000+00:00",
        "id": 1260695019321557012,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "I did test it since you can use a demo version",
        "timestamp": "2024-07-10 20:31:43.337000+00:00",
        "id": 1260694969618792479,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "i see, i have heard textract is really damn good but makes sense",
        "timestamp": "2024-07-10 20:31:20.429000+00:00",
        "id": 1260694873535811584,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "at least given how many pages i need to process",
        "timestamp": "2024-07-10 20:30:35.847000+00:00",
        "id": 1260694686545477713,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "alternative was their API or AWS Textract but it's wildly expensive",
        "timestamp": "2024-07-10 20:30:28.182000+00:00",
        "id": 1260694654395875358,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "gotcha",
        "timestamp": "2024-07-10 20:30:14.787000+00:00",
        "id": 1260694598213177444,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "so it's a bit slow",
        "timestamp": "2024-07-10 20:30:11.832000+00:00",
        "id": 1260694585819005013,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "ya everything is done locally",
        "timestamp": "2024-07-10 20:30:09.409000+00:00",
        "id": 1260694575656206336,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "oh interesting, you use their `system dependencies` yourself to OCR then",
        "timestamp": "2024-07-10 20:29:57.406000+00:00",
        "id": 1260694525312106506,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "so definitely a lot of finnagling and diving into their repo",
        "timestamp": "2024-07-10 20:29:45.310000+00:00",
        "id": 1260694474577940653,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "not their API",
        "timestamp": "2024-07-10 20:29:32.266000+00:00",
        "id": 1260694419867304100,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "I use the open source stuff",
        "timestamp": "2024-07-10 20:29:30.779000+00:00",
        "id": 1260694413630378004,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "https://docs.unstructured.io/open-source/introduction/quick-start#installation",
        "timestamp": "2024-07-10 20:29:27.649000+00:00",
        "id": 1260694400502206514,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "do you have a link  to the `unstructured` stuff you were using?",
        "timestamp": "2024-07-10 20:29:12.973000+00:00",
        "id": 1260694338946469960,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "LFG 🙂",
        "timestamp": "2024-07-10 20:29:12.643000+00:00",
        "id": 1260694337562345652,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "ya this image --> markdown is waaay better. look at this:\n\n# before \n```\nProposal Volumes Volume Title Page Limitation Volume I Volume II Volume III Executed Solicitation Documents (Section K) and SF 33 N/A Executive Summary (1 page) Technical Approach (15 pages) Experience (5 pages) Resumes of Key Personnel not subject to page limitations Management Approach (14 pages) Past Performance Questionnaires and References (with Narrative) Cost/Price Proposal 35 Pages Total N/A Volume IV N/A\n```\n\n# after\n```\n| Proposal Volumes | Volume Title                                                                                | Page Limitation |\n|------------------|---------------------------------------------------------------------------------------------|-----------------|\n| Volume I         | Executed Solicitation Documents (Section K) and SF 33                                       | N/A             |\n| Volume II        | Executive Summary (1 page) Technical Approach (15 pages) Experience (5 pages) Resumes of Key Personnel not subject to page limitations Management Approach (14 pages) | 35 Pages Total  |\n| Volume III       | Past Performance Questionnaires and References (with Narrative)                             | N/A             |\n| Volume IV        | Cost/Price Proposal                                                                         | N/A             |\n```",
        "timestamp": "2024-07-10 20:28:05.896000+00:00",
        "id": 1260694057605136404,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "gemini is kinda trash tbh but i havent tested all their models",
        "timestamp": "2024-07-10 20:27:25.907000+00:00",
        "id": 1260693889879113760,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "and gemini 😉",
        "timestamp": "2024-07-10 20:27:00.956000+00:00",
        "id": 1260693785227300998,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "oh baby",
        "timestamp": "2024-07-10 20:26:52.427000+00:00",
        "id": 1260693749453950996,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": ".aaronv",
        "content": "sonnet 3.5 also supports images",
        "timestamp": "2024-07-10 20:26:39.067000+00:00",
        "id": 1260693693418045480,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "Is GPT4o the only model that supports images?",
        "timestamp": "2024-07-10 20:26:29.800000+00:00",
        "id": 1260693654549303296,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "though as long as I can get it to markdown, then I can dump it into the actual text and downstream LLMs will likely parse it better as opposed to nonsensical text",
        "timestamp": "2024-07-10 20:23:59.614000+00:00",
        "id": 1260693024623693926,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "That's interesting... I'll need to think about how I can architect our system to handle that. I would need to persist this class to my DB but without a predefined schema, it might not be possible",
        "timestamp": "2024-07-10 20:23:38.147000+00:00",
        "id": 1260692934584434801,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "then you can render reach row as such.",
        "timestamp": "2024-07-10 20:20:38.254000+00:00",
        "id": 1260692180058509336,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "one idea:\n\nif you run the first call to get TableHeaders[] you can have it spit out:\n\n```rust\nclass Header {\n  name string\n  type ValueType\n}\n\nenum ValueType {\n  String\n  Int\n  Float\n  StringList\n  IntList\n  ...\n}\n```\n\nThen you can technically use dynamic types on the second call to return a MyResponseType[]\n\nWhich then is built to check the types of each item.",
        "timestamp": "2024-07-10 20:19:51.073000+00:00",
        "id": 1260691982167314453,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "I'm thinking i'll just ask it to return in markdown which downstream LLMs can use",
        "timestamp": "2024-07-10 20:16:41.226000+00:00",
        "id": 1260691185891020851,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "it really depends on how complicated the table is",
        "timestamp": "2024-07-10 20:15:52.653000+00:00",
        "id": 1260690982161088614,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "ok I am able to get images. Is there any suggestion for the baml class? I'm comfortable with just markdown but wondering if you have seen others have success with other prompts?",
        "timestamp": "2024-07-10 20:15:12.691000+00:00",
        "id": 1260690814548578314,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "good to know for us, i should write a cookbook on this!",
        "timestamp": "2024-07-10 19:57:56.996000+00:00",
        "id": 1260686470528630955,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "yep! basically!",
        "timestamp": "2024-07-10 19:57:42.427000+00:00",
        "id": 1260686409421950976,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "Then I guess use BAML to call GPT4o with the image and ask it to return the table in markdown (or some class I define)",
        "timestamp": "2024-07-10 19:57:34.899000+00:00",
        "id": 1260686377847095368,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "Ok, yea I was considering using something like this to turn the page into an image. I believe Unstructured even supports writing the tables to IMages itself",
        "timestamp": "2024-07-10 19:57:00.489000+00:00",
        "id": 1260686233521098753,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "```diff\n+ from PIL import Image as PILImage\n\ndef ...\n  # Convert pixmap to bytes\n  img_bytes = pix.tobytes(\"png\")\n    \n+ # Save the image to disk using PIL\n+ image = PILImage.open(BytesIO(img_bytes))\n+ image.save(output_image_path, format=\"PNG\")\n```",
        "timestamp": "2024-07-10 19:56:50.867000+00:00",
        "id": 1260686193163763793,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "You can then also use the PIL library to dump out the images to disk if you want to debug it better",
        "timestamp": "2024-07-10 19:55:36.508000+00:00",
        "id": 1260685881279381585,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "given that openai prefers image dimensions in mutliples of 512, i would highly recommend playing around with zoom factor to optimize for that",
        "timestamp": "2024-07-10 19:54:55.342000+00:00",
        "id": 1260685708616798230,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "not a PDF directly, but we support an image type.\n\nwe haven't built in file types yet as most models don't support them.\n\nBut i think you should be able to write a pretty small class that can get the right page out of a PDF:\n\nIt sounds like the library you'd need to use is: fitz (PyMuPDF)\n\n```python\nimport fitz  # PyMuPDF\nimport base64\nfrom io import BytesIO\nfrom baml_py import Image\n\ndef pdf_page_to_base64(pdf_path: str, page_number: int = 0, zoom: int = 2) -> Image:\n    \"\"\"\n    Convert a specified page of a PDF to a base64-encoded image.\n\n    :param pdf_path: Path to the PDF file.\n    :param page_number: Page number to convert (0-indexed).\n    :param zoom: Zoom factor for rendering the page.\n    :return: Baml Image\n    \"\"\"\n    # Open the PDF file\n    pdf_document = fitz.open(pdf_path)\n    \n    # Get the specified page\n    page = pdf_document.load_page(page_number)\n    \n    # Render page to a pixmap\n    pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))\n    \n    # Convert pixmap to bytes\n    img_bytes = pix.tobytes(\"png\")\n    \n    # Encode the image to base64\n    img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n\n    return Image.from_base64(\"image/png\", img_base64)\n```\n\nhttps://pymupdf.readthedocs.io/en/latest/pixmap.html#Pixmap.tobytes",
        "timestamp": "2024-07-10 19:52:45.342000+00:00",
        "id": 1260685163357016166,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "so ideally, i just take the page where the table exists that I want to extract",
        "timestamp": "2024-07-10 19:45:46.969000+00:00",
        "id": 1260683408573464607,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "these PDFs can be hundreds of pages long",
        "timestamp": "2024-07-10 19:45:38.385000+00:00",
        "id": 1260683372569825301,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "gabev2037",
        "content": "well, do you guys support ingesting a PDF?",
        "timestamp": "2024-07-10 19:45:30.634000+00:00",
        "id": 1260683340059512905,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "or did you have another thought in mind about what about would be a better more BAML-native way to do this?",
        "timestamp": "2024-07-10 19:42:57.721000+00:00",
        "id": 1260682698696032368,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "also if you want, you can pass in all the pages as parameters with another param `selected_page`",
        "timestamp": "2024-07-10 19:42:28.777000+00:00",
        "id": 1260682577296228352,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "Heres my ideas:\n\nlikely need a few functions to do a good job here:\n\n1. ExtractTableHeaders(page: string | image) -> TableHeader[]\n2. ExtractTable(page: string | image, headers: TableHeader[]) -> string  // This is a markdown string",
        "timestamp": "2024-07-10 19:41:44.458000+00:00",
        "id": 1260682391408742533,
        "parent_id": null,
        "thread_id": 1260681725982544035
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-10 19:41:44.031000+00:00",
        "id": 1260682389617905716,
        "parent_id": 1260681725982544035,
        "thread_id": 1260681725982544035
    },
    {
        "author": "dpaleka",
        "content": "hi! does this library support OpenAI-compatible APIs that do not have json/tools/functions mode?  i'm considering alternatives to `instructor` on a research project because i can't make it work with OpenRouter endpoints that do not support that param",
        "timestamp": "2024-07-09 00:46:19.412000+00:00",
        "id": 1260034266390073396,
        "parent_id": null,
        "thread_id": 1260034266390073396
    },
    {
        "author": ".aaronv",
        "content": "Hi, were you able to test out BAML? Do you have any feedback on your experience? We'd love to improve the product / address any issues!",
        "timestamp": "2024-07-11 17:26:19.310000+00:00",
        "id": 1261010699933581363,
        "parent_id": null,
        "thread_id": 1260034266390073396
    },
    {
        "author": ".aaronv",
        "content": "yeah definitely let us know",
        "timestamp": "2024-07-09 00:49:44.705000+00:00",
        "id": 1260035127451451442,
        "parent_id": null,
        "thread_id": 1260034266390073396
    },
    {
        "author": "dpaleka",
        "content": "ok i can test it on the models that don't work with `instructor` and report back if it fails",
        "timestamp": "2024-07-09 00:49:27.931000+00:00",
        "id": 1260035057096065055,
        "parent_id": null,
        "thread_id": 1260034266390073396
    },
    {
        "author": ".aaronv",
        "content": "let us know if you run into any issues, we try to respond pretty damn quickly",
        "timestamp": "2024-07-09 00:47:53.324000+00:00",
        "id": 1260034660285550643,
        "parent_id": null,
        "thread_id": 1260034266390073396
    },
    {
        "author": ".aaronv",
        "content": "yeah, it does! We have docs on this: https://docs.boundaryml.com/docs/snippets/clients/providers/other\n\nOur playground has a \"raw curl\" toggle where you can see the actual request that will be sent",
        "timestamp": "2024-07-09 00:47:22.236000+00:00",
        "id": 1260034529893286004,
        "parent_id": null,
        "thread_id": 1260034266390073396
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-09 00:47:22.048000+00:00",
        "id": 1260034529104494793,
        "parent_id": 1260034266390073396,
        "thread_id": 1260034266390073396
    },
    {
        "author": "etbyrd",
        "content": "How would you get baml running in just a basic new node project? Probably Webpack in a similar way to the NextJS setup but with using something like `node-loader` instead of `nextjs-node-loader`?",
        "timestamp": "2024-07-04 03:25:21.423000+00:00",
        "id": 1258262349157240913,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Here's a version of that SST project with the lambda layer:\n\nhttps://github.com/BoundaryML/baml-examples/tree/main/node-aws-lambda-sst\n\nIf you end up using something other than SST it will work very similarly",
        "timestamp": "2024-07-10 18:59:58.372000+00:00",
        "id": 1260671880122335397,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Of course! And thanks for your help! I guess this shows how badly I didn't want to have to write parsing/testing code and use BAML instead lol",
        "timestamp": "2024-07-10 00:56:43.212000+00:00",
        "id": 1260399270662504478,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Niiiice. Thanks for all the patience in getting this to work. Lmk if you have specific prompt engineering wuestions or whatnot at some point. Ill update our docs with this info and hopefully we can get the lambda layer working tomorrow",
        "timestamp": "2024-07-10 00:55:01.582000+00:00",
        "id": 1260398844395389000,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Yeah, you just need to install \"@boundaryml/baml-linux-x64-gnu\"",
        "timestamp": "2024-07-10 00:39:24.891000+00:00",
        "id": 1260394915628585041,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Also, I can confirm that deploying from Windows works perfectly fine",
        "timestamp": "2024-07-10 00:35:23.683000+00:00",
        "id": 1260393903929032846,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "And you might not even need to install baml itself",
        "timestamp": "2024-07-10 00:33:37.030000+00:00",
        "id": 1260393456593932429,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Awesome! I can also try writing a small doc on using SST/CDK to deploy but it really seems like it is jus this simple:\n\n```\nruntime: \"nodejs20.x\", \n    nodejs: {\n      install: [\"@boundaryml/baml\", \"@boundaryml/baml-linux-x64-gnu\"],\n      loader: {\n        '.node': 'file',\n      }\n    },\n```",
        "timestamp": "2024-07-10 00:33:29.371000+00:00",
        "id": 1260393424469889184,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Haha no worries. And yeah ill take a look at the layer config either tonight or tomorrow",
        "timestamp": "2024-07-10 00:32:11.830000+00:00",
        "id": 1260393099239231639,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "A lambda layer would be great, though - I already use that for accessing git at runtime",
        "timestamp": "2024-07-10 00:31:52.752000+00:00",
        "id": 1260393019220430859,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "but I cannot believe I didn't just try using node20 -_-",
        "timestamp": "2024-07-10 00:31:24.276000+00:00",
        "id": 1260392899783294976,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "It is definitely working now! Sorry for delay, there was another small issue on my end",
        "timestamp": "2024-07-10 00:31:10.783000+00:00",
        "id": 1260392843189420084,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "the good news is sst supports it: https://docs.sst.dev/advanced/lambda-layers",
        "timestamp": "2024-07-10 00:26:04.771000+00:00",
        "id": 1260391559682064384,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "lmk if it works for you! We can definitely publish a lambda layer soon to make the invocation faster",
        "timestamp": "2024-07-10 00:25:42.264000+00:00",
        "id": 1260391465281126532,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "just the nodejs install / loader config we had there",
        "timestamp": "2024-07-10 00:24:15.634000+00:00",
        "id": 1260391101928312833,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "like no hacks",
        "timestamp": "2024-07-10 00:24:06.001000+00:00",
        "id": 1260391061524713513,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "seems to have worked out of the box for me now with nodejs20",
        "timestamp": "2024-07-10 00:24:02.911000+00:00",
        "id": 1260391048564310079,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Sorry, was afk - I just used the Lambda UI",
        "timestamp": "2024-07-10 00:17:10.298000+00:00",
        "id": 1260389317939957801,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "oh nvm!",
        "timestamp": "2024-07-10 00:16:58.152000+00:00",
        "id": 1260389266995806220,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "what CLI command did you use to download the source code?",
        "timestamp": "2024-07-10 00:16:40.063000+00:00",
        "id": 1260389191125172274,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "with the same hack on the script you did to load the module directly",
        "timestamp": "2024-07-10 00:06:27.010000+00:00",
        "id": 1260386619794657321,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "(from amazon linux 2023) https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html#runtimes-supported",
        "timestamp": "2024-07-10 00:06:14.159000+00:00",
        "id": 1260386565893521511,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "basically nodejs20 runtime uses a newer version of glibc",
        "timestamp": "2024-07-10 00:05:59.815000+00:00",
        "id": 1260386505730428948,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "can you try with :\n\n```\n function: {\n          handler: \"packages/functions/src/get.handler\",\n          environment: {\n            OPENAI_API_KEY: \"...\",\n          },\n          runtime: \"nodejs20.x\",\n          nodejs: {\n            install: [\n              \"@boundaryml/baml\",\n              // \"baml-linux-x64-gnu\",\n              \"@boundaryml/baml-linux-x64-gnu\",\n            ],\n            esbuild: {\n              loader: {\n                \".node\": \"file\",\n              },\n            },\n          },\n        },\n      },\n\n```",
        "timestamp": "2024-07-10 00:05:46.034000+00:00",
        "id": 1260386447928852551,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "makes sense!",
        "timestamp": "2024-07-09 23:55:52.304000+00:00",
        "id": 1260383957644476459,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "but for now glibc issue is the blocker",
        "timestamp": "2024-07-09 23:53:41.725000+00:00",
        "id": 1260383409956585602,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "i think we can patch our index.js script to account for lambda runtimes",
        "timestamp": "2024-07-09 23:53:32.724000+00:00",
        "id": 1260383372203659314,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "nice - the platform and arch being empty is not great....must be a weird node issues somewhere",
        "timestamp": "2024-07-09 23:52:49.120000+00:00",
        "id": 1260383189315096678,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "ok the glibc version we can fix on our end, let me take a look at our build configs",
        "timestamp": "2024-07-09 23:52:07.230000+00:00",
        "id": 1260383013615960160,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "niiiice",
        "timestamp": "2024-07-09 23:51:55.375000+00:00",
        "id": 1260382963892486334,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Yep!",
        "timestamp": "2024-07-09 23:51:47.453000+00:00",
        "id": 1260382930665082910,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "did you modify the code and reupload it to test?",
        "timestamp": "2024-07-09 23:51:40.919000+00:00",
        "id": 1260382903259627541,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "ohh interesting",
        "timestamp": "2024-07-09 23:51:32.330000+00:00",
        "id": 1260382867234623560,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Lambda only seems to have glic 2.26",
        "timestamp": "2024-07-09 23:51:21.038000+00:00",
        "id": 1260382819872542832,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "So one issue looks like process.platform and process.arch are empty when requireNative checks. I then forced `return require(\"../baml-linux-x64-gnu\")` which gives:\n\n `/lib64/libm.so.6: version GLIBC_2.29 not found (required by /var/task/node_modules/@boundaryml/baml-linux-x64-gnu/baml.linux-x64-gnu.node)`\n\nSo, more progress lol",
        "timestamp": "2024-07-09 23:48:55.606000+00:00",
        "id": 1260382209886523513,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "will add some more logs",
        "timestamp": "2024-07-09 23:40:10.056000+00:00",
        "id": 1260380005570052097,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "yeah im wondering what binary our script is trying to get inside the lambda",
        "timestamp": "2024-07-09 23:40:07.022000+00:00",
        "id": 1260379992844533801,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "That is the unzipped structure of the function now",
        "timestamp": "2024-07-09 23:40:00.820000+00:00",
        "id": 1260379966831595691,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "",
        "timestamp": "2024-07-09 23:39:44.340000+00:00",
        "id": 1260379897709203497,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "function size jumped from 16kb to 9.8mb and then I downloaded the zip and checked",
        "timestamp": "2024-07-09 23:38:44.108000+00:00",
        "id": 1260379645078147082,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "how did you confirm it?",
        "timestamp": "2024-07-09 23:38:23.530000+00:00",
        "id": 1260379558767755274,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Progress - it still errors, but I can confirm that doing this:\n`install: [\"@boundaryml/baml\", \"@boundaryml/baml-linux-x64-gnu\"],`\n\nwill actually copy the node binary",
        "timestamp": "2024-07-09 23:38:05.516000+00:00",
        "id": 1260379483211300864,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "",
        "timestamp": "2024-07-09 23:17:04.310000+00:00",
        "id": 1260374193329930260,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "yep",
        "timestamp": "2024-07-09 23:16:45.323000+00:00",
        "id": 1260374113692811305,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "working on a fix",
        "timestamp": "2024-07-09 23:16:44.250000+00:00",
        "id": 1260374109192323163,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Yeah, you can look in the deployed lambda function and see that the binary is not there",
        "timestamp": "2024-07-09 23:16:42.706000+00:00",
        "id": 1260374102716453004,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "i figured out the likely issue",
        "timestamp": "2024-07-09 23:16:40.327000+00:00",
        "id": 1260374092738068500,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "awesome, thanks again for taking the time to look into this!",
        "timestamp": "2024-07-09 23:10:55.427000+00:00",
        "id": 1260372646122754118,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "oks i reproduced the error, seeing if i can fix it",
        "timestamp": "2024-07-09 23:10:38.919000+00:00",
        "id": 1260372576883048519,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "That doesn’t seem to work, I have tried file and copy, with and without install, as well as changing some esbuild options",
        "timestamp": "2024-07-09 23:08:55.001000+00:00",
        "id": 1260372141019369554,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "im testing my config now in prod",
        "timestamp": "2024-07-09 23:08:29.098000+00:00",
        "id": 1260372032374177833,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "my bad, shouldve noticed this earlier:\n\nyour config should be:\n```\n nodejs: {\n            install: [\"@boundaryml/baml\"],\n            esbuild: {\n              loader: {\n                \".node\": \"file\",\n              },\n            },\n          },\n```",
        "timestamp": "2024-07-09 23:00:56.418000+00:00",
        "id": 1260370133696778411,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I think I’m gonna try deploying with Python - it might just work out of the box",
        "timestamp": "2024-07-09 22:57:26.956000+00:00",
        "id": 1260369255149342884,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "make sense - I will stop messing with it for now then lol",
        "timestamp": "2024-07-09 19:08:40.479000+00:00",
        "id": 1260311682132082801,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "cool, wanna make sure you dont waste a lot of time on it since it's probably something ive seen before",
        "timestamp": "2024-07-09 19:08:04.689000+00:00",
        "id": 1260311532017946745,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Awesome, no rush - just wanted to provide more context 😄",
        "timestamp": "2024-07-09 19:07:40.449000+00:00",
        "id": 1260311430347886612,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "It runs locally with `npm run dev` but of course won't run in an deployment",
        "timestamp": "2024-07-09 19:07:23.100000+00:00",
        "id": 1260311357581037640,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "cool, ill start taking a look in around 2 hours",
        "timestamp": "2024-07-09 19:07:22.108000+00:00",
        "id": 1260311353420283976,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "This is with the most basic repro I can with just:\n\n```\nimport { StackContext, Api, Function } from \"sst/constructs\";\n\nexport function API({ stack }: StackContext) {\n  const theAnalyzeFunction = new Function(stack, \"analyzerFunction\", {\n    handler: \"packages/functions/src/analyzer.handler\",\n    nodejs: {\n      install: [\"@boundaryml/baml\"],\n      loader: {\n        '.node': 'copy',\n      }\n    },\n    environment: {\n      OCTOKIT_API_KEY: process.env.OCTOAI_API_KEY ?? \"\"\n    }\n  })\n\n  const api = new Api(stack, \"api\", {\n    routes: {\n      \"PUT /analyze\": theAnalyzeFunction,\n    },\n  });\n\n  stack.addOutputs({\n    ApiEndpoint: api.url,\n  });\n}\n```",
        "timestamp": "2024-07-09 19:07:02.596000+00:00",
        "id": 1260311271581028443,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "So I even tried deploying from an Amazon Linux 2 EC2 machine just to make sure it wasn't an issue with windows and I still get:\n\n```\nERROR    Unhandled Promise Rejection     \n{\n    \"errorType\": \"Runtime.UnhandledPromiseRejection\",\n    \"errorMessage\": \"Error: Failed to load native binding\",\n    \"reason\": {\n        \"errorType\": \"Error\",\n        \"errorMessage\": \"Failed to load native binding\",\n        \"stack\": [\n            \"Error: Failed to load native binding\",\n            \"    at Object.<anonymous> (/var/task/node_modules/@boundaryml/baml/native.js:359:11)\",\n            \"    at Module._compile (node:internal/modules/cjs/loader:1364:14)\",\n            \"    at Module._extensions..js (node:internal/modules/cjs/loader:1422:10)\",\n            \"    at Module.load (node:internal/modules/cjs/loader:1203:32)\",\n            \"    at Module._load (node:internal/modules/cjs/loader:1019:12)\",\n            \"    at Module.require (node:internal/modules/cjs/loader:1231:19)\",\n            \"    at require (node:internal/modules/helpers:177:18)\",\n            \"    at Object.<anonymous> (/var/task/node_modules/@boundaryml/baml/index.js:4:16)\",\n            \"    at Module._compile (node:internal/modules/cjs/loader:1364:14)\",\n            \"    at Module._extensions..js (node:internal/modules/cjs/loader:1422:10)\"\n        ]\n    },\n    \"promise\": {},\n    \"stack\": [\n        \"Runtime.UnhandledPromiseRejection: Error: Failed to load native binding\",\n        \"    at process.<anonymous> (file:///var/runtime/index.mjs:1276:17)\",\n        \"    at process.emit (node:events:517:28)\",\n        \"    at emit (node:internal/process/promises:149:20)\",\n        \"    at processPromiseRejections (node:internal/process/promises:283:27)\",\n        \"    at process.processTicksAndRejections (node:internal/process/task_queues:96:32)\"\n    ]\n}\n```",
        "timestamp": "2024-07-09 19:04:46.609000+00:00",
        "id": 1260310701210079288,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I believe it does deploy a stack but I think that the lambda functions are run in local debug mode by default",
        "timestamp": "2024-07-09 16:48:53.724000+00:00",
        "id": 1260276505531908187,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Awesome, thanks!",
        "timestamp": "2024-07-09 16:48:04.110000+00:00",
        "id": 1260276297435975752,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "It worked with npm run dev but i think npm run dev actually deploys a stack",
        "timestamp": "2024-07-09 15:38:43.934000+00:00",
        "id": 1260258848393003154,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Ill take a look at this today!",
        "timestamp": "2024-07-09 15:37:11.749000+00:00",
        "id": 1260258461741088921,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Sorry for rezzing this old thread but when you said you got this working did you actually deploy it or just use locally with npm run dev?\n\nI got distracted by some other things over the last few days and I realized that I never actually tested it working in a deployment, and unfortunately it doesn’t work - same missing binary error. \n\nI thought it could be because I am deploying from Windows but I deployed from Linux (WSL but should be fine?) and ensured that BAML was bundling its Linux binary. I tried tons of different configurations on the function but still to no avail. \n\nI know I could probably use a docker container but I’m wary about start times because this function will be called a ton. But if that is the only way for now, so be it - but I really doubt it…there has to be a way to bundle native node modules to lambda somehow.\n\nIf there is any way you could get a minimal deployed node18+ running that would be amazing…and maybe I could see what I did wrong.",
        "timestamp": "2024-07-09 08:46:41.910000+00:00",
        "id": 1260155156708720695,
        "parent_id": 1258312461694537758,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Yeah we can add it to our docs for sure",
        "timestamp": "2024-07-04 20:26:16.045000+00:00",
        "id": 1258519269663051848,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I should probably write this up and have a basic sst +baml example available",
        "timestamp": "2024-07-04 20:02:03.385000+00:00",
        "id": 1258513176765403216,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I can fully use baml in the pipeline now 😄",
        "timestamp": "2024-07-04 19:59:57.950000+00:00",
        "id": 1258512650653143332,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "also make sure `\"types\": [\"node\"],` is in the tsconfig",
        "timestamp": "2024-07-04 19:59:41.233000+00:00",
        "id": 1258512580536959017,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Alrighty, env does work. Just need to have env in the root, and pass:\n\n```\nenvironment: {\n      OCTOKIT_API_KEY: process.env.OCTOKIT_API_KEY ?? \"\"\n    }\n```\n\nand we are good!",
        "timestamp": "2024-07-04 19:59:18.539000+00:00",
        "id": 1258512485351166042,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I will be in on-prem distributions, but using OctoAI atm",
        "timestamp": "2024-07-04 19:57:22.918000+00:00",
        "id": 1258512000401543188,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Youre using bedrock no?",
        "timestamp": "2024-07-04 19:56:58.404000+00:00",
        "id": 1258511897582501998,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Sorry, please hold - I think there is a way to use env",
        "timestamp": "2024-07-04 19:56:45.468000+00:00",
        "id": 1258511843324858500,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Maybe we need the dynamic client thing to work?",
        "timestamp": "2024-07-04 19:56:41.294000+00:00",
        "id": 1258511825817829487,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "<@711679663746842796> mind taking a look?",
        "timestamp": "2024-07-04 19:56:13.608000+00:00",
        "id": 1258511709694333058,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Ah, \n\n```\nWe do not currently support any other mechanisms for providing authorization credentials, including but not limited to:\n\nexchanging refresh tokens for ephemeral authorization tokens\nfetching credentials from a secret storage service, such as AWS Secrets Manager or HashiCorp Vault\n```\n\nSo BAML won't let you change the api key at runtime - so I need to figure out a way to pass in the API key in the environment in the stack without storing it in the code",
        "timestamp": "2024-07-04 19:44:25.040000+00:00",
        "id": 1258508737744732332,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "This is seriously great, I really appreciate you taking so much time to help with this!",
        "timestamp": "2024-07-04 19:27:55.276000+00:00",
        "id": 1258504586373763195,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Nice",
        "timestamp": "2024-07-04 19:27:30.120000+00:00",
        "id": 1258504480861716480,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Btw if you are interested, you can use AWS SSM with SST to store the secrets",
        "timestamp": "2024-07-04 19:27:21.150000+00:00",
        "id": 1258504443238813716,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "The “init” command should worknow as well",
        "timestamp": "2024-07-04 19:27:15.627000+00:00",
        "id": 1258504420073672724,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "Awesome",
        "timestamp": "2024-07-04 19:26:46.110000+00:00",
        "id": 1258504296270663741,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "This is awesome, thanks! It works!\n\nWith 0.48, it seems like I just need:\n\n```\nconst aFunction = new Function(stack, \"myFunction\", {\n    handler: \"packages/functions/src/lambda.handler\",\n    nodejs: {\n      loader: {\n        '.node': 'copy',\n      }\n    },\n    environment: {\n      OCTOKIT_API_KEY: \"...\",\n    }\n  })\n```",
        "timestamp": "2024-07-04 19:26:27.631000+00:00",
        "id": 1258504218763853935,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "I ran it with “npm run dev”",
        "timestamp": "2024-07-04 06:48:17.900000+00:00",
        "id": 1258313421003292782,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "`curl -X GET -H \"Content-Type: application/json\" https://id123.execute-api.us-east-1.amazonaws.com/notes/123`",
        "timestamp": "2024-07-04 06:46:11.162000+00:00",
        "id": 1258312889425727510,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "not sure how you can safely add the environment variables -- you may want to look into that (instead of hardcoding it in here)",
        "timestamp": "2024-07-04 06:45:35.776000+00:00",
        "id": 1258312741005950989,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "you cna run:\n`npm uninstall @boundaryml/baml`\n`npm install @boundaryml/baml`",
        "timestamp": "2024-07-04 06:44:55.122000+00:00",
        "id": 1258312570490847232,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "I got it working:\n\nyou'll need @boundaryml/baml version 0.48.0\n\nAlso:\nExampleStack:\n```\nimport { Api, StackContext } from \"sst/constructs\";\n\nexport function ExampleStack({ stack }: StackContext) {\n  // Create the HTTP API\n  const api = new Api(stack, \"Api\", {\n    routes: {\n      // \"GET /notes\": \"packages/functions/src/list.handler\",\n      \"GET /notes/{id}\": {\n        function: {\n          handler: \"packages/functions/src/get.handler\",\n          environment: {\n            OPENAI_API_KEY: \"...\",\n          },\n          nodejs: {\n            install: [\"@boundaryml/baml\"],\n            esbuild: {\n              loader: {\n                \".node\": \"file\",\n              },\n            },\n          },\n        },\n      },\n\n      //\"packages/functions/src/get.handler\",\n      // \"PUT /notes/{id}\": \"packages/functions/src/update.handler\",\n    },\n  });\n\n  // Show the API endpoint in the output\n  stack.addOutputs({\n    ApiEndpoint: api.url,\n  });\n}\n```\npackages/functions/src/get.ts:\n```\nimport { APIGatewayProxyHandlerV2 } from \"aws-lambda\";\nimport { b } from \"../baml_client\";\n\nexport const handler: APIGatewayProxyHandlerV2 = async (event) => {\n  try {\n    const res = await b.ExtractResume(\n      \"Mark gonzalez, mark@hello.com. python. 5 years.\"\n    );\n    console.log(res);\n  } catch (e) {\n    console.log(e);\n  }\n  return {\n    statusCode: 200,\n    body: JSON.stringify({ message: \"Hello from get!\" }),\n  };\n};\n```",
        "timestamp": "2024-07-04 06:44:29.183000+00:00",
        "id": 1258312461694537758,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "no problem!",
        "timestamp": "2024-07-04 05:30:57.598000+00:00",
        "id": 1258293958166052885,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Thanks for the help!",
        "timestamp": "2024-07-04 05:30:37.682000+00:00",
        "id": 1258293874632429629,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yeah for sure, it isn't a blocker or anything for me, just really wanted to deploy it this way if possible 😄",
        "timestamp": "2024-07-04 05:30:35.319000+00:00",
        "id": 1258293864721027132,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "oks, yeah i would probably just wait until we can repro on our end -- this is kind of hairy stuff",
        "timestamp": "2024-07-04 05:30:08.729000+00:00",
        "id": 1258293753194479618,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yep, exact same",
        "timestamp": "2024-07-04 05:29:34.765000+00:00",
        "id": 1258293610739269712,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "let me make sure",
        "timestamp": "2024-07-04 05:28:26.347000+00:00",
        "id": 1258293323773251614,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yeah, seemingly",
        "timestamp": "2024-07-04 05:28:22.353000+00:00",
        "id": 1258293307021459517,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "and same error?",
        "timestamp": "2024-07-04 05:28:13.020000+00:00",
        "id": 1258293267875762218,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yep!",
        "timestamp": "2024-07-04 05:28:07.231000+00:00",
        "id": 1258293243595194461,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "in nodejs setting",
        "timestamp": "2024-07-04 05:28:01.749000+00:00",
        "id": 1258293220601761813,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "did you try both removing the `install` and adding it back in?",
        "timestamp": "2024-07-04 05:27:56.419000+00:00",
        "id": 1258293198246121503,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "one sec",
        "timestamp": "2024-07-04 05:25:43.879000+00:00",
        "id": 1258292642333327422,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I am also not using the example baml, I am using some from my other project, so maybe I should switch it to the boilerplate while I test this",
        "timestamp": "2024-07-04 05:25:43.466000+00:00",
        "id": 1258292640600952924,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "hmm this may be an issue on our end",
        "timestamp": "2024-07-04 05:25:42.738000+00:00",
        "id": 1258292637547499591,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yeah same error, I will try messing around with the loader based on what I find and see",
        "timestamp": "2024-07-04 05:25:20.006000+00:00",
        "id": 1258292542202450030,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "hmm",
        "timestamp": "2024-07-04 05:24:38.356000+00:00",
        "id": 1258292367509688320,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "in client.ts",
        "timestamp": "2024-07-04 05:24:30.508000+00:00",
        "id": 1258292334592790611,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "if you save a .baml file it will get overwritten again fyi",
        "timestamp": "2024-07-04 05:24:30.299000+00:00",
        "id": 1258292333716443208,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "because if I comment out everything, I get another error for ```\nError: this.ctx_manager.cloneContext is not a function\n```",
        "timestamp": "2024-07-04 05:24:24.902000+00:00",
        "id": 1258292311079522385,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "for now",
        "timestamp": "2024-07-04 05:24:19.568000+00:00",
        "id": 1258292288707366923,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "ok just comment out the whole file and do:\n\nexport {}",
        "timestamp": "2024-07-04 05:24:17.231000+00:00",
        "id": 1258292278905274368,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "But I bet this is a red herring and it is just being built correctly",
        "timestamp": "2024-07-04 05:24:02.047000+00:00",
        "id": 1258292215218831360,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "```\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnAsync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\n```",
        "timestamp": "2024-07-04 05:23:50.670000+00:00",
        "id": 1258292167500107848,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "which is the line 22 in your file?",
        "timestamp": "2024-07-04 05:23:11.684000+00:00",
        "id": 1258292003980967977,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Seems like the same errror",
        "timestamp": "2024-07-04 05:22:51.264000+00:00",
        "id": 1258291918333542410,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "inside the flush = () =>",
        "timestamp": "2024-07-04 05:22:07.361000+00:00",
        "id": 1258291734190755923,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "can you try just commenting this:\n\n`  DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.flush.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)()\n`",
        "timestamp": "2024-07-04 05:21:55.947000+00:00",
        "id": 1258291686317097030,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yep!",
        "timestamp": "2024-07-04 05:21:14.530000+00:00",
        "id": 1258291512601608274,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "```\n/*************************************************************************************************\n\nWelcome to Baml! To use this generated code, please run one of the following:\n\n$ npm install @boundaryml/baml\n$ yarn add @boundaryml/baml\n$ pnpm add @boundaryml/baml\n\n*************************************************************************************************/\n\n// This file was generated by BAML: do not edit it. Instead, edit the BAML\n// files and re-generate this code.\n//\n// tslint:disable\n// @ts-nocheck\n// biome-ignore format: autogenerated code\n/* eslint-disable */\nimport { BamlLogEvent } from '@boundaryml/baml';\nimport { DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX } from './globals';\n\nconst traceAsync =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnAsync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst traceSync =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnSync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst setTags =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.upsertTags.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst flush = () => {\n  DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.flush.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)()\n}\nconst onLogEvent = (callback: (event: BamlLogEvent) => void) =>\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.onLogEvent(callback)\n\nexport { traceAsync, traceSync, setTags, flush, onLogEvent }\n```",
        "timestamp": "2024-07-04 05:21:06.826000+00:00",
        "id": 1258291480288563210,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "and your @boundaryml/baml is 0.47.0 right?",
        "timestamp": "2024-07-04 05:21:03.293000+00:00",
        "id": 1258291465470345267,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "can you post the contents of baml_client/tracing.ts ?",
        "timestamp": "2024-07-04 05:20:47.086000+00:00",
        "id": 1258291397493260338,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Thanks so much for your time! I'll see if I can get it working tonight",
        "timestamp": "2024-07-04 05:20:07.273000+00:00",
        "id": 1258291230505439375,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Progress: seemingly getting a BAML error:\n\n```\nError: Cannot read properties of undefined (reading 'bind')\n       at <anonymous> (C:\\...\\packages\\functions\\baml\\baml_client\\tracing.ts:22:71)\n       at ModuleJob.run (node:internal/modules/esm/module_job:195:25)\n       at async ModuleLoader.import (node:internal/modules/esm/loader:337:24)\n       at async file:///C:/.../node_modules/.pnpm/sst@2.43.3_@aws-sdk+client-sso-oidc@3.609.0/node_modules/sst/support/nodejs-runtime/index.mjs:46:15\n```",
        "timestamp": "2024-07-04 05:19:46.360000+00:00",
        "id": 1258291142789955644,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "good search query: https://github.com/search?q=sst+%22new+Function%22+nodejs+language%3ATypeScript+&type=code&p=3 . Tomorrow ill have more time to take a look",
        "timestamp": "2024-07-04 05:19:13.525000+00:00",
        "id": 1258291005069721640,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "you may not even need \"Api\". Just the raw lambda function. def take a look at other projects",
        "timestamp": "2024-07-04 05:18:51.452000+00:00",
        "id": 1258290912488849448,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "https://github.com/winklerj/console/blob/c4373bd8991c84eb763a2588e23c7d10bd353db2/stacks/api.ts#L121",
        "timestamp": "2024-07-04 05:18:04.433000+00:00",
        "id": 1258290715277000775,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "right, but this totally makes sense, and I do see that is how people were able to get native modules running with other configurations so I bet this will work, trying it out now",
        "timestamp": "2024-07-04 05:13:53.895000+00:00",
        "id": 1258289664444469349,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "and their discord",
        "timestamp": "2024-07-04 05:13:06.194000+00:00",
        "id": 1258289464371839029,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "also i havent tried this, was just reading the parameters etc",
        "timestamp": "2024-07-04 05:13:02.059000+00:00",
        "id": 1258289447028658179,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "(edited above block with the nodejs thing)",
        "timestamp": "2024-07-04 05:12:40.556000+00:00",
        "id": 1258289356838539357,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "ohhh, duh, that makes so much sense",
        "timestamp": "2024-07-04 05:12:17.709000+00:00",
        "id": 1258289261011271692,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "```\nconst func = new Function(stack, \"MyFunction\", {\n  handler: \"src/lambda.handler\",\n  nodejs: {\n    // stuff here!\n  }\n});\n\nAPI(....) {\nroutes: {\n  \"GET /\": func\n```",
        "timestamp": "2024-07-04 05:11:38.405000+00:00",
        "id": 1258289096158347314,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "an api wraps around a function:\n\n`https://docs.sst.dev/constructs/Api#addroutes`",
        "timestamp": "2024-07-04 05:10:33.618000+00:00",
        "id": 1258288824421711982,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "'nodejs' isn't a property on API, so I probably have to go through a function but I am trying to figure out how. Currently looking through some more threads on sst",
        "timestamp": "2024-07-04 05:10:26.094000+00:00",
        "id": 1258288792863772732,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "API, something like this in the most basic example:\n\n```\nimport { Api, StackContext } from \"sst/constructs\";\n\nexport function API({ stack }: StackContext) {\n  const api = new Api(stack, \"api\", {\n    routes: {\n      \"GET /\": \"packages/functions/src/lambda.handler\"\n    }\n  });\n\n  stack.addOutputs({\n    ApiEndpoint: api.url,\n  });\n}\n\n```",
        "timestamp": "2024-07-04 05:06:53.840000+00:00",
        "id": 1258287902606102559,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "what construct are you using?",
        "timestamp": "2024-07-04 05:01:42.458000+00:00",
        "id": 1258286596575199283,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Hmm, so the Function construct seems to be the way to deploy a lambda locally? I would need to use the Api construct in order to get it to build and run when deployed? Or I am misunderstanding?\n\nLet me read through that other thread more and see",
        "timestamp": "2024-07-04 04:59:33.135000+00:00",
        "id": 1258286054155358208,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "our*",
        "timestamp": "2024-07-04 04:01:51.583000+00:00",
        "id": 1258271535353954375,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "if it does we'll add it to your docs 🙂",
        "timestamp": "2024-07-04 04:01:49.051000+00:00",
        "id": 1258271524733849621,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "lmk if that works",
        "timestamp": "2024-07-04 04:01:40.287000+00:00",
        "id": 1258271487975100477,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "relevant https://discord.com/channels/983865673656705025/1206068849313718302 (i searched for \"no loader is configured for \".node\" files\")",
        "timestamp": "2024-07-04 04:01:31.824000+00:00",
        "id": 1258271452478701708,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "```\nnodejs: {\n                    loader: {\n                        '.node': 'copy',\n                    },\n                    install: ['@boundaryml/baml']\n                }\n```\ntry it without the `install` and with it",
        "timestamp": "2024-07-04 04:00:59.385000+00:00",
        "id": 1258271316419678260,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Thanks for the support, I wish I knew more about this and could be more helpful lol",
        "timestamp": "2024-07-04 03:59:30.167000+00:00",
        "id": 1258270942212259911,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "oh very interesting, I'll try that out",
        "timestamp": "2024-07-04 03:59:15.856000+00:00",
        "id": 1258270882187575347,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "and / or also add the .node loader",
        "timestamp": "2024-07-04 03:58:44.092000+00:00",
        "id": 1258270748959572019,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "try adding @boundaryml/baml there",
        "timestamp": "2024-07-04 03:58:36.623000+00:00",
        "id": 1258270717632581673,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "probably something like this: https://docs.sst.dev/constructs/Function#install",
        "timestamp": "2024-07-04 03:58:26.924000+00:00",
        "id": 1258270676952023091,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I suppose, sst seems to have easy enough support for that: https://docs.sst.dev/containers\n\nBut I feel like there is probably a simple-ish setup with webpack to get this working. It would probably be almost exactly the same as a `npm init` project",
        "timestamp": "2024-07-04 03:55:47.245000+00:00",
        "id": 1258270007209754754,
        "parent_id": 1258269407206182932,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "that's what we did with the nextjs one",
        "timestamp": "2024-07-04 03:53:56.879000+00:00",
        "id": 1258269544301203566,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "yeah their webpack loader probably would need to be modified",
        "timestamp": "2024-07-04 03:53:50.851000+00:00",
        "id": 1258269519017939025,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I copied it over, and now I get:\n\n```\n No loader is configured for \".node\" files: packages/functions/node_modules/@boundaryml/baml-win32-x64-msvc/baml.win32-x64-msvc.node\n   packages/functions/node_modules/@boundaryml/baml/native.js\n```",
        "timestamp": "2024-07-04 03:53:29.607000+00:00",
        "id": 1258269429914009710,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "are you open to using a container?",
        "timestamp": "2024-07-04 03:53:24.193000+00:00",
        "id": 1258269407206182932,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "yeah, in the nextjs project I am using BAML in, I can see the `baml.win32-x64-msvc.node` in the node_modules, but not in this project",
        "timestamp": "2024-07-04 03:51:54.933000+00:00",
        "id": 1258269032822607903,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "i dont think it can find the native node module",
        "timestamp": "2024-07-04 03:48:03.195000+00:00",
        "id": 1258268060842721280,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "That package.json is:\n\n```\n{\n  \"name\": \"@baml-sst-test/functions\",\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"test\": \"sst bind vitest\",\n    \"typecheck\": \"tsc -noEmit\"\n  },\n  \"devDependencies\": {\n    \"@types/aws-lambda\": \"^8.10.140\",\n    \"@types/node\": \"^20.14.9\",\n    \"sst\": \"^2.43.3\",\n    \"vitest\": \"^1.6.0\"\n  },\n  \"dependencies\": {\n    \"@boundaryml/baml\": \"^0.47.0\"\n  }\n}\n```\n\nSo I imagine that the baml generate needs to be a part of the build step at some point?",
        "timestamp": "2024-07-04 03:45:27.884000+00:00",
        "id": 1258267409421176887,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "The extension is working as expected and generating the files, so I tried to import and run the BAML in the API and got this when building:\n\n```\n✖  Build failed packages/functions/src/lambda.handler\n   Could not resolve \"@boundaryml/baml\"\n   packages/functions/baml/baml_client/client.ts\n   18 │ import { BamlRuntime, FunctionResult, BamlCtxManager, BamlStream, Image } from \"@boundaryml/baml\"\n   Could not resolve \"@boundaryml/baml\"\n   packages/functions/baml/baml_client/globals.ts\n   18 │ import { BamlCtxManager, BamlRuntime } from '@boundaryml/baml'\n```",
        "timestamp": "2024-07-04 03:44:17.825000+00:00",
        "id": 1258267115572691034,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "and try to save it using the vscode extension. A `baml_client` should generate",
        "timestamp": "2024-07-04 03:39:17.914000+00:00",
        "id": 1258265857654521918,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "ok for now just copy this in a baml_src folder in packages/functions:\n\nmain.baml\n```\n// This is a BAML config file, which extends the Jinja2 templating language to write LLM functions.\n\nclass Resume {\n  name string\n  education Education[] @description(\"Extract in the same order listed\")\n  skills string[] @description(\"Only include programming languages\")\n}\n\nclass Education {\n  school string\n  degree string\n  year int\n}\n\nfunction ExtractResume(resume_text: string) -> Resume {\n  // see clients.baml\n  client GPT4o\n\n  // The prompt uses Jinja syntax. Change the models or this text and watch the prompt preview change!\n  prompt #\"\n    Parse the following resume and return a structured representation of the data in the schema below.\n\n    Resume:\n    ---\n    {{ resume_text }}\n    ---\n\n    {# special macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n\ntest Test1 {\n  functions [ExtractResume]\n  args {\n    resume_text #\"\n      John Doe\n\n      Education\n      - University of California, Berkeley\n        - B.S. in Computer Science\n        - 2020\n\n      Skills\n      - Python\n      - Java\n      - C++\n    \"#\n  }\n}\n\n\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model gpt-4o\n    api_key env.OPENAI_API_KEY\n  }\n}\n\ngenerator default {\n  output_type typescript\n  output_dir ../\n}\n```",
        "timestamp": "2024-07-04 03:38:45.683000+00:00",
        "id": 1258265722468040754,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I am on Windows, btw, if that causes weirdness",
        "timestamp": "2024-07-04 03:37:34.474000+00:00",
        "id": 1258265423795982346,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Yeah, trying to just add it and init the cli gives:\n\n```\nnode:internal/modules/cjs/loader:1143\n  throw err;\n  ^\n\nError: Cannot find module './native'\nRequire stack:\n```",
        "timestamp": "2024-07-04 03:36:07.762000+00:00",
        "id": 1258265060099231794,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "in that package.json",
        "timestamp": "2024-07-04 03:35:57.638000+00:00",
        "id": 1258265017636356096,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "exactly, it would be in packages/functions",
        "timestamp": "2024-07-04 03:35:54.756000+00:00",
        "id": 1258265005548240927,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "awesome, thanks!",
        "timestamp": "2024-07-04 03:35:06.993000+00:00",
        "id": 1258264805215567872,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "I see - I should install it in the specific package that it is using it probably, not the root? (packages/functions/)",
        "timestamp": "2024-07-04 03:35:00.678000+00:00",
        "id": 1258264778728804352,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "tomorrow we'll have some time to help you set that up",
        "timestamp": "2024-07-04 03:34:54.998000+00:00",
        "id": 1258264754905157725,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "`pnpm add @boundaryml/baml` \n`npx baml-cli init`",
        "timestamp": "2024-07-04 03:33:55.558000+00:00",
        "id": 1258264505595592745,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "if you try their basic Api construct and add @boundaryml/baml perhaps it may work out of the box (?)",
        "timestamp": "2024-07-04 03:33:32.965000+00:00",
        "id": 1258264410833817741,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "the basic gist is that BAML's @boundaryml/baml package just has a native node module that needs to be included if bundling.",
        "timestamp": "2024-07-04 03:33:12.948000+00:00",
        "id": 1258264326876172433,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "Yeah, I think sst uses express under the hood",
        "timestamp": "2024-07-04 03:31:55.224000+00:00",
        "id": 1258264000878350386,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "the webpack thing is only needed cause the project's getting bundled, but if it's just a simple node server with no bundling you shouldn't have to do anything fancy. I'm not sure if this is what theyre using to bundle things https://github.com/sst/sst/blob/master/packages/sst/src/runtime/handlers/node.ts",
        "timestamp": "2024-07-04 03:31:55.104000+00:00",
        "id": 1258264000374767656,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "If you aren't familiar, https://docs.sst.dev/ - it's basically a nice wrapper around the CDK",
        "timestamp": "2024-07-04 03:29:33.462000+00:00",
        "id": 1258263406285160459,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "But I bet if I can get it working in a simple node project I could figure out the other way",
        "timestamp": "2024-07-04 03:27:02.267000+00:00",
        "id": 1258262772127367178,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": "etbyrd",
        "content": "The goal is actually to use sst and deploy AWS apis",
        "timestamp": "2024-07-04 03:26:35.864000+00:00",
        "id": 1258262661385289769,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "are you trying to run it in an express app? How do you deploy it?",
        "timestamp": "2024-07-04 03:26:12.791000+00:00",
        "id": 1258262564610117673,
        "parent_id": null,
        "thread_id": 1258262349157240913
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-04 03:26:12.535000+00:00",
        "id": 1258262563536375870,
        "parent_id": 1258262349157240913,
        "thread_id": 1258262349157240913
    },
    {
        "author": "yungweedle",
        "content": "Do the tests only test output schema being correct? Can you also test that the actual output matches what you would expect?",
        "timestamp": "2024-07-03 05:06:02.896000+00:00",
        "id": 1257925301065748561,
        "parent_id": null,
        "thread_id": 1257925301065748561
    },
    {
        "author": ".aaronv",
        "content": "Gotcha, yeah for the moment you may want to write your own pytests. We will adds docs on that soon",
        "timestamp": "2024-07-03 05:08:14.726000+00:00",
        "id": 1257925854000975913,
        "parent_id": null,
        "thread_id": 1257925301065748561
    },
    {
        "author": "yungweedle",
        "content": "python currently",
        "timestamp": "2024-07-03 05:07:36.384000+00:00",
        "id": 1257925693182840885,
        "parent_id": null,
        "thread_id": 1257925301065748561
    },
    {
        "author": ".aaronv",
        "content": "And yes they only validate the schema",
        "timestamp": "2024-07-03 05:07:33.785000+00:00",
        "id": 1257925682281840640,
        "parent_id": null,
        "thread_id": 1257925301065748561
    },
    {
        "author": ".aaronv",
        "content": "Not yet! We will add assertion capabilities soon, but i dont currently have a timeline. Do you use python or typescript? You can also write programmatic tests",
        "timestamp": "2024-07-03 05:07:13.116000+00:00",
        "id": 1257925595589775391,
        "parent_id": null,
        "thread_id": 1257925301065748561
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-03 05:07:12.613000+00:00",
        "id": 1257925593480171602,
        "parent_id": 1257925301065748561,
        "thread_id": 1257925301065748561
    },
    {
        "author": "yungweedle",
        "content": "can you pass in the client into a function? or specific client parameters? for example (1) if i wanted to switch between gpt4o and sonnet3.5,  i would need to copy paste and define multiple functions using different clients and (2) if i wanted to run sonnet3.5 but with 0 and 0.3 temperature, i would need to copy paste and define multiple functions using the same client but different settings?",
        "timestamp": "2024-07-03 04:39:53.368000+00:00",
        "id": 1257918717988311110,
        "parent_id": null,
        "thread_id": 1257918717988311110
    },
    {
        "author": ".aaronv",
        "content": "we had some delays on this -- new ETA is tomorrow -- apologies on the delay!",
        "timestamp": "2024-07-03 18:52:10.582000+00:00",
        "id": 1258133203009667195,
        "parent_id": null,
        "thread_id": 1257918717988311110
    },
    {
        "author": ".aaronv",
        "content": "Currently you will need to use different functions. You could use a template_string to define yojr prompt only one time. But yes we will release a dynamic client capability shortly",
        "timestamp": "2024-07-03 04:44:34.824000+00:00",
        "id": 1257919898500464710,
        "parent_id": null,
        "thread_id": 1257918717988311110
    },
    {
        "author": "yungweedle",
        "content": "🙂",
        "timestamp": "2024-07-03 04:44:15.005000+00:00",
        "id": 1257919815373553674,
        "parent_id": null,
        "thread_id": 1257918717988311110
    },
    {
        "author": ".aaronv",
        "content": "This feature is coming in the next 4 hours or so",
        "timestamp": "2024-07-03 04:43:11.417000+00:00",
        "id": 1257919548666019952,
        "parent_id": null,
        "thread_id": 1257918717988311110
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-03 04:43:11.075000+00:00",
        "id": 1257919547231703142,
        "parent_id": 1257918717988311110,
        "thread_id": 1257918717988311110
    },
    {
        "author": "yungweedle",
        "content": "ah, i see it needs to be inside\n    generation_config {\n      temperature 0\n    }",
        "timestamp": "2024-07-03 04:04:15.836000+00:00",
        "id": 1257909752529293375,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "ah yes, we can get an easy patch in that allows you to configure base_url which would make it possible to edit that.\n\nWe're doing a release tonight, so i'll include that change! You can expect it tmrw.",
        "timestamp": "2024-07-03 04:25:48.570000+00:00",
        "id": 1257915174648811581,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "ah maybe the issue is that is v1beta api instead of v1",
        "timestamp": "2024-07-03 04:24:51.159000+00:00",
        "id": 1257914933849362534,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "it is reflected as such in the playground",
        "timestamp": "2024-07-03 04:24:39.569000+00:00",
        "id": 1257914885237379093,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "https://ai.google.dev/api/rest/v1beta/GenerationConfig",
        "timestamp": "2024-07-03 04:24:24.374000+00:00",
        "id": 1257914821505056768,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "when i look at the docs: https://ai.google.dev/api/rest/v1/GenerationConfig\n\nI don't see response_mime_type",
        "timestamp": "2024-07-03 04:22:54.930000+00:00",
        "id": 1257914446349729842,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "let me also try meanwhile!",
        "timestamp": "2024-07-03 04:21:57.530000+00:00",
        "id": 1257914205596815370,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "have you tried the raw_curl request option the playground? That can help debug the issue.",
        "timestamp": "2024-07-03 04:21:27.966000+00:00",
        "id": 1257914081596280842,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "safety_settings is supposed to be on the same level as generation_config",
        "timestamp": "2024-07-03 04:21:04.218000+00:00",
        "id": 1257913981990080612,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "response_mime_type is supposed to be in generation_config",
        "timestamp": "2024-07-03 04:20:49.832000+00:00",
        "id": 1257913921650819122,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "i'm having trouble passing in safety_settings and response_mime_type",
        "timestamp": "2024-07-03 04:20:40.803000+00:00",
        "id": 1257913883780452404,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "to help others*",
        "timestamp": "2024-07-03 04:13:50.866000+00:00",
        "id": 1257912164379787306,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "yea you'll need to pass in params exactly how gemini wants them! We just do passthrough of all params! Glad you got it! We should make a PR on our docs to fix this.",
        "timestamp": "2024-07-03 04:13:39.635000+00:00",
        "id": 1257912117273825311,
        "parent_id": null,
        "thread_id": 1257909752529293375
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-07-03 04:13:39.327000+00:00",
        "id": 1257912115981975626,
        "parent_id": 1257909752529293375,
        "thread_id": 1257909752529293375
    },
    {
        "author": "yungweedle",
        "content": "i get an error when I add it to options",
        "timestamp": "2024-07-03 04:02:04.931000+00:00",
        "id": 1257909203473924096,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "does Gemini support temperature?",
        "timestamp": "2024-07-03 04:01:56.479000+00:00",
        "id": 1257909168023539732,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "deoxykev",
        "content": "is json schema conversion still being developed? Use case is dynamically converting unknown json schema into BAML for structured extraction.\n\nI was working on an implementation using the python client but it  looks like someone is working on a base rust implementation which might be better. https://github.com/BoundaryML/baml/pull/655",
        "timestamp": "2024-07-02 17:59:55.748000+00:00",
        "id": 1257757666697150485,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "hellovai",
        "content": "This was really well thought out! 🙂 Quite appreciate the thoughts on this. Will be sharing the spec with you over the weekend sometime!",
        "timestamp": "2024-07-10 18:02:21.942000+00:00",
        "id": 1260657382804160633,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "<@711679663746842796> left some feedback on this https://github.com/BoundaryML/baml/issues/765#issuecomment-2221112650",
        "timestamp": "2024-07-10 17:51:03.120000+00:00",
        "id": 1260654535618330634,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "joatmon.pockets",
        "content": "https://github.com/BoundaryML/baml-examples/blob/b685347967b9ff956d68be7c516b536ae663f988/nextjs-starter/baml_src/rag.baml",
        "timestamp": "2024-07-09 21:51:12.414000+00:00",
        "id": 1260352584712196098,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "Cool, I sent an invite at 4 cst with our team. Looking forward to it!",
        "timestamp": "2024-07-09 15:36:44.718000+00:00",
        "id": 1260258348364730470,
        "parent_id": 1260251334721011792,
        "thread_id": 1257757666697150485
    },
    {
        "author": "hellovai",
        "content": "Yep! Based in Seattle!",
        "timestamp": "2024-07-09 15:08:52.535000+00:00",
        "id": 1260251334721011792,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "Are you a US company?",
        "timestamp": "2024-07-09 14:31:01.284000+00:00",
        "id": 1260241808403857458,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "hellovai",
        "content": "Oh, you're just an early riser! 🙂 \n\nIf its more helpful here's a calendly btw! https://calendly.com/boundary-founders/connect-45\n\nOtherwise feel free to suggest times!",
        "timestamp": "2024-07-09 14:30:38.061000+00:00",
        "id": 1260241710999404619,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "No, we are on American central time zone",
        "timestamp": "2024-07-09 14:29:44.751000+00:00",
        "id": 1260241487401193572,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "hellovai",
        "content": "would be happy to! I take it you are Europe timezones?",
        "timestamp": "2024-07-09 13:56:43.581000+00:00",
        "id": 1260233177771937793,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "I would be interested in a call, I think our entire team is interested. Can I get back to you with some availability slots?",
        "timestamp": "2024-07-09 13:56:03.334000+00:00",
        "id": 1260233008963649642,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "Yeah absolutely I can try this out and give feedback",
        "timestamp": "2024-07-09 13:50:36.861000+00:00",
        "id": 1260231639636906107,
        "parent_id": 1260036867953524807,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "Yes, preprocessing is totally ok!",
        "timestamp": "2024-07-09 13:47:11.667000+00:00",
        "id": 1260230778990628875,
        "parent_id": 1260002591266570350,
        "thread_id": 1257757666697150485
    },
    {
        "author": ".aaronv",
        "content": "Deoxy what we can do here for you is to actually give you some python code to generate BAML schemas + prompts, because you may actually need to change the definitions slightly (e.g. prompt engineer), and it's really hard to do if we just give you a straight up `typebuilder.from_json_schema`  that generates the output schema that Sam posted above. For example, you may want to remove all the redundant `id` descriptions.\n\nWe can give you a python script that generates the files if you are interested. I assume that these schemas probably don't change much.\n\nIf you want to chat more about your usecase in a quick call and see how we can help you get better results or learn about your problems definitely let us know.",
        "timestamp": "2024-07-09 00:56:39.673000+00:00",
        "id": 1260036867953524807,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "joatmon.pockets",
        "content": "Here's what it is for EpisodeOfCare: https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/episode-of-care.schema.txt",
        "timestamp": "2024-07-08 23:42:14.708000+00:00",
        "id": 1260018140532834314,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "joatmon.pockets",
        "content": "I think you're definitely going to want to modify the JSON schemas a bit before you feed them into BAML- here's the output format we would generate for Patient: https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/patient-schema.txt",
        "timestamp": "2024-07-08 23:40:32.942000+00:00",
        "id": 1260017713695297578,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "joatmon.pockets",
        "content": "A more specific example is `#/definitions/Extension`, which I suspect for your case you can just replace with `\"type\": \"string\"`\n\n```\n    \"Extension\": {\n      \"description\": \"Optional Extension Element - found in all resources.\",\n      \"properties\": {\n        ...,\n        \"extension\": {\n          \"description\": \"May be used to represent additional information that is not part of the basic definition of the element. To make the use of extensions safe and managable, there is a strict set of governance applied to the definition and use of extensions. Though any implementer can define an extension, there is a set of requirements that SHALL be met as part of the definition of the extension.\",\n          \"items\": {\n            \"$ref\": \"#/definitions/Extension\"\n          },\n          \"type\": \"array\"\n        },\n    }\n```\n\nYou can diff these two files to see an example of what I mean: https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/fhir-no-cycles.schema.json and https://gist.githubusercontent.com/sxlijin/7a41ff5faa5d17132e0b0181dae8c5cf/raw/705e590d246311ca9415db0ad18b137e9b0392af/fhir-original.schema.json",
        "timestamp": "2024-07-08 22:53:58.030000+00:00",
        "id": 1260005990984581160,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "joatmon.pockets",
        "content": "So I have JSON schema loading working with dynamic types, and am currently working on getting `fhir.schema.json` to load, and wanted to check whether or not you're OK with doing some pre-processing of it before feeding it into BAML.\n\n---\n\nContext: the FHIR schemas have some cycles in the type definitions, e.g. they have something that looks like this:\n\n```\nclass Reference {\n  identifier Identifier\n}\n\nclass Identifier {\n  assigner Reference\n}\n```\n\nBAML currently doesn't support this, and I'm wondering if you'd be OK with modifying the JSON schema that you feed into BAML to strip out these type cycles.\n\n---\n\nLet me know if this question makes sense, or if you'd be interested in hopping on a call to chat more about your use case",
        "timestamp": "2024-07-08 22:40:27.474000+00:00",
        "id": 1260002591266570350,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "Hey, I’m sorry I did not check. Usually these schemas are part of a larger standard, which contain the missing references",
        "timestamp": "2024-07-06 13:24:58.223000+00:00",
        "id": 1259138022524325932,
        "parent_id": 1258504600063836221,
        "thread_id": 1257757666697150485
    },
    {
        "author": "joatmon.pockets",
        "content": "Hey there! I'm picking the branch back up and just wanted to double-check - you can load `fhir.schema.json` into your code, right?\n\nThe reason I ask is because `patient.schema.json` and `episodeofcare.schema.json` don't seem to actually contain enough information to be fully resolveable, since they reference other root schemas but don't actually have metadata for how to find those other roots",
        "timestamp": "2024-07-04 19:27:58.540000+00:00",
        "id": 1258504600063836221,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "<@99252724855496704> certainly!\n\nschema_1: https://build.fhir.org/episodeofcare.schema.json.html\n\nschema_1_sample: https://build.fhir.org/episodeofcare-example.json.html\n\n---\n\nschema_2: https://build.fhir.org/patient.schema.json.html\n\nschema_2_sample: https://build.fhir.org/patient-example-b.json.html\n\n---\n\nand  behold: \nhttp://build.fhir.org/fhir.schema.json.zip\n\nthe mother of all json schemas\n\nthis should be an excellent test bench",
        "timestamp": "2024-07-02 20:27:56.503000+00:00",
        "id": 1257794915283243088,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "hellovai",
        "content": "<@1062441178022289479> could you share some samples of the JSONschemas you have? I want to make sure we can get it working for the strucutres you have!",
        "timestamp": "2024-07-02 19:18:56.399000+00:00",
        "id": 1257777550428471308,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": ".aaronv",
        "content": "our current estimate is for completing this is end of next week (Fri July 12) -- sorry we cant get to it earlier! That PR definitely is still a work in progress",
        "timestamp": "2024-07-02 18:22:11.720000+00:00",
        "id": 1257763270169727016,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "yeah the domain i'm working in has a well-defined ontology with json schemas for everything already",
        "timestamp": "2024-07-02 18:03:59.412000+00:00",
        "id": 1257758688697909318,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": ".aaronv",
        "content": "ahh gotcha, you just want a faster way rather than build it all out manually. Let me get back to you on the date for this, one sec",
        "timestamp": "2024-07-02 18:03:07.245000+00:00",
        "id": 1257758469893918920,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": "deoxykev",
        "content": "yeah I am parsing a json schema and creating the dynamic type using that feature",
        "timestamp": "2024-07-02 18:02:37.756000+00:00",
        "id": 1257758346207826042,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": ".aaronv",
        "content": "is this something that may help you? https://docs.boundaryml.com/docs/calling-baml/dynamic-types\n\nOr do you want to use json schemas specifically?",
        "timestamp": "2024-07-02 18:02:16.112000+00:00",
        "id": 1257758255426572419,
        "parent_id": null,
        "thread_id": 1257757666697150485
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-07-02 18:02:15.700000+00:00",
        "id": 1257758253698388128,
        "parent_id": 1257757666697150485,
        "thread_id": 1257757666697150485
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Hi team -- for deploying BAML, when do you think you'll  support alpine? We use alpine in our prod containers currently and are running into build errors",
        "timestamp": "2024-07-01 20:15:01.869000+00:00",
        "id": 1257429278367158284,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Sounds good, thx",
        "timestamp": "2024-08-06 16:01:14.166000+00:00",
        "id": 1270411371913285675,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "hellovai",
        "content": "Hi Ashwin, sorry for the delay on this. Some good news, we had a build compiling this AM for `aarch64-unknown-linux-musl` (Whic is what alpine uses - assuming you're using a aarch64 not x86 container). \n\nWe should know soon if it works on alpine.",
        "timestamp": "2024-08-06 14:48:19.134000+00:00",
        "id": 1270393021699195032,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Circling back on this -- does BAML support alpine linux now / plan to soon?",
        "timestamp": "2024-08-06 00:46:08.558000+00:00",
        "id": 1270181081101893675,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha, appreciate it!",
        "timestamp": "2024-07-01 20:21:46.127000+00:00",
        "id": 1257430973948100669,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "hellovai",
        "content": "We should be able to add this in within 2 weeks! I’ll confirm after we do scheduling on Thursday!",
        "timestamp": "2024-07-01 20:20:31.610000+00:00",
        "id": 1257430661401153567,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "ashwin.a.kumar",
        "content": "We can *probably* switch to a non-alpine distro without breaking things so hopefully that works in the meantime, but ideally we can just add it to the existing configuration and have it work",
        "timestamp": "2024-07-01 20:17:39.464000+00:00",
        "id": 1257429939368624158,
        "parent_id": null,
        "thread_id": 1257429278367158284
    },
    {
        "author": "ashwin.a.kumar",
        "content": "",
        "timestamp": "2024-07-01 20:17:39.215000+00:00",
        "id": 1257429938324242494,
        "parent_id": 1257429278367158284,
        "thread_id": 1257429278367158284
    },
    {
        "author": "yungweedle",
        "content": "are optional / default parameters in functions supported?",
        "timestamp": "2024-06-29 17:10:21.399000+00:00",
        "id": 1256658027776446494,
        "parent_id": null,
        "thread_id": 1256658027776446494
    },
    {
        "author": "hellovai",
        "content": "We only have default values for optionala for which it defaults to none",
        "timestamp": "2024-06-29 23:47:35.692000+00:00",
        "id": 1256757996046778478,
        "parent_id": null,
        "thread_id": 1256658027776446494
    },
    {
        "author": "hellovai",
        "content": "We don’t support those yet!",
        "timestamp": "2024-06-29 23:47:09.689000+00:00",
        "id": 1256757886982422558,
        "parent_id": null,
        "thread_id": 1256658027776446494
    },
    {
        "author": "yungweedle",
        "content": "what about default values",
        "timestamp": "2024-06-29 23:16:56.085000+00:00",
        "id": 1256750280175915089,
        "parent_id": null,
        "thread_id": 1256658027776446494
    },
    {
        "author": "hellovai",
        "content": "Yes! Params can be “string?” Or other variants of ?",
        "timestamp": "2024-06-29 17:19:08.075000+00:00",
        "id": 1256660236815700060,
        "parent_id": null,
        "thread_id": 1256658027776446494
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-29 17:19:07.453000+00:00",
        "id": 1256660234206711929,
        "parent_id": 1256658027776446494,
        "thread_id": 1256658027776446494
    },
    {
        "author": "hellovai",
        "content": "Defining Types in Python",
        "timestamp": "2024-06-29 07:37:02.494000+00:00",
        "id": 1256513748311478329,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "yungweedle",
        "content": "is it possible to define the data type inside python (and use it in a .baml file) instead of inside the baml file (which then flows to baml_client.types)",
        "timestamp": "2024-06-28 23:34:52.038000+00:00",
        "id": 1256392405184155759,
        "parent_id": null,
        "thread_id": 1256392405184155759
    },
    {
        "author": "hellovai",
        "content": "Sadly we don't support this as of yet. What some folks do is:\n\n1. define the types in BAML\n2. inherit from `baml_client.types` in their own python class so it keeps the data model in sync.\n\n(Also, we don't support all types in BAML yet (dict, tuple, set, and date))",
        "timestamp": "2024-06-29 07:37:04.365000+00:00",
        "id": 1256513756159148085,
        "parent_id": null,
        "thread_id": 1256392405184155759
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-29 07:37:02.494000+00:00",
        "id": 1256513748311478324,
        "parent_id": 1256392405184155759,
        "thread_id": 1256392405184155759
    },
    {
        "author": "ashwin.a.kumar",
        "content": "How can we provide dynamic types to BAML functions? I'd like to use an `enum`, but it seems like I can only use this statically with the enum variants hardcoded in the file -- I'm guessing a workaround would be to type it `string` and create a prompt in the @description annotation, but I'd rather avoid this if possible\n\n(relevant empty docs page: https://docs.boundaryml.com/docs/calling-baml/dynamic-types)",
        "timestamp": "2024-06-25 23:47:40.856000+00:00",
        "id": 1255308466243899412,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "<@99252724855496704> niice can you repost?",
        "timestamp": "2024-06-28 17:07:11.702000+00:00",
        "id": 1256294844263825408,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Just launched the feature on LinkedIn and shouted y'all out! \nhttps://www.linkedin.com/posts/ashwin-a-kumar_today-mercoa-came-alive-for-the-first-time-activity-7212497434235584512-B4em?utm_source=share&utm_medium=member_desktop",
        "timestamp": "2024-06-28 16:55:25.906000+00:00",
        "id": 1256291883940974622,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "always",
        "timestamp": "2024-06-28 06:37:10.244000+00:00",
        "id": 1256136293457596450,
        "parent_id": 1256136185160663112,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Haha thank you! And will do",
        "timestamp": "2024-06-28 06:36:53.304000+00:00",
        "id": 1256136222405955635,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "👀 ur awake too 👀",
        "timestamp": "2024-06-28 06:36:44.424000+00:00",
        "id": 1256136185160663112,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "lmk if you run into more issues",
        "timestamp": "2024-06-28 06:36:06.445000+00:00",
        "id": 1256136025865064468,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "niiice! congrats!",
        "timestamp": "2024-06-28 06:36:01.272000+00:00",
        "id": 1256136004168061021,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Forgot to mention this earlier today at work — with this schema + GPT4o and some(many) bug fixes we got the line item classifier to a launchable state and have it set to go out to some customers tomorrow, thanks for your help! Wouldn’t have been able to figure out the dynamic type building / prompt engineering without it and y’all were super responsive throughout",
        "timestamp": "2024-06-28 06:35:27.854000+00:00",
        "id": 1256135864002547732,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "So it never breaks",
        "timestamp": "2024-06-27 01:44:26.903000+00:00",
        "id": 1255700239600455821,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Yeah I just want to strongly type the whole thing based on what the user wants",
        "timestamp": "2024-06-27 01:44:23.394000+00:00",
        "id": 1255700224882769980,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Oh nvm i think i misread it",
        "timestamp": "2024-06-27 01:44:03.541000+00:00",
        "id": 1255700141612994561,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "You probably dont need the index at this point i guess. And why do you need the account id there in the output schema too?",
        "timestamp": "2024-06-27 01:43:30.627000+00:00",
        "id": 1255700003561803877,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "And I assume you'd be able to make something that doesn't cost as many tokens 😅",
        "timestamp": "2024-06-27 01:43:06.624000+00:00",
        "id": 1255699902885924945,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "It'd be nice if a user could specify something like `LineItemMetadata[4]` and have you guys do it under the hood, this feels like a common use case/failure mode",
        "timestamp": "2024-06-27 01:42:36.010000+00:00",
        "id": 1255699774481629204,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Nice, i like the approach",
        "timestamp": "2024-06-27 01:42:06.101000+00:00",
        "id": 1255699649034059817,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Yeah baml wont break it",
        "timestamp": "2024-06-27 01:42:00.066000+00:00",
        "id": 1255699623721570304,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Yep",
        "timestamp": "2024-06-27 01:41:48.036000+00:00",
        "id": 1255699573263962165,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "I think this is a little verbose and it took some time to hand-roll the dynamic key definitions, since you have to encode it this way and decode it -- but assuming BAML will never break the schema it means I don't have to validate the length of the array anymore",
        "timestamp": "2024-06-27 01:41:47.219000+00:00",
        "id": 1255699569837084703,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Did you build it with the dynamic typebuilder?",
        "timestamp": "2024-06-27 01:41:36.612000+00:00",
        "id": 1255699525348233296,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "[warning long message] \n\nI ended up at this prompt after a while of tinkering with the schema:\n```\nParse the following history of invoices to learn how metadata fields correlate with line items.\nPast Invoices\n---\n[\n  {\n    lineItems: [\n      {\n        index: 0\n        amount: 1,\n        currency: USD,\n        name: null,\n        description: 'Anakin Skywalker',\n        quantity: 1,\n        unitPrice: 1,\n        metadata: {\n          glAccountId: 'Savings',\n          forceAlignment: 'Sith',\n        }\n      },\n\n      ...\n\n    ],\n  },\n]\n---\nNow, parse the following invoice's line items and return a structured representation of the most likely corresponding metadata fields in the schema below.\nYou MUST provide one metadata object for each line item provided in the current invoice.\nCurrent Invoice\n---\n{\n  lineItems: [\n    {\n      index: 0\n      amount: 1,\n      currency: USD,\n      description: 'Anakin Skywalker',\n      quantity: 1,\n      unitPrice: 1,\n    },\n    {\n      index: 1\n      amount: 100,\n      currency: JPY,\n      description: 'Darth Vader',\n      quantity: 10,\n      unitPrice: 10,\n    },\n    {\n      index: 2\n      amount: 100,\n      currency: JPY,\n      description: 'Obi-Wan Kenobi',\n      quantity: 10,\n      unitPrice: 10,\n    },\n    {\n      index: 3\n      amount: 100,\n      currency: JPY,\n      description: 'Qui-Gon Jinn',\n      quantity: 10,\n      unitPrice: 10,\n    },\n  ],\n},\n---\nAnswer in JSON using this schema:\n{\n  lineItem0: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n  lineItem1: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n  lineItem2: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n  lineItem3: {\n    index: int,\n    glAccountId: 'Savings' or 'Checking' or 'Utilities',\n    forceAlignment: 'Jedi' or 'Sith',\n  },\n}\n```",
        "timestamp": "2024-06-27 01:40:28.483000+00:00",
        "id": 1255699239594496071,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Right right that makes sense lol",
        "timestamp": "2024-06-27 00:18:39.093000+00:00",
        "id": 1255678648120512533,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Oh no you use your own api keys haha",
        "timestamp": "2024-06-27 00:18:28.146000+00:00",
        "id": 1255678602205204533,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "OH sorry I thought you meant a Boundary price for using baml",
        "timestamp": "2024-06-27 00:18:16.079000+00:00",
        "id": 1255678551592534017,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Should be in openai docs! Ill link it in a bit",
        "timestamp": "2024-06-27 00:17:48.985000+00:00",
        "id": 1255678437952196609,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Ah I haven't tried looking for pricing info yet, where is that?",
        "timestamp": "2024-06-27 00:17:22.097000+00:00",
        "id": 1255678325175750686,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Depends on your budget",
        "timestamp": "2024-06-27 00:17:07.321000+00:00",
        "id": 1255678263200583690,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Yeah just fyi gpt4 is more expensive. See if gpt4o works",
        "timestamp": "2024-06-27 00:17:00.604000+00:00",
        "id": 1255678235027570710,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Ran it a bunch of times and it hasn't failed yet, I'm gonna add some harder test cases and maybe try the custom array idea",
        "timestamp": "2024-06-27 00:16:28.735000+00:00",
        "id": 1255678101359169586,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Either way ill check your prompt in a bit, i have some other ideas that have worked in the past",
        "timestamp": "2024-06-27 00:15:46.733000+00:00",
        "id": 1255677925190013008,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "I see, that's surprising",
        "timestamp": "2024-06-27 00:15:33.144000+00:00",
        "id": 1255677868193615882,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Not sure about internals but it has been bad in our experience",
        "timestamp": "2024-06-27 00:15:14.021000+00:00",
        "id": 1255677787986067476,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Gpt4turbo was like alpha version of gpt4o",
        "timestamp": "2024-06-27 00:14:59.728000+00:00",
        "id": 1255677728036884622,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "why is that?",
        "timestamp": "2024-06-27 00:14:30.212000+00:00",
        "id": 1255677604237934633,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Interesting",
        "timestamp": "2024-06-27 00:14:10.945000+00:00",
        "id": 1255677523426283520,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "In order of goodness it s gpt4, gpt4o, gpt4turbo",
        "timestamp": "2024-06-27 00:14:03.741000+00:00",
        "id": 1255677493210386484,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Oh? I thought gpt4turbo was supposed to be better",
        "timestamp": "2024-06-27 00:13:44.760000+00:00",
        "id": 1255677413598171136,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Try gpt4 first",
        "timestamp": "2024-06-27 00:13:26.571000+00:00",
        "id": 1255677337307975802,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Oh dont use gpt4 turbo, use gpt4 or gpt4o",
        "timestamp": "2024-06-27 00:13:22.970000+00:00",
        "id": 1255677322204413962,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "I was using GPT4Turbo",
        "timestamp": "2024-06-27 00:13:12.726000+00:00",
        "id": 1255677279238099085,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Ill take a look in a bit. Which model do you use?",
        "timestamp": "2024-06-27 00:13:01.239000+00:00",
        "id": 1255677231057997845,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "```\nExpected:\n[\n  { index: 0, glAccountId: 'Savings', forceAlignment: 'Sith' },\n  { index: 1, glAccountId: 'Checking', forceAlignment: 'Sith' },\n  { index: 2, glAccountId: 'Savings', forceAlignment: 'Jedi' },\n  { index: 3, glAccountId: 'Checking', forceAlignment: 'Jedi' }\n]\n\nPredicted: \n[\n  { index: 1, glAccountId: 'Checking', forceAlignment: 'Sith' },\n  { index: 2, glAccountId: 'Savings', forceAlignment: 'Jedi' },\n  { index: 3, glAccountId: 'Checking', forceAlignment: 'Jedi' }\n]\n```",
        "timestamp": "2024-06-27 00:10:23.431000+00:00",
        "id": 1255676569163403304,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Sure, one sec",
        "timestamp": "2024-06-27 00:08:18.080000+00:00",
        "id": 1255676043403071561,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Can you give me an example of a failing case with the expected value and what it gave you?",
        "timestamp": "2024-06-27 00:08:09.656000+00:00",
        "id": 1255676008070123605,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Full prompt ^",
        "timestamp": "2024-06-27 00:07:28.887000+00:00",
        "id": 1255675837072674876,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "```\nfunction PredictLineItemMetadata(past_invoices: string, current_invoice: string) -> LineItemMetadata[] {\n  client GPT4Turbo\n  prompt #\"\n    Parse the following history of invoices to learn how metadata fields correlate with line items.\n\n    Past Invoices\n    ---\n    {{ past_invoices }}\n    ---\n\n    Now, parse the following invoice's line items and return a structured representation of the most likely corresponding metadata fields in the schema below.\n    You MUST provide one metadata object for each line item provided in the current invoice.\n\n    Current Invoice\n    ---\n    {{ current_invoice }}\n    ---\n\n    {{ ctx.output_format }}\n  \"#\n}\n```",
        "timestamp": "2024-06-27 00:07:24.278000+00:00",
        "id": 1255675817741254696,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "No the mapping is always correct, it just doesn't map items that it can and should",
        "timestamp": "2024-06-27 00:07:18.947000+00:00",
        "id": 1255675795381158019,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "But when it misses line items does it just map them incorrectly?",
        "timestamp": "2024-06-27 00:06:52.125000+00:00",
        "id": 1255675682881667092,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Same if I use indices 1, 2, 3, 4",
        "timestamp": "2024-06-27 00:06:36.391000+00:00",
        "id": 1255675616888357028,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "If I give it indices 0, 1, 2, 3 it sometimes gives me 1, 2, 3 back",
        "timestamp": "2024-06-27 00:06:31.561000+00:00",
        "id": 1255675596629868556,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Does it skip numbers?",
        "timestamp": "2024-06-27 00:05:53.740000+00:00",
        "id": 1255675437997097021,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Instead of index can you make that some other id?",
        "timestamp": "2024-06-27 00:05:33.831000+00:00",
        "id": 1255675354492829707,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "```\n// Defining a data model dynamically\nclass LineItemMetadata {\n  index int\n\n  @@dynamic\n}\n```",
        "timestamp": "2024-06-27 00:05:16.919000+00:00",
        "id": 1255675283558760448,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "What schema are you using?",
        "timestamp": "2024-06-27 00:04:54.029000+00:00",
        "id": 1255675187551277166,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "One hacky idea I have is to dynamically generate a unique property with a templated name for each line item I want on a custom `LineItemMetadataArray` and use that instead of `LineItemMetadata[]`",
        "timestamp": "2024-06-27 00:04:51.894000+00:00",
        "id": 1255675178596438047,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "I actually have seen the problem of not generating the right number of metadata objects several times now, even after putting the index and a line in the prompt saying the model has to provide the correct number -- any ideas?",
        "timestamp": "2024-06-27 00:03:40.387000+00:00",
        "id": 1255674878674075710,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "For sure",
        "timestamp": "2024-06-26 22:48:01.753000+00:00",
        "id": 1255655842263470165,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "lmk if I can help with the prompt engineering",
        "timestamp": "2024-06-26 22:37:06.247000+00:00",
        "id": 1255653092871897200,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "There were some issues on my end, but now it mostly works -- there's some odd pathological cases I'm looking at now and trying to mitigate",
        "timestamp": "2024-06-26 22:28:26.374000+00:00",
        "id": 1255650912366624778,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Does it work end to end?",
        "timestamp": "2024-06-26 20:26:25.401000+00:00",
        "id": 1255620205980287077,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Nice that error is gone now, thanks",
        "timestamp": "2024-06-26 20:23:00.555000+00:00",
        "id": 1255619346793762878,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Yea updated vscode and ts, trying saving now",
        "timestamp": "2024-06-26 20:22:13.466000+00:00",
        "id": 1255619149288312915,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "The generated code had one bug",
        "timestamp": "2024-06-26 20:21:36.003000+00:00",
        "id": 1255618992157098084,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "And try rerunning",
        "timestamp": "2024-06-26 20:21:27.956000+00:00",
        "id": 1255618958405406761,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Save a baml file first",
        "timestamp": "2024-06-26 20:21:24.819000+00:00",
        "id": 1255618945248133321,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Did you update the vscode extension?",
        "timestamp": "2024-06-26 20:21:18.055000+00:00",
        "id": 1255618916877602826,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Same code as before",
        "timestamp": "2024-06-26 20:20:11.514000+00:00",
        "id": 1255618637784547399,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "That error is fixed now, but now I'm getting something else -- it seems like `tb.addEnum(key)` is silently returning undefined\n```\n FAIL  server/helpers/invoiceClassifiers.test.ts\n  Identical Invoices (rename this when I get more test in this describe\n    ✕ Identical Invoices (15 ms)\n\n  ● Identical Invoices (rename this when I get more test in this describe › Identical Invoices\n\n    TypeError: Cannot read properties of undefined (reading 'addValue')\n\n      24 |     console.log({ fieldEnum })\n      25 |     for (let variant in value) {\n    > 26 |       fieldEnum.addValue(variant)\n         |                 ^\n      27 |     }\n      28 |     tb.LineItemMetadata.addProperty(key, fieldEnum.type())\n      29 |   }\n\n      at server/helpers/invoiceClassifiers.ts:26:17\n      at fulfilled (server/helpers/invoiceClassifiers.ts:5:58)\n```",
        "timestamp": "2024-06-26 20:20:08.331000+00:00",
        "id": 1255618624434208808,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "try version 0.44.0 on both VSCode extension + the TS package",
        "timestamp": "2024-06-26 19:30:17.152000+00:00",
        "id": 1255606078519906567,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Amazing, thank you!",
        "timestamp": "2024-06-26 18:55:04.515000+00:00",
        "id": 1255597217478082671,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "fixed it! the fix will roll out in 20ish min",
        "timestamp": "2024-06-26 18:54:36.880000+00:00",
        "id": 1255597101568495659,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "i reproduced the bug, working on a fix",
        "timestamp": "2024-06-26 18:43:20.936000+00:00",
        "id": 1255594266454134957,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Also here's the stacktrace you saw on the call -- may be helpful since it seems like the root error is happening in your source code:\n\n```\n FAIL  server/helpers/invoiceClassifiers.test.ts (5.177 s)\n  Identical Invoices (rename this when I get more test in this describe\n    ✕ Identical Invoices (1 ms)\n\n  ● Identical Invoices (rename this when I get more test in this describe › Identical Invoices\n\n    TypeError: this.classes.has is not a function\n\n      72 |\n      73 |     addEnum<Name extends string>(name: Name): EnumBuilder<Name> {\n    > 74 |         this.tb.addEnum(name);\n         |                 ^\n      75 |     }\n      76 | }\n\n      at TypeBuilder.addEnum (node_modules/@boundaryml/baml/type_builder.js:49:26)\n      at TypeBuilder.addEnum (baml_client/type_builder.ts:74:17)\n      at server/helpers/invoiceClassifiers.ts:21:26\n      at fulfilled (server/helpers/invoiceClassifiers.ts:5:58)\n```",
        "timestamp": "2024-06-26 18:39:28.881000+00:00",
        "id": 1255593293144653885,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Re: call today:\n\n```\n// Dynamically construct LineItemMetadata type based on EntityMetadata table\nconst entityMetadata = await prisma.entityMetadata.findMany({\n  where: {\n    entityId,\n  },\n})\n\nlet tb = new TypeBuilder()\nfor (let { key, value } of entityMetadata) {\n  const fieldEnum = tb.addEnum(key)\n  for (let variant of value) {\n    fieldEnum.addValue(variant)\n  }\n  tb.LineItemMetadata.addProperty(key, fieldEnum.type())\n}\n\n...\n\nconst predictedLineItemMetadata = await b.PredictLineItemMetadata(llmPastInvoicesStr, llmCurrentInvoiceStr, { tb })\n```",
        "timestamp": "2024-06-26 18:27:33.281000+00:00",
        "id": 1255590291700977704,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "neuralcorrelate",
        "content": "Thanks, I'll give those suggestions a try",
        "timestamp": "2024-06-26 17:19:44.618000+00:00",
        "id": 1255573226491220058,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "you probably need to add a description on `Person`, if you are using this dynamic type builder you would need to do `addProperty(\"person\", tb.string().description(\"This can be many people\")`\n\nAnother option is to name this `people` not `person`. LLMs try to fix grammar like that.",
        "timestamp": "2024-06-26 15:31:02.224000+00:00",
        "id": 1255545869587976262,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "can you show me the code?",
        "timestamp": "2024-06-26 15:29:27.581000+00:00",
        "id": 1255545472626589706,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "neuralcorrelate",
        "content": "Hi, I gave this a try where my class has an attribute person Person[], Person being the class with characteristics. Person has name Name, etc. where Name is an enum. What I am finding is that when I use this approach, it never returns more than one Person class, whereas when I use name string in the Person class as opposed to an name enum, it does. Any idea why?",
        "timestamp": "2024-06-26 13:56:19.229000+00:00",
        "id": 1255522033379446784,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "It returns an array of a class — this should be enough for me to get an MVP thx!",
        "timestamp": "2024-06-26 02:02:42.405000+00:00",
        "id": 1255342446603141221,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Ooh I see I didn’t realize addEnum returned the enum😅 that makes sense",
        "timestamp": "2024-06-26 02:00:45.936000+00:00",
        "id": 1255341958096879748,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "you do need to declare an output class in BAML to do this, with @@dynamic attached. In this case here's our function:\n```\nclass Person {\n  @@dynamic\n}\n\nfunction ExtractPerson(input: string) -> Person {\n  client GPT4o\n  prompt #\"\n    Extract from this input:\n    {{ input }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n```\n```",
        "timestamp": "2024-06-26 01:25:51.399000+00:00",
        "id": 1255333172971831348,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "do use the import like this -- accessing the tb.__tb() may break for you in the future -- the underscore methods are internal",
        "timestamp": "2024-06-26 01:23:04.274000+00:00",
        "id": 1255332471998779614,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "here is a full example:\n```\nimport TypeBuilder from \"../../../baml_client/type_builder\";\nimport { Role } from \"../../../baml_client/types\";\n\nexport async function GET(request: Request) {\n  const tb = new TypeBuilder();\n\n  tb.Person.addProperty(\"hair_color\", tb.string());\n  const roleEnum = tb.addEnum(\"Role\");\n  roleEnum.addValue(\"Software Engineer\");\n  roleEnum.addValue(\"Intern\");\n\n  tb.Person.addProperty(\"role\", roleEnum.type());\n\n  const result = await b.ExtractPerson(\n    \"My name is Harry. I have black hair. I love skiing. I am 25 years old.\",\n    { tb: tb }\n  );\n  console.log(JSON.stringify(result, null, 2));\n```",
        "timestamp": "2024-06-26 01:22:29.595000+00:00",
        "id": 1255332326544773132,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "yes you can create an enum dynamically -- it doesnt need to be in BAML. Your function returns a class or enum?",
        "timestamp": "2024-06-26 01:19:09.096000+00:00",
        "id": 1255331485590884423,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Best I can find is doing something hacky like  `tb.__tb().getEnum(key).addProperty(value)` which I doubt works",
        "timestamp": "2024-06-26 01:12:20.187000+00:00",
        "id": 1255329770502094958,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "I found addEnum but I'm not sure how to then access that enum to add properties",
        "timestamp": "2024-06-26 01:09:08.930000+00:00",
        "id": 1255328968312361010,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Nice, got something working -- is it possible to dynamically create an `enum` and THEN dynamically add variants to that enum?",
        "timestamp": "2024-06-26 01:07:54.530000+00:00",
        "id": 1255328656256139365,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "the righ timport for typebuilder is:\n`import TypeBuilder from \"../../../baml_client/type_builder\"`",
        "timestamp": "2024-06-26 00:26:04.235000+00:00",
        "id": 1255318127315652678,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "other solutions folks have done is to use a string[] -- but you will need to validate it. If you use our typebuilder we can at least guarantee one of the enum values you define will be returned",
        "timestamp": "2024-06-25 23:53:23.408000+00:00",
        "id": 1255309903011123301,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Gotcha thanks, taking a look now",
        "timestamp": "2024-06-25 23:52:37.643000+00:00",
        "id": 1255309711058800721,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "make sure you enable BAML_LOG=baml_events or BAML_LOG=info as an environment variable to see the prompt in your terminal as you run things",
        "timestamp": "2024-06-25 23:51:53.793000+00:00",
        "id": 1255309527138697249,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "we are still working on docs. Let us know if you run into any issues -- this feature is relatively new. For example there is no way to test this using our Playground",
        "timestamp": "2024-06-25 23:50:59.760000+00:00",
        "id": 1255309300507611186,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "Here is a code sample:\nhttps://github.com/BoundaryML/baml/blob/canary/integ-tests/typescript/tests/integ-tests.test.ts#L240\n\nthe BAML definition class just needs to have @@dynamic .Works for enums or classes. See here\n\nhttps://github.com/BoundaryML/baml/blob/canary/integ-tests/baml_src/test-files/functions/output/class-dynamic.baml\n\n```\nimport TypeBuilder from '../baml_client/type_builder'\n\n it('should work with dynamics', async () => {\n    let tb = new TypeBuilder()\n    tb.Person.addProperty('last_name', tb.string().optional())\n    tb.Person.addProperty('height', tb.float().optional()).description('Height in meters')\n    tb.Hobby.addValue('CHESS')\n    tb.Hobby.listValues().map(([name, v]) => v.alias(name.toLowerCase()))\n    tb.Person.addProperty('hobbies', tb.Hobby.type().list().optional()).description(\n      'Some suggested hobbies they might be good at',\n    )\n\n    const res = await b.ExtractPeople(\n      \"My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop.\",\n      { tb },\n    )\n    expect(res.length).toBeGreaterThan(0)\n    console.log(res)\n  })\n```",
        "timestamp": "2024-06-25 23:50:28.902000+00:00",
        "id": 1255309171079905291,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "Typescript!",
        "timestamp": "2024-06-25 23:48:16.577000+00:00",
        "id": 1255308616068501566,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": ".aaronv",
        "content": "this is totally possible -- do you use typescript or python?",
        "timestamp": "2024-06-25 23:48:12.802000+00:00",
        "id": 1255308600234999879,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "In my use case, the user can define a couple possible values for a field at an administrative level, which are stored in a database and need to be dynamically pulled and provided to my BAML function.",
        "timestamp": "2024-06-25 23:48:01.165000+00:00",
        "id": 1255308551426146324,
        "parent_id": null,
        "thread_id": 1255308466243899412
    },
    {
        "author": "ashwin.a.kumar",
        "content": "",
        "timestamp": "2024-06-25 23:48:00.806000+00:00",
        "id": 1255308549920260097,
        "parent_id": 1255308466243899412,
        "thread_id": 1255308466243899412
    },
    {
        "author": "elijas_ai",
        "content": "Helix",
        "timestamp": "2024-06-25 13:57:24.778000+00:00",
        "id": 1255159920446275620,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "gabev2037",
        "content": "Is there support for classes with nested fields? i.e. something like \n\n```rust\nclass Section {\n   id string\n    text string\n    subsections Section[]\n}\n```",
        "timestamp": "2024-06-25 12:45:37.326000+00:00",
        "id": 1255141853683257394,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "gabev2037",
        "content": "In the real application, there's no limit to how nested the final structure can be :yikes!:\n\nFor now I'm adding the constraint that there can only be 2 levels of nesting, though ideally it would be more dynamic. That said, I can't imagine more than 3 levels for 95% of cases",
        "timestamp": "2024-06-26 13:02:13.924000+00:00",
        "id": 1255508421583704125,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "hellovai",
        "content": "How nested do you go? If it’s not infinite, you could just create a section and subsection class",
        "timestamp": "2024-06-25 14:19:08.902000+00:00",
        "id": 1255165390338785332,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "elijas_ai",
        "content": "(BAML would return Section[])",
        "timestamp": "2024-06-25 13:56:41.589000+00:00",
        "id": 1255159739298611221,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "elijas_ai",
        "content": "<@1049713528170364968> \nyou could try to do with indexes\n\nclass Section {\n    id int\n    text string\n    subsection_ids int[]\n}",
        "timestamp": "2024-06-25 13:56:15.838000+00:00",
        "id": 1255159631290826822,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "gabev2037",
        "content": "rip",
        "timestamp": "2024-06-25 13:05:14.643000+00:00",
        "id": 1255146791708397598,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "hellovai",
        "content": "Sadly not. To support that we need to do a bit more work and testing on the prompt side as well",
        "timestamp": "2024-06-25 13:04:37.015000+00:00",
        "id": 1255146633885253693,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "gabev2037",
        "content": "I run into dependency cycle issues",
        "timestamp": "2024-06-25 12:45:50.964000+00:00",
        "id": 1255141910884913275,
        "parent_id": null,
        "thread_id": 1255141853683257394
    },
    {
        "author": "gabev2037",
        "content": "",
        "timestamp": "2024-06-25 12:45:50.733000+00:00",
        "id": 1255141909916160030,
        "parent_id": 1255141853683257394,
        "thread_id": 1255141853683257394
    },
    {
        "author": "elijas_ai",
        "content": "My friend is using https://helix-editor.com/ IDE, what would be the best way to use BAML with it?",
        "timestamp": "2024-06-25 11:21:26.649000+00:00",
        "id": 1255120669608513641,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "nice!",
        "timestamp": "2024-07-01 22:23:54.572000+00:00",
        "id": 1257461711674540213,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "no issues, it's on latest 0.45.0",
        "timestamp": "2024-07-01 22:23:45.077000+00:00",
        "id": 1257461671849496608,
        "parent_id": 1257461275215003708,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "you should get them on cursor 😉",
        "timestamp": "2024-07-01 22:22:23.364000+00:00",
        "id": 1257461329120460912,
        "parent_id": 1257461211843268618,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "ah yes, cursor should work! Do you have issues updating the BAML extension via cursor btw?",
        "timestamp": "2024-07-01 22:22:10.512000+00:00",
        "id": 1257461275215003708,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "(My teammate is using Helix though)",
        "timestamp": "2024-07-01 22:21:55.403000+00:00",
        "id": 1257461211843268618,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "awesome, in that case let me take another stab at the syntax highlighting w/ tree-sitter grammars.\n\nThe playground is something we're looking to enable for non-vscode devs via a:\n```\nbaml-cli playground \n```",
        "timestamp": "2024-07-01 22:21:43.541000+00:00",
        "id": 1257461162090696725,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "I'm personally using Cursor (LLM-enabled fork of VSCode) and not VSCode, though, but it has been baml extension has been 100% compatible so far",
        "timestamp": "2024-07-01 22:21:26.337000+00:00",
        "id": 1257461089931890779,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "I highly appreciate for taking a look!\n\nI think (2) would the top priority, with (1) as second-place priority, and (3) least important due to file-save-hook doing most of the value\n\nHowever, I do completely understand the need for BAML to focus on core user base and features (VSCode), so definitely feel free to prioritize to maximize the overall success of BAML 🙌 \n\nThanks again 🚀",
        "timestamp": "2024-07-01 22:20:04.271000+00:00",
        "id": 1257460745722138624,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "For your team, whats the order of preference here in terms of most to least importance btw?\n\n1. playground\n2. syntax highlighting\n3. auto-generating the client code to keep python / TS in sync with BAML",
        "timestamp": "2024-07-01 19:24:57.595000+00:00",
        "id": 1257416677528834058,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "I was basing off of: https://github.com/tree-sitter/tree-sitter-javascript/blob/master/grammar.js",
        "timestamp": "2024-07-01 19:23:34.305000+00:00",
        "id": 1257416328185384999,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "Hey Elijas! Gave it a try and its a bigger refactor job and i thought for getting the full language server (with the playground) 😦 \n\nBut here's what i recommend that would be a good stop gap:\n\n1. Add an auto compile hook in Helix for on save events in BAML file (run `baml-cli generate` from your python env or `npx baml-cli generate`)\n2. Adding a customer tree-sitter grammar file for syntax highlighting (attaching below - I'm still trying to figure out how helix actually runs the grammar, but was able to get the mostly grammar working atleast)",
        "timestamp": "2024-07-01 19:22:23.698000+00:00",
        "id": 1257416032038027304,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "<@99252724855496704> Hey, do you have any updates?\nAny idea how hard it would be to add helix support ourselves?",
        "timestamp": "2024-07-01 18:53:12.111000+00:00",
        "id": 1257408685349798010,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "This is super sweet, thanks for all the amazing work you do",
        "timestamp": "2024-06-25 23:58:17.150000+00:00",
        "id": 1255311135054368808,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "Nvmind. Found it. Seems like tree sitter grammars",
        "timestamp": "2024-06-25 14:31:03.014000+00:00",
        "id": 1255168385541607455,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "Do you know how you get syntax highlighting in helix",
        "timestamp": "2024-06-25 14:22:01.431000+00:00",
        "id": 1255166113977733291,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "This is the best workaround tho: https://docs.boundaryml.com/docs/get-started/quickstart/editors-other",
        "timestamp": "2024-06-25 14:17:56.180000+00:00",
        "id": 1255165085320609852,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "I’ll take a stab at this over the weekend and get back to you! But it looks achievable.",
        "timestamp": "2024-06-25 14:17:44.702000+00:00",
        "id": 1255165037178261565,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "Right now we bundle our language server in with our extension. I think if we pull it out and ship it as a separate npm package it could work!",
        "timestamp": "2024-06-25 14:16:56.732000+00:00",
        "id": 1255164835977625610,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "hellovai",
        "content": "Sorry I was reading the helix docs!",
        "timestamp": "2024-06-25 14:15:58.404000+00:00",
        "id": 1255164591332393142,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "<@99252724855496704> i.e. is there maybe a Language Server I could connect?",
        "timestamp": "2024-06-25 13:57:25.579000+00:00",
        "id": 1255159923805782196,
        "parent_id": null,
        "thread_id": 1255120669608513641
    },
    {
        "author": "elijas_ai",
        "content": "",
        "timestamp": "2024-06-25 13:57:24.778000+00:00",
        "id": 1255159920446275614,
        "parent_id": 1255120669608513641,
        "thread_id": 1255120669608513641
    },
    {
        "author": "yungweedle",
        "content": "do you commit the autogenerated python files from baml to git?",
        "timestamp": "2024-06-24 02:16:57.565000+00:00",
        "id": 1254621257673216001,
        "parent_id": null,
        "thread_id": 1254621257673216001
    },
    {
        "author": "deoxykev",
        "content": "commiting autogenerated code sometimes causes annoying merge conficts if there are multiple people working on baml prompts...",
        "timestamp": "2024-07-10 20:05:26.920000+00:00",
        "id": 1260688357646794764,
        "parent_id": null,
        "thread_id": 1254621257673216001
    },
    {
        "author": "hellovai",
        "content": "some people do! but you can also .gitignore them and just run the generate command in your build script via `baml-cli generate`\n\nhttps://docs.boundaryml.com/docs/get-started/deploying/docker",
        "timestamp": "2024-06-24 02:17:46.272000+00:00",
        "id": 1254621461965045760,
        "parent_id": null,
        "thread_id": 1254621257673216001
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-24 02:17:45.827000+00:00",
        "id": 1254621460098580621,
        "parent_id": 1254621257673216001,
        "thread_id": 1254621257673216001
    },
    {
        "author": "yungweedle",
        "content": "Hi, is there a built in way of providing sample outputs into prompts?",
        "timestamp": "2024-06-24 01:52:07.119000+00:00",
        "id": 1254615006289592453,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "It’s hard to say one or the other upfront",
        "timestamp": "2024-06-25 03:26:42.635000+00:00",
        "id": 1255001198994391134,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "I think I would try and experiment. In some cases you will need the format but in others you may not!",
        "timestamp": "2024-06-25 03:26:27.723000+00:00",
        "id": 1255001136448933989,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "It’s more tokens right",
        "timestamp": "2024-06-25 03:25:05.115000+00:00",
        "id": 1255000789966131301,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "Is it good practice to do that so that it matches with the output format?",
        "timestamp": "2024-06-25 03:24:54.981000+00:00",
        "id": 1255000747461050440,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "we should add these to our docs! 🥲",
        "timestamp": "2024-06-25 03:05:55.261000+00:00",
        "id": 1254995967128637511,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions",
        "timestamp": "2024-06-25 03:05:47.915000+00:00",
        "id": 1254995936317542510,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "yes! You can do this:\n\n```\n{{ variable || pprint }}\n```\n\nhere's the complete list of built ins!",
        "timestamp": "2024-06-25 03:05:46.493000+00:00",
        "id": 1254995930352980018,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "Is there a way of jsonify it like in jinja",
        "timestamp": "2024-06-25 02:21:31.383000+00:00",
        "id": 1254984794014482495,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "When I pass in an object, it renders it in one line instead of pretty json format",
        "timestamp": "2024-06-25 02:21:12.050000+00:00",
        "id": 1254984712926007356,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "there should be an open playground button over the baml function:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#opening-baml-playground",
        "timestamp": "2024-06-24 02:58:46.845000+00:00",
        "id": 1254631782356095048,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "How do you use the playground in vscode?",
        "timestamp": "2024-06-24 02:58:12.611000+00:00",
        "id": 1254631638768287834,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "sorry in the loop it should be:\n```\n   {% for example in examples %}\n   {{ example }}\n   {% endfor %}\n```\n\nyou can do more interestuing thigns like:\n\n```\n   {% for example in examples %}\n   {{ _.role('user') }}\n   Example {{ loop.index }}:\n   {{ example }}\n   {% endfor %}\n```",
        "timestamp": "2024-06-24 02:47:11.262000+00:00",
        "id": 1254628864869531740,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "They run the same code*",
        "timestamp": "2024-06-24 02:42:36.367000+00:00",
        "id": 1254627711876337685,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "both 🙂 They are the same thing!",
        "timestamp": "2024-06-24 02:42:29.585000+00:00",
        "id": 1254627683430826005,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "is the playground in vscode? Or do you mean promptfiddle?",
        "timestamp": "2024-06-24 02:42:12.109000+00:00",
        "id": 1254627610130907207,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "do you have an example of the prompt you are able to share? Can give better advice with a more specific example",
        "timestamp": "2024-06-24 01:57:53.811000+00:00",
        "id": 1254616460421234730,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "also, as long as the type of the examples is the same as your output type, then it shouldn't matter too much",
        "timestamp": "2024-06-24 01:57:36.110000+00:00",
        "id": 1254616386177601617,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "docs on loops: https://docs.boundaryml.com/docs/snippets/prompt-syntax/loops",
        "timestamp": "2024-06-24 01:57:09.355000+00:00",
        "id": 1254616273958993971,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "the playground should show you what the final prompt looks like!",
        "timestamp": "2024-06-24 01:56:45.132000+00:00",
        "id": 1254616172360630367,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "then you can actually do:\n\n```\nfunction(arg1: string, examples: OutputType[]) -> OutputType {\n  client GPT4\n  prompt #\"\n  ...\n   {% for example in examples %}\n   example\n   {% endfor %}\n  \"#\n}\n```",
        "timestamp": "2024-06-24 01:56:32.775000+00:00",
        "id": 1254616120531615794,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "Or is the str representation good enough",
        "timestamp": "2024-06-24 01:56:20.912000+00:00",
        "id": 1254616070774325278,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "yungweedle",
        "content": "Do I need to format it in a particular way",
        "timestamp": "2024-06-24 01:55:42.277000+00:00",
        "id": 1254615908727652524,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "from <@201399017161097216> Not at the moment, the best way is to add an input argument with your examples. You would need to pass them in from your code and print them out in the prompt.\n\nfunction(arg1: string, examples: OutputType[]) -> OutputType",
        "timestamp": "2024-06-24 01:55:24.394000+00:00",
        "id": 1254615833720918116,
        "parent_id": null,
        "thread_id": 1254615006289592453
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-24 01:55:24.087000+00:00",
        "id": 1254615832433000489,
        "parent_id": 1254615006289592453,
        "thread_id": 1254615006289592453
    },
    {
        "author": "gabev2037",
        "content": "I have a field which is a list of baml objects\n\n```rust\nclass Contact {\nname string?\nemail string?\n}\n\nclass Requirement {\n  text string\n  contacts Contact[]\n}\n```\nI assumed that if no contacts were able to be inferred, it would return an empty list. Instead, I am seeing the following from the output (ignore the `celery_worker_rfp-1`\n```\n\"contacts\": [\ncelery_worker_rfp-1           |           {\ncelery_worker_rfp-1           |             \"name\": null,\ncelery_worker_rfp-1           |             \"email\": null,\ncelery_worker_rfp-1           |             \"phone\": null\ncelery_worker_rfp-1           |           }\ncelery_worker_rfp-1           |         ],\n```\n\nAny idea why that is?",
        "timestamp": "2024-06-22 17:49:26.366000+00:00",
        "id": 1254131148221841428,
        "parent_id": null,
        "thread_id": 1254131148221841428
    },
    {
        "author": "hellovai",
        "content": "from a language perspective, and from the model's perspective, every field being Null is  a valid option since that is a valid class definition given the types of the field.",
        "timestamp": "2024-06-22 17:55:09.990000+00:00",
        "id": 1254132589485232260,
        "parent_id": null,
        "thread_id": 1254131148221841428
    },
    {
        "author": "hellovai",
        "content": "no it doesn't, just an idea of what we could do",
        "timestamp": "2024-06-22 17:53:24.122000+00:00",
        "id": 1254132145442521189,
        "parent_id": null,
        "thread_id": 1254131148221841428
    },
    {
        "author": "gabev2037",
        "content": "This already exists?",
        "timestamp": "2024-06-22 17:53:14.688000+00:00",
        "id": 1254132105873592372,
        "parent_id": null,
        "thread_id": 1254131148221841428
    },
    {
        "author": "hellovai",
        "content": "this is just prompt engineering. \n\nI think we can add an attribute to a field that helps you auto filter some of those out.\n(e.g. in class Contact)\n\n```\nclass Contact {\n  name string?\n  email string?\n  @@require_atleast_one_field\n}\n```\n(not the exact one, but you get the idea)",
        "timestamp": "2024-06-22 17:52:59.044000+00:00",
        "id": 1254132040257896568,
        "parent_id": null,
        "thread_id": 1254131148221841428
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-22 17:52:58.612000+00:00",
        "id": 1254132038446092330,
        "parent_id": 1254131148221841428,
        "thread_id": 1254131148221841428
    },
    {
        "author": "nebuleto",
        "content": "Is there any way to input any files(which is not image) to LLM with Baml?",
        "timestamp": "2024-06-21 23:29:55.420000+00:00",
        "id": 1253854446018887770,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "and it will just work as well FYI",
        "timestamp": "2024-06-21 23:39:19.703000+00:00",
        "id": 1253856812793594007,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "if you end up wanting to use images, you can make content a `string | image` type",
        "timestamp": "2024-06-21 23:39:12.098000+00:00",
        "id": 1253856780895649922,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "glad that helped! 🙂",
        "timestamp": "2024-06-21 23:38:53.178000+00:00",
        "id": 1253856701539684353,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "nebuleto",
        "content": "I can solve with this way. Thanks!",
        "timestamp": "2024-06-21 23:38:44.042000+00:00",
        "id": 1253856663220518975,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "then in typescript:\n\n```typescript\nimport {b} from 'baml_client'\nimport {File} from 'baml_client/types'\n\nasync function foo(files: string[]) {\n  const loaded_files: File[] = loadFiles(files)\n  await b.DoSomething(\"summarize the main points\")\n}\n```",
        "timestamp": "2024-06-21 23:38:13.108000+00:00",
        "id": 1253856533473787974,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "and the best way to do this, yea i think only gemini supports other files, but for now the method we recommend doing the file loading / OCR / convering to an image and loading prior to baml, then just passing in the files as a string param to a baml function.\n\n```rust\nclass File {\n  name string\n  content string\n}\n\nfunction DoSomething(goal: string, files: File[]) -> string {\n  client GPT4o\n  prompt #\"\n     Do {{ goal }}\n     \n     {% for file in files %}\n     {{ _.role('user') }}\n     Title: {{ file.name }}\n     {{ file.content }}\n     {% endfor %}\n  \"#\n}\n```",
        "timestamp": "2024-06-21 23:36:40.545000+00:00",
        "id": 1253856145236295780,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": ".aaronv",
        "content": "Anthropic models only support text and images:\n\nBut gemini does support files! Vaibhav will chime in on that in a sec",
        "timestamp": "2024-06-21 23:36:05.782000+00:00",
        "id": 1253855999429836872,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "nebuleto",
        "content": "I'm planning to use Gemini 1.5 Flash/Pro, Claude 3 Haiku/3 Sonnet/3.5 Sonnet, GPT-4o. I think this models supports to input files.",
        "timestamp": "2024-06-21 23:33:27.913000+00:00",
        "id": 1253855337279258704,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "and what models are you planning on using?",
        "timestamp": "2024-06-21 23:31:56.282000+00:00",
        "id": 1253854952950861958,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "nebuleto",
        "content": "I'm using typescript, and I want to input multiple pdf files or html files.",
        "timestamp": "2024-06-21 23:31:32.638000+00:00",
        "id": 1253854853780734095,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "and what language are you trying to interface with",
        "timestamp": "2024-06-21 23:30:36.348000+00:00",
        "id": 1253854617683492985,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "hi haze, what kind of files are you talking about?",
        "timestamp": "2024-06-21 23:30:29.034000+00:00",
        "id": 1253854587006222388,
        "parent_id": null,
        "thread_id": 1253854446018887770
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-21 23:30:28.741000+00:00",
        "id": 1253854585777295465,
        "parent_id": 1253854446018887770,
        "thread_id": 1253854446018887770
    },
    {
        "author": "elijas_ai",
        "content": "Can we pass Model name to be used through function argument?",
        "timestamp": "2024-06-21 22:05:24.476000+00:00",
        "id": 1253833176938188820,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "cool np. For now you could create 2 funcitons and copy the prompt over. We'll make it easier very very soon.",
        "timestamp": "2024-06-21 22:57:22.030000+00:00",
        "id": 1253846252907397233,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": "elijas_ai",
        "content": "🙌",
        "timestamp": "2024-06-21 22:56:46.748000+00:00",
        "id": 1253846104924098612,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": "elijas_ai",
        "content": "Thank you",
        "timestamp": "2024-06-21 22:56:35.243000+00:00",
        "id": 1253846056668500049,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": "elijas_ai",
        "content": "Awesome! No rush, though",
        "timestamp": "2024-06-21 22:56:33.034000+00:00",
        "id": 1253846047403413544,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "yes we have a way of doing it -- though it's going out on the next release (tonight). CC <@99252724855496704>",
        "timestamp": "2024-06-21 22:55:58.767000+00:00",
        "id": 1253845903677198507,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": "elijas_ai",
        "content": "yes",
        "timestamp": "2024-06-21 22:55:21.218000+00:00",
        "id": 1253845746185277552,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "oh you want to change the client at runtime",
        "timestamp": "2024-06-21 22:55:17.331000+00:00",
        "id": 1253845729881882716,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": "elijas_ai",
        "content": "I want to give user ability to select to gpt4o or sonnet3.5",
        "timestamp": "2024-06-21 22:55:02.451000+00:00",
        "id": 1253845667470770309,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "you can try adding {{ ctx.client }} and see the playgorund preview to see all the available properties",
        "timestamp": "2024-06-21 22:08:18.481000+00:00",
        "id": 1253833906768318516,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "in the prompt",
        "timestamp": "2024-06-21 22:08:03.075000+00:00",
        "id": 1253833842150609048,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "what are you trying to do?\n\nWe definitely support accessing your client provider and name like this:\n\n{{ ctx.client.name }}",
        "timestamp": "2024-06-21 22:08:01.114000+00:00",
        "id": 1253833833925840967,
        "parent_id": null,
        "thread_id": 1253833176938188820
    },
    {
        "author": ".aaronv",
        "content": "",
        "timestamp": "2024-06-21 22:08:00.933000+00:00",
        "id": 1253833833166536775,
        "parent_id": 1253833176938188820,
        "thread_id": 1253833176938188820
    },
    {
        "author": "gabev2037",
        "content": "Are there docs for implementing dynamic types? I would like to pass in an Enum I've defined in application code",
        "timestamp": "2024-06-21 18:12:07.795000+00:00",
        "id": 1253774470603083946,
        "parent_id": null,
        "thread_id": 1253774470603083946
    },
    {
        "author": "hellovai",
        "content": "quick huddle?",
        "timestamp": "2024-06-21 18:21:01.279000+00:00",
        "id": 1253776708197158924,
        "parent_id": null,
        "thread_id": 1253774470603083946
    },
    {
        "author": "gabev2037",
        "content": "my case is\n\n# Application code\n```python\nclass DocumentType(Enum):\n   NOTION\n   GDRIVE\n```\n\n# Baml code\n```rust\nenum DocumentType {\n   @@dynamic\n}\n\nfunction getDocumentType(document_text: string) -> DocumentType {\n\n}\n```\n\nSo when I call the code in application, I can expect the return type to be the `DocumentType` I defined in Application code OR a `str`?",
        "timestamp": "2024-06-21 18:20:38.308000+00:00",
        "id": 1253776611849932902,
        "parent_id": null,
        "thread_id": 1253774470603083946
    },
    {
        "author": "hellovai",
        "content": "the auto complete should work!\n\nThe return tyep of anything of type Tool auto becomes:\n```\nTools | str\n```",
        "timestamp": "2024-06-21 18:17:57.173000+00:00",
        "id": 1253775936000884778,
        "parent_id": null,
        "thread_id": 1253774470603083946
    },
    {
        "author": "gabev2037",
        "content": "what does it look like when baml returns the output, will it be the same type as the class I pass in?",
        "timestamp": "2024-06-21 18:17:15.362000+00:00",
        "id": 1253775760632578128,
        "parent_id": null,
        "thread_id": 1253774470603083946
    },
    {
        "author": "hellovai",
        "content": "we're working on docs! Should out out by EOD.\n\ntl;dr:\n```python\nfrom baml_client.type_builder import TypeBuilder\n\ntb = TypeBuilder()\nfor _, row in df.iterrows():\n  if row[\"Parent\"] in root_categories:\n    tb.Tools.add_value(row['Categories'])\nselected = await b.Classify(tool, description, count=1, baml_options={ \"tb\": tb })\n```\n\n```rust\n// Defining a data model.\nenum Tools {\n  // We'll define these in python\n  @@dynamic\n\n  @@alias(ToolCategory)\n}\n\nclass Classification {\n  category Tools\n  reason string\n}\n\n\n\n// Creating a function to extract the resume from a string.\nfunction Classify(tool: string, description: string, count: int) -> Classification[] {\n  client FastOpenAI\n  prompt #\"\n    Given tools and their descriptions, classify the tools into categories.\n    {{ ctx.output_format(enum_value_prefix=null) }}\n\n    {% if count > 1 %}\n    Give me the {{ count }} best options.\n    {% endif %}\n\n    {{ _.role('user') }}\n    Tool: {{ tool }}\n    Description: {{ description }}\n  \"#\n}\n```\n\nHere's a repo example:\nhttps://github.com/BoundaryML/example-massive-categorizer/blob/main/classifier.ipynb",
        "timestamp": "2024-06-21 18:14:33.993000+00:00",
        "id": 1253775083801940019,
        "parent_id": null,
        "thread_id": 1253774470603083946
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-21 18:14:33.499000+00:00",
        "id": 1253775081730080778,
        "parent_id": 1253774470603083946,
        "thread_id": 1253774470603083946
    },
    {
        "author": "gabev2037",
        "content": "Does baml support datetimes? Or should we still use the `string` types with descriptions that specify ISO 8601 Timestamp format?",
        "timestamp": "2024-06-21 16:44:54.253000+00:00",
        "id": 1253752519537201263,
        "parent_id": null,
        "thread_id": 1253752519537201263
    },
    {
        "author": "hellovai",
        "content": "string is the way!",
        "timestamp": "2024-06-21 16:50:40.782000+00:00",
        "id": 1253753972985036870,
        "parent_id": null,
        "thread_id": 1253752519537201263
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-21 16:50:40.363000+00:00",
        "id": 1253753971227492373,
        "parent_id": 1253752519537201263,
        "thread_id": 1253752519537201263
    },
    {
        "author": "hellovai",
        "content": "Multi-agent",
        "timestamp": "2024-06-20 14:53:30.773000+00:00",
        "id": 1253362099124568137,
        "parent_id": null,
        "thread_id": 1253362099124568137
    },
    {
        "author": "hellovai",
        "content": "in typescript you would do:\n```typescript\nimport {b} from \"baml_client\"\nimport {Category} from \"baml_client/types\"\n\n\nasync function pipeline(query: string) {\n  const category = await b.PickBestCategory(query);\n  if (category == Category.TroubleShooting) {\n    return await TroubleShootingAgent(query);\n  }\n  if (category == Category.Buying) {\n    return await BuyingAgent(query);\n  }\n\n  return \"Sorry! I can't help you with that!\";\n}\n\n```\n\nLet me get you an example that also holds onto conversation history, so you can see what it looks like in a chat app",
        "timestamp": "2024-06-20 15:01:42.266000+00:00",
        "id": 1253364160595623998,
        "parent_id": null,
        "thread_id": 1253362099124568137
    },
    {
        "author": "hellovai",
        "content": "<@346578058389749762> the way we do multi-agents in baml is more like code you would write, even if you weren't doing ai. If you wanted to do a router that takes a query then picks one of many agents to pass it down to you would do something like this:\n\n```rust\nenum Category {\n  TroubleShooting @description(#\"a short description\"#)\n  Buying @description(#\"a short description\"#)\n}\n\nfunction PickBestCategory(query: string) -> Category {\n  client GPT4o\n  prompt #\"\n    {{ ctx.output_format }}\n\n    {{ _.role('user') }}\n    {{query}}\n  \"#\n}\n```\n\nthen in python:\n\n```python\nfrom baml_client import b\nfrom baml_client.types import Category\n\nasync def pipeline(query: str):\n   category = await b.PickBestCategory(query)\n\n   if category == Category.TroubleShooting:\n      # code here to handle this however you want\n   elif category == Category.Buying\n      # So on...\n```",
        "timestamp": "2024-06-20 14:53:31.527000+00:00",
        "id": 1253362102287077406,
        "parent_id": null,
        "thread_id": 1253362099124568137
    },
    {
        "author": "hellovai",
        "content": "using images",
        "timestamp": "2024-06-20 14:47:06.066000+00:00",
        "id": 1253360485546594357,
        "parent_id": null,
        "thread_id": null
    },
    {
        "author": "mariustrovik",
        "content": "Can I feed locally stored images to the LLM? If yes, could you give me some pointers on how to go about this (Class, function and main.py)? The goal would be to feed images of financial tables to the LLM and receive JSON back which I can store on my computer 🙏 \n\nI attempted something like this, but I cannot figure out how to pass the image data to the b.ExtractTable-function:\n\n```import asyncio\nimport json\nfrom pathlib import Path\nimport base64\nfrom baml_client import b\n\nasync def main():\n    try:\n        # Local path to the image\n        image_path = Path(\"C:/Users/Trovi/Documents/Projects/ML/baml_table_extraction/Page_131_High_Res_Image.png\")\n\n        # Read the image file\n        with open(image_path, 'rb') as image_file:\n            image_data = image_file.read()\n\n        # Encode the image to base64\n        image_base64 = base64.b64encode(image_data).decode('utf-8')\n\n        # Extract table from the image using the base64 string\n        table = await b.ExtractTable(image_base64) \n        \n        with open('extracted_table.json', 'w', encoding='utf-8') as file:\n            json.dump(table, file, ensure_ascii=False, indent=4)\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())`\n\n\n-------resume.baml--------\n// Function to extract the table from an image.\nfunction ExtractTable(first_image: image ) -> Table {\n  client GPT4o\n  prompt #\"\n    {{ _.role(\"user\")}}\n\n    Task: Analyze the provided image and extract the table data.\n\n    {{ first_image }}\n    {{ ctx.output_format }}\n\n  \"#\n}```",
        "timestamp": "2024-06-20 14:29:01.790000+00:00",
        "id": 1253355937763295232,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "mariustrovik",
        "content": "I see, this makes sense. In the beginning I had a feeling that the magic of Baml was \"I will give you perfect JSON, no matter what, you just sit back and relax\", but now I see that there is a structure to it and \"things actually make sense\" (And that's preferable of course :D). Thank you for guiding me past through this bumpy patch in my Baml-journey.\n\nIf I can be of any help, please let me know. I do not believe I am up to par when it comes to contributing on the coding side, but perhaps user feedback or something could be of help.",
        "timestamp": "2024-06-20 19:48:18.305000+00:00",
        "id": 1253436286010785854,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "ah no, there's no extra work to be done in the python code.\n\nIn the playground we just are calling the equivalent of `model_dump()` for you in the UI 🙂 \n\nits more so the type of the function have you a `Table` class.\n\nwhen you do a `print(result)`, the default print method for the `Table` class uses the method pydantic defines (which does the `row=[ ...]`\n\nyou can still acess it via:\n```\nfor row in result.row:\n  for item in row:\n    print(item)\n  print(\"END\")\n```",
        "timestamp": "2024-06-20 19:42:46.803000+00:00",
        "id": 1253434895590756352,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "mariustrovik",
        "content": "Ah, very interesting. I had a feeling I was causing this myself. The syntax of building classes was a bit of a mystery and I tried my best to get something that would work. A list of lists gave no errors, so I stuck with it. I figured it would convert to JSON just like in the Playground. \n\nIt's great that the Playground is so quick and fail-proof; it gives the user an early sense of accomplishment. But do I understand correctly that there are some hidden \"One size fits all\"-parsing steps applied in the Playground that I have to create myself in the main.py? Such as the ones you suggest: \n´result.model_dump() -> key value dict´\n´result.model_dump_json() -> str´",
        "timestamp": "2024-06-20 19:40:13.684000+00:00",
        "id": 1253434253362991105,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "<@827968359856341023> i think you're actually getting what you want. We use pydantic (https://docs.pydantic.dev/2.7/) for all the models you generate. So you have a class called `Table` with a property called `row` of type `List[List[int]]`. \n\nTo print a pydantic model to json or as a dictionary:\n```\nresult.model_dump() -> key value dict\nresult.model_dump_json() -> str\n```",
        "timestamp": "2024-06-20 19:22:11.755000+00:00",
        "id": 1253429715423854693,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "mariustrovik",
        "content": "Thanks for looking into this 🙏",
        "timestamp": "2024-06-20 18:42:05.577000+00:00",
        "id": 1253419623181717545,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "Will report back and figure out why the parser struggled!",
        "timestamp": "2024-06-20 18:30:12.450000+00:00",
        "id": 1253416632110420081,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "FYI, i created an issue after getting a partial repro on something similar: https://github.com/BoundaryML/baml/issues/702",
        "timestamp": "2024-06-20 18:29:59.863000+00:00",
        "id": 1253416579316711435,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "(screenshots are ok btw)",
        "timestamp": "2024-06-20 18:18:43.501000+00:00",
        "id": 1253413742449000549,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "(in python vs the playground)",
        "timestamp": "2024-06-20 18:18:37.952000+00:00",
        "id": 1253413719174807592,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "like the raw output you get out of the prompt?",
        "timestamp": "2024-06-20 18:18:29.082000+00:00",
        "id": 1253413681971069050,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "can you share a bit more about what the outputs are?",
        "timestamp": "2024-06-20 18:18:11.574000+00:00",
        "id": 1253413608537198665,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "mariustrovik",
        "content": "```\n---main.py---\nimport asyncio\nimport base64\nfrom pathlib import Path\nimport json\nimport baml_py\nfrom baml_client import b\nfrom baml_client.types import Table\n\nasync def main():\n    try:\n        # Local path to the image\n        image_path = Path(\"C:/Users/Trovi/Documents/Projects/ML/baml_table_extraction/Page_131_High_Res_Image_Partial1.png\")\n\n        # Read the image file\n        with open(image_path, 'rb') as image_file:\n            image_data = image_file.read()\n\n        # Encode the image to base64\n        base64_image = base64.b64encode(image_data).decode('utf-8')\n        mime_type = \"image/png\"  # Ensure this matches the actual image type\n\n        # Create an Image object using the from_url method with base64 data\n        Image = baml_py.Image.from_url(f'data:{mime_type};base64,{base64_image}')\n\n        result = await b.ExtractTable(Image)\n        \n        print(result)\n\n        # Save the result to a JSON file\n        output_path = Path(\"extracted_table.json\")\n        with open(output_path, 'w', encoding='utf-8') as json_file:\n            json.dump(result, json_file, ensure_ascii=False, indent=4)\n\n        print(f\"Results saved to {output_path}\")\n\n        assert isinstance(result, Table)\n\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```",
        "timestamp": "2024-06-20 17:10:02.658000+00:00",
        "id": 1253396458380726322,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "mariustrovik",
        "content": "```\n---resume.baml---\nclass Table {\n  row string[][] @description(#\"\n    The rows of the table, each containing the data for each column.\n  \"#)\n}\n\n// Function to extract the table from an image.\nfunction ExtractTable(first_image: image ) -> Table {\n  client GPT4o\n  prompt #\"\n    {{ _.role(\"user\")}}\n\n    Task: Analyze the provided image and extract the table data to JSON format:\n\n    {{ first_image }}\n\n    Extract this data in JSON format as described below:\n\n    {{ ctx.output_format }}\n\n    Before generating the output, validate the structure and content for accuracy and completeness.\n  \"#\n}\n\ntest table_extraction {\n  functions [ExtractTable]\n  args {\n    first_image { url \"https://i.postimg.cc/Xq4BjrSk/Page-131-High-Res-Image-Partial1.png\"}\n  }\n}\n```",
        "timestamp": "2024-06-20 17:09:37.668000+00:00",
        "id": 1253396353564934165,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "mariustrovik",
        "content": "Thanks for the quick reply. This works!\n\nI have now set up my main.py and I am able to send the image to the LLM and get a response. I do have one more question though:\n\nWhy does the Playground give me perfect JSON, however the function output (result) in main.py gives me a list of list?",
        "timestamp": "2024-06-20 17:09:12.493000+00:00",
        "id": 1253396247973199932,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "great point, our docs are missing!\n\nHere's how you do it in python: \nYou can access example code in python here (this has a lot of examples we use in our unit tests): https://github.com/BoundaryML/baml/blob/b19f04a059ba18d54544cb278b6990b95170d3f3/integ-tests/python/test_functions.py#L96\n\n```python\n\n\nres = await b.TestImageInput(\n        img=baml_py.Image.from_url(\n\"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n        )\n    )\n```\n\nthere's also a `Image.from_base64` but that has a bug that we'll be patching in today's release! (`0.42`). \n\nInstead you can just do:\n```\nbaml_py.Image.from_url(f'data:{mime_type};base64,{base64_image}')\n```",
        "timestamp": "2024-06-20 14:47:06.408000+00:00",
        "id": 1253360486980915341,
        "parent_id": null,
        "thread_id": 1253355937763295232
    },
    {
        "author": "hellovai",
        "content": "",
        "timestamp": "2024-06-20 14:47:05.961000+00:00",
        "id": 1253360485105930283,
        "parent_id": 1253355937763295232,
        "thread_id": 1253355937763295232
    }
]