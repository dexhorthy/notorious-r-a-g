[
    {
        "thread_id": 1253471884142444615,
        "thread_name": "Did something change in how we are",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:09:45.561000+00:00",
                "content": "Did something change in how we are supposed to be capturing traces for new functions? I am not seeing my newer functions in the dashboard"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:10:27.280000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:10:27.695000+00:00",
                "content": "it should just be @trace are you doing this for baml or non-baml functions btw?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:10:41.085000+00:00",
                "content": "is this for dynamic types?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:11:08.499000+00:00",
                "content": ">it should just be @trace are you doing this for baml or non-baml functions btw?\n\nI decorate a normal python function with the `@trace`, though within that function is a baml call\n\n>is this for dynamic types?\nNo"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:11:41.322000+00:00",
                "content": "I know the code is being executed properly. I'm simply not seeing the trace persist in the dashboard"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:12:15.542000+00:00",
                "content": "got it! ok let me try and repro:\n\nother quick points, is this dealing with threads in anyways?\nAnd is this on a lambda function?\n\nif its on a lambda, you may need to add a flush at the end"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:12:18.012000+00:00",
                "content": "Maybe my baml keys got rotated or something?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:12:29.397000+00:00",
                "content": "This is completed in a celery worker"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:12:43.010000+00:00",
                "content": "ah yes, same thing would be needed!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:12:49.983000+00:00",
                "content": "huddle? I can check the code rq!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:12:58.432000+00:00",
                "content": "ya"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-20 22:39:29.612000+00:00",
                "content": "<@99252724855496704> looks like the worker is stalling following the LLM response?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-20 22:39:54.241000+00:00",
                "content": "back on OH!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-21 20:02:45.754000+00:00",
                "content": "This is becoming a blocker. Is there a baml log level I can use to just see the input and output of the LLM?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-21 20:13:59.716000+00:00",
                "content": "BAML_LOG=baml_events"
            }
        ]
    },
    {
        "thread_id": 1254123002707312640,
        "thread_name": "Found a bug where I executed a test case",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:17:04.324000+00:00",
                "content": "Found a bug where I executed a test case in the BAML playground , saw the parsed LLM Response as it was streaming, then it collapsed into a list and won't let me expand"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:18:43.163000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:18:43.532000+00:00",
                "content": "its actually not that it collapsed, its that it was parseable for a while, but eventually failed to parse! We should expose some better flags for this. See a similar issue where i ran into something like this and was confused myself\nhttps://github.com/BoundaryML/baml/issues/702"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:20:43.264000+00:00",
                "content": "Oh interesting... could it be due to the use of dynamic types failing with test cases?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:21:30.120000+00:00",
                "content": "oh interesting, i haven't tested what happens for dynamic types in test cases when they are empty. likely we will say \"Failed to parse\"..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:21:39.765000+00:00",
                "content": "oh intersting, yes that is a bit busted UX"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:21:42.200000+00:00",
                "content": "great catch"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:22:04.946000+00:00",
                "content": "I guess this will bump up our priority for supporting dynamic types in tests!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:22:12.911000+00:00",
                "content": "In my example, the ouptut object requires a field which is a dynamic type. In the test case, I don't explicitly provide a typebuilder (not sure if that's even feasible). When I inspect the Raw LLM Response, I got random values for the compliance_type"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:22:25.240000+00:00",
                "content": "yep, thats 100% the issue ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:22:40.202000+00:00",
                "content": "for now the workaround for dynamic types is to write tests in python natively using pytest"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:23:23.845000+00:00",
                "content": "also we should expose errors better for sure, if you had seen \"field\" doesn't match \"TypeName\", you likely would have jumpted to teh conclusion faster!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:23:44.090000+00:00",
                "content": "Is the github issue you linked also due to dynamic types?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:23:53.782000+00:00",
                "content": "I am happy to open a github issue if you'd like"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:24:06.817000+00:00",
                "content": "Then I'll update my twitter to be an #OSSContributor"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:24:17.421000+00:00",
                "content": "lol yes ðŸ˜‰ feel free to do so"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:24:41.005000+00:00",
                "content": "its two separate issues,:\n1. better visibility in the playground about hte parser\n2. adding dynamic types to tests"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-22 17:24:51.056000+00:00",
                "content": "the issue i linked is related to 1"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:25:54.947000+00:00",
                "content": "https://github.com/BoundaryML/baml/issues/711"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-06-22 17:25:56.490000+00:00",
                "content": "cool"
            }
        ]
    },
    {
        "thread_id": 1254630701098340434,
        "thread_name": "Unspecified error code: 2",
        "messages": [
            {
                "author": "anaygupta2004",
                "timestamp": "2024-06-24 02:54:29.053000+00:00",
                "content": "Unspecified error code: 2\nerror sending request: JsValue(TypeError: Failed to fetch\nTypeError: Failed to fetch\n    at __wbg_fetch_bc7c8e27076a5c84 (https://file+.vscode-resource.vscode-cdn.net/Users/anaygupta/.vscode/extensions/boundary.baml-extension-0.41.1/web-panel/dist/assets/baml_schema_build.js:2110:17)\n    at reqwest::wasm::client::js_fetch::h03b1f10135eadd3a (wasm://wasm/0184b172:wasm-function[4721]:0x44eb6c)\n    at reqwest::wasm::client::fetch::{{closure}}::h03ff948f525e2e4c (wasm://wasm/0184b172:wasm-function[276]:0x11c7a5)\n    at baml_runtime::internal::llm_client::orchestrator::stream::orchestrate_stream::{{closure}}::hbcba85f0330e39e2 (wasm://wasm/0184b172:wasm-function[137]:0x48d69)\n    at wasm_bindgen_futures::future_to_promise::{{closure}}::{{closure}}::h76d4d7eb80aef6b1 (wasm://wasm/0184b172:wasm-function[147]:0x6b6cf)\n    at wasm_bindgen_futures::queue::Queue:ðŸ†•:{{closure}}::hbf0d8c47c43a70e4 (wasm://wasm/0184b172:wasm-function[2293]:0x3889ce)\n    at <dyn core::ops::function::FnMut<(A,)>+Output = R as wasm_bindgen::closure::WasmClosure>::describe::invoke::hce4ea887f3b31f4c (wasm://wasm/0184b172:wasm-function[6891]:0x4829d3)\n    at __wbg_adapter_52 (https://file+.vscode-resource.vscode-cdn.net/Users/anaygupta/.vscode/extensions/boundary.baml-extension-0.41.1/web-panel/dist/assets/baml_schema_build.js:253:12)\n    at real (https://file+.vscode-resource.vscode-cdn.net/Users/anaygupta/.vscode/extensions/boundary.baml-extension-0.41.1/web-panel/dist/assets/baml_schema_build.js:232:16))\n\nCheck the webview network tab for more details. Command Palette -> Open webview developer tools."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:55:24.016000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-06-24 02:55:24.464000+00:00",
                "content": "oh yea.. we get this error if you haven't set up env variables someimte!\n\nDid you do this:\nhttps://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#setting-env-variables\n\nAlso general debugging tips for the VSCode playground:\nhttps://docs.boundaryml.com/docs/get-started/debugging/vscode-playground"
            }
        ]
    },
    {
        "thread_id": 1258100572364607558,
        "thread_name": "baml version issue",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:42:30.830000+00:00",
                "content": "```ERROR: Could not find a version that satisfies the requirement baml-py==0.46.0 (from versions: 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 0.30.0, 0.30.2, 0.30.3, 0.30.4, 0.31.0, 0.32.0, 0.32.1, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, 0.36.0, 0.37.0, 0.38.0, 0.39.0, 0.40.0, 0.41.0, 0.43.0, 0.44.0)```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:48:42.340000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:48:42.705000+00:00",
                "content": "Hmm we published 0.46.0 here https://pypi.org/project/baml-py/ -- do you use poetry?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:48:56.907000+00:00",
                "content": "yes"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:48:59.649000+00:00",
                "content": "poetry was fine"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:49:11.846000+00:00",
                "content": "it was in the container with the pip install command that it couldn't find the proper release"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:49:23.721000+00:00",
                "content": "`RUN pip3 install --no-cache-dir -r requirements.txt`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:49:47.636000+00:00",
                "content": "can you add an echo \"hello\" at the top of the dockerfile temporarily? It probably cached the previous layers"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:50:03.908000+00:00",
                "content": "so it means it doesnt have knowledge of baml-py"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:57:06.456000+00:00",
                "content": "```\n2.126 ERROR: Could not find a version that satisfies the requirement baml-py==0.46.0 (from versions: 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 0.30.0, 0.30.2, 0.30.3, 0.30.4, 0.31.0, 0.32.0, 0.32.1, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, 0.36.0, 0.37.0, 0.38.0, 0.39.0, 0.40.0, 0.41.0, 0.43.0, 0.44.0)\n2.126 ERROR: No matching distribution found for baml-py==0.46.0\n2.239\n2.239 [notice] A new release of pip is available: 23.0.1 -> 24.1.1\n2.239 [notice] To update, run: pip install --upgrade pip\n------\nfailed to solve: process \"/bin/bash -o pipefail -c pip3 install --no-cache-dir -r requirements.txt\" did not complete successfully: exit code: 1\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:57:19.977000+00:00",
                "content": "threw the `RUN echo \"hello\"` at the top of Dockerfile"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:57:38.413000+00:00",
                "content": "hmm ok one sec"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:03:18.690000+00:00",
                "content": "can you try running pip3 cache purge in that dockerfile before installing"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:05:13.495000+00:00",
                "content": "no dice"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:14:36.605000+00:00",
                "content": "<@711679663746842796>  mind assisting here?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:15:44.403000+00:00",
                "content": "looking- it's strange that i don't see 0.45.0 in the list of available versions either"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:16:19.722000+00:00",
                "content": "but poetry seems to work?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:16:36.638000+00:00",
                "content": "0.46.0 btw"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:17:27.272000+00:00",
                "content": "yes, but when `pip` fails (above), not only does it miss 0.46.0 but it doesn't find 0.45.0"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:21:14.460000+00:00",
                "content": "hmm"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:21:21.776000+00:00",
                "content": "`pip` and `poetry` both work for me"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:21:45.058000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:22:05.766000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:22:24.255000+00:00",
                "content": "<@1049713528170364968> can you post your full dockerfile (if theres no sensitive info)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:23:07.696000+00:00",
                "content": "```\n# Use a smaller base image\nFROM python:3.10-slim AS builder\n\nWORKDIR /app\n\nENV PYTHONPATH=\"/app:${PYTHONPATH}\"\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n# Combine RUN commands to reduce the number of layers\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends apt-transport-https ca-certificates curl gnupg tesseract-ocr tesseract-ocr-eng libtesseract-dev poppler-utils libmagic-dev ffmpeg libsm6 libxext6 gcc python3-dev build-essential && \\\n    curl -sLf --retry 3 --tlsv1.2 --proto \"=https\" 'https://packages.doppler.com/public/cli/gpg.DE2A7741A397C129.key' | gpg --dearmor -o /usr/share/keyrings/doppler-archive-keyring.gpg && \\\n    echo \"deb [signed-by=/usr/share/keyrings/doppler-archive-keyring.gpg] https://packages.doppler.com/public/cli/deb/debian any-version main\" | tee /etc/apt/sources.list.d/doppler-cli.list && \\\n    apt-get update && \\\n    apt-get -y --no-install-recommends install doppler && \\\n    rm -rf /var/lib/apt/lists/*  # Clean up in the same layer\n\nCOPY requirements.txt .\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n\n# This auto generates the baml_client package\nCOPY baml_src/ /app/baml_src/\nRUN rm -rf /app/baml_src/__tests__\nRUN baml-cli generate\n\nCOPY runner.py .\nCOPY prefect_deployment.py .\nCOPY api_startup.sh /app/api_startup.sh\nCOPY celery_startup.sh /app/celery_startup.sh\nCOPY celery_startup_imports.sh /app/celery_startup_imports.sh\nCOPY celery_startup_integrations.sh /app/celery_startup_integrations.sh\nCOPY celery_startup_rfp.sh /app/celery_startup_rfp.sh\n\n# Copy the rest of the application files into src/ directory\nCOPY src/ ./src/\nCMD [\"/bin/bash\", \"/app/api_startup.sh\"]\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:24:26.490000+00:00",
                "content": "Also gonna restart docker since historically it has given me many headaches"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:25:10.802000+00:00",
                "content": "try this before the pip3 install:\n`RUN pip install --upgrade pip`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:25:16.674000+00:00",
                "content": "with pip3"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:25:53.791000+00:00",
                "content": "`pip3 install --upgrade pip`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:26:06.378000+00:00",
                "content": "ive run into this same issue btw -- i believe this is how i fixed it"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:26:43.089000+00:00",
                "content": "ðŸ˜¦"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:26:45.668000+00:00",
                "content": "no dice"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:26:55.345000+00:00",
                "content": "ok, i have the repro"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:28:08.838000+00:00",
                "content": "there's definitely something stupid we're all missing - maybe a pip registry thing somewhere"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:33:23.493000+00:00",
                "content": "it's likely b/c we are missing the arm64 linux build (will debug this with sam)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 17:36:16.247000+00:00",
                "content": "yep, i just figured that out too lol"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 18:22:04.641000+00:00",
                "content": "status update: i've figured out why our arm64-linux build was broken and am chanting the necessary github-workflow-cross-compilation-shell-commands to make it work. the requisite sacrifice of cpu time is (hopefully) paid."
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:24:41.358000+00:00",
                "content": "https://tenor.com/view/wizard-elmo-elmo-fire-fire-chaos-chaos-fire-gif-26561992"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 21:41:19.477000+00:00",
                "content": "you should be good to go now!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-03 21:42:12.673000+00:00",
                "content": ""
            }
        ]
    },
    {
        "thread_id": 1258101107805392896,
        "thread_name": "stream_options",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:44:38.489000+00:00",
                "content": "Also:\n```\nRaw LLM Response:\nUnspecified error code: 400\nRequest failed: {\n  \"error\": {\n    \"message\": \"Unrecognized request argument supplied: stream_options\",\n    \"type\": \"invalid_request_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n\n\nCheck the webview network tab for more details. Command Palette -> Open webview developer tools. \n```\n\nSeeing this in playground? Don't see anywhere that I'm setting a `stream_options` parameter? This is while running a test in the playground"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:51:22.015000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:51:22.302000+00:00",
                "content": "which client are you using? barebones openai?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:52:05.516000+00:00",
                "content": "```\nclient<llm> QualityLargeOutput {\n  provider baml-fallback\n  options {\n    strategy [\n      Claude35SonnetLargeTokenOutput,\n      Gemini15Pro,\n      AzureGPT4TURBO\n    ]\n  }\n}\n```\n\n```\nclient<llm> Claude35SonnetLargeTokenOutput {\n  provider anthropic\n  retry_policy ZenfetchDefaultPolicy\n  options {\n    model claude-3-5-sonnet-20240620\n    max_tokens 20000\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:52:50.279000+00:00",
                "content": "ok one sec"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:53:27.222000+00:00",
                "content": "`http://localhost:56701/chat/completions?api-version=2024-03-01-preview`"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:53:32.162000+00:00",
                "content": "seeing localhost get hit? is this expected"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:53:36.040000+00:00",
                "content": "yep"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:54:02.892000+00:00",
                "content": "the extension starts a proxy server since anthropic calls dont work on the browser (vscode webview)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:54:04.648000+00:00",
                "content": "wait did they restrict claude 3-5 sonnet to 4096 tokens?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:54:20.688000+00:00",
                "content": "the stream_options looks like an error on our end"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:54:34.511000+00:00",
                "content": "```\n{\n    \"type\": \"error\",\n    \"error\": {\n        \"type\": \"invalid_request_error\",\n        \"message\": \"max_tokens: 20000 > 4096, which is the maximum allowed number of output tokens for claude-3-5-sonnet-20240620\"\n    }\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:55:22.778000+00:00",
                "content": "the max output has always been 5096"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:55:24.723000+00:00",
                "content": "4096"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:55:33.193000+00:00",
                "content": "it's the input that's like 200k"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:55:59.475000+00:00",
                "content": "yea I guess the silly part is I put max tokens as 20k lol"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 16:56:27.412000+00:00",
                "content": "yep that fixed it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:56:37.671000+00:00",
                "content": "it was probably falling back"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:56:57.156000+00:00",
                "content": "but it means we still have an issue -- does Gemini15Pro work for you if you use it by itself?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 16:57:16.913000+00:00",
                "content": "my guess is that it fell back 2 levels to azuregpt4, and we have an issue with streaming that client"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:01:05.174000+00:00",
                "content": "gotcha, I haven't tried gemini15pro on it's own recently, I can give it a go though"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:01:17.036000+00:00",
                "content": "also does the test output get propagated to boundary dashboard?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:02:20.795000+00:00",
                "content": "yeah if you have your BOUNDARY_PROJECT_ID and BOUNDARY_SECRET set in the environment vars in the playground"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:03:02.715000+00:00",
                "content": "When I click on the `Env Vars` button, nothing happpens ðŸ˜¦"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:03:28.094000+00:00",
                "content": "omg sorry about that"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:03:34.354000+00:00",
                "content": "ill patch it"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:05:19.688000+00:00",
                "content": "thank!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:41:12.922000+00:00",
                "content": "will publish the extension fix as soon as Azure lets me (currently down): https://status.dev.azure.com/_event/519117906"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:43:30.504000+00:00",
                "content": "not a good day for the devs today"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 17:43:34.531000+00:00",
                "content": "Apollo io was down earlier"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 17:43:44.413000+00:00",
                "content": "haha everything is suddenly broken today"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 18:08:29.759000+00:00",
                "content": "fixed the env vars button in 0.46.1. the playground it a bit wonky when starting up (you have to type in a baml file for it to update the very first time) so ew'll be fixing that soon as well"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:17:30.377000+00:00",
                "content": "dope"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:17:32.217000+00:00",
                "content": "thank"
            }
        ]
    },
    {
        "thread_id": 1258132317961195745,
        "thread_name": "json files",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:48:39.570000+00:00",
                "content": "Seems like I can't place a json file in my `baml_src`, is that intentional?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 18:49:08.450000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 18:49:08.756000+00:00",
                "content": "does it error out? We probably just need to ignore them"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:49:36.210000+00:00",
                "content": "Error validating: A BAML test file must be in a `__tests__` directory., 0, 0, 0, 0\ndiagnostics length: 47"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 18:49:52.300000+00:00",
                "content": "Yea. What kind do json file are you trying to add?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:50:17.097000+00:00",
                "content": "completely unrelated to baml, I just wanna have a json file for the purposes of storing the data my baml function generated in the test case"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 18:52:05.117000+00:00",
                "content": "Got it. Yea we should just ignore them!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 18:56:50.982000+00:00",
                "content": "also a nit but it's kinda annoying that everytime there's an issue with baml, when I save, it automatically opens the Ouptut pane"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 19:00:52.424000+00:00",
                "content": "Hmm ill take a look at that"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-05 18:33:52.989000+00:00",
                "content": "we fixed this issue i believe -- if you have 0.47.0 as vscode extension (the latest one). LMK if you do have that version installed"
            }
        ]
    },
    {
        "thread_id": 1258139770404737044,
        "thread_name": "Test hanging",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 19:18:16.371000+00:00",
                "content": "I'm also finding sometimes that the playground tests hang... not sure if the response got cut prematurely or something?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 19:22:36.412000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 19:22:36.852000+00:00",
                "content": "Yeah so the response shows up in the web developer panel network logs but somehow we dont render it. Definitely a known issue with our web assembly build"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 19:22:59.897000+00:00",
                "content": "so i can access it in web developer panel network?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 19:25:25.782000+00:00",
                "content": "Yep the response shoild be there when it hangs"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 19:25:38.780000+00:00",
                "content": "How often does it happen for you?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-03 19:25:49.473000+00:00",
                "content": "every other request"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-03 19:26:02.878000+00:00",
                "content": "Ooof ok we ll prioritize it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 19:26:50.377000+00:00",
                "content": "Does the curl request work reliably?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-03 19:27:19.592000+00:00",
                "content": "As in if you curl it does it ever have network issues? (Thereâ€™s a raw curl toggle in the playground)"
            }
        ]
    },
    {
        "thread_id": 1258513096029241367,
        "thread_name": "Potential Parsing issue",
        "messages": [
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:01:44.136000+00:00",
                "content": "I am noticing that in some cases, the raw LLM response is correct but the parsed one is wrong.  In this case, the parsed output has correct extracted values removed for no reason. Any idea what might be happening?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:02:39.554000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:02:40.012000+00:00",
                "content": "Mind posting an example?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:03:12.815000+00:00",
                "content": "Or a screenshot"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:03:59.662000+00:00",
                "content": ""
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:04:38.502000+00:00",
                "content": "actors is empty in the parsed response"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:04:49.173000+00:00",
                "content": "Whats your schema?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:05:53.357000+00:00",
                "content": "Can you make sure your vscode extension version is 0.47 or up?"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:06:39.865000+00:00",
                "content": "it is 0.47"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:07:10.952000+00:00",
                "content": "how do I make code blocks on discord?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:07:34.326000+00:00",
                "content": "Add three backtics:`"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:07:48.874000+00:00",
                "content": "```\nclass ActorSubject {\n  actors Actor[] @description(#\"\n      Actors of query, these can be people, political parties, geographic regions, etc.\n  \"#)\n  subject string[] @description(\"Subject of query, thing being acted on\")\n  dates string[] @description(#\"\n    Convert any dates detected to year-month-day format.\n    If month and day are not specified, default to year-01-01.\n    If the dates being referred to are after a specified date, e.g., \"since Feb 2021\", return as \">2021-02-01\", where as \"before Feb 2021\" you would return as \"<2021-02-01\"\n    If a date range is specified, return in this scheme \"year-month-day - year-month-day\", e.g., \"between July 2021 and January 2023\", would be \"2021-07-01 - 2023-01-01\".\n    Return each date or date range detected.\n  \"#)\n }\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:08:14.843000+00:00",
                "content": "Mind also posting the schema of Actor?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:08:33.174000+00:00",
                "content": "Will take a look at this ASAP"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:08:40.279000+00:00",
                "content": "```class Actor {\n  person string @description(#\"\n      Name of a person. If the name is ONLY initials, such as \"MP\", return nothing.\n      If \"MP\" comes before a name, return only the name. For example for \"MP Starmer\" return \"Starmer\".\n  \"#)\n  party Party @description(#\"\n      British political party.\n  \"#)\n  region Region @description(#\"\n      Region of the UK the actor is located. This should be included in wheen the region is the actor, or directly linked to the actor.\n      Examples: \n      \"What do MPs in Cambridge think about the government's house building policy?\", here Region is \"East of England\" as this is the place the MPs are located. \n      \"What does Labour think about Tory policies in Cambridge?\", here Region should not be returned, since here the subject is about Tory policies in Cambridge, and Region for the actor Labour is not specified.\n  \"#)\n  gender Gender\n}```"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:09:51.894000+00:00",
                "content": "```class Party {\n  name string @description(#\"\n    Name as in query\n  \"#)\n  official PartyOfficial\n}\n\nenum PartyOfficial {\n  Labour\n  Conservative\n  Liberal_Democrat\n  Green_Party\n  Reform_Party\n  Labour_Co_op\n  Social_Democratic\n  Independent\n  Scottish_National_Party\n}\n\nenum Region {\n  England\n  London\n  North_East\n  North_West\n  Yorkshire\n  East_Midlands\n  West_Midlands\n  South_East\n  East_of_England\n  South_West\n  Scotland\n  Wales\n  Northern_Ireland\n}```"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:10:48.695000+00:00",
                "content": "This was the query \"What did the Tories say about Brexit between December 2015 and Feb 2016?\""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:11:11.653000+00:00",
                "content": "Thanks we will look at this now, parsing issues are our top priority"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 20:11:32.600000+00:00",
                "content": "Fantastic, thanks!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 20:27:42.342000+00:00",
                "content": "We dug into it, and it turns out that this parsing is technically correct, but we don't surface enough information about what's happening for the user (you) to understand _why_ we're tossing it away, and we definitely need to work on that"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 20:27:53.256000+00:00",
                "content": "First, real quick, here's a schema that seems to work for me:"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 20:29:55.593000+00:00",
                "content": "```\n\nclass Actor {\n  person string? @description(#\"\n      Name of a person. If the name is ONLY initials, such as \"MP\", return nothing.\n      If \"MP\" comes before a name, return only the name. For example for \"MP Starmer\" return \"Starmer\".\n  \"#)\n  party Party @description(#\"\n      British political party.\n  \"#)\n  region Region? @description(#\"\n      Region of the UK the actor is located. This should be included in wheen the region is the actor, or directly linked to the actor.\n      Examples: \n      \"What do MPs in Cambridge think about the government's house building policy?\", here Region is \"East of England\" as this is the place the MPs are located. \n      \"What does Labour think about Tory policies in Cambridge?\", here Region should not be returned, since here the subject is about Tory policies in Cambridge, and Region for the actor Labour is not specified.\n  \"#)\n  gender Gender?\n}\nclass Party {\n  name string @description(#\"\n    Name as in query\n  \"#)\n  official PartyOfficial?\n}\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 20:30:37.871000+00:00",
                "content": "the changes specifically were making `person`, `region`, `gender`, and `official` optional"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-04 20:32:21.735000+00:00",
                "content": "Potential Parsing issue"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 20:36:45.978000+00:00",
                "content": "Explanation:\n\nWhen we parse the LLM's response, using the type contract you've defined, we treat that type contract as _strict_ for the final parse- if any fields are missing or unavailable in an object, that rolls up all the way and results in us considering the object unparseable.\n\nSo for example here, because `actors[0].party.official`  isn't a valid `PartyOfficial` value, when we try to parse the thing at `actors[0]`, our parser decides that `actors[0]` is unparseable and drops it\n\nWhile streaming the LLM response back, though, we parse everything as `Partial<ActorSubject>`, because while streaming a response, we can only show fields if we allow fields in `ActorSubject` to be (recursively) missing"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-04 20:37:27.446000+00:00",
                "content": "Let me know if this makes sense or not- it's a weird thing to wrap your head around and we're still working on making this more intuitive"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-04 21:30:22.365000+00:00",
                "content": "@sam That was very helpful. Thanks for looking into it!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-23 17:48:59.425000+00:00",
                "content": "<@401078332751478784>  have you been able to use BAML for your use-case? Did anything go well, or bad? Any feedback helps!"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-23 18:37:21.287000+00:00",
                "content": "<@201399017161097216> it went quite well! the recent update allowing dynamic typing was very useful, my only grips are with the vs code extension autogenerating the baml_client directory in the wrong location, but it looks like that was fixed in the last update! as I look into more advanced applications I am sure I'll have more feedback to give."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-23 18:38:06.267000+00:00",
                "content": "sweet, you can always change the location of the generation with the `output_dir` config in the `generator` block as well. Thanks for the feedback"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-23 18:38:34.702000+00:00",
                "content": "yeah baml-cli generate worked correctly, it was the auto generate on save that wasn't"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-23 18:38:49.791000+00:00",
                "content": "ah gotcha, do you use windows or macos?"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-23 18:38:55.318000+00:00",
                "content": "windows"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-23 18:39:02.596000+00:00",
                "content": "ok cool we'll look into it, thanks!"
            }
        ]
    },
    {
        "thread_id": 1259810992750333993,
        "thread_name": "I am trying to setup an azure-openai",
        "messages": [
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 09:59:06.828000+00:00",
                "content": "I am trying to setup an azure-openai endpoint as a BAML client.\n```baml\nclient<llm> GPT4o {\n  provider azure-openai\n  options {\n    base_url \"https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o\"\n    api-version \"2024-02-15-preview\"\n    api_key env.AZURE_OPENAI_KEY\n  }\n}\n```\nThere is an issue. BAML generated curl looks like:\n```sh\ncurl -X POST 'https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o/chat/completions' -H \"api-key: ***\" -H \"content-type: application/json\" -d \"{\n  \\\"max_tokens\\\": 4096,\n  \\\"api-version\\\": \\\"2024-02-15-preview\\\",\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": \\\"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string,\\n  experience: string[],\\n  skills: string[],\\n}\\\"\n    }\n  ]\n}\"\n```\nThis throws a 404. Changing it to (notice the shift in api-version placement)\n```sh\ncurl -X POST 'https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview' -H \"api-key: ***\" -H \"content-type: application/json\" -d \"{\n  \\\"max_tokens\\\": 4096,\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": \\\"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string,\\n  experience: string[],\\n  skills: string[],\\n}\\\"\n    }\n  ]\n}\"\n```\nnow returns a 200 with the correct data."
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 10:06:39.988000+00:00",
                "content": ""
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 10:07:55.282000+00:00",
                "content": "Aditionally if I change `api-version \"2024-02-15-preview\" -> api_version \"2024-02-15-preview\"` in my BAML file and try again, I get:\n\n```sh\ncurl -X POST 'https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o/chat/completions' -H \"api-key: ***\" -H \"content-type: application/json\" -d \"{\n  \\\"max_tokens\\\": 4096,\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": \\\"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string,\\n  experience: string[],\\n  skills: string[],\\n}\\\"\n    }\n  ]\n}\"\n```\nWhich returns 400."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 13:04:03.922000+00:00",
                "content": "Youâ€™ll need to use the underscore variant! https://docs.boundaryml.com/docs/snippets/clients/providers/azure Iâ€™ll work on adding the - version as well"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 13:04:21.919000+00:00",
                "content": "Let me know if youâ€™re able to get it working with that!"
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 14:34:28.646000+00:00",
                "content": "With the udnerscore variant\n```\nclient<llm> GPT4o {\n  provider azure-openai\n  options {\n    // base_url \"https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o\"\n    resource_name \"avayl-se2\"\n    deployment_id \"gpt-4o\"\n    api_version \"2024-02-15-preview\"\n    api_key env.AZURE_OPENAI_KEY\n  }\n}\n```\nI get \n```\nUnspecified error code: 400\nRequest failed: {\n  \"error\": {\n    \"message\": \"Unknown parameter: 'stream_options'.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"stream_options\",\n    \"code\": \"unknown_parameter\"\n  }\n}\n\nCheck the webview network tab for more details. Command Palette -> Open webview developer tools.\n```"
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 14:36:57.095000+00:00",
                "content": "What's weird is when I do the api_version (as suggested in the docs) the raw curl looks like. There is no api-version anywhere in there:\n```\ncurl -X POST 'https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o/chat/completions' -H \"api-key: ***\" -H \"content-type: application/json\" -d \"{\n  \\\"max_tokens\\\": 4096,\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": \\\"...\\\"\n    }\n  ]\n}\"\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:39:04.871000+00:00",
                "content": "oh interesting, it seems that there are 2 bugs here:\n\n1. the curl doesn't match what we actually do\n2. azure looks to have changed how streaming works\n\n<@417144266163224577> can take on #1 and we'll patch that in ASAP\nFor 2, I'll debug and find out what happened! with the params.\n\nCan you try the curl request with `https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:40:49.733000+00:00",
                "content": "oh that isn't the bug, the issue is the azure API doesn't support all the parameters of the OpenAI spec as they say the do ðŸ˜¦ \n\nI'll get a patch for this out within a 15 mins!"
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 14:41:34.649000+00:00",
                "content": "with the `https://avayl-se2.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview` it works fine"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:55:19.469000+00:00",
                "content": "This fixes it!\n\nhttps://github.com/BoundaryML/baml/pull/760"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:59:39.365000+00:00",
                "content": "waiting on some checks to pass, then will issue the release (looks like about 1 hour sadly, i forgot we hadn't updated our release flow)"
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 15:06:12.354000+00:00",
                "content": "No worries, thanks for the effort"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 18:38:08.167000+00:00",
                "content": "this is now patched in 0.49! See https://discord.com/channels/1119368998161752075/1119375433666920530/1259941306747326616"
            },
            {
                "author": "anish.pi",
                "timestamp": "2024-07-08 19:30:23.743000+00:00",
                "content": "the cURL rendering is fixed! Issue was related to how we rendered query parameters, which is unique to Azure"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 19:58:05.479000+00:00",
                "content": "we'll ship this soon as well!"
            }
        ]
    },
    {
        "thread_id": 1259822967475671041,
        "thread_name": "As a solution I thought about using",
        "messages": [
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 10:46:41.825000+00:00",
                "content": "As a solution I thought about using litellm as a compatability layer. I used one of the models we have set up. For some reason baml cuts the response short.\n```sh\ncurl -X POST 'http://0.0.0.0:4000/chat/completions' -H \"authorization: Bearer ***\" -H \"content-type: application/json\" -d \"{\n  \\\"model\\\": \\\"gemini-1.5-pro-001\\\",\n  \\\"stream\\\": \\\"false\\\",\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": \\\"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string,\\n  experience: string[],\\n  skills: string[],\\n}\\\"\n    }\n  ]\n}\"\n```\nGives back\n```\n\n{\"id\":\"chatcmpl-04f898b3-4f90-41f1-8b2e-fc1522f8a8be\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"message\":{\"content\":\"`|``json\\n{\\n  \\\"name\\\": \\\"Vaibhav Gupta\\\",\\n  \\\"email\\\": \\\"vbv@boundaryml.com\\\",\\n  \\\"experience\\\": [\\n    \\\"Founder at BoundaryML\\\",\\n    \\\"CV Engineer at Google\\\",\\n    \\\"CV Engineer at Microsoft\\\"\\n  ],\\n  \\\"skills\\\": [\\n    \\\"Rust\\\",\\n    \\\"C++\\\"\\n  ]\\n}\\n\\``|`\",\"role\":\"assistant\"}}],\"created\":1720435274,\"model\":\"gemini-1.5-pro-001\",\"object\":\"chat.completion\",\"system_fingerprint\":null,\"usage\":{\"prompt_tokens\":85,\"completion_tokens\":86,\"total_tokens\":171}}\n\n```\n\n(added vertical lines for formatting)."
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 10:57:44.898000+00:00",
                "content": ""
            },
            {
                "author": "foxicution",
                "timestamp": "2024-07-08 10:57:45.299000+00:00",
                "content": "Sems like an issue with \\```json\\``` that gemini likes to add. Adding \"Avoid adding \\```json\\```\" fixes the issue, but might be a good case for baml to handle."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 13:05:20.418000+00:00",
                "content": "Itâ€™s due to max tokens. Azure by default sets it to 16 (ridiculously small)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 13:06:13.847000+00:00",
                "content": "Youâ€™ll need to manually configure that. We configure azure for you by default to default higher than 16, but most others donâ€™t"
            }
        ]
    },
    {
        "thread_id": 1259875502013812746,
        "thread_name": "Generator bug with None",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 14:15:27.035000+00:00",
                "content": "Have you guys seen this? \n```\ncelery_worker_rfp-1           |   File \"/app/baml_client/partial_types.py\", line 21, in <module>\ncelery_worker_rfp-1           |     from . import types\ncelery_worker_rfp-1           |   File \"/app/baml_client/types.py\", line 59\ncelery_worker_rfp-1           |     None = \"None\"\ncelery_worker_rfp-1           |     ^^^^\ncelery_worker_rfp-1           | SyntaxError: cannot assign to Non\n```\n\nI'm on baml-py 0.48.0"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:16:08.913000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:16:09.455000+00:00",
                "content": "this looks like a generator bug! \n\nCan you share the baml class def?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 14:16:24.850000+00:00",
                "content": "idk which class is failing"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:16:30.040000+00:00",
                "content": "Generator bug with None"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:17:19.847000+00:00",
                "content": "can you paste the lines around line 58 in types.py? If you don't have access, no worries, we should be able to create a ticket for this easily"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:17:30.874000+00:00",
                "content": "should be patched this AM!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 14:17:55.814000+00:00",
                "content": "found it"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 14:18:02.169000+00:00",
                "content": "```rust\nenum SecurityClearance {\n    None\n    Confidential\n    Secret\n    TopSecret\n    TopSecretSCI\n}\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 14:18:23.788000+00:00",
                "content": "```python\nclass SecurityClearance(str, Enum):\n    \n    None = \"None\"\n    Confidential = \"Confidential\"\n    Secret = \"Secret\"\n    TopSecret = \"TopSecret\"\n    TopSecretSCI = \"TopSecretSCI\"\n```\n\nI guess `None` is a reserved word"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:18:28.724000+00:00",
                "content": "oh, its cause you can't name something None due to Python naming!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:18:44.811000+00:00",
                "content": "yep, it would work if you generated TS code, but not when generating Python"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 14:18:58.853000+00:00",
                "content": "ye that fixed it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:19:13.900000+00:00",
                "content": "Better ticket here is for us to generate different reserved keyword list based on what langauge you're generating!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:19:28.321000+00:00",
                "content": "I'll note that down, thanks for flagging!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 14:21:19.962000+00:00",
                "content": "Issue filed: https://github.com/BoundaryML/baml/issues/759"
            }
        ]
    },
    {
        "thread_id": 1259932474268585984,
        "thread_name": "Hi guys,",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 18:01:50.279000+00:00",
                "content": "Hi guys, \nI'm not able to download my inputs as test cases from the baml dashboard. Getting \n```\nzsh: parse error near `}' \n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 18:02:36.362000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 18:02:36.713000+00:00",
                "content": "can you hop on office hours and share screen here?"
            }
        ]
    },
    {
        "thread_id": 1259934324690522152,
        "thread_name": "Diagnose",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 18:09:11.454000+00:00",
                "content": "Is the diagnose feature still a thing?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 18:11:22.237000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 18:11:22.696000+00:00",
                "content": "it should be! It looks like its broken due to some updates we did to the prompt! <@201399017161097216> we should patch this EOD or tmrw."
            }
        ]
    },
    {
        "thread_id": 1259945072020164619,
        "thread_name": "Error rendering Create-test icon",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 18:51:53.817000+00:00",
                "content": "Error rendering Create-test icon\nCan't download my inputs as a test in the dashboard"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 19:58:19.057000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 19:58:19.529000+00:00",
                "content": "<@201399017161097216> !"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 19:58:43.252000+00:00",
                "content": "will take this on ASAP"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 22:24:42.907000+00:00",
                "content": "It was working just a few hours ago ðŸ˜¿"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 22:25:17.524000+00:00",
                "content": "can you check the beta site?\n\n(just add  `beta.` to the URL prefix)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-08 22:26:19.813000+00:00",
                "content": "nope ðŸ˜¦"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 22:26:55.037000+00:00",
                "content": "yea, its most likely just a bug with our rendering on the site for a complex data structure. Aaron should be able to get to it ASAP!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:38:42.388000+00:00",
                "content": "I patched a bug on this (happened when an input key was null). The change is out now"
            }
        ]
    },
    {
        "thread_id": null,
        "thread_name": null,
        "messages": [
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 19:58:19.057000+00:00",
                "content": "Error rendering Create-test icon"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-12 18:43:29.855000+00:00",
                "content": "<@733001033340551218> when you get a chance, can you share the deserialization error you ran into?"
            },
            {
                "author": "etbyrd",
                "timestamp": "2024-07-12 18:47:44.840000+00:00",
                "content": "Oh yeah absolutely - I'll dm you"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:03:02.995000+00:00",
                "content": "I am setting an environmental variable"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 22:56:34.935000+00:00",
                "content": "Here's my code:\n```\nclientRegistry.addLlmClient('EG_AI_Endpoint', 'openai', {\n    base_url: \"http://localhost:8080\",\n    model: 'gpt-4o',\n    api_key: 'skipped_because_we_are_using_custom_proxy_endpoint',\n    headers: {\n      \"Session-Token\": session_token\n    }\n  });\n  clientRegistry.setPrimary('EG_AI_Endpoint');\n```\n\"model\" is there, but I get an error that it's not. Womp womp."
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 22:56:40.551000+00:00",
                "content": "(Testing locally)"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 22:57:37.244000+00:00",
                "content": "```â¨¯ Error: LLM call failed: LLMErrorResponse { client: \"EG_AI_Endpoint\", model: None, prompt: Chat([RenderedChatMessage { role: \"system\",  ...```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:21:18.816000+00:00",
                "content": "Working on an OCR pipeline with GPT4o,"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 04:02:13.687000+00:00",
                "content": "existing classes on dynamic types"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 18:26:22.531000+00:00",
                "content": "return type string"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:27:43.476000+00:00",
                "content": "Happy to provide more from console logs if helpful"
            },
            {
                "author": "roze_sha",
                "timestamp": "2024-09-05 09:06:40.782000+00:00",
                "content": "Hi, Is the openai-generic provider deprecated?"
            },
            {
                "author": "roze_sha",
                "timestamp": "2024-09-05 09:08:20.758000+00:00",
                "content": "this is my client\n```ts\nclient<llm> Groq {\n  provider openai-generic\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.OPENAI_API_KEY\n    model \"mixtral-8x7b-32768\"\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:38:24.528000+00:00",
                "content": "openai-generic"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:02:07.868000+00:00",
                "content": "is there a way to set the default"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:58:35.629000+00:00",
                "content": "For some reason baml doesn't want to"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 07:24:02.017000+00:00",
                "content": "Have people been able to use Groq?"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 07:25:03.597000+00:00",
                "content": "```\nmessage: \"Request failed: {\\\"error\\\":{\\\"message\\\":\\\"Invalid API Key\\\",\\\"type\\\":\\\"invalid_request_error\\\",\\\"code\\\":\\\"invalid_api_key\\\"}}\\n\", code: InvalidAuthentication ...\n```\ndespite in the logs I can see my GROQ API Key: \"api_key\": String(\"gsk...\")"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:34:39.841000+00:00",
                "content": "Groq"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 19:12:29.747000+00:00",
                "content": "cache control validation"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-20 08:47:07.255000+00:00",
                "content": "Hello, I was wondering if an improvement in configurability / strategy for retry is something in progress? I generate graphs thanks ot BAML  but sometimes the links are not generated and I would like some ways to invalidate the model and a retry ? also I tried to use the BAML_LOG=info (in my .env file) but failed seeing some logs, maybe I'm doing wrong..."
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 08:49:35.679000+00:00",
                "content": "I'm using a simple code and prompt for that and works, the logs you need to set:\n\nBAML_LOG=info\nBOUNDARY_PROJECT_ID=test\nBOUNDARY_SECRET=test"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 08:49:49.839000+00:00",
                "content": "they said in other channel that is due to a bug(the need to set the variables boundary_project and secret, you jsut need to put a random string an works)"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-20 08:55:16.299000+00:00",
                "content": "Actually I created an account and used a real project_id and secret and have my logs, THANK YOU"
            },
            {
                "author": "andrewcka",
                "timestamp": "2024-09-20 09:00:45.059000+00:00",
                "content": "Regarding the retrying you can catch it with BamlValidationError, and then send it back to the prompt using a template, for example:\n\n{% if fail %}\n\nYou encountered the following error while attempting this task previously:\n\n{{ failure }}\n\nBefore solving the error, let's analyze why it occurred, plan how to solve it within  <thinking> xml tags and then immediately solve it.\n\n{% endif %}"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 14:41:30.063000+00:00",
                "content": "We should put this on our docs somewhere..."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-20 20:42:29.931000+00:00",
                "content": "We are about to fix this"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 20:42:59.687000+00:00",
                "content": "Is this anthropic specific"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 20:43:16.760000+00:00",
                "content": "yep! see thread"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 20:43:31.460000+00:00",
                "content": "yeah just saw that, I appreciate the quick response!"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 20:43:37.989000+00:00",
                "content": "ðŸš€"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 22:17:51.497000+00:00",
                "content": "`\n  File \"/Users/devk/Developer/instigate.ai/backend/src/chatFunctions/instigate.py\", line 7, in instigate\n    response = b.ExtractInstigation(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/devk/Developer/instigate.ai/backend/src/baml_client/sync_client.py\", line 72, in ExtractInstigation\n    raw = self.__runtime.call_function_sync(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nbaml_py.BamlError: client GPT4 could not resolve options.api_key\n\nCaused by:\n    Failed to resolve expression Identifier(ENV(\"OPENAI_KEY\")) with error: unset env variable 'OPENAI_KEY'\n^CINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [23733]\n\n\n$ source .env\necho $OPENAI_KEY\nsk-proj-[rest of key]`\n\nI have been running into this above error. It seems like the baml_cli_client is not detecting my keys in my .env. I've tried setting them in the playground where my functions run and in my .env. Am not sure why this is happening, as my python code itself can load in the keys. Any ideas?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:01:31.355000+00:00",
                "content": "Parsing"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 17:34:22.780000+00:00",
                "content": "raw details"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-27 16:49:13.804000+00:00",
                "content": "Does BAML have timeouts enabled? I can see some of my LLM calls are hanging and it prevents the system from moving forward"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-27 17:41:40.319000+00:00",
                "content": "Hey guys, I found a bug in BAML"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-27 19:06:10.641000+00:00",
                "content": "Panic related to logging"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:51:07.628000+00:00",
                "content": "multimodal"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 01:30:24.033000+00:00",
                "content": "hey, is there a way to set up a client"
            },
            {
                "author": "moonboie",
                "timestamp": "2024-10-07 14:45:12.029000+00:00",
                "content": "Locally on my mac everything works fine"
            },
            {
                "author": "editnori",
                "timestamp": "2024-10-10 20:30:00.637000+00:00",
                "content": "hi everyone quick question if i wanted to test a finetuned gpt-4o-mini model do i update the clients.baml file and add it there? or i specify the client as openai/(ft:gpt-40-mini...) client from the prompt baml file? or both?"
            },
            {
                "author": "editnori",
                "timestamp": "2024-10-10 20:37:28.135000+00:00",
                "content": "figured it out! nevermind"
            }
        ]
    },
    {
        "thread_id": 1259980627546210395,
        "thread_name": "Hi y'all -- today all of our tests",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:13:10.915000+00:00",
                "content": "Hi y'all -- today all of our tests involving BAML started failing when I went to add a new feature, and it seems to be caused by generating BAML code with v0.48.0 of the vscode extension (error below):\n```\nTypeError: Cannot read properties of undefined (reading 'bind')\n\n      20 |\n      21 | const traceAsync =\n    > 22 | DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnAsync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\n         |                                                                       ^\n      23 | const traceSync =\n      24 | DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnSync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\n      25 | const setTags =\n\n      at Object.<anonymous> (server/helpers/baml/baml_client/tracing.ts:22:71)\n      at Object.<anonymous> (server/helpers/baml/baml_client/index.ts:20:1)\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:14:47.648000+00:00",
                "content": ""
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:14:47.899000+00:00",
                "content": "Going back to the same source code and baml_client/ code generated from v0.44.0 of the VSCode extension works, but as soon as I hit save to generate a new baml_client/ there are some diffs that cause tests to fail"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 21:17:36.468000+00:00",
                "content": "thanks for flagging this. \n\nThe issue is due to your version for teh playground and typescript for BAML not matching.\n\nCan you check what version your runtime is at?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 21:17:40.540000+00:00",
                "content": "in the playground"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 21:18:01.200000+00:00",
                "content": "and then you can be sure to install that version in your npm packages"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:19:35.848000+00:00",
                "content": "Does that mean we should update the version number in package.json manually each time the vscode extension updates?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 21:19:46.015000+00:00",
                "content": "`npm update @boundaryml/baml`\n\nTo get the latest one.\n\nThen in VSCode Extensions, you'll need to check what version you have of BAML"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 21:20:16.702000+00:00",
                "content": "yea for now, that will help with consistency, we're working on a fix that will help you maintain better parity (or even just tell VSCode to use the local install you have for generating the code)"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:20:27.945000+00:00",
                "content": "gotcha ok"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:20:32.477000+00:00",
                "content": "trying that now"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:32:24.172000+00:00",
                "content": "Ok noticing some very weird things happening:\n- When I rollback the VSCode extension to v0.44.0 and with typescript at v0.44.0 it previously was everything worked (as expected) \n- When I updated typescript to v0.49.0 and ran tests again it *still* works with VSCode at v0.44.0\n- And when I update VSCode to v0.49.0 it breaks again"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 21:32:59.262000+00:00",
                "content": "odd, would you mind hopping on the office hours channel? May be helpful to screenshare"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-07-08 21:33:39.471000+00:00",
                "content": "sure, one sec"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 22:50:54.396000+00:00",
                "content": "Hi <@1173670148155125774>, we patched just VScode to 0.49.1 and it should work now! Please let me know if you have further issues. (that is compatible with npm @boundaryml/baml 0.49.0)"
            }
        ]
    },
    {
        "thread_id": 1259996352868515992,
        "thread_name": "@Foxicution mentioned that geminiâ€™s",
        "messages": [
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-08 22:15:40.124000+00:00",
                "content": "<@233274532855676928> mentioned that geminiâ€™s response that is surrounded with \n\nâ€˜â€™â€™json\n...\nâ€˜â€™â€™\n\nTrips up baml due to unexpected syntax? I could be wrong though, havent seen the error first hand\n\n(Note backticks replaced for discord compatibility)\nâ€”â€”-\nEDIT: SOLVED (CLOSED)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 22:16:36.402000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 22:16:36.640000+00:00",
                "content": "Could you elaborate more on this? \n\nwhat do you mean by unexpected syntax."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-08 22:16:57.373000+00:00",
                "content": "and what is the return type of the function you are using here?"
            },
            {
                "author": "elijas_ai",
                "timestamp": "2024-07-08 22:49:53.126000+00:00",
                "content": "Nevermind looks like it works now, thanks for a quick response!ðŸ™Œ"
            }
        ]
    },
    {
        "thread_id": 1260036992628953121,
        "thread_name": "stream logging",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 00:57:09.398000+00:00",
                "content": "Hey Gang, \nFor some reason, I'm not seeing the full stream get logged. Is there a known reason for this? Is my streaming code off? \n```python\n    stream = baml.stream.StreamRFPChatResponse(\n        chat_stream_input=RFPChatStreamInput(\n            conversation=baml_conversation,\n            reference_documents=ZenfetchBotDocumentBaseList(\n                list_of_documents=document_info_for_final_stream\n            ),\n            proposal_text=user_message.proposal_text,\n            proposal_sections_user_highlighted=proposal_sections,\n            requirements_to_fulfill=user_message.requirements_to_fulfill,\n            writing_plan=user_message.writing_plan,\n        )\n    )\n\n    try:\n        prev = \"\"\n        async for part in stream:\n            if part is None:\n                continue\n            try:\n                delta = part[len(prev) :]\n                await websocket.send_text(delta)\n                logger.info(f\"Sending delta: {delta}\")\n                prev = part\n            except Exception as e:\n                logger.error(\n                    \"Failed to send streaming response for user: %s\\t Exception: %s\",\n                    chat.user_id,\n                    str(e),\n                )\n                raise WebSocketDisconnect(1001) from e\n        final_output = await stream.get_final_response()\n        chat_messages_sent.append(final_output)\n    except WebSocketDisconnect as e:\n        raise e\n    except Exception as e:\n        logger.error(\n            \"Failed to stream final response for user: %s\\t Exception: %s\",\n            chat.user_id,\n            str(e),\n        )\n```\n\nFor context: The baml function returns a pure string"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:58:07.247000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:58:07.440000+00:00",
                "content": "do you see something get logged at all, or 0 logs?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 00:58:27.690000+00:00",
                "content": "I'm seeing the last 50% of the messages get logged"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 00:58:36.178000+00:00",
                "content": "for the deltas"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:59:05.473000+00:00",
                "content": "ohh like your literal logger.info(..) only starts printing halfway thru the stream"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 00:59:13.988000+00:00",
                "content": "well, boundary is default logging the entire prompt and parsed response so it actually might be covering some of the other delta logs"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 00:59:18.453000+00:00",
                "content": "is there a way to turn off all boundary logs?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 00:59:54.940000+00:00",
                "content": "remove BAML_LOG from your env vars to remove everything we print out"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 01:00:37.185000+00:00",
                "content": "is the first delta perhaps chunkier than the other ones? or definitely cut off?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 01:01:19.461000+00:00",
                "content": "stream logging"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 01:12:07.671000+00:00",
                "content": "one second, I'm updating the log thing and testing"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-09 01:13:30.740000+00:00",
                "content": "ok confirmed it was working fine, issue is client side. Thanks for jumping in quickly <@201399017161097216> !"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-09 01:13:45.392000+00:00",
                "content": "np! ðŸ«¡"
            }
        ]
    },
    {
        "thread_id": 1261010103524528219,
        "thread_name": "Potential tracing issues",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 17:23:57.115000+00:00",
                "content": "Was there any progress made on the tracing for workers? Remember we talked about this a while back related to flushing"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:24:38.996000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:24:39.227000+00:00",
                "content": "this should be resolved in the latest release. Let me know if you have issues!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:24:50.721000+00:00",
                "content": "Release 0.49+"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 17:24:53.337000+00:00",
                "content": "Do I need to use the `flush()` command?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:25:20.610000+00:00",
                "content": "you likely should in your top most worker (outside of the @trace call)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:25:37.900000+00:00",
                "content": "otherwise what can happen is the process will get killed before we get a chance to guarantee the logs get sent"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 17:25:44.829000+00:00",
                "content": "You have 2 minutes to show me?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 17:25:48.487000+00:00",
                "content": "yes sir!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 19:15:04.456000+00:00",
                "content": "spent almost 2 hours debugging something niche in docker"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 19:15:08.420000+00:00",
                "content": "but got around to this;\nTask extract_text_from_document_and_divide_into_pages[3ab9b8d7-578f-4e29-b177-fa0ec8e1fddf] retry: Retry in 0s: BamlError('Failed to flush BAML traces\\n\\nCaused by:\\n    BatchProcessor worker thread did not finish in time')"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 19:15:19.857000+00:00",
                "content": "celery worker seemed to finish though the baml flush failed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 19:16:23.277000+00:00",
                "content": "got it, are you seeing the events the dashboard however?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 19:16:37.834000+00:00",
                "content": "we can increase beyond the default timeout there to make it better to use."
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 19:16:57.088000+00:00",
                "content": "no not seeing it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 19:17:32.406000+00:00",
                "content": "<@711679663746842796> can you file an issue and track this down?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 19:20:08.188000+00:00",
                "content": "That would be greatly appreciated"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-11 19:20:13.247000+00:00",
                "content": "happy to share the code in a DM if that helps"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 19:21:25.963000+00:00",
                "content": "that would be great, can you share teh sync code you use for your celery worker with <@711679663746842796> ?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-11 19:21:33.128000+00:00",
                "content": "(to make it wait for your task)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-13 04:21:30.285000+00:00",
                "content": "Just to update: investigation is still in progress, have definitely learned more information, but there's something real weird going on here that we haven't been able to nail down yet"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-15 01:47:04.609000+00:00",
                "content": "appreciate you guys"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-16 13:11:08.316000+00:00",
                "content": "<@1049713528170364968> We're still investigating this. we know what the likely culprit is (celery spins up many hosts and then the way they share variables may be screwing up some of the state BAML uses which leads to messages being lost) hopefully we can get a patch working today!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-16 13:54:05.560000+00:00",
                "content": "That would be a lifesaver"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-16 13:55:20.321000+00:00",
                "content": "Major major unlock if this can work and I can trace my workers again. The bulk of my product quality relies on Baml functions inside these workers. Right now I don't have a good way to iterate on the prompts and application code since it's really hard to visualize the LLM prompts and responses. The context window in some cases is so absurdly large that logging in my terminal only gets me maybe 50% of the way there"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:25:50.066000+00:00",
                "content": "Would there potentially be a concurrency limit if I'm running a number of baml tasks concurrently within my celery worker?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:27:30.532000+00:00",
                "content": "We currently start a new thread per baml stream, unfortunately (we were seeing async bugs without this), so there's a soft limit that depends on how many `threading.Thread` instances you can spawn"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:29:54.071000+00:00",
                "content": "Are you asking because you're running into something?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:29:55.587000+00:00",
                "content": "Gotcha, it seems as though baml is hanging though in certain situations (not sure if this is related to the tracing issue) \n\nI have the following \n\n```python\nasync def extract_requirements(doc):\n   try:\n      print('starting', doc.id) \n      baml.DoSomething(doc)\n      print('completed', doc.id)\n    except:\n      stuff\n\ntasks = [extract_requirements(doc) for doc in docs]\nresults = await asyncio.gather(*tasks)\n```\n\nI'm seeing 11 prints of starting for each doc and not a single completed ðŸ˜¦"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:30:12.732000+00:00",
                "content": "`await baml.DoSomething(doc)`"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:30:17.192000+00:00",
                "content": "ya i do that"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:30:22.029000+00:00",
                "content": "should have clarified lol"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:30:35.857000+00:00",
                "content": "Sometimes when I restart my worker, it will work fine"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:30:43.148000+00:00",
                "content": "but I'm not sure how to catch this stalling behavior"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:33:14.650000+00:00",
                "content": "Thinking... the threading issue doesn't apply in your case because you're not using streaming"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:35:19.294000+00:00",
                "content": "You can crank `BAML_LOG=debug` on (we don't really have any stability guarantees on this log level, so I wouldn't depend on it being consistent over time) which will dump out a lot more about what's happening under the hood"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-17 21:35:52.780000+00:00",
                "content": "crazy how celery's lack of concurrency support for the last 4 years has gotten so much more attention due to GPT https://github.com/celery/celery/issues/6552"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:36:22.543000+00:00",
                "content": "Hah"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-17 21:38:11.269000+00:00",
                "content": "Also, just to let you know: we've found something in Celery that smells really funny and v's working on a fix that we're hoping we can confirm tonight. (we suspect there's some serialization happening of things that don't support it)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 07:22:40.906000+00:00",
                "content": "<@1049713528170364968> would you able to share your celery set up?\n\nI want to check and see if i'm reproing the exact same issue as you"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 09:26:23.456000+00:00",
                "content": "Ok I've finally got it. This seems be a bug of how celery doesn't play well with async natively and due to us being a native binary it gets a bit more complicated.\n\nKey things to ask:\n1. are you able to use `eventlet` for your celery task isntead of `multiprocessing` (if you don't configure it manually, then `multiprocessing` is the default - which appears to be one of the issues)\n2. are you able to switch your functions to purely sync? Other than baml do you have other async calls that must be async? (we can give you a patch of something like: `b.sync.YourFunction(...)` to make BAML sync)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 09:27:20.283000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-18 09:27:39.915000+00:00",
                "content": "(tested on docker-desktop with celery FYI!)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-18 13:35:35.420000+00:00",
                "content": "> are you able to use eventlet for your celery task isntead of multiprocessing (if you don't configure it manually, then multiprocessing is the default - which appears to be one of the issues)\n\nI don't see why not? Unless there's something tricky with the configuration I should be able to do this\n\n> are you able to switch your functions to purely sync? Other than baml do you have other async calls that must be async? (we can give you a patch of something like: b.sync.YourFunction(...) to make BAML sync)\n\nThat should be doable. My priority is observability over speed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 03:01:17.740000+00:00",
                "content": "Ok! great update. The code is now tested and validated:\n\nI can now call BAML functions sync from celery code and it works with tracing!!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 03:01:57.075000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 03:02:55.446000+00:00",
                "content": "What I'll do now is prep the merge (`0.51.0` will fix this for you!)\n\nI'll also send over a repo so you have a reference of a celery project that works with BAML."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 08:44:45.945000+00:00",
                "content": "Ok, the commit is merged! \n\nhttps://github.com/BoundaryML/baml/pull/803\n\nLast few things and we should be able to get you 0.51.0 to you"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-19 10:39:26.041000+00:00",
                "content": "sick!!!!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 10:59:54.320000+00:00",
                "content": "FYI, may need a bit more time (will merge in around noon)\n\nSuper tired but just got all the tests passing! So we'll take care of the release when I'm up! \nSorry for the delay, but that issue (and quite a few more - including better stream stability) are resolved now!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-19 11:06:08.659000+00:00",
                "content": "Ya take ur time"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-19 11:06:14.702000+00:00",
                "content": "Have other things I can work on"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-19 19:31:57.821000+00:00",
                "content": "Want to quickly ask whether this merge will happen anytime in the next hour. It's ok if not, just wanna know whether I should switch context to another priority ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 19:38:57.207000+00:00",
                "content": "Merge is planned for after lunch! (We wanted to do a pretty heavy testing just in case - eta 1.5 hours!)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-19 21:56:12.955000+00:00",
                "content": "sorry took a while! (aaron is still on it and almost done. there's just a lot of changes we made in this realease and we underestimated the testing load)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-19 22:03:52.530000+00:00",
                "content": "all gucci"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-19 22:03:59.594000+00:00",
                "content": "i'm becoming a dev ops engineering in the meantime 8)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 00:54:30.842000+00:00",
                "content": "its out!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 01:33:24.854000+00:00",
                "content": "Thanks! Will probs not get to it until tomorrow though Iâ€™ll keep yall posted"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 17:51:08.409000+00:00",
                "content": "So is the idea that now in my generator I need to declare the \n```\n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"sync\"\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 17:51:35.912000+00:00",
                "content": "then all my baml calls, instead of being `poem = await baml.Generate()` it will just be `poem = baml.Generate`"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 17:56:26.950000+00:00",
                "content": "Btw i'm getting the following boundary error in my IDE \n\n```\nProperty not known: \"default_client_mode\". Did you mean one of these: \"output_type\", \"output_dir\", \"version\"?baml\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 18:04:58.906000+00:00",
                "content": "Ok so I think I figured it out\nIn application code, I now am doing\n```\nfrom baml_client.async_client import b as baml\nfrom baml_client.sync_client import b as baml_sync_client\n```\n\nStill doesn't look like the generator allows a `default_client_mode` though"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 18:21:30.021000+00:00",
                "content": "My celery worker crashes as soon as It gets to the synchronous baml function... i managed to convert the celery task to be synchronous, though i'm getting a `celery_worker_rfp-1 exited with code 255`\n\nseems like the container dies as soon as the `baml.Function()` is called (at least based on my logs)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 18:28:33.966000+00:00",
                "content": "Ok, moving to the `eventlet` pool seemed to fix the problem ðŸ˜„"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 18:29:36.162000+00:00",
                "content": "and we got tracing unlocked!!!! WOOOOOO thank you baml team !!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:10:51.954000+00:00",
                "content": "Yay!!! So excited it all works"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:11:12.146000+00:00",
                "content": "Also I guess the crash is better than the silent failure now for non eventlet"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:11:29.347000+00:00",
                "content": "Thanks for checking everything!!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:11:45.321000+00:00",
                "content": "Longest living thread to date for baml haha"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 19:12:42.690000+00:00",
                "content": "Doesn't help that I spam the thread ðŸ«£ \nDo you want me to cut an issue for the `default_client_mode`?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:13:01.066000+00:00",
                "content": "Whatâ€™s the issue?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:13:21.157000+00:00",
                "content": "Oh yes. That would be great"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:13:49.182000+00:00",
                "content": "But you donâ€™t need to modify default client mode"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:13:56.777000+00:00",
                "content": "It should work regardless!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 19:14:10.552000+00:00",
                "content": "Just explicitly use sync client"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-20 19:15:40.043000+00:00",
                "content": "https://github.com/BoundaryML/baml/issues/813"
            }
        ]
    },
    {
        "thread_id": 1261473093881434193,
        "thread_name": "did something happen with the 0.50",
        "messages": [
            {
                "author": "yungweedle",
                "timestamp": "2024-07-13 00:03:42.618000+00:00",
                "content": "did something happen with the 0.50 update? i am now getting this error: \nTypeError: BamlRuntime.call_function() missing 1 required positional argument: 'cb'"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-13 00:04:26.408000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-13 00:04:26.781000+00:00",
                "content": "Oof - can you try re-running `baml-cli generate`?"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-13 00:05:10.535000+00:00",
                "content": "oh hmm. that works now. my cursor used to auto run that i suppose"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-13 00:05:21.913000+00:00",
                "content": "every time i saved"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-13 00:05:24.866000+00:00",
                "content": "basically the vscode extension updated to 0.50 but you have to make sure your python package is also 0.50.0, we will be notifying you of out-of-sync issues soon"
            },
            {
                "author": "yungweedle",
                "timestamp": "2024-07-13 00:05:35.337000+00:00",
                "content": "i only updated my python lib"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-13 00:05:46.080000+00:00",
                "content": "ah yes cursor sometimes doesnt autoupdate"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-13 00:06:27.400000+00:00",
                "content": "our next update to the extension will have more configs as to whether you want it to generate the `baml_client` or let you do it yourself using the python package. \n\nAlso we did add dynamic client params now: https://docs.boundaryml.com/docs/calling-baml/client-registry"
            }
        ]
    },
    {
        "thread_id": 1264142006297296947,
        "thread_name": "I am setting an environmental variable",
        "messages": [
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 08:49:00.924000+00:00",
                "content": "I am setting an environmental variable AZURE_OPENAI_API_KEY but I am finding that my baml client isn't working with api_key env.AZURE_OPENAI_API_KEY, but will work if I manually specify the key in baml file. openai.AzureOpenAI works with the environmental variable. Am I missing something?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:03:02.995000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:03:03.975000+00:00",
                "content": "ah yes! you need to do the following:\n\n```sh\npip install dotenv-cli\ndotenv -e .env python app.py\n```\n\nisntead of just `python app.py`\n\nBasically we require setting up env vars ahead of time:  see https://docs.boundaryml.com/docs/calling-baml/set-env-vars"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:03:22.406000+00:00",
                "content": "there is also a known issue https://github.com/BoundaryML/baml/issues/805"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 15:13:25.238000+00:00",
                "content": "Oh interesting... I could have sworn setting an OPENAI_API_KEY with ```setx OPENAI_API_KEY \"keyhere\"``` was working with an regular openai client"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 15:13:36.579000+00:00",
                "content": "I'll look into using dotenv"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:14:06.935000+00:00",
                "content": "i would print out os.environ!\n\nwe noted that in powershell it doesn't always propagate env variables to python on windows sometimes"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 15:14:28.907000+00:00",
                "content": "I did print it out and it was set correctly"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:14:35.023000+00:00",
                "content": "(we basically just pass down all of os.environ to BAML at the time of construction fyi)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:14:39.133000+00:00",
                "content": "oh intesting"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:14:46.676000+00:00",
                "content": "i'll take a look and see why thats the case!"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 15:14:47.528000+00:00",
                "content": "and it works with openai.AzureOpenAi"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 15:15:24.275000+00:00",
                "content": "well, the langchain_openai AzureChatOpenAI wrapper"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:16:09.936000+00:00",
                "content": "oh interesting! Yea, we're likely doing something slightly different somehow."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:16:21.917000+00:00",
                "content": "could you share the commands you are running?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:17:30.547000+00:00",
                "content": "and then also paste a copy of yoru directory tree from the path you running the command from:\n```\nShow-Tree e:\\data â€“depth 2\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-20 15:17:50.645000+00:00",
                "content": "I'll use that to file a ticket so i can repro and then identify what the bug is"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-20 15:20:51.670000+00:00",
                "content": "going out at the moment but I'll provide more details when I get back"
            },
            {
                "author": "neuralcorrelate",
                "timestamp": "2024-07-23 18:34:22.233000+00:00",
                "content": "hi sorry, I forgot to get back to you. I decided to just use dotenv as it made more sense from a best practices perspective."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 18:41:24.985000+00:00",
                "content": "amazing! Glad it worked for you ðŸ™‚"
            }
        ]
    },
    {
        "thread_id": 1265006191319318609,
        "thread_name": "dyanmic clients (client registry)",
        "messages": [
            {
                "author": "jyothi_36203",
                "timestamp": "2024-07-22 18:02:58.689000+00:00",
                "content": "Is it possible to pass client as a variable to functions like below ?\nfunction ExtractResume(resume_text: string, myclient: client) -> Resume {\n  \n  //client MyClient"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-22 18:04:54.563000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-22 18:04:54.831000+00:00",
                "content": "We'll be enabling this soon native in BAML, but for now we have some ways to do this in a native language: \n\nhttps://docs.boundaryml.com/docs/calling-baml/client-registry\n\nexample:\n\n```python\nfrom baml_py import ClientRegistry\nasync def run():\n    cr = ClientRegistry()\n    # Sets MyAmazingClient as the primary client\n    # assuming you've already defined MyAmazingClient as a client in BAML\n    cr.set_primary('MyAmazingClient')\n    # ExtractResume will now use MyAmazingClient as the calling client\n    res = await b.ExtractResume(\"...\", { \"client_registry\": cr })\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-22 18:05:23.272000+00:00",
                "content": "alternatively, you can also create clients at runtime from python by following the example on the docs"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-22 18:05:44.543000+00:00",
                "content": "(e.g. then pass in an API key of your user or dynamically set temperature)"
            },
            {
                "author": "jyothi_36203",
                "timestamp": "2024-07-22 18:07:14.400000+00:00",
                "content": "Thank you for the quick response, this is helpful!!!"
            }
        ]
    },
    {
        "thread_id": 1265442385823797280,
        "thread_name": "Client Registry â€” BAML Documentation",
        "messages": [
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 22:56:15.563000+00:00",
                "content": "Hello! Newbie here, but I love this library. Problem is, I'm trying to use a custom ClientRegistry in TypeScript, and it's not working.\n\nDocs: https://docs.boundaryml.com/docs/calling-baml/client-registry\n\nFirst off, the TS function signature for addLlmClient in the example on the link above doesn't match the actual signature in the code (using 0.51.1)\n\nEXPECTED (based on docs):\ncr.addLlmClient({ name: 'MyAmazingClient', provider: 'openai', options: {\n        model: \"gpt-4o\",\n        temperature: 0.7,\n        api_key: \"sk-...\"\n    }})\n\nACTUAL:\nexport class ClientRegistry {\n  constructor()\n  addLlmClient(name: string, provider: string, options: { [string]: any }, retryPolicy?: string | undefined | null): void\n  setPrimary(primary: string): void\n}\n\n\nBut the actual seems fine, so I adjusted to the syntax. \n\nNext problem: the TS generated for my function doesn't have the correct syntax to use the custom registry I passed.\n\nGENERATED:\n```\nasync LocalGetRecipe(\n      arg: string,\n      __baml_options__?: { tb?: TypeBuilder, clientRegistry?: ClientRegistry }\n  ): Promise<Recipe> {\n    const raw = await this.runtime.callFunction(\n      \"LocalGetRecipe\",\n      {\n        \"arg\": arg\n      },\n      this.ctx_manager.cloneContext(),\n      __baml_options__?.tb?.__tb(),\n      __baml_options__?.cr, <-- DOES NOT EXIST\n    )\n    return raw.parsed() as Recipe\n  }\n  \n}\n```\nEXPECTED:\n```\nasync LocalGetRecipe(\n      arg: string,\n      __baml_options__?: { tb?: TypeBuilder, clientRegistry?: ClientRegistry }\n  ): Promise<Recipe> {\n    const raw = await this.runtime.callFunction(\n      \"LocalGetRecipe\",\n      {\n        \"arg\": arg\n      },\n      this.ctx_manager.cloneContext(),\n      __baml_options__?.tb?.__tb(),\n      __baml_options__?.clientRegistry, <-- CORRECTED TO THIS MANUALLY\n    )\n    return raw.parsed() as Recipe\n  }\n  \n}\n```\nOnce I make that correction manually in the generated TS, it does seem to ATTEMPT to use my custom client. But it says I'm missing a \"model\" param. (I'm not.)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-23 22:57:05.405000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-23 22:57:05.690000+00:00",
                "content": "FYI, you can use ```triple backtick``` to style code blocks"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-07-23 22:57:33.310000+00:00",
                "content": "(Still reading through your code to figure out what's going on)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 22:59:05.693000+00:00",
                "content": "Thanks for sharing <@799378605858750476> i thnk the new update (going out tmrw am!) should fix that shortly! We added a few more tests for client registry.\n\nThat said, what version of BAML are you using?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 22:59:14.596000+00:00",
                "content": "on VScode and Typescript respectively"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 22:59:30.090000+00:00",
                "content": "in vscode you can see your version in the bottom right"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 23:00:29.358000+00:00",
                "content": "Also, if its helpful, I'm the office hours channel and we can screenshare to see if we can debug faster! (the current release should support client registry)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 23:02:23.571000+00:00",
                "content": "```ts\n  it('should work with dynamic client', async () => {\n    const clientRegistry = new ClientRegistry()\n    clientRegistry.addLlmClient('myClient', 'openai', {\n      model: 'gpt-3.5-turbo',\n    })\n    clientRegistry.setPrimary('myClient')\n\n    await b.TestOllama('hi', {\n      clientRegistry,\n    })\n  })\n```\n\nIn my case, I'm using:\n```sh\n$ pnpm show @boundaryml/baml\n\n@boundaryml/baml@0.51.1 | MIT | deps: none | versions: 45\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 23:06:30.034000+00:00",
                "content": "And for that client, are you able to make it work in the playground?\n\nI see you are able to call the model. w/ your endpoint, but you aren't able to see the actual request.\nOne simple thing to try is you define the client in BAML directly, you can open the playground and see the actual CURL request we are making. If that curl requests works for you in terminal, but not in BAML, then we likely have a bug. Could you see if that is the case?"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 23:34:31.040000+00:00",
                "content": "If I manually set up the client in BAML (not using a custom ClientRegistry) then yes it seems to make the CURL properly.\n```\nclient<llm> LOCAL_EG_GPT4o {\n  provider openai\n  options {\n    base_url \"http://localhost:8080\"\n    model gpt-4o\n    api_key \"skipped_because_we_are_using_local_endpoint\"\n    headers {\n      Session-Token \"(redacted)\"\n    }\n  }\n}\n```\n\n```function LocalGetRecipe(arg: string) -> Recipe {\n    client LOCAL_EG_GPT4o\n    prompt #\"\n        Generate a recipe for a {{arg}}.\n\n        {{ ctx.output_format }}\n    \"#\n}\n```\n\n```\ncurl -X POST 'http://localhost:8080/chat/completions' -H \"session-token: [redacted]\" -H \"authorization: Bearer skipped_because_we_are_using_local_endpoint\" -H \"content-type: application/json\" -d \"{\n  \\\"model\\\": \\\"gpt-4o\\\",\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n...\n```"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 23:35:16.327000+00:00",
                "content": "Yeah sorry silly my for not formatting properly. Shame bell."
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 23:36:19.010000+00:00",
                "content": "TS: `\"@boundaryml/baml\": \"^0.51.1\",`\nVS: `VSCode Runtime Version: 0.51.1`"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-23 23:38:08.101000+00:00",
                "content": "Very generous! Have to do some parent duty but will be back online as soon as possible. Good to see you again, Vaibhav! (We connected via EurasiaGroup.)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-23 23:45:14.543000+00:00",
                "content": "Oh! Great to see you again! ðŸ™‚ Just tag me and I'll come back online. If its easier, we can set up a predetermined time tmrw? say 9 am PST?"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-24 00:18:12.110000+00:00",
                "content": "That'd be great! Should I swing by office hours then, or find you some other way?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 00:21:42.891000+00:00",
                "content": "I can send over a zoom link!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-24 00:21:49.924000+00:00",
                "content": "I still have you email ðŸ™‚"
            }
        ]
    },
    {
        "thread_id": 1268260423925366846,
        "thread_name": "variables in prompts",
        "messages": [
            {
                "author": "bsachs10",
                "timestamp": "2024-07-31 17:34:08.203000+00:00",
                "content": "This might be a silly question, but is there a way to create a static string and reuse it across test cases? I tried using a template string with no params, but it didn't work. It just gets rendered as \"{{Sample_Article()}}\" in the CURL.\n\n```\n\ntemplate_string Sample_Article() #\"\n  Sample article...\n\"#\n\ntest ArticleTest {\n  functions [AnalyzeClientMentions]\n  args {\n    client_name \"OpenAI\"\n    article \"{{Sample_Article()}}\"\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 17:35:04.775000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 17:35:05.082000+00:00",
                "content": "You want variables ðŸ™‚"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-31 17:35:22.516000+00:00",
                "content": "Indeed"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-31 17:35:44.303000+00:00",
                "content": "Is there a syntax for that? I tried `string ...` and it didn't work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 17:37:15.304000+00:00",
                "content": "we are working on this! This will be similar to the client thing i showed you earlier and should land at the same time"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-07-31 17:37:23.188000+00:00",
                "content": "but for now you'll need to copy and paste"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-07-31 17:37:24.662000+00:00",
                "content": "Ah, got it"
            }
        ]
    },
    {
        "thread_id": 1268312244488372336,
        "thread_name": "dashboard crash",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:00:03.188000+00:00",
                "content": "Hey gang, \ni think my boundary dashboard may have crashed ( I am loading quite a lot of text for this one trace)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:00:24.973000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:00:25.263000+00:00",
                "content": "taking a look"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:00:41.076000+00:00",
                "content": "https://app.boundaryml.com/dashboard/projects/proj_4d5d8c28-b9df-435e-8b6e-48f2566cadbb/drilldown?start_time=2024-07-17T20%3A05%3A15.748Z&end_time=2024-07-31T20%3A59%3A23.369Z&eid=80551477-88f8-492f-9169-ec8cd6596a0e&s_eid=7f682bd9-710e-4ebb-b201-baf19cdced3d&test=false&onlyRootEvents=true"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:01:34.004000+00:00",
                "content": "it loaded for me now -- was it an intermittent failure or consistent crash? Which browser are you on?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:02:17.096000+00:00",
                "content": "hmm i think the URL isn't even showing for the trace I'm intersted in. I'm interestd inthe 4:48 pm one"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:03:11+00:00",
                "content": "the ExtractProposal... function?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:03:34.663000+00:00",
                "content": "ohhh yeah it aint loading for me"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:03:42.454000+00:00",
                "content": "ye"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:03:53.902000+00:00",
                "content": "oks cool ill take a look, maybe just need to adjust our timeout"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:04:01.719000+00:00",
                "content": "probably because there are a ton of LLM calls within that trace. I really just want to see the first one right now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:04:42.772000+00:00",
                "content": "oks let me see if i have a quick fix, if not i can pull up that entry manually if you need it ASAP"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:15:25.357000+00:00",
                "content": "ok it loaded now for me"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:15:27.036000+00:00",
                "content": "just takes a while"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:15:34.671000+00:00",
                "content": "i increased our connection timeout"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:16:16.215000+00:00",
                "content": "thanks!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:16:19.011000+00:00",
                "content": "ok seems to crash after a while, let me increase it further"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:16:41.633000+00:00",
                "content": "oof, ya sorry usually my traces won't be this large"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:16:41.972000+00:00",
                "content": "sorry, it's in beta irght now"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:16:47.040000+00:00",
                "content": "nw"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:16:47.499000+00:00",
                "content": "beta.app.boundaryml.com"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:19:24.628000+00:00",
                "content": "I clicked on the trace though nothing seems to happen, can't tell from network logs if something is going on"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:20:43.646000+00:00",
                "content": "so the issue is that our dashboard is firing off like 200 requests hehe"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:20:51.607000+00:00",
                "content": "im gonna make it lazily load the requests until you click each one"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:21:02.756000+00:00",
                "content": "give me like 10min"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:23:42.934000+00:00",
                "content": "okie thanks!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:37:27.492000+00:00",
                "content": "do you know roughly how much context there is in each prompt? like kilobytes wise"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 21:47:13.091000+00:00",
                "content": "actually nvm i got the info in the DB. Got one thing to load but still slow to load any other traces so still looking into it"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 21:52:34.725000+00:00",
                "content": "ok thanks!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 22:31:34.379000+00:00",
                "content": "ok heres the current state...\n\nit loads if you right click one of the llm events and load it in a new tab, but it takes a while to load and once it does it crashes after some seconds (although perhaps you can copy some of the input/output at that point). It's horrible UX though.\n\nI'm looking at what's causing it to crash shortly after the data is rendered, im thinking it's a nextJS issue"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 22:31:43.393000+00:00",
                "content": "so for example this is one of the sub-tasks https://beta.app.boundaryml.com/dashboard/projects/proj_4d5d8c28-b9df-435e-8b6e-48f2566cadbb/drilldown?start_time=2024-07-17T20%3A05%3A15.748Z&end_time=2024-07-31T22%3A29%3A50.821Z&eid=6685f439-f03e-430d-bed3-f953a02071a2&s_eid=d5e08da4-ad1d-4811-820b-da1ce04562df&test=false&onlyRootEvents=true"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-07-31 22:38:35.766000+00:00",
                "content": "I actually just want to see the very first generate proposal content task, any chance you have that one?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 22:38:48.773000+00:00",
                "content": "yeah give me a sec"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-07-31 22:40:26.488000+00:00",
                "content": "ill DM you"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:29:57.036000+00:00",
                "content": "<@1049713528170364968> do you expect a lot of big payloads like this in the future? If you're unable to debug your new pipeline due to these crashes lmk and i can prioritize fixing things next week"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-01 18:55:33.382000+00:00",
                "content": "Yea the proposals we generate are massive"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-01 18:55:48.157000+00:00",
                "content": "Is there something I could do w.r.t. the trace to break it up more?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-01 18:56:09.295000+00:00",
                "content": "If you gave me a stop gap solution, like a CSV export or something, I could work with that for the time being"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:56:19.162000+00:00",
                "content": "let me see what we can do in the meantime"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-01 18:56:41.559000+00:00",
                "content": "that would be great!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 22:39:42.330000+00:00",
                "content": "ok I think I fixed it, lmk if a request still crashes for you. (for my own personal future note, i just migrated nextjs from 13 -> 14)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 22:40:01.094000+00:00",
                "content": "it just takes a bit to load those requests but i added a loading bar at the top to give you more feedback when stuff is loading"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 23:13:17.683000+00:00",
                "content": "we can add some more improvements to pre-load some of the smaller traces -- for now i erred on the side of loading things more lazily."
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-02 14:45:12.408000+00:00",
                "content": "That works, thanks aaron!"
            }
        ]
    },
    {
        "thread_id": 1268359563397304330,
        "thread_name": "Working on an OCR pipeline with GPT4o,",
        "messages": [
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 00:08:04.895000+00:00",
                "content": "Working on an OCR pipeline with GPT4o, and somehow we're going over the input context window size in the API call with a few images of invoice PDFs encoded using BAML's Image type (API requests got rejected because they were ~200k tokens long :o) -- any ideas why the input is so massive?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:21:18.816000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:21:19.212000+00:00",
                "content": "do you know what your image dimensions are?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 00:23:22.543000+00:00",
                "content": "Not sure, I can try to get that info"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:23:45.045000+00:00",
                "content": "yeah see how big the images are since thats how tokens are calculated"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 00:24:00.025000+00:00",
                "content": "But I doubt that's the root cause, the docs seems to imply that 768x2048 is the largest possible image it will ever actually convert into tiles for 4o"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 00:24:04.003000+00:00",
                "content": "They do some resizing"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 00:24:16.879000+00:00",
                "content": "https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:28:05.673000+00:00",
                "content": "how many images are you adding in?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:29:34.644000+00:00",
                "content": "actually, do you use Image.fromBase64? or fromUrl?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:30:13.878000+00:00",
                "content": "i think i know what the issue is.\n\nCan you try and use the fromURL with openai? and if you use a base64 make sure you format the url in the base64 format openai specifies:\n\n`                \"image_url\": {\"url\": f\"data:image/x-png;base64,{base64_image}\", \"detail\": \"low\"},`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:30:24.922000+00:00",
                "content": "so `Image.fromUrl(...,...)`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 00:30:40.798000+00:00",
                "content": "and lmk if that works"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 01:34:28.175000+00:00",
                "content": "Sorry, had to hop on a call"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 01:35:02.114000+00:00",
                "content": "Yes, I'm using fromBase64 -- I saw some suspicious things in the error message that looked like it was passing base64 to the LLM as a string instead of as that format you linked"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 01:35:10.708000+00:00",
                "content": "Will try what you said"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 01:41:25.005000+00:00",
                "content": "Yes i think thats whats happening"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 01:41:30.496000+00:00",
                "content": "Try fromUrl"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:13:37.077000+00:00",
                "content": "Just got back to this -- the same error is happening"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:14:16.125000+00:00",
                "content": "mind sharing a snippet of how you build the prompt? How many images do you add to the prompt?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:14:29.680000+00:00",
                "content": "Another potential confounding variable is I'm passing a image[] instead of image"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:14:30.489000+00:00",
                "content": "can you add BAML_LOG=debug and DM me your curl request?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:14:38.694000+00:00",
                "content": "sure"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:14:48.185000+00:00",
                "content": "oh interesting, let me check that on our end too"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:15:18.294000+00:00",
                "content": "```\nfunction ExtractInvoiceData(pdfImages: image[], pdfText: string[]) -> Invoice {\n  client DocumentOcrClient\n  prompt #\"\n    Extract data from the following invoice and return a structured representation of the data in the schema below.\n\n    Invoice:\n    ---\n    {{ pdfImages }}\n    {{ pdfText}}\n    ---\n\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:16:19.756000+00:00",
                "content": "can you write the images as a for loop:\n\n```\n{% for i in pdfImages %}\n{{ i }}\n{%endfor%}\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:16:36.876000+00:00",
                "content": "Sure, same for text?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:17:10.551000+00:00",
                "content": "wait is your text 1:1 with the image?"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:17:58.380000+00:00",
                "content": "One sec, let me check impl -- I think so"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:19:20.384000+00:00",
                "content": "If it's a PDF yes because we push PDF and extracted text together, otherwise text is an empty array"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:20:02.971000+00:00",
                "content": "```\nfunction ExtractInvoiceData(pdfImages: image[], pdfText: string[]) -> Invoice {\n  client DocumentOcrClient\n  prompt #\"\n    Extract data from the following invoice and return a structured representation of the data in the schema below.\n    {{ ctx.output_format }}\n    \n    {{ _.role('user') }}\n    {% for img in pdfImages %}\n    Item:\n    {{ img }}\n    {% if pdfText %}{{ pdfText[loop.index0] }}{% endif %}\n    {% endfor %}\n  \"#\n}\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:21:49.489000+00:00",
                "content": "Should there not be an Invoice: --- {whatever} --- wrapper in the user message?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:22:02.783000+00:00",
                "content": "probably dont need it"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:23:31.405000+00:00",
                "content": "Interesting, didn't know jinja lets you get the index of the loop like that lol"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:23:47.760000+00:00",
                "content": "So this plus fromUrl should format it correctly?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:23:57.241000+00:00",
                "content": "yeah"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:24:00.734000+00:00",
                "content": "Ok trying now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:24:28.214000+00:00",
                "content": "basically we were serializing the wholeass base64 string into the prompt (our bad, we will fix asap https://github.com/BoundaryML/baml/issues/853)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:24:52.829000+00:00",
                "content": "so your intuition about the array being the cause was correct, ty"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:24:55.070000+00:00",
                "content": "Yeahhh not ideal lol, that makes sense"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:24:58.203000+00:00",
                "content": "Np"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:25:08.040000+00:00",
                "content": "I was like theres no way I'm making a thousand tiles"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:25:11.183000+00:00",
                "content": "you can still enable BAML_LOG=debug to see the full request"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:25:43.180000+00:00",
                "content": "(though it is super noisy fyi)"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:25:58.401000+00:00",
                "content": "gotcha ok will enable that for this test"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:56:11.847000+00:00",
                "content": "Sorry forgot to update, that ended up working -- now I'm working on some optimizations (aliasing, descriptions) to make it faster/more consistent"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:57:01.761000+00:00",
                "content": "Seems like the LLM kind of gave up halfway through the invoice, there are 90 line items and it got roughly the first 54 -- hoping aliasing will help with that, open to any other ideas"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:58:08.093000+00:00",
                "content": "is this for the first image? Yoiu may need to prompt \"Make sure every row is accounted for in the invoice\". Our playground test is a bit wonky since you have to add a url or a base64 but id recommend setting up 1 test up in the playground so you can iterate on the prompt faster"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:58:34.155000+00:00",
                "content": "you can also try and only add 1-2 images per LLM call"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 18:59:26.754000+00:00",
                "content": "or first make sure it works with 1 image before you add multiple"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 18:59:59.408000+00:00",
                "content": "Yep that makes sense, the current plan is to just try to get something working and use gpt4o-mini to see if we can reliably get the whole invoice in one LLM call (bigger output window), starting with single page invoices and moving up from there"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:00:14.172000+00:00",
                "content": "ah gotcha, you want to ensure it has all the context of previous pages"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:00:17.843000+00:00",
                "content": "like the column headers"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:00:20.895000+00:00",
                "content": "Yeah exactly"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:00:26.081000+00:00",
                "content": "Some invoices have that, some invoices dont"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:01:18.125000+00:00",
                "content": "Our current OCR pipeline does some tricks with sending the first and last page along with individual pages for long invoices"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:01:43.025000+00:00",
                "content": "And merging the LLM's answers into one final answer"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:02:48.548000+00:00",
                "content": "there's def many things you can try:\n- rerun the same function but ask it to extract any that are missing\n- lower the amount of data\n- prompt engineering\n- have the LLM write out all the row IDs first, then ask it to extract the specific info for each row"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:03:43.198000+00:00",
                "content": "we will be adding file refs to tests in the next couple days so youll be able to test better using:\n\n```\ntest .. {\n  args {\n    image { file: \"../myfile.png\" }\n  }\n}\n```"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:04:08.526000+00:00",
                "content": "Ooh nice"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:04:14.696000+00:00",
                "content": "That will be useful"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:04:37.079000+00:00",
                "content": "Thx for the ideas!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:05:00.881000+00:00",
                "content": "do you use gpt4 or claude? i found claude to be better at some vision extraction tasks"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:05:33.771000+00:00",
                "content": "Currently using 4o in production\\"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:05:59.745000+00:00",
                "content": "What's the output token limit of claude? That seems to be a major limiter / cause of extra engineering complexity"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:06:32.160000+00:00",
                "content": "all of the models have 4096 output tokens, except for a new gpt4 alpha model that just came out 2 days ago"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:06:35.044000+00:00",
                "content": "4o mini has 16,384 tokens, 4o has 4096 which causes problems sometimes for long invoices"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:06:36.626000+00:00",
                "content": "gotcha"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-01 19:07:00.913000+00:00",
                "content": "whoa nice, i didnt know mini had more"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:07:11.883000+00:00",
                "content": "Yeahh that's the main reason we want to switch"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:07:19+00:00",
                "content": "Also it's really cheap idk how the hell they do that"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 19:08:26.577000+00:00",
                "content": "```\ngpt-4o-mini\n$0.150 / 1M input tokens\n$0.075 / 1M input tokens\n$0.600 / 1M output tokens\n$0.300 / 1M output tokens\n```\nhttps://openai.com/api/pricing/"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-01 20:07:26.056000+00:00",
                "content": "FYI- 4o-mini apparently artificially inflates token counts for image inputs, so that images cost the same on 4o-mini as they do on 4o"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-01 20:07:49.514000+00:00",
                "content": "Just came across this from their head of DX: https://x.com/romainhuet/status/1814054938986885550?t=AMFK4svMvCluYqAXUqRDMQ&s=19"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:23:45.044000+00:00",
                "content": "Yeah we saw that, def misleading -- still useful for our purposes since we feed both images and text to inference (the latter of which will still cost less)"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:24:46.091000+00:00",
                "content": "And ultimately the big differentiator for us is output token limit / cost since we would consistently hit the limit without prompt engineering for large input invoices"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-01 22:27:20.194000+00:00",
                "content": "... actually unless the text tokenizer is ALSO meaningfully different? I think (hope) that isn't the case"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-02 02:26:06.681000+00:00",
                "content": "Conclusion: 4o mini had markedly worse performance at vision extraction, so we switched back to 4o and had zero problems after some prompt engineering -- currently going to stick with 4o because we have a bunch of credits and it's been proven to be good so far, but might switch to sonnet later"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 02:46:34.881000+00:00",
                "content": "Niice, thanks for the update ðŸ™‚"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 02:47:10.915000+00:00",
                "content": "And yeah not sure what tokenizer they used for each model, wish theyd be more â€œopenâ€ about it"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-02 18:39:33.980000+00:00",
                "content": "Lol"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-02 18:40:35.694000+00:00",
                "content": "Also just launched this on linkedin with a bigger shoutout than last time lol: https://www.linkedin.com/posts/ashwin-a-kumar_how-i-deleted-1000-lines-of-code-this-week-activity-7225200892008972288-9dg5?utm_source=share&utm_medium=member_desktop"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 18:41:11.008000+00:00",
                "content": "amazingggg thanks for the shoutout, will repost this everywhere haha"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-02 18:41:19.291000+00:00",
                "content": "Haha of course!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 18:41:53.574000+00:00",
                "content": "love the short and sweet one-liner \"BAML, a domain-specific language for prompting LLMs\""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-02 18:42:06.538000+00:00",
                "content": "4o-mini and 4o both use the o200k tokenizer"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-08-02 18:42:38.570000+00:00",
                "content": "if you dig into the tiktoken library you can see source code implying as much"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 19:01:56.321000+00:00",
                "content": "fyi <@1173670148155125774> we do have a map<string, string> type now, not sure if you saw it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 19:04:57.370000+00:00",
                "content": "btw do you validate the line-items match the total? That's a really good way to ensure the data extracted is 100% accurate"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-02 19:15:20.191000+00:00",
                "content": "Yeah I saw that, still wanted to strongly type the metadata to avoid hallucinations since we do have an organization-level metadata schema"
            },
            {
                "author": "ashwin.a.kumar",
                "timestamp": "2024-08-02 19:15:50.667000+00:00",
                "content": "And yep that's a step we take in post-processing, if they don't match we add an \"Unknown\" line item with the difference and tell the user"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-02 22:29:52.988000+00:00",
                "content": "<@1173670148155125774> our example apps are all here: https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter"
            }
        ]
    },
    {
        "thread_id": 1269446726910214194,
        "thread_name": "Next js env vars",
        "messages": [
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:08:04.875000+00:00",
                "content": "having trouble with env vars in a next.js (typescript) project.\n```\nError: client GPT4o could not resolve options.api_key\n```\n\nclients.baml:\n```\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n```\n\nfile in question:\n```\nimport dotenv from 'dotenv'\ndotenv.config()\n\nconsole.log(process.env.OPENAI_API_KEY)\n\nimport { b } from '../../../baml_client'\n```\nthe correct key is logged to the console, so env vars are being set.\n\n*edit: it also works correctly if i hardcode the api key in the clients.baml file, so the issue is `env.OPENAI_API_KEY` in that baml file\n\nhas anyone had issues with next.js projects? i had another path alias issue with generated baml files, so not sure if there is a bigger issue going on here..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 00:12:11.727000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 00:12:12.388000+00:00",
                "content": "<@201399017161097216> this looks like the same dotenv issue. We should file a bug. Recommended fix is to use dotenv cli not the dotenv package"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:14:47.328000+00:00",
                "content": "How do you start your local nextjs server? Try using dotenv cli before that server start command to inject the vars. Ill look into this tomorrow"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:16:04.538000+00:00",
                "content": "thanks guys, `dotenv -e .env -- npx vitest <relative path>` works for me"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:16:12.615000+00:00",
                "content": "appreciate the quick response"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:17:00.407000+00:00",
                "content": "Sweeet. Let us know if you have any suggestions on improvements or feature ideas!"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:21:37.651000+00:00",
                "content": "will do!\n\nnot sure if this is related, but i had a path alias issue when using `import { b } from '@/baml_client'`\n\ntsconfig.json:\n```\n\"paths\": {\n      \"@/*\": [\"./*\"]\n    }\n```\nide has no issues with that, but i get this error when testing it:\n`Error: Failed to load url @/baml_client (resolved id: @/baml_client)`\n\nall my other standard imports seem to work correctly.\n\ni resolved it by just using relative path references"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:34:05.114000+00:00",
                "content": "Is baml_client in the same level in the filetree as app/ or public/?"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:36:45.084000+00:00",
                "content": "it's at the project root. i tried moving it inside /src but baml-generate couldn't find it"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:39:55.444000+00:00",
                "content": "specifically:\n```\nError generating clients\n/Users/.../node_modules/@boundaryml/baml/cli.js:5\n  baml.invoke_runtime_cli(process.argv.slice(1))\n       ^\n\nError: \"./baml_src\" does not exist (expected a directory containing BAML files)\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:40:34.021000+00:00",
                "content": "Can you try â€œâ€”from <the path>â€ in the cli?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:41:55.730000+00:00",
                "content": "You can also specify where baml_client is generated via the â€œoutput_typeâ€ in the generator"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:42:10.734000+00:00",
                "content": "Sorry â€œoutput_dirâ€"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:44:08.176000+00:00",
                "content": "i might be misunderstanding\n\nnpm run baml-generate --from ./src\n```\n> <project> baml-generate\n> baml-cli generate ./src\n\nerror: unexpected argument './src' found\n\nUsage: baml-cli generate [OPTIONS]\n\nFor more information, try '--help'.\n```\n\nnpm run baml-generate --help\n```\nRun arbitrary package scripts\n\nUsage:\nnpm run-script <command> [-- <args>]\n\nOptions:\n[-w|--workspace <workspace-name> [-w|--workspace <workspace-name> ...]]\n[-ws|--workspaces] [--include-workspace-root] [--if-present] [--ignore-scripts]\n[--foreground-scripts] [--script-shell <script-shell>]\n\naliases: run, rum, urn\n\nRun \"npm help run-script\" for more info\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 00:44:43.650000+00:00",
                "content": "The â€”from field should point to the directory of baml_src"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 00:45:00.366000+00:00",
                "content": "Weâ€™ll be working on improving our docs for this. Good catch!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:45:18.271000+00:00",
                "content": "`baml-cli generate --from ./src`"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:49:11.556000+00:00",
                "content": "`--from` is still giving `error: unexpected argument`\n\nbut changing `output_dir \"../src\"` does output correctly, but for some reason i'm still getting\n`Error: Failed to load url @/baml_client (resolved id: @/baml_client)`\n\neven though the imports work correctly in the ide with the correct tsconfig path aliases"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:49:31.906000+00:00",
                "content": "i'll just keep it at the project root and use relative path references"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:50:14.344000+00:00",
                "content": "is your project setup using create-next-app or is it using some other build system like vite etc?"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:50:31.152000+00:00",
                "content": "yea, create-next-app"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:52:23.226000+00:00",
                "content": "can you run `baml-cli --version` or `npx baml-cli --version`"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:53:12.530000+00:00",
                "content": "ah, i just noticed i've been using `npm run baml-generate` (from the docs)"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:53:20.003000+00:00",
                "content": "actually don't have baml-cli installed *edit: globally, it works in package scripts"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 00:53:42.962000+00:00",
                "content": "what version is your @boundaryml/baml package?"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 00:54:40.225000+00:00",
                "content": "`\"@boundaryml/baml\": \"^0.52.1\",`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 01:00:04.739000+00:00",
                "content": "hmm strange, maybe try using it with npx. Our tsconfig is the same as the one you have so not sure why it's not resolving at the moment https://github.com/BoundaryML/baml-examples/blob/main/nextjs-starter/tsconfig.json , https://github.com/BoundaryML/baml-examples/blob/main/nextjs-starter/app/actions/streamable_objects.tsx#L3 will have to see if i can repro in another way"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-04 01:01:37.465000+00:00",
                "content": "if you do get import aliases working for other TypeScript directories but not the baml_client something must be weird since baml_client should be like any other TS directory"
            },
            {
                "author": "swabbie.eth",
                "timestamp": "2024-08-04 05:15:41.064000+00:00",
                "content": "iâ€™ll run some more tests soon. itâ€™s working well enough as it is but it would be nice to understand if itâ€™s just a simple issue or unrelated. thanks for your help!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 05:53:51.647000+00:00",
                "content": "ah yes. (apologies in advance for the word volmit, answer is at the bottom)\n\nin if you're using `npm run baml-generate` then you need to edit the package.json file to modify the `baml-generate` script.\n\n`baml-cli` is actually available in `./node_modules/.bin/baml-cli` which comes after you install `@boundaryml/baml`\n\nthe script you added in auto runs:\n```\nbaml-cli generate\n```\nfor you.\n\nYou can modify that script to add additional args like `--from path/to/baml_src`\n\nif you are using npm you need to do something like the following:\n```\nnpm run baml-generate -- --from path/to/baml_src\n```\n\nor update the script to:\n```\n\"scripts: {\n  \"baml-generate\": \"baml-cli generate -- --from path/to/baml-src\"\n}\n```\n\n(npm requires teh extra `--` , if you use pnpm or yarn you don't need the extra `--`)\n\nThe reason the base command is likely not working as you'd expect is that you need to modify the generator itself. then you shoudn't need to modify the command.\n\nUpdate the generator as such:\n```\ngenerator target {\n    output_type \"typescript\"\n    \n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../src\"\n    default_client_mode \"async\"\n    version \"0.50.0\"\n}\n```\n\nThen `baml_client` will be generated inside of the `<root>/src` folder.\n\nto do this, `baml_src` should be a sibling to `src`. Path to `baml_src` should be `<root>/baml_src`."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 05:53:56.577000+00:00",
                "content": "Hope this helps!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-04 05:54:07.035000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#define-a-generator-clause"
            }
        ]
    },
    {
        "thread_id": 1270828445739253872,
        "thread_name": "Vercel is down!",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 19:38:32.315000+00:00",
                "content": "Is the boundary dashboard down?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 19:40:29.656000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 19:40:29.921000+00:00",
                "content": "We're hosted on vercel and it seems they are down\nhttps://www.vercel-status.com/"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-07 19:40:44.300000+00:00",
                "content": "we'll be dropping logs during this window sadly"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-07 19:41:27.354000+00:00",
                "content": "* not necessarily, ill test out our endpoint"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-08-07 19:45:54.205000+00:00",
                "content": "looks like it's back"
            }
        ]
    },
    {
        "thread_id": 1272263337446608978,
        "thread_name": "Versio nmismatch",
        "messages": [
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 18:40:17.157000+00:00",
                "content": "Hey might be a silly question, but I'm trying to setup baml in my node project, but running into a runtime error: `TypeError: this.ctx_manager.get is not a function`.\n\nLooking at the code that gets generated in the baml client:\n```\nclass BamlStreamClient {\n  constructor(private runtime: BamlRuntime, private ctx_manager: BamlCtxManager) {}\n\n  \n  ExtractEvaluation(\n      evaluation_text: string\n  ): BamlStream<(Partial<DesignEval> | null), DesignEval> {\n    const raw = this.runtime.streamFunction(\n      \"ExtractEvaluation\",\n      {\n        \"evaluation_text\": evaluation_text,\n      },\n      undefined,\n      this.ctx_manager.get(),\n    )\n    return new BamlStream<(Partial<DesignEval> | null), DesignEval>(\n      raw,\n      (a): a is (Partial<DesignEval> | null) => a,\n      (a): a is DesignEval => a,\n      this.ctx_manager.get(),\n    )\n  }\n  ```\n\nit seems different than the examples I'm seeing in github\n\n```class BamlStreamClient {\n  constructor(private runtime: BamlRuntime, private ctx_manager: BamlCtxManager) {}\n\n  \n  ExtractResume(\n      resume: string,\n      __baml_options__?: { tb?: TypeBuilder }\n  ): BamlStream<RecursivePartialNull<Resume>, Resume> {\n    const raw = this.runtime.streamFunction(\n      \"ExtractResume\",\n      {\n        \"resume\": resume\n      },\n      undefined,\n      this.ctx_manager.cloneContext(),\n      __baml_options__?.tb?.__tb(),\n    )\n    return new BamlStream<RecursivePartialNull<Resume>, Resume>(\n      raw,\n      (a): a is RecursivePartialNull<Resume> => a,\n      (a): a is Resume => a,\n      this.ctx_manager.cloneContext(),\n      __baml_options__?.tb?.__tb(),\n    )\n  }\n  \n}```\nI'm using version 0.53.0 in my package and have baml-client CLI version of 0.20.0.  Not sure there's a compatibility issue or if there's more setup required? Any help is appreciated"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:41:35.608000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:41:35.868000+00:00",
                "content": "Hi Erik, can you uninstall and reinstall the @boundaryml/baml package and try rerunning `npx baml-cli --version` ?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:42:21.849000+00:00",
                "content": "and is your VSCode extension version 0.53.0 as well?"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 18:43:58.756000+00:00",
                "content": "yea tried that before and got the same results, and yea it is. let me try again.  A prompt comes up asking to choose a version of \"@boundaryml/baml-darwin-x64\", is there a specific version of that i should choose?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:44:27.411000+00:00",
                "content": "can you paste what the prompt is asking?"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 18:44:43.528000+00:00",
                "content": "```\n? Please choose a version of \"@boundaryml/baml-darwin-x64\" from this list: (Use arrow keys)\nâ¯ 0.0.22 \n  0.0.21 \n  0.0.20 \n  0.0.18 \n  0.0.17 \n  0.0.15 \n  0.0.14 \n  0.0.13 \n  0.0.9 \n  0.0.6 \n  0.0.5 \n  0.0.4 \n  0.0.3 \nCouldn't find any versions for \"@boundaryml/baml-win32-arm64-msvc\" that matches \"0.53.0\"\n? Please choose a version of \"@boundaryml/baml-win32-arm64-msvc\" from this list: (Use arrow keys)\nâ¯ 0.0.22 \n  0.0.21 \n  0.0.20 \n  0.0.18 \n  0.0.17 \n  0.0.15 \n  0.0.14 \n  0.0.13 \n  0.0.9 \n  0.0.6 \n  0.0.5 \n  0.0.4 \n  0.0.3 \n  0.0.2 \n  0.0.1 \n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:45:16.277000+00:00",
                "content": "do you use macos, apple silicon chip, or intel chip?"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 18:45:27.657000+00:00",
                "content": "macos, intel chip"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:45:49.116000+00:00",
                "content": "ok one sec, let me take a look at our npm configs"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 18:46:41.959000+00:00",
                "content": "is there a required macos version? I'm still on Monterey (can't upgrade past because laptop is ~10 yrs old)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:47:31.912000+00:00",
                "content": "that should be fine, it seems our release process has an outdated binary published for macos intel chips, let me try and remedy that now and publish a new version"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 18:50:25.495000+00:00",
                "content": "oh ok gotcha, thank you I really appreciate it and the quick help! You guys are doing good here"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 18:51:03.748000+00:00",
                "content": "np! will update you once we publish the new version"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 21:47:55.110000+00:00",
                "content": "this should be fixed in like 30min or so! Will be published under version 0.53.1"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 21:57:51.383000+00:00",
                "content": "awesome! thank you so much"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 22:08:31.247000+00:00",
                "content": "0.53.1 is now published"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-11 22:47:34.724000+00:00",
                "content": "works now, thank you!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 22:47:59.364000+00:00",
                "content": "sweeet"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-11 22:48:06.642000+00:00",
                "content": "just curious, howd you hear about BAML?"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-13 00:26:25.439000+00:00",
                "content": "hey sorry just saw this, I met one of your founders at a pitch event"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-13 00:27:07.069000+00:00",
                "content": "nice"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 00:28:23.232000+00:00",
                "content": "ðŸ‘‹ðŸ¾ hi Erik! likely saw me blabbing about ðŸ™‚"
            },
            {
                "author": "erikth2355",
                "timestamp": "2024-08-13 00:37:57.821000+00:00",
                "content": "Yea met you at Dent AI, was with DesignScout"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-13 00:38:35.098000+00:00",
                "content": "ah yes ðŸ™‚ Glad to provided that context. Excited to see how the demos with design interviews comes around! (unless I'm mixing up Design Scout!)"
            }
        ]
    },
    {
        "thread_id": 1274084989281239132,
        "thread_name": "Baml generate",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:18:52.799000+00:00",
                "content": "Newbie question: I'm getting this error for python:\n\n`ModuleNotFoundError: No module named 'baml_client'`\n\nI believe I following the standard installation:\n`poetry run baml-cli init`\n\nNote that for this example, https://docs.boundaryml.com/docs/calling-baml/client-registry, `client_registry`\ngets imported fine."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:19:28.044000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:19:28.812000+00:00",
                "content": "Can you try running baml-cli generate first?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:20:18.257000+00:00",
                "content": "`Generated 1 baml_client`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:20:22.619000+00:00",
                "content": "Alternatively you can also just hit cmd + s in a baml file in vscode and that will do it for you as long as you have the baml extension installed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:20:31.582000+00:00",
                "content": "Yep! Now you should be able to run the code!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:21:09.344000+00:00",
                "content": "Basically we generate python code that gives you the interface you need to use the underlying baml functions natively in python."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:21:51.949000+00:00",
                "content": "If you change your baml files, you need to regenerate, which is why we recommend the extension as it runs it for you automatically!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:22:08.598000+00:00",
                "content": "I have installed the VScode extension.."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-16 19:22:44.205000+00:00",
                "content": "are you on a windows machine?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:22:48.774000+00:00",
                "content": "Do I put my own baml file inside the src folder?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:22:53.793000+00:00",
                "content": "Yep!"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:23:27.474000+00:00",
                "content": "I'm on a ubuntu machine"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:23:35.390000+00:00",
                "content": "The extension only updates baml_client automatically when you save a baml file"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:24:57.410000+00:00",
                "content": "I have the following code:\n\n`client<llm> MyClient {\n  provider openai\n  options {\n    api_key env.SEEKR_API_KEY\n    model \"meta-llama/Meta-Llama-3-70B-Instruct\"\n    base_url \"http://l...seekr.com:8001/v1/inference\"\n    temperature 0.1\n  }\n}\n\nclass Joke {\n    setup string @description(\"\"The setup of the joke\")\n    punchline string @description(\"The punchline to the joke\")\n}\n\nfunction ExtractJoke(text: string) -> Joke {\n  // LLM client with params you want (not pictured)\n  client MyClient\n\n  // BAML prompts use Jinja syntax\n  prompt #\"\n    Tell me a joke about cats, and return a structured representation of the data in the schema below.\n\n    Joke:\n    ---\n    {{ text }}\n    ---\n\n    {# special Jinja macro to print the output instructions. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 19:26:18.072000+00:00",
                "content": "I'm getting the following error:\n\n`Error generating clients\nTraceback (most recent call last):\n  File \"/home/akhaled/.cache/pypoetry/virtualenvs/rag-engine-FvHXmK8v-py3.10/bin/baml-cli\", line 8, in <module>\n    sys.exit(invoke_runtime_cli())\nbaml_py.BamlError: error: Error validating: This line is not a valid field or attribute definition. A valid class property looks like: 'myProperty string[] @description(\"This is a description\")'\n  -->  ./baml_src/baml_exp.baml:12\n   | \n11 | class Joke {\n12 |     setup string @description(\"\"The setup of the joke\")\n13 |     punchline string @description(\"The punchline to the joke\")\n   | \n`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:27:35.535000+00:00",
                "content": "Youâ€™ve got extra quotes on your description ðŸ™‚ â€œâ€the set upâ€¦â€"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 19:29:16.423000+00:00",
                "content": "Would have been sick if we could provide a better error, but compiler errors are hard ðŸ˜¦"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-08-16 21:17:42.107000+00:00",
                "content": "update: this cat joke example from langchain, https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/#openai, previously was failing to provide any output with the existing setup. I'm able to get a valid output (joke) using baml now ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-16 21:18:05.875000+00:00",
                "content": "yay!!! ðŸ˜„"
            }
        ]
    },
    {
        "thread_id": 1274836321948663939,
        "thread_name": "existing classes on dynamic types",
        "messages": [
            {
                "author": ".tonylll",
                "timestamp": "2024-08-18 21:04:24.466000+00:00",
                "content": "When dynamically creating classes in python hsing add_class is there a way to add an existing class as a property for the new type? The add_property method seems to need a fieldtype but not sure how to get that from the existing â€œnon dynamic â€œ type?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 04:02:13.566000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 04:02:14.075000+00:00",
                "content": "Let me try a quick thing and make sure it works. But this is an interestingt case i think i must've missed!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 04:08:46.254000+00:00",
                "content": "```\ncls = tb._tb.class_(\"<class_name>\").field()\n\nmy_new_class.add_property(\"name\", cls)\n```\n\nyou can then pass that in! But i'll file a bug so we can improve those ergonomics. Using the inner `_tb` is def a design bug on our end."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 04:10:28.941000+00:00",
                "content": "alternatively, you can mark the inner class as `@dynamic` and then you'll be able to use it more ergonomically - though it will technically be incorrect since it shouldn't be dynamic.\n\n```\nmy_new_class.add_property(\"name\", tb.YourClass.type())\n```"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 09:48:29.408000+00:00",
                "content": "nice, yeh will play with these ideas"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 10:51:52.218000+00:00",
                "content": "yeh this is cool, I made the inner class dynamic and used your alternative solution. Congrats on this project by the way, we've built similar things but I do like yours"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 10:52:18.452000+00:00",
                "content": "awesome, I'm glad it worked for you!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 10:52:45.521000+00:00",
                "content": "curious what did you build and anything you think we should add to BAML that you guys added?"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 11:26:47.627000+00:00",
                "content": "ive been looking at refactoring a pre-release report builder featue using baml, we have a different approach (xml based) to get reliable structured and annotated data. Im really interested in using the dynamic flexibility to adapt our flows a little"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 11:27:05.439000+00:00",
                "content": "so very focused on mixing dynamic classes with static classes"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 11:27:45.051000+00:00",
                "content": "one thing I would ask for is date types, doing this as the moment:\ndate string @description(#\"\n    A date in the format YYYY-MM-DD.\n  \"#)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 11:28:59.407000+00:00",
                "content": "ah yes. Datetypes are a great idea. We've been contemplating them for a bit (date vs datetime). \n\nFYI what we find works nicely is `@description(#\"ISO8601 format\"#)`"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 11:29:14.886000+00:00",
                "content": "ah thats cool, nice idea"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 11:29:41.848000+00:00",
                "content": "yea llms seem to handle that format quite well (even small ones)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 11:30:53.810000+00:00",
                "content": "also, as you port over code and see any impact, please share your findings over BAML vs XML. \n\nWhen we did an initial study, we found XML to be far less reliable, but could be completely wrong for certain use cases"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 13:06:54.966000+00:00",
                "content": "result = b.ReportGenerator(lease_document, {\"tb\": tb})\n\nif im running something like this with dynamic classes, whats the best way to get access to teh raw prompt? Can i get the raw prompt without running the query itself?"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 13:07:39.810000+00:00",
                "content": "actually also how do i get the raw reply?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 13:15:52.481000+00:00",
                "content": "we're working on an SDK to support that. a raw interface to everything. We'll have an ETA on that friday morning as we have a planning session scheduled for thurday with the team."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 13:16:29.134000+00:00",
                "content": "you can use th following env variable for debugabuility:\n\n```\nexport BAML_LOG=info\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 13:16:39.152000+00:00",
                "content": "that will dump things out to terminal"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 20:40:51.121000+00:00",
                "content": "Awesome, looking forward to seeing it. Would definitely err on providing as much as possible!"
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 20:46:29.143000+00:00",
                "content": "Token counting is another one"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:46:53.943000+00:00",
                "content": "yep, we have all the data we provide in promptfiddle + a bit more planned for this."
            },
            {
                "author": ".tonylll",
                "timestamp": "2024-08-19 20:48:15.001000+00:00",
                "content": "Cool. By the way what timezone are you in? Long hours"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:48:32.880000+00:00",
                "content": "PST ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:48:50.373000+00:00",
                "content": "I was just up late"
            }
        ]
    },
    {
        "thread_id": 1275086015752704151,
        "thread_name": "Hey team, I'm attempting to run BAML on",
        "messages": [
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 13:36:36.108000+00:00",
                "content": "Hey team, I'm attempting to run BAML on [Modal](https://modal.com/) and encountering some weird errors (works fine locally...)\n\n```  File \"/root/sentiment_analysis/process.py\", line 20, in sentiment_analysis\n    return b.AnalyseReview(comment, ctx.name, ctx.notes, '\\n\\n'.join(ctx.sample_reviews))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/sentiment_analysis/baml_client/sync_client.py\", line 82, in AnalyseReview\n    return coerce(mdl, raw.parsed())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/root/sentiment_analysis/baml_client/sync_client.py\", line 31, in coerce\n    return cls.model_validate({\"inner\": parsed}).inner # type: ignore\n           ^^^^^^^^^^^^^^^^^^\nAttributeError: type object 'AnalyseReviewReturnType' has no attribute 'model_validate'\n```\n\nAny idea what might be going wrong?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 13:38:15.939000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 13:38:16.499000+00:00",
                "content": "AttributeError: type object 'AnalyseReviewReturnType' has no attribute 'model_validate'\n\nthis implies that the wrong version of pydantic is installed there.\n\nWe base it off of `pydantic 2.0+` can you check what version you have installed there?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 13:40:03.152000+00:00",
                "content": "from claude:\n\nTo check the version of Pydantic installed on your system, you can use one of the following methods:\n\n1. Using Python:\n\nOpen a Python interpreter and run:\n\n```python\nimport pydantic\nprint(pydantic.__version__)\n```\n\n2. Using pip:\n\nOpen a terminal or command prompt and run:\n\n```\npip show pydantic\n```\n\nThis will display information about the installed Pydantic package, including its version.\n\n3. Using the command line (if Pydantic is installed):\n\n```\npython -c \"import pydantic; print(pydantic.__version__)\"\n```\n\nWould you like me to explain any of these methods in more detail?"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 13:42:43.603000+00:00",
                "content": "Ah, amazing! Thank you! User error. I forgot to explicitly specify pydantic so it fell back to the an ancient 1.10 version (why, modal?). Working perfectly now. Sorry, the error message threw me off and felt pydantic related but quite unhelpful."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 14:09:03.108000+00:00",
                "content": "no worries at all ðŸ™‚"
            }
        ]
    },
    {
        "thread_id": 1275183389535895563,
        "thread_name": "potential parsing bug",
        "messages": [
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 20:03:31.827000+00:00",
                "content": "Hey guys, I've noticed a weird scenario where BAML is _discarding_ a good result (in this screenshot the json response looks about perfect to my ðŸ‘€).... the streaming response (visually) appears to be populated and then the final output disappears. The same behaviour is present (for same input) when using the python sync client."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:04:43.455000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:04:43.753000+00:00",
                "content": "can you check if some other field which is required is missing?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:05:29.016000+00:00",
                "content": "usually whne we've seen these bugs, its due to an inner field not parsing correctly and hence we drop the the schema and return an empty array"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 20:07:33.864000+00:00",
                "content": "Ah! I see... one of those fields was an `Enum` and the LLM returned a response outside the specified set ðŸ¤”"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:08:35.391000+00:00",
                "content": "we should ideally provide better visibility around this, but sadly a backlog item for sure ðŸ˜¢"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 20:08:38.832000+00:00",
                "content": "For completness, it was `emotion` which was specified as:\n```enum Emotion {\n    Anger\n    Disgust\n    Fear\n    Joy\n    Sadness\n    Surprise\n    Disappointment\n    Neutral @description(#\"\n        Always choose \"neutral\" when the content is factual or does not express any detectable emotion.\n    \"#)\n}```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:09:07.421000+00:00",
                "content": "oh yes, in a scenario like this, i recommend adding Other option"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 20:09:19.789000+00:00",
                "content": "Yeah, I was surprised that BAML (playground, anyhow) reported it as \"Passed\""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:09:27.681000+00:00",
                "content": "or you can use symbol tuning"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:10:04.892000+00:00",
                "content": "see: baml_src/prompt-engineering/symbol-tuning.baml\non: https://www.promptfiddle.com/"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:10:33.382000+00:00",
                "content": "but also, i would give the llm an out, cause what if its not one of those sentiments?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:11:30.116000+00:00",
                "content": "yea, our passed atm only means that the tests ran and we were able to parse something.\n\nI think we can improve it once we release asserts for tests which will make passed much stronger"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 20:12:20.886000+00:00",
                "content": "Interesting technique! I've not heard of that, but will be sure to try. In this case I think I'll replace the enum with a string and some instructions. I suppose we want \"soft-adherence\" in this case (not sure if there is a prompt engineering term for this)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:12:45.603000+00:00",
                "content": "oh if you want soft adhrence you can do:\n\n`Emotion | string`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:13:19.057000+00:00",
                "content": "I feel like we should have a cookbook ðŸ˜‚"
            }
        ]
    },
    {
        "thread_id": 1275184006941380720,
        "thread_name": "404 errors",
        "messages": [
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 20:05:59.028000+00:00",
                "content": "Another problem we've noticed are these 404 errors(?):\n\n```baml_py.BamlError: LLM call failed: LLMErrorResponse { client: \"GPT4\", model: None, prompt: ..., request_options: {\"model\": String(\"gpt-4o-2024-08-06\")}, start_time: SystemTime { tv_sec: 1724075686, tv_nsec: 158735420 }, latency: 364.208411ms, message: \"Request failed: {\n  \\\"error\\\": {\n    \\\"message\\\": \\\"Engine not found\\\",\n    \\\"type\\\": \\\"invalid_request_error\\\",\n    \\\"param\\\": null,\n    \\\"code\\\": null\n  }\n}\", code: Other(404) }```\n\nAny idea what might trigger that? More generally, does anyone have any guidelines for best practice on error handling? _Should_ we wrap BAML calls to catch for rate limits, parse failures, other errors? Does BAML have an internal retry mechanism?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:07:35.553000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:07:35.900000+00:00",
                "content": "we do support retry policies and even more complex things like fallbacks (try a different model if one fails). \n\nhttps://docs.boundaryml.com/docs/snippets/clients/fallback\nhttps://docs.boundaryml.com/docs/snippets/clients/retry\n\nAs a general design, i would recommend putting a single try catch at the very top of your function that your webserver can handle"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 20:07:52.525000+00:00",
                "content": "so if it fails you give out some standard error message"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-19 20:39:32.685000+00:00",
                "content": "Other folks are running into the same error with openai intermittently. Seems like an isue on their end"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-19 20:40:03.921000+00:00",
                "content": "Or does this happen to you everytime?"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-19 21:39:16.818000+00:00",
                "content": "It seems intermittent for sure, but, I've not seen _this particular error_ before with marvin (not that I've been paying super close attention). We'll put some diagnostics around it and keep an eye on it."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-19 21:40:11.228000+00:00",
                "content": "if you can get the raw curl from the playground in BAML, you'll see that openai is directly returning that error code. Sadly it seems to have sprung up recently. (A few other customers have spotted it as well)"
            }
        ]
    },
    {
        "thread_id": 1275885679875723265,
        "thread_name": "Parsing bug",
        "messages": [
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 18:34:10.892000+00:00",
                "content": "Noticed in some initial testing through prompt fiddle that the parsed response dropped an element from a json array. But in the raw LLM response, it includes the correct output. Thought I would let y'all know"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-21 18:34:47.660000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-21 18:34:48.092000+00:00",
                "content": "can you share the links or screenshots?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-21 18:35:09.885000+00:00",
                "content": "Usually we've seen that the parsing worked, but some required fields in the inner ojbect were missing, so the object was dropped"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-21 18:40:00.639000+00:00",
                "content": "away from desk rn but yeah in this case it was an internal object within an internal array. Model 4o-mini"
            },
            {
                "author": "unsignedint.",
                "timestamp": "2024-08-21 22:41:52.612000+00:00",
                "content": "<@342454629021253632> like this? https://discord.com/channels/1119368998161752075/1253172325205934181/1275183389535895563"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-22 14:00:13.993000+00:00",
                "content": "Yeah, except in my case it was an array nested within an internal field, and it only dropped off the last element"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 14:15:18.351000+00:00",
                "content": "My gut says the last element was incompelte in some way (perhaps it didn't meet one of the criterias)? If you get a chance to find it again, i would love to take a look.\n\nParsing bugs are a P0 for us! and we'll fix them ASAP."
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-22 14:18:37.035000+00:00",
                "content": "Gotcha. Unfortunately the prompt-fiddle page was closed and the error lost, but will try to recreate"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 14:19:05.984000+00:00",
                "content": "no worries if its too much work!"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-22 19:08:24.029000+00:00",
                "content": "Ok, recreated the error. Ran the same test 30 times and seeing this error every time, except this time it only includes 1 element in the internal array (should be 4) in parsed response. Produces the same result regardless of raw response malformed or valid. Note that the raw response is correct most of the time"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 19:08:46.920000+00:00",
                "content": "can you screenshot it here?"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-22 19:10:59.273000+00:00",
                "content": "raw response is long but verified accurate. Note in parsed response only 1 element (collapsed)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 19:11:16.182000+00:00",
                "content": "can you share the prompt fiddle link with me?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 19:11:28.248000+00:00",
                "content": "alternatively can we hop on discord call and you can screenshare rq?"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-22 19:12:02.842000+00:00",
                "content": "It's in vs code. Happy to hop on a call"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-22 19:12:13.355000+00:00",
                "content": "awesome, I'm on the office hours channel"
            },
            {
                "author": "jawnathonjones",
                "timestamp": "2024-08-22 19:12:55.615000+00:00",
                "content": "one second"
            }
        ]
    },
    {
        "thread_id": 1277324182581153905,
        "thread_name": "return type string",
        "messages": [
            {
                "author": "bsachs10",
                "timestamp": "2024-08-25 17:50:16.663000+00:00",
                "content": "FYI - this might not be a bug at all and just expected behavior, but thought I'd share. When running these tests, the strings that were ultimately returned had quotation marks around them.\n\n```\nclass Message {\n    role string\n    content string\n}\n\n\nfunction Describe_Chatbot_Thread(messages: Message[]) -> string {\n  client GPT4o\n  prompt #\"\n    Your job is to create a 4-7 word title for a thread between a user and a chatbot. Below is the thread so far. Use that to create a title for this chat thread. \n    \n    --\n    {% for message in messages %}\n      {{ message.role }}: {{ message.content }}\n    {% endfor %}\n    --\n\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest Describe_Chatbot_Thread_Ukraine {\n  functions [Describe_Chatbot_Thread] \n  args {\n    messages [\n      { role: \"user\", content: \"Summarize the political situation that led to the recent conflict in Ukraine\" }\n    ]\n  }\n} \n```\n\nParsed LLM Response: `\"\"Origins of Ukraine's Recent Conflict\"\"`\n\nSometimes the quotation marks were not there ,and sometiems they were. Here's an example:\n\nI fixed this by adding `Do not include quotation marks or other formatting.` to the prompt, which seemed to work, but I also trimmed off quotation marks programmatically in my code just in case."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 18:26:22.531000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-25 18:26:23.021000+00:00",
                "content": "Thanks for flagging! yes, this is expected behavior. if your return type is a string, we do our best to not be fancy and just give you the raw text. If you're already stripping the quotes out programatically (yay!), then you don't need to spend the extra tokens for `Do not include quotation marks or other formatting. ` ðŸ™‚"
            }
        ]
    },
    {
        "thread_id": 1277439234575171635,
        "thread_name": "Npm not building",
        "messages": [
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:27:27.196000+00:00",
                "content": "(Feel free to let me know if I am posting in the wrong place!)\n\nI'm suddenly seeing this error in my console and am unable to run locally  (NextJS + BAML). Not seen this before. I tried to run baml-generate to make sure all was well, and it generated without errors. But when I load a page of my NextJS app that references any `b` server action file, even if it doesn't invoke the action, the page fails to load. I bet I'm missing something obvious here!\n`Error: Failed to load native binding`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:34:08.207000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:34:11.341000+00:00",
                "content": "Can you try npm uninstall and install again? Havenâ€™t seen this error before"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:37:11.407000+00:00",
                "content": "Alternatively if itâ€™s a new node app you just started, I think you have to update the nextjs config file to allow for native node bindings. If you look at our typescript docs it links to a typescript project which has the updated nextjs config file as well."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:37:22.125000+00:00",
                "content": "(On my phone so unable to link)"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:38:34.165000+00:00",
                "content": "Tried clearing node_modules entirely and reinstalling. Nothing."
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:38:46.371000+00:00",
                "content": "I checked next config. Looks fine"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:39:50.177000+00:00",
                "content": "This is a weird one. I isolated the page causing the issue, which is a server actions file. It has a `b` call in it, and when I comment out the line ( `await b.Describe_Chatbot_Thread(messages)`) so `b` isn't referenced, the error goes away. I'm not calling the code, but just having it in there causes Next to compile BAML I guess, which throws the error. But oddly enough when I use that same await on another working page, it is fine. \n\nI'll have to do a few more tests to isolate this bad boy."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:40:23.877000+00:00",
                "content": "Hmm. Iâ€™ll be home in 30. Can help diagnose then!"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:40:36.313000+00:00",
                "content": "Hoepfully I have it fixed by then! Otherwise it'll be a long night ðŸ™‚"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:40:44.176000+00:00",
                "content": "Don't let me ruin yours"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:41:15.970000+00:00",
                "content": "Haha. Iâ€™ll be working anyways then! But I do hope itâ€™s able to be fixed ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 01:41:33.441000+00:00",
                "content": "Do you see any syntax errors in baml_client"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:46:53.157000+00:00",
                "content": "No errors in baml_client"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:47:08.664000+00:00",
                "content": "I'll keep looking. Must be something I did."
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:50:42.226000+00:00",
                "content": "FYI - logs"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 01:59:02.293000+00:00",
                "content": "Update: This message (about it not throwing an error one very page that references `b`) was incorrect. The test page I made just didn't call any server actions so it didn't trigger the error.\n\nBasically, if a client page loads and runs a server action, even one that does absolutely nothing, but that same server action file has a baml function in use such that it forces the page to compile, that triggers the fatal error. \n\nI tried commenting out recent changes to BAML src too. So far, that didn't help either."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:04:14.020000+00:00",
                "content": "ok back!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:04:35.080000+00:00",
                "content": "i can hop on zoom or discord call to help. Might be faster than copying and pasting logs"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 02:04:56.768000+00:00",
                "content": "I mean, if you're open to it, but I feel bad roping you into this!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:05:32.039000+00:00",
                "content": "sent over a zoom link via DM"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-08-26 02:25:41.680000+00:00",
                "content": "issue was solved! just a minor bug in the nextjs.config file"
            },
            {
                "author": "bsachs10",
                "timestamp": "2024-08-26 02:26:33.059000+00:00",
                "content": "thank you!"
            }
        ]
    },
    {
        "thread_id": 1278932241195733002,
        "thread_name": "Hey all, I'm trying to run BAML in prod",
        "messages": [
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:20:07.713000+00:00",
                "content": "Hey all, I'm trying to run BAML in prod with SST (AWS Lambdas) I've got them configured to run with `x86_x64` and install the baml layer, however I'm running into this error:\n\n```\nUnhandled Promise Rejection    {\n  \"errorType\": \"Runtime.UnhandledPromiseRejection\",\n  \"errorMessage\": \"Error: Failed to load native binding\",\n  \"reason\": {\n    \"errorType\": \"Error\",\n    \"errorMessage\": \"Failed to load native binding\",\n    \"stack\": [\n      \"Error: Failed to load native binding\",\n      \"    at Object.<anonymous> (/var/task/node_modules/@boundaryml/baml/native.js:359:11)\",\n      \"    at Module._compile (node:internal/modules/cjs/loader:1358:14)\",\n      \"    at Module._extensions..js (node:internal/modules/cjs/loader:1416:10)\",\n      \"    at Module.load (node:internal/modules/cjs/loader:1208:32)\",\n      \"    at Module._load (node:internal/modules/cjs/loader:1024:12)\",\n      \"    at Module.require (node:internal/modules/cjs/loader:1233:19)\",\n      \"    at require (node:internal/modules/helpers:179:18)\",\n      \"    at Object.<anonymous> (/var/task/node_modules/@boundaryml/baml/index.js:4:16)\",\n      \"    at Module._compile (node:internal/modules/cjs/loader:1358:14)\",\n      \"    at Module._extensions..js (node:internal/modules/cjs/loader:1416:10)\"\n    ]\n  },\n  \"promise\": {},\n  \"stack\": [\n    \"Runtime.UnhandledPromiseRejection: Error: Failed to load native binding\",\n    \"    at process.<anonymous> (file:///var/runtime/index.mjs:1276:17)\",\n    \"    at process.emit (node:events:519:28)\",\n    \"    at emitUnhandledRejection (node:internal/process/promises:250:13)\",\n    \"    at throwUnhandledRejectionsMode (node:internal/process/promises:385:19)\",\n    \"    at processPromiseRejections (node:internal/process/promises:470:17)\",\n    \"    at process.processTicksAndRejections (node:internal/process/task_queues:96:32)\"\n  ]\n}\n```\n\nI'm guessing my layer is not getting built correctly. <@201399017161097216> have you seen this?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:20:44.889000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:20:45.082000+00:00",
                "content": "can you paste your sst config?"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:21:12.925000+00:00",
                "content": "That might be kinda hard, it's pretty big and spread out"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:21:20.684000+00:00",
                "content": "ah one sec"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:21:42.933000+00:00",
                "content": "Oh, I might be building the lambda layer incorrectly"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:21:51.412000+00:00",
                "content": "Should I be using this script?\nhttps://github.com/BoundaryML/baml-examples/blob/main/node-aws-lambda-sst/prepare-layer.js"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:22:10.759000+00:00",
                "content": "yeah"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:22:20.571000+00:00",
                "content": "so it will add the native module into the zip file"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:22:35.783000+00:00",
                "content": "ah ha"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:22:44.745000+00:00",
                "content": "Let me try that"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:23:58.360000+00:00",
                "content": "yeah it's a bit of a manual process but once the layer zip working you shouldnt need to change it much afterwards unless you update baml (or you can make this part of your build scripts)"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:24:36.177000+00:00",
                "content": "I'm using seed.dev (Build by the SST team) to deploy my sst apps. So I'll have to check to see what type of things I can change during the CI process"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:26:03.922000+00:00",
                "content": "Hmm.. getting this locally"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:26:34.967000+00:00",
                "content": "you use pnpm right?"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:26:39.718000+00:00",
                "content": "yeah"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:26:42.784000+00:00",
                "content": "in a monorepo"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:28:07.842000+00:00",
                "content": "maybe try this version:"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:28:16.769000+00:00",
                "content": "Changed all the npm run commands in the script to pnpm, seems to be working"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:28:59.598000+00:00",
                "content": "nice, yeah you can just open up the layer and make sure it has the right structure and the binary is there"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:29:41.323000+00:00",
                "content": "Almost made it...\n```\n\nDone in 4.4s\nRemoved @boundaryml/baml-linux-x64-gnu from package.json and node_modules\nnode:fs:1633\n  const stats = binding.lstat(\n                        ^\n\nError: ENOENT: no such file or directory, lstat '/Users/seawatts/src/github.com/co-founder/mainsail/node_modules/@boundaryml/baml-linux-x64-gnu'\n    at lstatSync (node:fs:1633:25)\n    at getStatsSync (node:internal/fs/cp/cp-sync:120:19)\n    at checkPathsSync (node:internal/fs/cp/cp-sync:72:33)\n    at cpSyncFn (node:internal/fs/cp/cp-sync:58:42)\n    at Object.cpSync (node:fs:3055:3)\n    at copyPackage (file:///Users/seawatts/src/github.com/co-founder/mainsail/prepare-baml-layer.js:74:6)\n    at file:///Users/seawatts/src/github.com/co-founder/mainsail/prepare-baml-layer.js:79:1\n    at ModuleJob.run (node:internal/modules/esm/module_job:222:25)\n    at async ModuleLoader.import (node:internal/modules/esm/loader:316:24)\n    at async asyncRunEntryPointWithESMLoader (node:internal/modules/run_main:123:5) {\n  errno: -2,\n  code: 'ENOENT',\n  syscall: 'lstat',\n  path: '/Users/seawatts/src/github.com/co-founder/mainsail/node_modules/@boundaryml/baml-linux-x64-gnu'\n}\n\nNode.js v20.13.1\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:30:51.001000+00:00",
                "content": "it seems the script runs all the way no? and then that error apepars"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:30:58.616000+00:00",
                "content": "are you on linux?"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:31:06.838000+00:00",
                "content": "mac"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:31:20.037000+00:00",
                "content": "It fails at \n\n```\n\n// Copy the dependencies\ncopyPackage(\"baml-linux-x64-gnu\");\ncopyPackage(\"baml\");\n```"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:31:27.223000+00:00",
                "content": "I don't see a zip file in the folder"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:32:00.719000+00:00",
                "content": "Oh, I'm trying to run it locally, not in CI"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:32:08.741000+00:00",
                "content": "the CI environment is linux"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:32:12.861000+00:00",
                "content": "yeah that's fine"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:32:29.365000+00:00",
                "content": "it should still work if you run locally, one sec, it seems pnpm has diff structure on where node_modules lives"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:32:35.580000+00:00",
                "content": "I'm on a M1 Pro (Arm chip) if that matters"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:32:54.830000+00:00",
                "content": "Yeah, that's true"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:33:34.005000+00:00",
                "content": "you can try\n\n```\nfunction copyPackage(packageName) {\n  const pnpmStoreDir = execSync('pnpm store path', { encoding: 'utf8' }).trim();\n  const packageDir = path.join(pnpmStoreDir, 'node_modules', '@boundaryml', packageName);\n  \n  if (!fs.existsSync(packageDir)) {\n    console.error(`Package directory not found: ${packageDir}`);\n    return;\n  }\n\n  const destDir = path.join(layerNodeModulesDir, \"@boundaryml\", packageName);\n  fs.cpSync(packageDir, destDir, { recursive: true });\n  console.log(`Copied ${packageName} to layer`);\n}\n```"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:33:36.196000+00:00",
                "content": ""
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:34:02.181000+00:00",
                "content": "Trying now"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:34:23.438000+00:00",
                "content": "ðŸ¤£"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:34:46.313000+00:00",
                "content": "haha yeah basically you are forcing your machine to download the binary for linux temporarily for this script"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:34:56.240000+00:00",
                "content": "in CI you wouldnt need to do the cleanup / or the --force"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:35:22.712000+00:00",
                "content": "I see"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:35:31.113000+00:00",
                "content": "Progress was made:\n\n```\n\nDone in 3.2s\nRemoved @boundaryml/baml-linux-x64-gnu from package.json and node_modules\nCleaning up...\nâ€‰ERR_PNPM_CANNOT_REMOVE_MISSING_DEPSâ€‰ Cannot remove '@boundaryml/baml-linux-x64-gnu': no such dependency found\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:35:51.868000+00:00",
                "content": "you could probably ignore the cleanup IIRC"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:36:02.247000+00:00",
                "content": "It looks like the baml-layer.zip was put in `layers/baml-layer.zip` instead of `baml-layer/baml-layer/baml-layer.zip`"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:37:28.920000+00:00",
                "content": "Manually put it in there for now, pushing to prod, I'll keep you posted"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 04:37:43.241000+00:00",
                "content": "Waiting for Cloudformation ................ ðŸ¤£ðŸ¤¦â€â™‚ï¸"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:38:22.833000+00:00",
                "content": "yeah it should be layers/baml-layer.zip i believe. You can also try and run our SST example and just see what the layer looks like exactly (though it does use npm)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 04:39:06.635000+00:00",
                "content": "you do need to use nodejs20.x runtime btw"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:25:32.784000+00:00",
                "content": "Looks like it needs to be at `layers/baml-layer/baml-layer.zip`"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:25:57.209000+00:00",
                "content": "Got it building and deploying, however still facing the same issue. Looked inside the zip and it's empty"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 05:26:39.081000+00:00",
                "content": "So the original script isnt copying the binary? Can you print out the dirs at each step and not do the cleanup so you can see where the files are?"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:39:24.447000+00:00",
                "content": "Cursor ftw"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:39:27.099000+00:00",
                "content": "```\n\nfunction copyPackage(packageName) {\n  const packageDir = path.join(\n    \"node_modules\",\n    \".pnpm\",\n    `@boundaryml+${packageName}@${bamlVersion}`,\n    \"node_modules\",\n    \"@boundaryml\",\n    packageName,\n  );\n\n  if (!fs.existsSync(packageDir)) {\n    console.error(`Package directory not found: ${packageDir}`);\n    return;\n  }\n\n  const destDir = path.join(layerNodeModulesDir, \"@boundaryml\", packageName);\n  fs.mkdirSync(path.dirname(destDir), { recursive: true });\n  fs.cpSync(packageDir, destDir, { recursive: true });\n  console.log(`Copied ${packageName} to layer`);\n}\n```"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:39:29.742000+00:00",
                "content": "That works"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 05:39:52.801000+00:00",
                "content": "Sweeet"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:40:01.564000+00:00",
                "content": "I still need to have it put the zip file in `layers/baml-layer/baml-layer.zip`"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:40:14.864000+00:00",
                "content": "Otherwise it says it can't find it"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:40:27.873000+00:00",
                "content": "```\n\n  const bamlLayer = new LayerVersion(stack, \"BamlLayer\", {\n    code: Code.fromAsset(\"layers/baml-layer\"),\n  });\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 05:40:33.243000+00:00",
                "content": "Theres some paths at the top of the scripy ylu can modify"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:55:36.693000+00:00",
                "content": "Dang, still getting the same error"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 05:56:17.953000+00:00",
                "content": "Can you potentially create the layer manually in the aws console and see if it works?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 05:56:32.279000+00:00",
                "content": "Or download the existing thing that got uploaded and see if it has all the right files"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:57:00.832000+00:00",
                "content": "checking now"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 05:58:18.911000+00:00",
                "content": "Do you know how I can download the layer from the aws consol?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 05:58:55.233000+00:00",
                "content": "Yes one sec"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:01:52.751000+00:00",
                "content": "Found it"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:02:18.442000+00:00",
                "content": ""
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:02:28.220000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:03:07.108000+00:00",
                "content": "this is the layer i created in my example"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:03:13.323000+00:00",
                "content": ""
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:03:21.604000+00:00",
                "content": "Looks good to me"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:04:17.652000+00:00",
                "content": ""
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:04:22.301000+00:00",
                "content": ""
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:04:43.604000+00:00",
                "content": "Yeah, i've got all that"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:05:25.516000+00:00",
                "content": "can you get on a call?"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:07:53.349000+00:00",
                "content": "Not at the moment. Does tomorrow morning work?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:08:00.132000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:08:01.833000+00:00",
                "content": "yeah tomorrow works!"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:08:47.117000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:09:08.088000+00:00",
                "content": "you can also see if it works without a layer by uncomenting line 38 on the file i sent and using empty array: \n and setting installPackages to [@boundaryml/baml]"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:31:00.547000+00:00",
                "content": "Trying now"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:31:10.527000+00:00",
                "content": "Before that: here is my lambda code"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:31:20.324000+00:00",
                "content": "Should the layer be in there?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:32:04.009000+00:00",
                "content": "no, there shouldnt be any baml stuff in there:"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:32:31.597000+00:00",
                "content": "it seems you have installPackages not set to empty array"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:32:44.833000+00:00",
                "content": "nodejs.install: []"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:35:07.987000+00:00",
                "content": "Trying this now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:35:31.238000+00:00",
                "content": "you do need the external one set:\nhttps://github.com/BoundaryML/baml-examples/blob/main/node-aws-lambda-sst/stacks/ExampleStack.ts"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:36:50.010000+00:00",
                "content": "wack a mole lol"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:37:14.214000+00:00",
                "content": "Oh AWS......"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:37:18.624000+00:00",
                "content": "ðŸ¤¦â€â™‚ï¸"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:37:28.983000+00:00",
                "content": "Love AWS and hate AWS all at the same time"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:38:14.248000+00:00",
                "content": "haha yeah, that config shouuuld work!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:38:31.767000+00:00",
                "content": "ill be on tomorrow!"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:42:45.201000+00:00",
                "content": ""
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 06:42:58.945000+00:00",
                "content": "haha damnit"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:50:41.220000+00:00",
                "content": "It seems the lambda layer isnt there?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-08-30 06:50:55.922000+00:00",
                "content": "But either way we can hop on a call and resolve quicker tomorrow"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-08-30 07:00:08.849000+00:00",
                "content": "Sounds great, thanks for the prompt feedback and help!"
            }
        ]
    },
    {
        "thread_id": 1279885795615965305,
        "thread_name": "NExtJS issue",
        "messages": [
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:29:12.803000+00:00",
                "content": "hey friends, i'm trying to add in BAML to an existing Next.js project, following the example next.js codebase which works fine -- getting an error. Am I missing an import or something silly? ðŸ™‚"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:29:36.163000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:29:36.398000+00:00",
                "content": "taking a look..."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:30:41.308000+00:00",
                "content": "can you paste your nextjs.config.ts ?"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:31:16.125000+00:00",
                "content": "/** @type {import('next').NextConfig} */\nconst nextConfig = {\n    images: {\n        domains: ['outpost.chat'],\n    },\n    experimental: {\n        serverComponentsExternalPackages: [\"@boundaryml/baml-core\"]\n    },\n    webpack: (config, { dev, isServer, webpack, nextRuntime }) => {\n        config.module.rules.push({\n            test: /\\.node$/,\n            use: [\n                {\n                    loader: \"nextjs-node-loader\",\n                    options: {\n                        outputPath: config.output.path,\n                    },\n                },\n            ],\n        });\n        return config;\n    },\n    async rewrites() {\n        return [\n            ...\n        ];\n    },\n    async redirects() {\n        return [\n            {\n                ...\n    },\n};\n\nexport default nextConfig;"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:31:52.783000+00:00",
                "content": "assume you meant .mjs : )"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:32:06.718000+00:00",
                "content": "maybe discord removed a character but this should have a backslash:"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:32:55.595000+00:00",
                "content": ""
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:33:16.425000+00:00",
                "content": "looks like discord"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:34:02.613000+00:00",
                "content": "and you're on @boundaryml/baml 0.54.0 right?"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:34:26.850000+00:00",
                "content": ""
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:34:31.669000+00:00",
                "content": "ðŸ™‚"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:34:57.163000+00:00",
                "content": "do you have time for a Discord chat?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 19:35:10.620000+00:00",
                "content": "maybe it'll be faster to debug there. My next question is which package manager you use haha"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:36:05.605000+00:00",
                "content": "of course"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 19:36:08.973000+00:00",
                "content": "and npm"
            }
        ]
    },
    {
        "thread_id": 1279908155249262662,
        "thread_name": "access models",
        "messages": [
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 20:58:03.755000+00:00",
                "content": "n00b question, is there a way to access the models defined in the BAML file outside of the BAML file, or better yet define them and then import them? \n\nGot a Next.js app leveraging streaming and a very basic action to return the objects parsed, but want to carry through the same model without re-writing it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 20:58:53.348000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 20:58:53.760000+00:00",
                "content": "you can import them from baml_client\n\nimport { Resume } from \"@/baml_client\""
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 20:59:06.936000+00:00",
                "content": "ðŸ˜„ '"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 20:59:12.090000+00:00",
                "content": "thanks"
            }
        ]
    },
    {
        "thread_id": 1279933464363143199,
        "thread_name": "Vercel deployment",
        "messages": [
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 22:38:37.918000+00:00",
                "content": "have ya'll had success deploying out to vercel? `npm run build` works fine locally, tried updating vercel variables to match build scripts"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 22:39:06.772000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 22:39:07.195000+00:00",
                "content": "Can you change nodejs to 20?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 22:39:11.810000+00:00",
                "content": "In the project settings"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 22:39:19.997000+00:00",
                "content": "Lmk if it was already at 20"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 22:41:18.516000+00:00",
                "content": "already set at 20.x"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-01 22:42:20.918000+00:00",
                "content": "Hmm it seems it s trying to deploy to windows machines. Im currently out but will be back in an hour to unblock you"
            },
            {
                "author": "dean.coffee",
                "timestamp": "2024-09-01 23:00:04.967000+00:00",
                "content": "hehe no worries, thanks aaron ðŸ™‚"
            }
        ]
    },
    {
        "thread_id": 1280276046075396108,
        "thread_name": "baml with ruby",
        "messages": [
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-02 21:19:55.763000+00:00",
                "content": "Hey, I am unable to install baml in my rails app, can you please help? Here is the error log\n```Could not find gems matching 'baml (>= 0.38.0)' valid for all resolution platforms (aarch64-linux, arm-linux, arm64-darwin, x86-linux, x86_64-darwin, x86_64-linux) in rubygems repository https://rubygems.org/ or\ninstalled locally.\n\nThe source contains the following gems matching 'baml (>= 0.38.0)':\n  * baml-0.38.0-aarch64-linux\n  * baml-0.38.0-arm64-darwin\n  * baml-0.38.0-x86_64-darwin\n  * baml-0.38.0-x86_64-linux\n  * baml-0.39.0-aarch64-linux\n  * baml-0.39.0-arm64-darwin\n  * baml-0.39.0-x86_64-darwin\n  * baml-0.39.0-x86_64-linux\n  * baml-0.51.1-aarch64-linux\n  * baml-0.51.1-arm64-darwin\n  * baml-0.51.1-x86_64-darwin\n  * baml-0.51.1-x86_64-linux\n  * baml-0.52.0-aarch64-linux\n  * baml-0.52.0-arm64-darwin\n  * baml-0.52.0-x86_64-darwin\n  * baml-0.52.0-x86_64-linux\n  * baml-0.52.1-aarch64-linux\n  * baml-0.52.1-arm64-darwin\n  * baml-0.52.1-x86_64-darwin\n  * baml-0.52.1-x86_64-linux\n  * baml-0.53.0-aarch64-linux\n  * baml-0.53.0-arm64-darwin\n  * baml-0.53.0-x86_64-darwin\n  * baml-0.53.0-x86_64-linux\n  * baml-0.53.1-aarch64-linux\n  * baml-0.53.1-arm64-darwin\n  * baml-0.53.1-x86_64-darwin\n  * baml-0.53.1-x86_64-linux\n  * baml-0.54.0-aarch64-linux\n  * baml-0.54.0-arm64-darwin\n  * baml-0.54.0-x86_64-darwin\n  * baml-0.54.0-x86_64-linux```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:23:17.986000+00:00",
                "content": ""
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:23:18.332000+00:00",
                "content": "Hi there - can you run `ruby -e \"puts RbConfig::CONFIG['platform']\"` and let us know what you get?"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-02 21:23:37.441000+00:00",
                "content": "`arm64-darwin23`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:27:54.299000+00:00",
                "content": "Thanks for the report! Unfortunately it'll be a few days before we can look into this one specifically - we are, however, adding a new BAML-over-HTTP API that will allow you to expose your BAML functions over HTTP through `baml-cli dev` and then generate an OpenAPI client that calls it"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:27:58.854000+00:00",
                "content": "Do you think that would work for you?"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-02 21:38:00.674000+00:00",
                "content": "Whatever can get me started asap works."
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-02 21:38:27.669000+00:00",
                "content": "I'd prefer a ruby client because I want it to be triggered by my rails stack"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:38:53.380000+00:00",
                "content": "you'll be able to generate a ruby client!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:39:13.165000+00:00",
                "content": "We'll be releasing in the next few days, so I'll circle back on this thread when we do"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-02 21:40:40.730000+00:00",
                "content": "alrighty, i'll be waiting!"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-02 21:44:49.852000+00:00",
                "content": "can you mail me at sudhanshu@noscrubs.io in case I dont check discord"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:45:39.673000+00:00",
                "content": "We'll try!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 04:59:24.477000+00:00",
                "content": "Marked! Weâ€™ll reply over email as well!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 19:25:49.038000+00:00",
                "content": "<@533212392004386826> can you see if you can `bundle install` now? I was able to do it on baml 0.54.1+"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-03 20:05:13.366000+00:00",
                "content": "```\nFetching gem metadata from https://rubygems.org/........\nCould not find gems matching 'baml (= 0.54.1)' valid for all resolution platforms (aarch64-linux, arm-linux, arm64-darwin, x86-linux, x86_64-darwin, x86_64-linux) in rubygems repository https://rubygems.org/ or\ninstalled locally.\n\nThe source contains the following gems matching 'baml (= 0.54.1)':\n  * baml-0.54.1-aarch64-linux\n  * baml-0.54.1-aarch64-linux-musl\n  * baml-0.54.1-arm-linux\n  * baml-0.54.1-arm64-darwin\n  * baml-0.54.1-x86_64-linux\n  * baml-0.54.1-x86_64-linux-musl\n```\n\nnah, still see the same error"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-03 20:06:32.090000+00:00",
                "content": "why are you guys building platform specific releases? ðŸ¤” I have never seen that in 3yrs of working with rails"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-03 21:31:44.673000+00:00",
                "content": "Ah, i see we're still missing x86_64 darwin for you.\n\n\nAnd yea we need platform specific releases as BAML is actually implemented all in rust. we then generate bindings under the hood to make interface seamlessly with a language.\n\nThis is how we are able to gurantee the exact same performance and stability accross python, ts, and ruby (and soon via a REST client like the one sam mentioned above)."
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-03 23:25:04.474000+00:00",
                "content": "alright"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-04 19:41:07.919000+00:00",
                "content": "Alternatively: we don't want to be nokogiri ðŸ˜…"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-04 20:31:12.528000+00:00",
                "content": "lol"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-04 20:31:21.513000+00:00",
                "content": "lmk when you guys have something i can use"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-05 21:38:42.222000+00:00",
                "content": "anything yet? sorry for the push but I need to start working on this project and dont want to write all these transformations manually"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-05 21:40:22.130000+00:00",
                "content": "totally hear you! we're getting closer - we were hammering out some extreme awkwardness with the generated types yesterday"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-05 21:40:42.734000+00:00",
                "content": "will be pushing up a ruby example today and hopefully release in the next day or two"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-05 21:41:03.480000+00:00",
                "content": "here's the full status update:"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-05 21:43:15.953000+00:00",
                "content": "Just following up- we're getting closer and closer to the release! Expect it some time this week; we'll update this thread when we do.\n\nTo give you a sense of the current status, we're doing final passes now (e.g. reviewing generated code and making sure that we can preserve backwards compatibility for future releases).\n\nThings you can expect:\n\nwe've implemented OpenAPI codegen that can also run openapi-generator-cli for you automatically\nwe've implemented a server with hot reload that will re-generate everything when you edit BAML files\nwe've added not only quickstart docs, but also a docker-compose example (preview)\nwe've published example code for Go, Java, PHP, Rust at https://github.com/BoundaryML/baml-examples"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-05 21:43:41+00:00",
                "content": "I have told you guys how much i look forward to Baml a few months ago. But i was telling these guys (Langtail) in Jan this year to build what you guys have built."
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-05 21:46:53.182000+00:00",
                "content": "just throw a random idea at you, probably high level: \nBaml should be able to call an API given an openapi specification of an API - select the right API, populate structured data for parameters.\n\nthat's it, now i have praised you guys long enough - give me the library!!!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-05 21:47:41.977000+00:00",
                "content": "ðŸ«¡"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-06 11:54:44.326000+00:00",
                "content": "fwiw i tried installing baml via yarn and got a similar error\n`âž¤ YN0082: â”‚ @boundaryml/baml-win32-arm64-msvc@npm:0.54.2: No candidates found`\n\nNot sure why it is looking for win32 binaries when i am on m2 mac"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-06 12:00:11.136000+00:00",
                "content": "python worked fine"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-06 14:51:40.815000+00:00",
                "content": "thats odd, let me checkout the npm package and figure out whats happening. that package should be optional, so i think npm should work anyways even with that error, but we should fix that"
            },
            {
                "author": "sudhanshug",
                "timestamp": "2024-09-06 15:46:32.418000+00:00",
                "content": "npm works fine, i am using yarn berry"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-09 21:57:13.416000+00:00",
                "content": "<@533212392004386826> BAML 0.55.0 is out, which means you can now try out BAML from Ruby (without having to deal with the platform install issues)!\n\n(Emailing as well shortly)\n\n- [announcement](https://discord.com/channels/1119368998161752075/1119375433666920530/1282818005084016721)\n- [quickstart docs](https://docs.boundaryml.com/docs/get-started/quickstart/openapi)\n- [baml-examples for ruby-openapi](https://github.com/BoundaryML/baml-examples/tree/main/ruby-openapi-starter)"
            }
        ]
    },
    {
        "thread_id": 1280277876981104722,
        "thread_name": "Hey there, I'm running into an issue:",
        "messages": [
            {
                "author": "toobulkeh",
                "timestamp": "2024-09-02 21:27:12.285000+00:00",
                "content": "Hey there, I'm running into an issue:\n`â¨¯ ./node_modules/.pnpm/@boundaryml+baml@0.54.0/node_modules/@boundaryml/baml/async_context_vars.js:5:1\nModule not found: Can't resolve 'async_hooks'`"
            },
            {
                "author": "toobulkeh",
                "timestamp": "2024-09-02 21:27:38.650000+00:00",
                "content": ""
            },
            {
                "author": "toobulkeh",
                "timestamp": "2024-09-02 21:27:39.022000+00:00",
                "content": "I tried `pnpm add async_hooks`"
            },
            {
                "author": "toobulkeh",
                "timestamp": "2024-09-02 21:27:56.247000+00:00",
                "content": "Also tried to switch baml_client to sync mode"
            },
            {
                "author": "toobulkeh",
                "timestamp": "2024-09-02 21:36:02.910000+00:00",
                "content": "should BAML only be run in server components?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-02 21:45:26.632000+00:00",
                "content": "Yes, BAML only works in server components"
            }
        ]
    },
    {
        "thread_id": 1281178892727746615,
        "thread_name": "openai-generic",
        "messages": [
            {
                "author": "roze_sha",
                "timestamp": "2024-09-05 09:07:31.179000+00:00",
                "content": "im getting the following error:\n```shell\nPS C:\\Users\\Sha Roze\\Misc Work\\autoDBDoc\\autoDBDoc\\src\\generate-docs> npx baml-cli generate\nError generating clients\nC:\\Users\\Sha Roze\\Misc Work\\autoDBDoc\\autoDBDoc\\node_modules\\@boundaryml\\baml\\cli.js:5\n  baml.invoke_runtime_cli(process.argv.slice(1))\n       ^\n\nError: error: client provider openai-generic not found. Did you mean one of these: `openai`, `azure-openai`, `anthropic`?\n  -->  ./baml_src\\clients.baml:54\n   | \n53 | client<llm> Groq {\n54 |   provider openai-generic\n   | \n\n\n    at Object.<anonymous> (C:\\Users\\Sha Roze\\Misc Work\\autoDBDoc\\autoDBDoc\\node_modules\\@boundaryml\\baml\\cli.js:5:8)\n    at Module._compile (node:internal/modules/cjs/loader:1504:14)\n    at Module._extensions..js (node:internal/modules/cjs/loader:1588:10)\n    at Module.load (node:internal/modules/cjs/loader:1282:32)\n    at Module._load (node:internal/modules/cjs/loader:1098:12)\n    at TracingChannel.traceSync (node:diagnostics_channel:315:14)\n    at wrapModuleLoad (node:internal/modules/cjs/loader:215:24)\n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:158:5)\n    at node:internal/main/run_main_module:30:49 {\n  code: 'GenericFailure'\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:38:24.406000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 13:38:30.482000+00:00",
                "content": "hi! Can you check what version of BAML you're on? It should exist on 0.54+!"
            },
            {
                "author": "roze_sha",
                "timestamp": "2024-09-06 04:41:12.291000+00:00",
                "content": "@boundaryml/baml@0.54.0"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-06 14:52:44.866000+00:00",
                "content": "can you update to the latest one? I think 0.54.1 is the one that fixed this!\n\nhttps://github.com/BoundaryML/baml/blob/canary/CHANGELOG.md#0541---2024-09-03"
            }
        ]
    },
    {
        "thread_id": 1281265100933693513,
        "thread_name": "replit",
        "messages": [
            {
                "author": "brandburner",
                "timestamp": "2024-09-05 14:50:04.817000+00:00",
                "content": "I'm using Replit and `baml-cli generate` doesn't generate the baml_client folder. It completes and prints 'Generated 1 baml_client' but no folder there and `from baml_client.sync_client import b` won't resolve. any workarounds?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:51:25.868000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:51:26.276000+00:00",
                "content": "can you do:\n\nBAML_LOG=info baml-cli generate"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:51:33.095000+00:00",
                "content": "and see what it dumps out?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:51:54.109000+00:00",
                "content": "we are working on adding a few more default log statements there to help debug issues like this better"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:52:12.813000+00:00",
                "content": "alternatively, for this, it may be quicker to do a screenshare in the office hours channel"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:52:29.281000+00:00",
                "content": "(I'm online if you want to join that)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:54:46.013000+00:00",
                "content": "just tried and it appears to work for me!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 14:55:10.637000+00:00",
                "content": "```ts\n// generators.baml\n\n// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"python/pydantic\"\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.54.1\"\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    // Regardless of this setting, you can always explicitly call either of the following:\n    // - b.sync.FunctionName()\n    // - b.async_.FunctionName() (note the underscore to avoid a keyword conflict)\n    default_client_mode sync\n}\n```"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-05 15:07:44.992000+00:00",
                "content": "ahh thanks - that located the issue. It was generating the folder inside another subfolder from a previous run in this workspace. something must have been cached somewhere."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-05 15:08:22.416000+00:00",
                "content": "ðŸ‘ðŸ¾"
            }
        ]
    },
    {
        "thread_id": 1282056987860930671,
        "thread_name": "Catching",
        "messages": [
            {
                "author": "brandburner",
                "timestamp": "2024-09-07 19:16:45.373000+00:00",
                "content": "I'm trying to get prompt caching working with Claude but I'm doing something wrong ðŸ™‚ All my model requests fail with thiis error:\n\n`Request failed: {\\\"type\\\":\\\"error\\\",\\\"error\\\":{\\\"type\\\":\\\"invalid_request_error\\\",\\\"message\\\":\\\"allowed_role_metadata: Extra inputs are not permitted\\\"}}\", code: Other(400) `\n\ndoes this function and client definition look correct?\n\n`function ExtractEntities(dialogue: string) -> Entity[] {\n  client ClaudeWithCaching\n  prompt #\"\n    {{ _.role('system', cache_control={\"type\": \"ephemeral\"}) }}\n    You are an expert in analyzing drama scripts. Extract and describe the entities (characters, objects, locations) mentioned in the given dialogue.\n\n    {{ _.role('user') }}\n    Analyze the following dialogue and extract the entities:\n    {{ dialogue }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\nclient<llm> ClaudeWithCaching {\n  provider anthropic\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n    allowed_role_metadata [\"cache_control\"]\n    headers {\"anthropic-beta\" \"prompt-caching-2024-07-31\"}\n  }\n}`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:17:24.387000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:17:25.119000+00:00",
                "content": "What version of BAML are you on?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:17:57.224000+00:00",
                "content": "I think itâ€™s only on 0.54+"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-07 19:23:06.062000+00:00",
                "content": "ahh I see. I have 0.53.0"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-07 19:23:41.626000+00:00",
                "content": "Weâ€™ll start adding version to our features from now on"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-07 19:28:58.080000+00:00",
                "content": "I upgraded and so far so good - thanks so much!"
            }
        ]
    },
    {
        "thread_id": 1282760230169477171,
        "thread_name": "version mismatch",
        "messages": [
            {
                "author": "simontam0",
                "timestamp": "2024-09-09 17:51:11.405000+00:00",
                "content": "hi there, I seem to be having some conflicts in running the baml extension for vs code v0.54.1 and my baml-py v0.54.2. I'm seeing this my vs code. Any thoughts? Note that using extension v0.53.0 seems to be fine"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 17:52:51.689000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 17:52:51.942000+00:00",
                "content": "you can set the vscode setting to use your python version and you'll encounter this issue much less often!\n\nhttps://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#vscode-settings"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-09-09 18:28:24.321000+00:00",
                "content": "we're using Conda on MacOS, not sure what the right format for using the right environment variable needs to be. Using this doesn't work\n\"baml.cliPath\":\"${env:CONDA_PREFIX}/bin/baml-cli\","
            }
        ]
    },
    {
        "thread_id": 1282773249884688405,
        "thread_name": "is there a way to do an optional array",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 18:42:55.547000+00:00",
                "content": "is there a way to do an optional array of strings? like `string[]?` ? the VSCode extension errors if I try that:\n```\nSaving file: file:///Users/airhorns/Code/gadget/packages/data-science/src/baml_src/dataModeller.baml\nsaving urifile:///Users/airhorns/Code/gadget/packages/data-science/src/baml_src/dataModeller.baml   /Users/airhorns/Code/gadget/packages/data-science/src/baml_src/dataModeller.baml\n[Error - 6:41:59 PM] Error updating runtime: \n[Error - 6:41:59 PM] Error linting \"\"\nbaml/message{\n  \"message\": \"\",\n  \"type\": \"error\"\n}\nbaml config {\n  \"cliPath\": null,\n  \"generateCodeOnSave\": \"always\",\n  \"restartTSServerOnSave\": false,\n  \"fileWatcher\": false,\n  \"trace\": {\n    \"server\": \"off\"\n  },\n  \"bamlPanelOpen\": false\n}\n[Error - 6:41:59 PM] Error occurred while generating BAML client code:\nError: BAML Generate failed. Project has errors.\n[Error - 6:41:59 PM] Error occurred while generating codelenses:\nError: null pointer passed to rust\n[Error - 6:42:36 PM] Error occurred while generating codelenses:\nError: null pointer passed to rust\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 18:43:47.866000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 18:43:48.094000+00:00",
                "content": "To get around this you can do string[] | null -- we'll look at supporting this with \"?\". Seems like folks do prefer just having nulls instead of empty lists as the default"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 18:44:38.514000+00:00",
                "content": "btw our default behavior is to actually inject an empty array if we dont detect that field is present"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 18:44:42.957000+00:00",
                "content": "ah null works!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 18:45:15.721000+00:00",
                "content": "and yeah, in my case that's less than ideal as an empty list means something different than null for the upstream system i want to send the data to, so i'd have to post process it myself to strip those empty lists out"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 18:45:37.833000+00:00",
                "content": "ahhh fair enough!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 18:46:13.881000+00:00",
                "content": "i just saw in the docs that it says optional arrays aren't supported, my bad for not reading further! i think the error message there could be clearer though ðŸ™‚"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 18:46:14.181000+00:00",
                "content": "have you ran BAML with a really big schema?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 18:46:22.213000+00:00",
                "content": "agreed"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 20:29:49.179000+00:00",
                "content": "yeah, i am indeed running on a very big schema, 150 properties by openai's measurement so not like super huge"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 20:30:12.487000+00:00",
                "content": "do you folks have any recommendations for how to do that post processing step to coerce back to the types one might use with zod or similar?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:31:23.033000+00:00",
                "content": "it is in the works! Let me link you to our proposal"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 20:31:31.705000+00:00",
                "content": "zod has really handy transforms you can use to manipulate the input at any point in the schema for example, i find that super great for encapsulating little fiddly bits of how the LLM performs best vs getting back to the shape that the rest of the computers want"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:31:38.926000+00:00",
                "content": "are you looking to validate the fields or to transform one type to another?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 20:32:45.804000+00:00",
                "content": "i need to prune the nulls and/or empty arrays that come back from a BAML call to get back to undefineds or whatever the rest of the system wants. i can definitely just write out a bunch of logic to do it i was just thinking its kind of brittle and unfortunate, and requires me to re-define a second version of all the types that has been cleaned up"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 20:34:23.868000+00:00",
                "content": "i guess a related question: is the metadata about the type tree available at runtime such that i could write a general purpose transformer that adapts to whatever my BAML types are already?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:42:29.750000+00:00",
                "content": "makes sense. there's a couple of options:\n\n1. We could see if we can create a `typescript/zod` generator type so the all the baml classes etc get transformed into Zod schemas instead of typescript interfaces, and you can then attach transforms to those objects in whichever way you want\n\n2. We also are planning on adding \"transforms\" in BAML files themselves but will take some time (perhaps 2 months) to land. Though the cool thing is that it'll work for every language.\n\n3. Perhaps the way we handle undefined / nulls by default in typescript could be improved -- like maybe you want only `undefined` instead of `null | undefined` ?\n\nIs there one of these 3 that speaks to you the most?\n\nThe semi-related feature to this is validations (attached here) -- we are working on adding that in the next 4 weeks."
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:39:56.131000+00:00",
                "content": "validations is super exciting, thats great"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:42:42.198000+00:00",
                "content": "annnnd i think the first thing sounds the best to me but i dont want to be too needy. the second thing doesn't sound super great to me tbh, i dont really care about multi-language portability, and in fact i would rather write my transforms in typescript, thats the langauge that i know already, i have my nice lint rules and prettier set up for it, my whole team knows it already, etc etc, i feel like i am already going to struggle to sell all of BAML to my whole team and having to tell them to use a less featureful language that doesn't exist yet makes it a bit harder sell too you know \n\nand then for undefined handling yeah -- it'd be great to be able to specify types as `string[]?`, which is either a list of strings or nothing at all, so i dont have to prune out nulls. openai's structured outputs has the same problem, but at least there they are already using a zod-to-json-schema transformer so i can easily fix the thing back up in zod land"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 21:45:08.150000+00:00",
                "content": "If you find a feature useful theres a big chance others do too so no worries â€” i could see ourselves supporting 1 and 2 to make things less scary for devs using baml (and i agree it s easier to sell â€œthis just converts the types to zodâ€. )\n\nWill get back to you on this, it may be like 3 days of work"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:45:21.829000+00:00",
                "content": "roger"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:46:37.554000+00:00",
                "content": "one other thing about the zod-based approach is that walking a zod schema to transform it is pretty clunky. there's no imperative way to ninja in and change something, its all immutable, so you have to build your own walker that traverses the whole thing and returns new elements all the way up the schema tree. they dont offer utilites for that either. so if you did just spit out zod types, i am not sure it'd be all that much easier to manipulate them"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:47:23.073000+00:00",
                "content": "for the openai structured outputs problem, i am doing this kind of thing right now:\n\n```\nconst SomeType = z.object({\n  someField: z.string().nullable().transform(value => isNull(value) ? : undefined : value)\n});\n```"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:47:57.712000+00:00",
                "content": "and in my world its actually like 5 types deep from the root or whatever, so colocating it is kinda nice"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 22:24:01.049000+00:00",
                "content": "so just so i understand:\n\n1. Zod's transforms are good and useful, but could be better\n2. It may still be nice for BAML to output zod schemas (let me know if this one is correct).\n2. If BAML's future native transforms don't look too scary (e.g. imagine we have simple expressions like isNull, operators like '&&, !==' etc) AND they're way easier to use and more powerful, that does seem worth learning them. I know this one is vague right now but we're still designing.\n\nOnce we start designing our native transforms it'd be sweet to get your take on them.\n\nA huge problem with LLMs is definitely that the type you use to prompt may not be the same type your actual code needs, so this is interesting to us"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 22:26:51.391000+00:00",
                "content": "you also mentioned \" it'd be great to be able to specify types as string[]?, which is either a list of strings or nothing at all,\"\n\nby nothing at all, you mean you want only `undefined`s in your types, correct? We could also add a `generator` config that says `useUndefinedInsteadOfNull` (or something like that), if that fixes one subset of problems in working with TypeScript."
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-10 01:23:20.158000+00:00",
                "content": "> Zod's transforms are good and useful, but could be better\nyes, what i like about them is that they are colocated with the rest of the schema definition, and super nice and type safe \n> It may still be nice for BAML to output zod schemas\nhonestly i don't quite see the value. the reason i keep bringing it up is because it is the alternative i have been using and offers stuff BAML doesnt at the moment (transforms), but i think because of the aforementioned struggle to manipulate zod schemas, i dont quite see the value of you folks generating them. i think if you _accepted_ them as input types and we could define our schemas in zod, have BAML do the SAP thing, and spit the result back into the zod schema so it's transforms could take over, that'd be spectacular\n> If BAML's future native transforms don't look too scary (e.g. imagine we have simple expressions like isNull, operators like '&&, !==' etc) AND they're way easier to use and more powerful, that does seem worth learning them. \njust being honest in the hopes it is helpful, but truthfully i struggle to see myself ever being stoked to use a different transform language than the native language that the rest of my system is defined in. there's a gigantic laundry list of reasons why IMO: team familiarity, shared team tooling like linting / loggers / tracing, all of the code available on NPM, ability to add in systems smarts like an LRU cache or a redis cache, breakpoint debugging and other editor support,  etc etc etc. i am always going to prefer the uncontroversial approach that preserves optionality and is less special for me and my team unless you can offer something that is 10 or 100x better you know, and in this case i struggle to see doing transforms in BAML as somehow 10x better than doing it outside. if there was a fundamental reason that the transforms being in BAML raised output quality or reliability i could see it, but i think by nature the transforms are after, not during generation"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-10 02:30:34.942000+00:00",
                "content": "Gotcha, thanks for all the feedback! This is really useful. Will discuss with the team."
            }
        ]
    },
    {
        "thread_id": 1282789374190157886,
        "thread_name": "is there a way to set the default",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 19:46:59.881000+00:00",
                "content": "is there a way to set the default ClientRegistry / TypeBuilder for all calls such that we dont have to pass them every time?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:02:07.761000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:02:08.341000+00:00",
                "content": "not at the moment -- will note this for our roadmap. Do you just want to override all the clients youve ever defined in any BAML file?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 20:02:27.954000+00:00",
                "content": "a couple use cases for me:\n - i have an enum that i already have a different source of truth for at runtime, and so i want to pass it down to BAML always\n - i have a pre-existing system of model selectors that know what model to use as a default for each little unit of work, but then have some overrides based on some ambient context now and then (for experiments or user preferences or what have you), and it'd be nice to wire that up once"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-09 20:46:05.445000+00:00",
                "content": "oks, thanks for sharing. I think our \"client registry\" could have way better interface/api in general. I agree it'd be good to not have to send that param every single time -- ill add this to the roadmap"
            }
        ]
    },
    {
        "thread_id": 1282820050138697778,
        "thread_name": "one last one for today: is there any",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 21:48:53.597000+00:00",
                "content": "one last one for today: is there any ninja way to hook in to see the underlying LLM calls in order to export them to a custom tracing provider today? I was chatting with Vaibhav on a call and I know that you fine folks have a monitoring product, but if we're already using our own or a custom one (dont even get me started), can we access the raw requests some how or another? and i mean specifically during evals / in production, not just in the playground"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 21:57:22.105000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 21:57:22.647000+00:00",
                "content": "we have a hook that might be useful here. on_log_event\n\nPyhton:\nhttps://github.com/BoundaryML/baml/blob/21545f2a4d9b3987134d98ac720705dde2045290/integ-tests/python/tests/test_functions.py#L880\n\nTs:\nhttps://github.com/BoundaryML/baml/blob/21545f2a4d9b3987134d98ac720705dde2045290/integ-tests/typescript/tests/integ-tests.test.ts#L540"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-09 21:57:31.176000+00:00",
                "content": "its not super documented yet"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-09 22:24:13.799000+00:00",
                "content": "ðŸ™ thanks!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-10 01:36:00.328000+00:00",
                "content": "it doesn't seem like my callback that i pass to that thing ever gets called, do I need to set any env vars or anything to get that to do its thing?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-10 07:05:47.069000+00:00",
                "content": "Oh interesting. I realized this only fires when I have the boundary studio env vars enabled"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-10 07:06:16.419000+00:00",
                "content": "Let me see if I can just get you an easy set up for that"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-10 23:28:28.554000+00:00",
                "content": "ðŸ™"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-19 19:08:15.076000+00:00",
                "content": "<@99252724855496704> any chance you got around to making this callback or any others work?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 01:22:09.721000+00:00",
                "content": "Hey <@262014624122011648>, we had some other higher priority work for some companies on our paid tier, so we sadly had to deprioritize this work. We're currently focusing on:\n\n1. Validations \n2. Better types (literals, recursive types, and anonymous types)\n3. Support for baml in all languages (i.e. `baml-cli serve`)\n\nLikely the next high priority item that will come up is returning the event id of the internal BAML event, and then we will be working on baml-raw. But one key discussion we (as a team) will be having is where to share our public roadmap, so all of this is easier to see."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 01:23:07.781000+00:00",
                "content": "If this is super blocking, I can recommend some strategies for how to write a python decorator that auto wraps around every function generated by baml client so you can do caching and such yoruself automatically."
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-20 11:16:49.431000+00:00",
                "content": "ah it wasn't actually the caching bit i was interested in most, ive actually done that already, but the inner LLM events so i can see what the heck the model is actually doing"
            }
        ]
    },
    {
        "thread_id": 1283122691129217026,
        "thread_name": "docker build issue",
        "messages": [
            {
                "author": "samos123",
                "timestamp": "2024-09-10 17:51:28.832000+00:00",
                "content": "Suddenly getting this error on our docker build that used to work:\n```\nStep #0: \u001b[0m\u001b[91mModuleNotFoundError: No module named 'baml_cli'\n```\nThis was the main diff:\n    // The BAML VSCode extension version should also match this version.\n    version \"0.53.1\" # before\n    version \"0.54.1\" # with error"
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 17:55:18.799000+00:00",
                "content": ""
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 17:55:19.160000+00:00",
                "content": "I think it might just be due to me not freezing pip dependencies"
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 17:55:29.641000+00:00",
                "content": "let me try freezing to older version and see if that fixes it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-10 17:56:36.573000+00:00",
                "content": "cool, <@711679663746842796> can hop on a discord call w/ you / help debug if necessary"
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 18:00:26.265000+00:00",
                "content": "this fixes it"
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 18:00:39.183000+00:00",
                "content": "so I think newer version of baml-py breaks existing docker files"
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 18:01:25.784000+00:00",
                "content": "I am good for now, but probably something that should be fixed in baml itself?"
            },
            {
                "author": "samos123",
                "timestamp": "2024-09-10 18:02:03.042000+00:00",
                "content": "this is my Dockerfile:\n```\nFROM python:3.12-slim\n\nENV PORT=8080\nENV ENV=production\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY main.py .\nCOPY baml_src baml_src\nRUN baml-cli generate --from baml_src\n\nCMD [\"sh\", \"-c\", \"fastapi run /app/main.py --port $PORT\"]\n```\nit should be easy to reproduce if you specify baml-py dependency without a specific version."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-10 18:14:00.889000+00:00",
                "content": "thanks for sending your docker file, we'll try and repro and get a fix out!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 18:14:11.735000+00:00",
                "content": "Fix is releasing as we speak!"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-10 18:15:32.040000+00:00",
                "content": "PyPI cache has been updated, should all be good now!"
            }
        ]
    },
    {
        "thread_id": 1283423465780543508,
        "thread_name": "i am getting this error in my production",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 13:46:39.097000+00:00",
                "content": "i am getting this error in my production infrastructure for BAML originated calls:\n```\n{\"model\": String(\"gpt-4o\")}, start_time: SystemTime { tv_sec: 1726060708, tv_nsec: 108434621 }, latency: 29.769216ms, message: \"reqwest::Error { kind: Request, url: Url { scheme: \\\"https\\\", cannot_be_a_base: false, username: \\\"\\\", password: None, host: Some(Domain(\\\"api.openai.com\\\")), port: None, path: \\\"/v1/chat/completions\\\", query: None, fragment: None }, source: hyper_util::client::legacy::Error(Connect, Ssl(Error { code: ErrorCode(5), cause: Some(Ssl(ErrorStack([Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 167772294, library: \\\"SSL routines\\\", function: \\\"(unknown function)\\\", reason: \\\"certificate verify failed\\\", file: \\\"ssl/statem/statem_clnt.c\\\", line: 2092 }]))) }, X509VerifyResult { code: 20, error: \\\"unable to get local issuer certificate\\\" })) }\", code: Other(2) }\n```\nthat work fine locally. calls to openai made by not-BAML from the same instance also work fine (using langchain js), so i know the system SSL trust store is fine. any idea what thats about?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 13:49:27.465000+00:00",
                "content": ""
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 13:49:27.644000+00:00",
                "content": "ah i think it is `ca-certificates` missing from the instance in prod but my macOS machine locally having a fine trust store, and then node being special and having its own non-system trust store which is why my JS calls worked ok, will try adding that and seeing if it fixes it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 13:51:22.912000+00:00",
                "content": "This maybbe relevant: https://github.com/BoundaryML/baml/pull/901"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 14:58:34.321000+00:00",
                "content": "ah ha, it would appear so! are we able to use prerelease versions of BAML easily or do we gotta wait for a release?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 15:08:02.527000+00:00",
                "content": "and do you folks know if you're using your own CA cert bundle or the system's?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:15:50.501000+00:00",
                "content": "<@711679663746842796> Mind doing a patch release?\n\nWe can get this out ASAP -- it takes like 20ish mins to release.\n\nWe are using the system's CA cert bundle on our own demos (like on our nextjs or flyio containers we've setup in the past). But also we've recently used more anthropic LLMs so something could've changed with OpenAI endpoints"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:16:18.062000+00:00",
                "content": "how do you deploy your app?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 17:16:43.226000+00:00",
                "content": "iirc when i put that PR together^ my digging was that we're using the system's cert bundles"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:13:46.244000+00:00",
                "content": "fwiw adding `ca-certificates` and running `sudo update-ca-certificates` didn't fix it for me, and i would say i am running a pretty stock debian setup and getting this issue"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:14:05.399000+00:00",
                "content": "if you folks find any other hints as to what might be going on please let me know"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 18:14:41.009000+00:00",
                "content": "0.55.2 is now out! You should be able to DANGER_ACCEPT_INVALID_CERTS now"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 18:15:09.811000+00:00",
                "content": "is there a specific base image youâ€™re using? or just debian:latest?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:16:20.245000+00:00",
                "content": "using the `debian-11-bullseye-v20230509` base image from GCP"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:16:42.293000+00:00",
                "content": "with this stuff added:\n```\napt-get update\napt-get install -y \\\n  btrfs-progs \\\n  buildkite-agent=3.57.0-7410 \\\n  ca-certificates \\\n  coreutils \\\n  curl \\\n  dnsmasq \\\n  dnsutils \\\n  fonts-liberation \\\n  git \\\n  htop \\\n  jq \\\n  libasound2 \\\n  libcurl3-gnutls \\\n  libcurl3-nss \\\n  libcurl4 \\\n  libgbm-dev \\\n  libgconf-2-4 \\\n  libgtk-3-0 \\\n  libgtk2.0-0 \\\n  libnotify-dev \\\n  libnss3 \\\n  libxss1 \\\n  libxtst6 \\\n  locales \\\n  moreutils \\\n  openssl \\\n  unzip \\\n  vim \\\n  wget \\\n  xauth \\\n  xdg-utils \\\n  xvfb\n```"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-11 18:17:10.741000+00:00",
                "content": "is there any lower level logging i can turn on that would show what cert bundles are being used that you know of?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 18:20:18.370000+00:00",
                "content": "iâ€™ll check- i know we have trace logging that allows digging into super low level stuff, but itâ€™s really noisy"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 18:29:56.983000+00:00",
                "content": "yeah, i don't see anything in `BAML_LOG=trace`, sadly (`BAML_LOG=reqwest=trace,hyper_util=trace` show the http connection details)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 22:37:55.419000+00:00",
                "content": "Were you unblocked Harry?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-12 16:41:15.792000+00:00",
                "content": "<@262014624122011648>  let us know if you're still blocked from deploying BAML -- we take any of these as P0"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-12 19:21:44.241000+00:00",
                "content": "i wasnt able to figure out why I was getting the cert error but DANGER_ACCEPT_INVALID_CERTS works as a workaround for now"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-09-12 19:22:27.624000+00:00",
                "content": "`fetch`-ing in JS land and the openai sdk works fine from the same system so i am guessing there is some config that isn't making it down to `reqwest` quite right"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-12 19:23:44.351000+00:00",
                "content": "yep we figured out the potential issue so we'll patch that soon, ty for the patience"
            }
        ]
    },
    {
        "thread_id": 1283433887518949389,
        "thread_name": "Prompt issue",
        "messages": [
            {
                "author": "charizard_98",
                "timestamp": "2024-09-11 14:28:03.833000+00:00",
                "content": "Here is an error I got:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-11 14:32:08.979000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-11 14:32:09.663000+00:00",
                "content": "The issue here is that you donâ€™t have an enum category that handles any other scenarios"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-11 14:32:29.773000+00:00",
                "content": "So the user said thanks, and thereâ€™s no valid enumeration that matches the expectations"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-11 14:32:36.873000+00:00",
                "content": "Chat*"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-11 14:39:19.802000+00:00",
                "content": "Oh okay. My understanding was that it will categorize based on the whole chat. I think when I ran the same conversation before, and I said Thanks at the end, it returned Sales"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-11 14:40:53.312000+00:00",
                "content": "Ah got it. For that youâ€™ll need a bit more prompt engineering likely"
            },
            {
                "author": "charizard_98",
                "timestamp": "2024-09-11 14:44:40.700000+00:00",
                "content": "Yeah! I was just testing out with various prompts. Thanks!"
            }
        ]
    },
    {
        "thread_id": 1283448217295388682,
        "thread_name": "For some reason baml doesn't want to",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 15:25:00.318000+00:00",
                "content": "For some reason baml doesn't want to compile my prompts even tho in vscode it says there are no error in the grammar.  The baml file is a bit too long to post here \n\nThis is the error I have got: \n```\nalex4o@mitsuki Â± $ BAML_LOG=trace pnpm baml-cli generate                             main ~/Projects/mandel-npm-package\n[2024-09-11T15:27:07Z TRACE baml_runtime::runtime::runtime_interface] Reading files from ./baml_src\n[2024-09-11T15:27:07Z ERROR baml_runtime::cli::generate] Error generating clients: \n/Users/alex4o/Projects/mandel-npm-package/node_modules/.pnpm/@boundaryml+baml@0.55.1/node_modules/@boundaryml/baml/cli.j\n/Users/alex4o/Projects/mandel-npm-package/node_modules/.pnpm/@boundaryml+baml@0.55.1/node_modules/@boundaryml/baml/cli.js:9\n  baml.invoke_runtime_cli(process.argv.slice(1))\n       ^\n\nError\n    at Object.<anonymous> (/Users/alex4o/Projects/mandel-npm-package/node_modules/.pnpm/@boundaryml+baml@0.55.1/node_mod\n    at Object.<anonymous> (/Users/alex4o/Projects/mandel-npm-package/node_modules/.pnpm/@boundaryml+baml@0.55.1/node_modules/@boundaryml/baml/cli.js:9:8)\n    at Module._compile (node:internal/modules/cjs/loader:1546:14)\n    at Module._extensions..js (node:internal/modules/cjs/loader:1691:10)\n    at Module.load (node:internal/modules/cjs/loader:1317:32)\n    at Module._load (node:internal/modules/cjs/loader:1127:12)\n    at TracingChannel.traceSync (node:diagnostics_channel:315:14)\n    at wrapModuleLoad (node:internal/modules/cjs/loader:217:24)\n    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:166:5)\n    at node:internal/main/run_main_module:30:49 {\n  code: 'GenericFailure'\n}\n\nNode.js v22.7.0\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:58:35.250000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:58:36.956000+00:00",
                "content": "can you share a github gist or do a file upload with the file?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:58:44.232000+00:00",
                "content": "you can also DM me the file!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-11 15:59:39.702000+00:00",
                "content": "does your generator also include `version 0.55.1` ?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 18:25:12.144000+00:00",
                "content": "https://gist.github.com/alex4o/73c9fcff2ff0e866334fb6d906dbc352 this is the baml file I added (cursor wrote) that is breaking it"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 18:25:30.932000+00:00",
                "content": "sry for the late reply I was OOF"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 18:37:59.826000+00:00",
                "content": "Adding this didn't help at all I get the same error\n```\n    version \"0.55.1\"\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 18:58:39.517000+00:00",
                "content": "Do you have a copy of your `generators.baml` as well?\n\nUnfortunately I can't reproduce what you're seeing ðŸ˜¦"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:07:52.544000+00:00",
                "content": "Also, we can try using other `baml-cli` versions (you'll need to update `version` in your BAML file each time):\n\nPrevious release:\n`pnpx @boundaryml/baml@0.54.2 generate`\n\nLatest release:\n`pnpx @boundaryml/baml@0.55.2 generate`"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:39:32.865000+00:00",
                "content": "my generators file isn't very special\n```\n\n// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"typescript\"\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.55.1\"\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    // Regardless of this setting, you can always explicitly call either of the following:\n    // - b.sync.FunctionName()\n    // - b.async_.FunctionName() (note the underscore to avoid a keyword conflict)\n    default_client_mode async\n}\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:40:30.331000+00:00",
                "content": "Can you try with `pnpx @boundaryml/baml@0.54.2 generate`?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:41:04.412000+00:00",
                "content": "It didn't work trying now with 55.2"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:42:30.080000+00:00",
                "content": "Hmm"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:42:32.847000+00:00",
                "content": "I get the same error with both of them, I trired deleting all of my node_modules, maybe I should try deleting the pnpm cache"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:44:32.964000+00:00",
                "content": "the generator inside of vscode (I assuem it uses the same on installed in the project) didn't work as well"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:44:53+00:00",
                "content": "How about `0.53.1`?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:45:28.225000+00:00",
                "content": "Also happy to get on a video call right now, if you're free"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:49:14.468000+00:00",
                "content": "I don't know if that is good or not but 0.53.1 gave me a list of errors"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:49:55.934000+00:00",
                "content": "Can you email me the actual file? sam@boundaryml.com\n\nI think something got lost in the gist upload that's resulting in the lack of repro"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:50:51.454000+00:00",
                "content": "Also, does `RUST_BACKTRACE=1 pnpx @boundaryml/baml@0.55.2 generate` do anything?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:51:33.024000+00:00",
                "content": "No it is not, it is that there are some classes defined in a separate file"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:51:45.395000+00:00",
                "content": "Hm - what errors?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:52:43.246000+00:00",
                "content": "I will join the offce ours if that is not a problem for you"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-11 19:55:22.417000+00:00",
                "content": "[2024-09-11T19:51:41Z ERROR baml_runtime::cli::generate] Error generating clients: \n    \n    Stack backtrace:\n       0: std::backtrace::Backtrace::capture\n       1: anyhow::error::<impl anyhow::Error>::msg\n       2: internal_baml_core::ir::repr::WithRepr::node\n       3: <core::iter::adapters::GenericShunt<I,R> as core::iter::traits::iterator::Iterator>::next\n       4: internal_baml_core::ir::repr::IntermediateRepr::from_parser_database\n       5: baml_runtime::runtime::runtime_interface::<impl baml_runtime::runtime_interface::RuntimeConstructor for baml_runtime::runtime::InternalBamlRuntime>::from_directory\n       6: baml_runtime::BamlRuntime::from_directory\n       7: baml_runtime::cli::generate::GenerateArgs::run\n       8: baml::__napi__run_cli\n       9: __ZN6v8impl12_GLOBAL__N_123FunctionCallbackWrapper6InvokeERKN2v820FunctionCallbackInfoINS2_5ValueEEE\n/Users/alex4o/Library/Caches/pnpm/dlx/c26bqtjdwf3mlufct7ebsbiwgm/191e29ad7c9-11967/node_modules/.pnpm/@boundaryml+baml@0.55.2/node_modules/@boundaryml/baml/cli.js:9\n  baml.invoke_runtime_cli(process.argv.slice(1))\n       ^"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 19:59:20.605000+00:00",
                "content": "https://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#vscode-settings"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-11 20:01:44.258000+00:00",
                "content": "`\"baml.cliPath\": \"${workspaceFolder}/node_modules/.bin/baml-cli\"`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 01:02:48.882000+00:00",
                "content": "<@220444767841026049> this should be fixed in 0.55.3!"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-12 06:51:29.771000+00:00",
                "content": "Great"
            }
        ]
    },
    {
        "thread_id": 1283878294600548468,
        "thread_name": "Bug with onboarding",
        "messages": [
            {
                "author": "hellovai",
                "timestamp": "2024-09-12 19:53:58.738000+00:00",
                "content": "<@711679663746842796>  <@678728259252387840> \n\nHey, testing BAML with VS code and running into a couple of issues:\n1. The extension is not available in Cursor, needed to manually download it from App Store and then drag and drop it.\n2. Ran `poetry run baml-cli generate init` per documentation on the extension page, but got an error:\n$ poetry run baml-cli generate init\nerror: unexpected argument 'init' found\n\n3. VS Code installation link in the readme is broken - https://github.com/boundaryml/baml?tab=readme-ov-file\n\n4. I have the extension installed, how do you open the extension panel to run the tests?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-12 19:54:14.318000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-12 19:54:14.575000+00:00",
                "content": "Sam can you get on office hours with Art?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-12 20:03:27.470000+00:00",
                "content": "he's there now"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 20:04:52.029000+00:00",
                "content": "<@678728259252387840> am here now- was out at lunch"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 20:05:22.372000+00:00",
                "content": "> Ran poetry run baml-cli generate init per documentation on the extension page, but got an error:\nThis is wrong, we'll fix this: it should be `poetry run baml-cli init`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 20:06:49.623000+00:00",
                "content": "> I have the extension installed, how do you open the extension panel to run the tests?\nif you click on any test in your baml files, you can click \"Run Test\", otherwise you should be able to cmd-shift-p/ctrl-shift-p to `baml: open playground`"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 20:09:20.629000+00:00",
                "content": "dropping off office hours, but tag me if you're around!"
            },
            {
                "author": "hashart",
                "timestamp": "2024-09-12 20:51:45.485000+00:00",
                "content": "thanks! I should have looked into documenration originally intstead of relying on the VS Code page.\n\nI was able to init and generate the client from the cli, I moved `baml_client` to subfolder: `src/llm`. Baml files are still in `{PROJECT_ROOT}/baml_src`.\n\nWhen I open the playground i get the next error:\n```\nNo baml projects loaded yet\nOpen a baml file or wait for the extension to finish loading!\n```\nI don't see any project files after running init. I am I doing something wrong?"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-12 20:52:27.695000+00:00",
                "content": "am in office hours now"
            },
            {
                "author": "hashart",
                "timestamp": "2024-09-13 21:42:39.399000+00:00",
                "content": "I have another issue when loading environment variables from .env in the root of the projects.\nI have a small test script that I am running:\n```\nimport os\n\nfrom dotenv import load_dotenv\n\nfrom src.llm.baml_client.sync_client import b\nfrom src.llm.baml_client.types import DealMeta\n\nload_dotenv()\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\nassert ANTHROPIC_API_KEY, \"ANTHROPIC_API_KEY is not set\"\n\nif __name__ == \"__main__\":\n    email_body_example = \"\"\"\nHi <PERSON>,\nCan you or <PERSON> tour us through suite 6F tomorrow (<DATE_TIME> at around 10am?)Â  we are happy to self tour if there is a key on site, or if the Tenant is still there.\nThanks,\n**<PERSON>**\n(d) <PHONE_NUMBER>\n(m) <PHONE_NUMBER>\n<URL>\n\"\"\".strip()\n\n    response = b.ExtractDealNameAndStage(email_body_example)\n    print(response)\n```\nWith this command:\n```\npoetry run python -m src.llm.deal_extraction\n```\n\nbut it fails with the next error:\n```\n    raw = self.__runtime.call_function_sync(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nbaml_py.BamlError: client Sonnet could not resolve options.api_key\n\nCaused by:\n    Failed to resolve expression Identifier(ENV(\"ANTHROPIC_API_KEY\")) with error: unset env variable 'ANTHROPIC_API_KEY'\n```\n\nI moved \n```\nfrom src.llm.baml_client.sync_client import b\nfrom src.llm.baml_client.types import DealMeta\n```\nbelow `load_dotenv()` and now it works. But it's a bit weird that the imports rely in environment variables, don't you think? I would expect an initializer for the sync client that you can call explicetly in your code that will pull env variables after you done initializing them"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-13 22:20:30.316000+00:00",
                "content": "Yes, totally agree - this has been on our list for a while, and our users should never have to deal with this. The challenge is really just prioritization here, but I'll try to make sure we set aside time to tackle this one soon!"
            },
            {
                "author": "hashart",
                "timestamp": "2024-09-13 22:22:08.684000+00:00",
                "content": "I see, thanks. I am glad it's on your radar"
            }
        ]
    },
    {
        "thread_id": 1284124974671466526,
        "thread_name": "Baml parsing errors?",
        "messages": [
            {
                "author": ".alex4o",
                "timestamp": "2024-09-13 12:14:11.850000+00:00",
                "content": "Hey is thera a way to know why hasn't baml parsed a bit of the output?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-13 12:14:59.526000+00:00",
                "content": ""
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-13 12:17:08.635000+00:00",
                "content": "https://gist.github.com/alex4o/87003c74991a9a9bc4b05ff200808103 Is there a way to find out why the details section is not part of the parsed result"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-13 12:24:55.875000+00:00",
                "content": "side note: \nIf I start a test in the baml Playground and then move to a different tab the extraction hangs"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-13 14:09:12.619000+00:00",
                "content": "oh! I think i know what hte bug there is, that's good to know when that happens. if press \"all tests\" and then come back to \"test results\" that should fix that"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-13 14:10:36.231000+00:00",
                "content": "In order to figure out why we didn't parse something, for now its just inspection, but sam has something in the working that will help out with this!\n\nSee the screenshot on this PR: https://github.com/BoundaryML/baml/pull/897"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-15 21:29:17.038000+00:00",
                "content": "That looks cool but please make it syntax highlithed it would be much much easier to read if there are some color so you can anchor your focus on what is important instead of having to parse the whole thing in your head"
            },
            {
                "author": "gitzalytics",
                "timestamp": "2024-09-19 04:42:33.629000+00:00",
                "content": "<@220444767841026049>  do you have a mispelling in your class definition? I had labeled a field \"explantionj\" and the ai in response correctly wrote explanation and it broke the entire parse"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:18:01.912000+00:00",
                "content": "oh yea spelling mistakes are a common cause. One thing we've been debating is should we allow for 1-2 character mismatches during parsing? <@399378437648678913>  and <@220444767841026049> curious if that would be desired? Or perhaps the answer is we bundle in a spell-checker?"
            },
            {
                "author": ".alex4o",
                "timestamp": "2024-09-19 06:20:03.176000+00:00",
                "content": "Maybe both? Anyway, if you are going to do a fuzzy key match that should be a decorator not the default"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:21:07.985000+00:00",
                "content": "oh thats true, yea i think adding an attribute to a key that allows for fuzzy matching or maybe even an expression you can write to write a custom matcher?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-19 06:21:24.987000+00:00",
                "content": "spell-checker seems like an easy lift"
            },
            {
                "author": "gitzalytics",
                "timestamp": "2024-09-19 22:34:15.738000+00:00",
                "content": "Lmao spell checker would help but could be confusing in code"
            },
            {
                "author": "gitzalytics",
                "timestamp": "2024-09-19 22:34:59.200000+00:00",
                "content": "More parser error detail would help like I had no idea where to even start which is why I joined the discord."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 01:17:33.288000+00:00",
                "content": "ah got it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 01:17:49.684000+00:00",
                "content": "i think we prototyped something about that, so we may wrap that work up soon"
            }
        ]
    },
    {
        "thread_id": 1284178082801909760,
        "thread_name": "Gemma image input",
        "messages": [
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-13 15:45:13.815000+00:00",
                "content": "hi, just a noob question - is there a reason why ollama models can't see the image? \n\n```\nbaml_py.BamlValidationError: Parsing error: Client: Ollama (gemma:2b) - 7644ms. StopReason: stop\n---PROMPT---\n[chat] user: Extract the value and unit of the required 'height' from the image below.\n\nBe advised - 'height' could be represented in other ways as arrows, with similar names etc.\n\n-------<image_placeholder base64>-------\n\nAnswer in JSON using this schema:\n{\n  // remove any unit information\n  extracted_value: string,\n  // Use full unit name\n  extracted_value_unit: string,\n}\n\n-------\nBefore you answer, explain your reasoning in 3 sentences.\n\nIf no entity is found, please respond with empty string for both fields in the expected json format. Respond like this even if you cannot see the image.\n\n---LLM REPLY---\nI cannot see the image, so I cannot extract the required value and unit. Therefore, I cannot generate the JSON response you requested.\n---Parsed Response---\nFailed to coerce value: Error parsing '<root>': Missing required fields: extracted_value, extracted_value_unit\n```\n\nThis is how I call it\n\n```\nimage_path = Path(SAVE_FOLDER) / Path(row[\"image_link\"]).name\n    with open(image_path, 'rb') as image_file:\n        image_data = image_file.read()\n        base64_image = base64.b64encode(image_data).decode('utf-8')\n        pred = b.ExtractEntityInfo(Image.from_base64('image/jpg', base64_image), entity_name=row[\"entity_name\"])\n        breakpoint()\n```\n\nThe above code works with other closed models though"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-13 15:50:10.358000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-13 15:50:10.776000+00:00",
                "content": "Gemma 2b doesnt seem to be a multimodal model. You can try Pixtral"
            },
            {
                "author": "philosopherstone",
                "timestamp": "2024-09-13 16:03:13.400000+00:00",
                "content": "ah! it looks like it cannot be installed with ollama."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-13 17:06:07.978000+00:00",
                "content": "Yeah so pixtral is currently the only model that supports image inputs if i recall correctly"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-13 17:06:25.376000+00:00",
                "content": "Perhaps vllm or llmstudio may make pixtral work soon"
            }
        ]
    },
    {
        "thread_id": 1285391386123829279,
        "thread_name": "ollama issue",
        "messages": [
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:06:27.869000+00:00",
                "content": "I've baml working with openai, but when i use ollama, it's not working. More here:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:06:45.316000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:06:45.922000+00:00",
                "content": "is this in the playground?"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:06:49.897000+00:00",
                "content": "local"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:06:58.469000+00:00",
                "content": "it seems that ollama doesnt like to get the request only in the system"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:07:08.311000+00:00",
                "content": "sorry i meant in VSCode extension or in python / typescript*"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:07:08.864000+00:00",
                "content": "so it returns empty"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:07:14.008000+00:00",
                "content": "vscode extension with typescript"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:07:46.272000+00:00",
                "content": "Ah, can you try a few more things:\n1. check if the CURL request in BAML works"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:07:57.610000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:08:00.249000+00:00",
                "content": "(RAW curl button)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:08:12.873000+00:00",
                "content": "oh yes, I saw this today too! It seems like a regression! \n\n<@711679663746842796>"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:08:16.181000+00:00",
                "content": "yeah that's what I was doing. I tried it out myself in the terminal and it returns empty"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:08:32.770000+00:00",
                "content": "if I add another role that asks a question then the LLM responds"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:09:05.069000+00:00",
                "content": "so the issue may be that LLMs with Ollama doesn't reply if the request is only using the system role and it may be expecting at least one user role message"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:09:15.933000+00:00",
                "content": "oh interesting. i wonder if ollama is having an issue here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:09:32.630000+00:00",
                "content": "let me file a bug on them, this seems like an ollama issue here"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:09:39.958000+00:00",
                "content": "thanks V!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:09:53.837000+00:00",
                "content": "You can try VLLM meanwhile for local models as well"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:09:59.199000+00:00",
                "content": "okay"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:10:26.026000+00:00",
                "content": "can you paste the CURL request here"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:10:58.970000+00:00",
                "content": "curl -X POST 'http://localhost:11434/v1/chat/completions' -H \"content-type: application/json\" -d \"{\n  \\\"model\\\": \\\"llama3.1:8b-instruct-q4_1\\\",\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": [\n        {\n          \\\"type\\\": \\\"text\\\",\n          \\\"text\\\": \\\"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string or null,\\n  experience: string[],\\n  skills: string[],\\n}\\\"\n        }\n      ]\n    }\n  ],\n  \\\"stream\\\": true\n}\""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:11:35.748000+00:00",
                "content": "if i change `system` for `user` then it works\n\n```{\n  \"model\": \"llama3.1:8b-instruct-q4_1\",\n  \"created_at\": \"2024-09-17T00:11:19.039472Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Here is the extracted data in JSON format:\\n\\n```\\n{\\n  \\\"name\\\": \\\"Vaibhav Gupta\\\",\\n  \\\"email\\\": \\\"vbv@boundaryml.com\\\",\\n  \\\"experience\\\": [\\n    \\\"Founder at BoundaryML\\\",\\n    \\\"CV Engineer at Google\\\",\\n    \\\"CV Engineer at Microsoft\\\"\\n  ],\\n  \\\"skills\\\": [\\n    \\\"Rust\\\",\\n    \\\"C++\\\"\\n  ]\\n}\\n```\"\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 2581172417,\n  \"load_duration\": 807514459,\n  \"prompt_eval_count\": 88,\n  \"prompt_eval_duration\": 204998000,\n  \"eval_count\": 83,\n  \"eval_duration\": 1567787000\n}```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:12:20.215000+00:00",
                "content": "hmm that's odd it works for me locall"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:13:10.224000+00:00",
                "content": ""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:13:45.702000+00:00",
                "content": "it gives me this\n\n```\ndata: {\"id\":\"chatcmpl-62\",\"object\":\"chat.completion.chunk\",\"created\":1726532005,\"model\":\"llama3.1:8b-instruct-q4_1\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:14:35.079000+00:00",
                "content": "hmm i'll debug more (at an event atm, so will be when I'm back home"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:14:35.265000+00:00",
                "content": "weird!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 00:14:35.984000+00:00",
                "content": "("
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:15:34.993000+00:00",
                "content": "```\nollama --version\nollama version is 0.3.10\n```"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 00:15:43.557000+00:00",
                "content": "I will ask more people to try it out"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 00:56:47.156000+00:00",
                "content": "Sorry, just saw this! (was chasing down a Windows bug)\n\nIIRC ollama has some per-model quirks with its OpenAPI compatibility - I remember llama3.1 not working when I put out a blog post for us about this a few months back, even though llama3 did"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 00:57:30.530000+00:00",
                "content": "It looks like `llama3` still works for me with ollama 0.3.10:\n\n```\n curl -X POST 'http://localhost:11434/v1/chat/completions' -H \"content-type: application/json\" -d '{\n  \"model\": \"llama3\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string or null,\\n  experience: string[],\\n  skills: string[],\\n}\"\n        }\n      ]\n    }\n  ],\n  \"stream\": true\n}'\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 00:59:15.349000+00:00",
                "content": "Checking against `llama3.1:8b-instruct-q4_1` right now..."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 01:00:43.948000+00:00",
                "content": "Yeah, I see the same behavior as you <@526965020031188994> with `llama3.1:8b-instruct-q4_1`:\n\n```\ncurl -X POST 'http://localhost:11434/v1/chat/completions' -H \"content-type: application/json\" -d '{\n  \"model\": \"llama3.1:8b-instruct-q4_1\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Extract from this content:\\nVaibhav Gupta\\nvbv@boundaryml.com\\n\\nExperience:\\n- Founder at BoundaryML\\n- CV Engineer at Google\\n- CV Engineer at Microsoft\\n\\nSkills:\\n- Rust\\n- C++\\n\\nAnswer in JSON using this schema:\\n{\\n  name: string,\\n  email: string or null,\\n  experience: string[],\\n  skills: string[],\\n}\"\n        }\n      ]\n    }\n  ],\n  \"stream\": true\n}'\ndata: {\"id\":\"chatcmpl-630\",\"object\":\"chat.completion.chunk\",\"created\":1726534797,\"model\":\"llama3.1:8b-instruct-q4_1\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 01:01:12.725000+00:00",
                "content": "FYI <@526965020031188994> one way to make it user role by default is to do this:\n\n```\nclient<llm> MyClient {\n  provider ollama\n  options {\n     model \"llama3.1:8b-instruct-q4_1\"\n     default_role \"user\"\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 01:01:50.943000+00:00",
                "content": "I think its likely a but with ollama + the instruct model. I suspect that model is dropping the system prompt"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 01:02:52.094000+00:00",
                "content": "Looking at the modelfiles..."
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 01:03:13.304000+00:00",
                "content": "Here's llama3:\n\n```\nollama show --modelfile llama3\n# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this, replace FROM with:\n# FROM llama3:latest\n\nFROM /Users/sam/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa\nTEMPLATE \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>\"\nPARAMETER num_keep 24\nPARAMETER stop <|start_header_id|>\nPARAMETER stop <|end_header_id|>\nPARAMETER stop <|eot_id|>\n```"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-17 01:03:42.150000+00:00",
                "content": "And here's 3.1 instruct:\n\n```\nollama show --modelfile llama3.1:8b-instruct-q4_1\n# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this, replace FROM with:\n# FROM llama3.1:8b-instruct-q4_1\n\nFROM /Users/sam/.ollama/models/blobs/sha256-5bf327c162140db656bda216d5e5997bb2b9c889373cd45a36732cc5d1276677\nTEMPLATE \"\"\"{{- if or .System .Tools }}<|start_header_id|>system<|end_header_id|>\n{{- if .System }}\n\n{{ .System }}\n{{- end }}\n{{- if .Tools }}\n\nCutting Knowledge Date: December 2023\n\nWhen you receive a tool call response, use the output to format an answer to the orginal user question.\n\nYou are a helpful assistant with tool calling capabilities.\n{{- end }}<|eot_id|>\n{{- end }}\n{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1 }}\n{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n{{- if and $.Tools $last }}\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n\n{{ range $.Tools }}\n{{- . }}\n{{ end }}\nQuestion: {{ .Content }}<|eot_id|>\n{{- else }}\n\n{{ .Content }}<|eot_id|>\n{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ end }}\n{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n{{- if .ToolCalls }}\n{{ range .ToolCalls }}\n{\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\n{{- else }}\n\n{{ .Content }}\n{{- end }}{{ if not $last }}<|eot_id|>{{ end }}\n{{- else if eq .Role \"tool\" }}<|start_header_id|>ipython<|end_header_id|>\n\n{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ end }}\n{{- end }}\n{{- end }}\"\"\"\nPARAMETER stop <|start_header_id|>\nPARAMETER stop <|end_header_id|>\nPARAMETER stop <|eot_id|>\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-17 01:04:13.729000+00:00",
                "content": "https://github.com/ollama/ollama/issues/6835"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-17 01:53:39.209000+00:00",
                "content": "perf! thank you guys ðŸ™"
            }
        ]
    },
    {
        "thread_id": 1285863732878245921,
        "thread_name": "Groq",
        "messages": [
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 07:23:24.111000+00:00",
                "content": "I failed using the groq client provider as a baml client (unless I set my GROQ_API_KEY)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:34:39.730000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:34:40.442000+00:00",
                "content": "Have people been able to use Groq?\nmessage: \"Request failed: {\\\"error\\\":{\\\"message\\\":\\\"Invalid API Key\\\",\\\"type\\\":\\\"invalid_request_error\\\",\\\"code\\\":\\\"invalid_api_key\\\"}}\\n\", code: InvalidAuthentication ...\n\ndespite in the logs I can see my GROQ API Key: \"api_key\": String(\"gsk...\")\n\n\nit should work!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:35:05.541000+00:00",
                "content": "in the BAML Playground in VSCode you should be able to see the Raw CURL request"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:35:15.102000+00:00",
                "content": "which you can try running directly in terminal first"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:35:37.320000+00:00",
                "content": "are you using the \"openai\" or \"openai-generic\" provider?"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 13:40:36.901000+00:00",
                "content": "```\nclient<llm> GROQMixtral {\n  provider openai-generic\n  retry_policy Retry3\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.GROQ_API_KEY\n    model \"mixtral-8x7b-32768\"\n  }\n}\n```"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 13:41:07.394000+00:00",
                "content": "openai-generic, should I try openai ? i'm testing right now with a raw CURL request"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:41:15.461000+00:00",
                "content": "that should be correct!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:42:13.492000+00:00",
                "content": "if its helpful i can hop on office hours rq"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 13:42:21.632000+00:00",
                "content": "the curl request works"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:42:40.339000+00:00",
                "content": "hmm. have you set env vars in the playground yet?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:42:50.416000+00:00",
                "content": "There's a button called \"Env Vars\""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:43:57.985000+00:00",
                "content": "Also, in your python / ts program, how are you setting up environment variables?\n\nYou'll need to ensure that you set up env vars prior to importing from `baml_client`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:44:21.186000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/set-env-vars"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:44:28.858000+00:00",
                "content": "Tahts for python / ts code"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 13:44:50.983000+00:00",
                "content": "For BAML Playground: https://docs.boundaryml.com/docs/get-started/quickstart/editors-vscode#setting-env-variables"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:00:40.757000+00:00",
                "content": "I do export `export GROQ_API_KEY=gsk_.....`"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:00:50.571000+00:00",
                "content": "ah ok"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:00:52.700000+00:00",
                "content": "thank you"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:00:55.905000+00:00",
                "content": "i'll test"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:05:18.520000+00:00",
                "content": "oh that should work"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:05:22.250000+00:00",
                "content": "with export hmm"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:05:28.042000+00:00",
                "content": "do you have the playground work"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:07:01.754000+00:00",
                "content": "I test"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:11:57.658000+00:00",
                "content": "it fails but maybe i'm doing wrong"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:12:13.974000+00:00",
                "content": "do you want to get on a quick call and screenshare?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:12:17.438000+00:00",
                "content": "might be faster"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:12:22.133000+00:00",
                "content": "https://www.promptfiddle.com/BAML-Examples-OJmOR"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:12:30.105000+00:00",
                "content": "you are talking about this playground?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:12:42.682000+00:00",
                "content": "I was talking about the one in VSCode actually! ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:12:44.532000+00:00",
                "content": "let me try"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:12:55.086000+00:00",
                "content": "I changed to openai and have a new error"
            },
            {
                "author": "julienlesbegueries",
                "timestamp": "2024-09-18 14:13:03.496000+00:00",
                "content": "but in the docs it's openai-generic"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:13:27.360000+00:00",
                "content": "\"openai-generic\" is def the right one. let me try"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:13:40.066000+00:00",
                "content": "can you share a temporary API key for groq rq?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 14:13:56.610000+00:00",
                "content": "(over DMs)"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:18:23.257000+00:00",
                "content": "was this issue resolved? I getting the same error."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:19:39.780000+00:00",
                "content": "We are issuing a release tonight but hereâ€™s the fix:"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:19:42.269000+00:00",
                "content": "client<llm> GROQMixtral {\n  provider openai-generic\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    headers {\n      authorization env.MY_API_KEY\n    }\n    default_role \"user\"\n    model \"mixtral-8x7b-32768\"\n  }\n}"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:11:56.587000+00:00",
                "content": "<@825315731012583424>"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:12:17.340000+00:00",
                "content": "but you don't need the headers thing if you're on `0.56` +"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:13:30.417000+00:00",
                "content": "<@99252724855496704> Ahh, that worked. I added the default_role."
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:13:50.542000+00:00",
                "content": "Thank you!"
            }
        ]
    },
    {
        "thread_id": 1286037377483018271,
        "thread_name": "cache control validation",
        "messages": [
            {
                "author": "brandburner",
                "timestamp": "2024-09-18 18:53:24.211000+00:00",
                "content": "Can I confirm that this client definition is valid for use with Claude's context caching?\n\n`client<llm> ClaudeWithCaching {\n  provider anthropic\n  retry_policy RetryPolicy\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n    allowed_role_metadata [\"cache_control\"]\n    headers {\"anthropic-beta\" \"prompt-caching-2024-07-31\"}\n  }\n}\n\nfunction ExtractEntities(dialogue: string, story_context: string, basics: SceneBasics) -> Entity[] {\n  client ClaudeWithCaching\n  prompt #\"\n    {{ _.role('system', cache_control={\"type\": \"ephemeral\"}) }}\n    System Role: You are an expert script editor, dramaturg, and story supervisor. You will use your knowledge to analyze the scene, extracting relevant entities and providing descriptions. Instructions for Entity Extraction:\nPlease extract and describe the entities (characters, objects, locations, and concepts) mentioned in the dialogue. Use the most complete version of their names, based on your story knowledge, but **do not** include irrelevant information about your own role or background. Make descriptions standalone, but specific to the scene.\n    Cached story context to refer to when processing individual scenes: {{ story_context }}\n    {{ _.role('user') }}\n    Extract and describe all significant entities (characters, locations, objects, concepts) mentioned in this scene:\n    Basics: {{ basics }}\n    Dialogue: {{ dialogue }}\n    {{ ctx.output_format }}\n  \"#\n}`\n\nspecifically how the system and user `.roles` are set up where the `cache_control `parameter is passed in? thanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 19:12:29.747000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-18 19:12:30.124000+00:00",
                "content": "yep! That should do it! You can check the raw curl we create in the playground"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-18 19:58:07.221000+00:00",
                "content": "thanks!"
            }
        ]
    },
    {
        "thread_id": 1286092772100538422,
        "thread_name": "rust/wasm build hitting package cycle detection",
        "messages": [
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:33:31.317000+00:00",
                "content": "not a fix but more like jfyi if i use the baml rust from another rust project i hit this error https://github.com/tkaitchuck/aHash/issues/95"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:34:05.328000+00:00",
                "content": ""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:34:05.635000+00:00",
                "content": "the troublemaker seems to be `serde-serialize` that's part of the wasm"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-18 22:34:40.472000+00:00",
                "content": "if there's a version we should bump, happy to bump it for you"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-18 22:35:22.613000+00:00",
                "content": "note that `baml-runtime/Cargo.toml` has some separation of wasm from non-wasm dependencies"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-18 22:36:00.143000+00:00",
                "content": "rust/wasm build hitting package cycle detection"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:36:09.037000+00:00",
                "content": "yeah i saw it but rust is weird, and it tries to check the dependencies before compiling"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:37:23.858000+00:00",
                "content": "someone from the team told me that it seems that `serde-serialize` is deprecated for another feature. I will try to find the thread"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:38:58.082000+00:00",
                "content": "https://github.com/rustwasm/gloo/pull/239"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:43:54.307000+00:00",
                "content": ""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:44:21.014000+00:00",
                "content": "i think they recommend using this https://docs.rs/serde-wasm-bindgen/latest/serde_wasm_bindgen/ instead of the feature"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 22:47:52.105000+00:00",
                "content": "if i find a minimal change that can be done, im going to try to send a PR to you guys"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 23:13:57.969000+00:00",
                "content": "<@711679663746842796> https://github.com/BoundaryML/baml/pull/967/files ðŸ™  no rush"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-18 23:15:55.401000+00:00",
                "content": "can you tack a comment onto `wasm-bindgen` warning us to not add `serde-serialize` in the future?\n\n(there's probably also a way to do this with `static_assertions` but i don't care about that)"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-18 23:16:18.616000+00:00",
                "content": "you'll also want to pull `canary` and merge/rebase it"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 23:16:46.691000+00:00",
                "content": "for sure"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-18 23:21:04.208000+00:00",
                "content": "done  âœ…"
            },
            {
                "author": "joatmon.pockets",
                "timestamp": "2024-09-18 23:25:33.572000+00:00",
                "content": "thanks for the PR!"
            }
        ]
    },
    {
        "thread_id": 1286547310964379679,
        "thread_name": "Azure question",
        "messages": [
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:39:41.827000+00:00",
                "content": "I am trying to run a baml function on the same LLM with 2 clients, OpenAI and AzureOpenAI with long prompt that takes ~21k prompt tokens, ~7k completion tokens and returns an array.\n\n- Using OpenAI with baml, it works flawlessly.\n- Using AzureOpenAI with baml, it quite often it'll just stop midway through the LLM Reply and the Parsed Response will be blank.\n- Using AzureOpenAI and just making a POST request to the baseurl with the same prompt returns a complete response (much like OpenAI with baml). \n\nWhat is causing baml to interrupt the generation when using AzureOpenAI but not when using OpenAI?\n\nThis is my `clients.baml`\n```\nclient<llm> GPT4OMINI {\n  provider azure-openai\n  options {\n    resource_name env.AZURE_OPENAI_RESOURCE_NAME\n    deployment_id env.AZURE_MODELNAME\n    api_version env.AZURE_OPENAI_API_VERSION\n    api_key env.AZURE_OPENAI_API_KEY\n    temperature 0.1\n    }\n}\nclient<llm> MYOPENAI {\n  provider \"openai\"\n  options {\n    api_key env.OPENAI_API_KEY\n    model \"gpt-4o-mini\"\n    temperature 0.1\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:40:48.119000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:40:49.612000+00:00",
                "content": "Are you setting max tokens for azure?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:40:55.859000+00:00",
                "content": "Yep! Thatâ€™s it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:41:09.022000+00:00",
                "content": "Just need to explicitly set max tokens to bigger"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:41:30.659000+00:00",
                "content": "is the default max tokens lower for azure than for openai?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:41:56.757000+00:00",
                "content": "Yea. Azure sets it to just 16 or something. We set it to 2kish"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:42:13.153000+00:00",
                "content": "If you hit raw curl, youâ€™ll see the request we are making"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:42:13.359000+00:00",
                "content": "that makes sense"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:42:17.443000+00:00",
                "content": "And you can modify it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:42:24.818000+00:00",
                "content": "By adding it into options"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:43:23.134000+00:00",
                "content": "They set it to 16 or 16384?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:43:47.963000+00:00",
                "content": "16 or something I think"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:43:56.555000+00:00",
                "content": "damn that nothing"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 04:44:10.333000+00:00",
                "content": "Yeaâ€¦ idk why their default is so bad"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 04:52:32.841000+00:00",
                "content": "```client<llm> GPT4OMINI {\n  provider azure-openai\n  options {\n    resource_name env.AZURE_OPENAI_RESOURCE_NAME\n    deployment_id env.AZURE_MODELNAME\n    api_version env.AZURE_OPENAI_API_VERSION\n    api_key env.AZURE_OPENAI_API_KEY\n    max_tokens 16384\n    temperature 0.1\n    }\n}```\nThis didnt change anything, 21k prompt tokens and it just stopped after just 641 completion tokens"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:00:12.229000+00:00",
                "content": "<@99252724855496704> in the docs for Azure, there isnt a way to define max tokens. Is there something wrong with my client code?\n\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/azure"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:01:03.533000+00:00",
                "content": "Itâ€™s just forwarded like temperature in grok"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:01:10.654000+00:00",
                "content": "We should add more examples ðŸ˜…"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:01:19.211000+00:00",
                "content": "Iâ€™ll be home in 15!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:10:11.536000+00:00",
                "content": "```\nclient<llm> MyClient {\n  provider azure-openai\n  options {\n    ... // what you have\n    max_tokens 4096\n  }\n}\n```"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:12:52.842000+00:00",
                "content": "yep thats what I've done. looking at the raw it gets like ~1k completion tokens and then just stops"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:13:09.086000+00:00",
                "content": "hmm thats so odd"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:13:17.974000+00:00",
                "content": "can you check what the raw curl looks like"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:13:18.063000+00:00",
                "content": "and it  says StopReason: unknown"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:13:25.535000+00:00",
                "content": "the playground should show it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:13:31.758000+00:00",
                "content": "(you can uncheck streaming)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:13:43.965000+00:00",
                "content": ""
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:14:42.732000+00:00",
                "content": "its massive, ill try to send the relevant bit"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:14:57.574000+00:00",
                "content": "i meant, can you try and run it locally in your terminal"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:15:01.451000+00:00",
                "content": "that works for you?"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:18:12.631000+00:00",
                "content": "yep, its just that it takes a minute to generate"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:18:43.472000+00:00",
                "content": "hmm thats super odd. so if that work, is that what happens if you run in python as well?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:19:15.985000+00:00",
                "content": "if its helpful would you be down to hop on a call and share screen"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:19:27.190000+00:00",
                "content": "I can join in about 30 mins!"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:21:33.299000+00:00",
                "content": "The openai response is what it should respond like and the azure response just cuts off after 1000ish tokens"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:22:34.965000+00:00",
                "content": "yep if we can get on a call, thatll be great"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:22:59.792000+00:00",
                "content": "perfect can you send me a calendar invite for 30 mins from now?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 05:23:02.956000+00:00",
                "content": "vbv@boundaryml.com"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-20 05:24:44.693000+00:00",
                "content": "sentðŸ‘"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:22:52.386000+00:00",
                "content": "It turns out the issue is we hit content filters on azure, and we don't raise exceptions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 06:23:04.151000+00:00",
                "content": "we should fix that and abort if finish_reason is content filter"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-10-08 09:38:43.435000+00:00",
                "content": "<@99252724855496704> is this still an issue in baml 0.59.0?"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-10-08 10:09:20.008000+00:00",
                "content": "the raising an exception bit not the content filter bit"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-08 13:34:47.823000+00:00",
                "content": "shoot, sorry this slipped and we haven't yet fixed this.\n\nLet me take a look today"
            }
        ]
    },
    {
        "thread_id": 1286789199961526469,
        "thread_name": "anthropic error",
        "messages": [
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 20:40:52.658000+00:00",
                "content": "Just encountered this error, any ideas?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 20:42:46.558000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 20:42:46.835000+00:00",
                "content": "yea... anthropic recently pushed a change to their API that breaks calling it from brwosers (your python/ts code is unimpacted)\n\ntonights release will fix this.\n\nMitigation is\n```\nclient<llm> Foo {\n  provider anthropic\n  options {\n    .. // what you have already\n    headers { \n      \"dangerous-direct-browser-access\" \"true\"\n    }\n  }\n}\n```"
            }
        ]
    },
    {
        "thread_id": 1286813960636600330,
        "thread_name": "Env var issue",
        "messages": [
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 22:19:16.063000+00:00",
                "content": "when i manually export my key in my terminal the baml_client uses it successfully."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-20 22:22:20.060000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-20 22:22:20.460000+00:00",
                "content": "How do you load env vars in your project? You have to makesure to call load_dotenv (if using python) before you import the baml client"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 22:22:42.888000+00:00",
                "content": "just realized my problem is not coming from baml, my bad on that"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-20 22:23:14.325000+00:00",
                "content": "No worries, but do make sure to import the baml client after env vars have been loaded in your program"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 22:23:26.167000+00:00",
                "content": "gotcha"
            },
            {
                "author": "swagbot300",
                "timestamp": "2024-09-20 22:41:15.260000+00:00",
                "content": "Seems like you were right. Once i imported my packages with the baml_client after the load_dotenv, it worked."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 22:54:37.241000+00:00",
                "content": "ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 22:54:46.564000+00:00",
                "content": "we recommend setting dotenv-cli instead of dotenv"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 22:54:56.225000+00:00",
                "content": "its a bit more robust and will match your deployment set up better as well"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-20 22:55:20.897000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/set-env-vars"
            }
        ]
    },
    {
        "thread_id": 1287021571638300705,
        "thread_name": "Parsing",
        "messages": [
            {
                "author": "sidd065",
                "timestamp": "2024-09-21 12:04:14.383000+00:00",
                "content": "I updated a project with prompts written for baml 0.53.1 to 0.56.1 and the `Parsed Response` has become less reliable. \n\n- One of the attributes is a multi-line string, the LLM sometimes writes it in a single line with `\\n`s or sometimes in multiple lines with triple double quotes `\"\"\"` at the start and end.\n- The prompt makes the LLM \"reason\" before writing the JSON object as this has improved the response quality but this may be breaking the new response parser as when I remove the reasoning bit from the prompt the new response parser starts working more often.\n\nbaml 0.53.1 is able to consistently handle both of these situations pretty much every time I have run it, baml 0.56.1 and 0.55.3 have just returned empty lists despite the LLM having written a JSON object at the end of its response."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:01:31.355000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:01:32.026000+00:00",
                "content": "Can you share an example?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:02:12.183000+00:00",
                "content": "Iâ€™ll add a unit test and patch it!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:21:47.091000+00:00",
                "content": "And is this in the VSCode Playground? Or in terminal?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:32:02.571000+00:00",
                "content": "Ok, Iâ€™ve found a minimum repro and am addressing this!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 13:57:08.553000+00:00",
                "content": "Actually no repro found. If you could share the example, would be great."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 14:27:43.582000+00:00",
                "content": "Sorry for the spam! We generally treat any parser bugs as mission critical.\n\nFor context, here's  my test case which looks to pass:\nhttps://github.com/BoundaryML/baml/pull/977/files\n\nThoughts on how to make it closer to your scenario?"
            },
            {
                "author": "sidd065",
                "timestamp": "2024-09-21 15:14:31.430000+00:00",
                "content": "Sorry i missed this, i thought i wouldnt get a reply over the weekend, ill get you a example in a bit"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:29:04.476000+00:00",
                "content": "maybe we should be setting that expectation... ðŸ˜…"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:23:15.619000+00:00",
                "content": "Updating thread for anyone else: this may or may not be a parser regression, but we've found one area of improvement for updating the parser for a new scenario.\n\nWhen the LLM outputs something like a code-block that has some code that could look like JSON, we don't continue searching the non-code block parts of the response to find other JSON candidates."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 23:06:17.534000+00:00",
                "content": "Resolved: https://github.com/BoundaryML/baml/pull/977"
            }
        ]
    },
    {
        "thread_id": 1287076150895575242,
        "thread_name": "introspection in python",
        "messages": [
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:41:07.092000+00:00",
                "content": "Hi all, loving BAML. First question here. How can I use the python library to introspect the client details. I have an extraction function, using ollama and testing with various small models. I want to pull the model and options defined in clients.baml  via python. Is this possible?\n\nHow do I access these details in Python?\n\n```\nclient<llm> QwenTwoFive32B {\n  provider ollama\n  options {\n    model qwen2.5:32b-instruct-q4_K_S\n    // temperature .5\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:46:01.886000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:46:02.980000+00:00",
                "content": "Glad you're enjoying BAML! Sadly right now this isn't possible, but, what you  can do for easy testing in python is use Client Registry to create clients at runtime.\n\nhttps://docs.boundaryml.com/docs/calling-baml/client-registry"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:46:09.479000+00:00",
                "content": "Would this solve your problem?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:46:43.736000+00:00",
                "content": "also FYI, i would recommend putting models with `.` in the name with a `\"` so:\n\n```\nmodel \"qwen2.5:32b-instruct-q4_K_S\"\n```"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:47:23.442000+00:00",
                "content": "Oh cool, yes that would do the trick. I saw that but didnt pay much attention to it. So I could specify my model specs in the python script, and it would create the client dynamically. Which because its defined in the python script, would know what my specs are"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:47:24.186000+00:00",
                "content": "Nice"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:47:31.023000+00:00",
                "content": "Thanks for the tip!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:47:33.213000+00:00",
                "content": "yep! ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:47:52.725000+00:00",
                "content": "the only annoying part would be actually swapping the model out"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:48:07.891000+00:00",
                "content": "but we're working on making things easier to use for this kind of thing actually"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:52:50.651000+00:00",
                "content": "What I am missing, couldnt I just define the model in an env?\n\n```\ndotenv.load_dotenv()\n\nclient_name = os.getenv(\"MODEL_NAME\")\nmodel_name = os.getenv(\"MODEL_NAME\")\nmodel_temp = os.getenv(\"MODEL_TEMP\")\n\ndef run():\n    cr = ClientRegistry()\n\n    cr.add_llm_client(name=client_name, provider='ollama', options={\n        \"model\": model_name,\n        \"temperature\": 0.7,\n        \"api_key\": \"useless-key\"\n    })\n\n    cr.set_primary(client_name)\n\n    res = b.MySpecialFunction(\"...\", { \"client_registry\": cr })\n```"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:53:07.171000+00:00",
                "content": "This isn't too annoying. Actually I find it more ergonomic than messing with clients.baml"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:54:14.833000+00:00",
                "content": "perfect! thats really good context"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:54:27.384000+00:00",
                "content": "oh you don't need an api_key"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:54:29.787000+00:00",
                "content": "for ollama"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:55:05.245000+00:00",
                "content": "Ha, yes I'm aware. It's a habit from using ollama's openai compatible endpoints"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:56:14.410000+00:00",
                "content": "did it work after that?\n\nAlso, for dotenv.load_dotenv(), you'll need to make sure you load it up before you import baml_client"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:57:01.677000+00:00",
                "content": "Ah, good catch. I'm aware of that issue too. Side note, is that why I cannot reliably get baml to capture my api keys for groq and openai for example?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:57:34.585000+00:00",
                "content": "yep! we're working on making it so you can just do:\n\nb.reset_env_vars({...})"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:57:40.330000+00:00",
                "content": "but for now, the order matters"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:57:49.043000+00:00",
                "content": "What I find strange is that BAML 0.55.3 wont even accept when I add the env variables directly in the fiddle window"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 15:58:04.391000+00:00",
                "content": "I have everything correctly added, but I still get the api key error"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:58:20.710000+00:00",
                "content": "there's a checkbox for `Raw Curl` in the playground"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:58:35.787000+00:00",
                "content": "can you take a look at that request and see if it looks correct?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:58:52.342000+00:00",
                "content": "you can check the streaming and non-streaming versions"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 15:58:57.982000+00:00",
                "content": "and just try running them in terminal"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:01:35.929000+00:00",
                "content": "Seems correct"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:01:40.796000+00:00",
                "content": "I'll try in terminal"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:03:05.078000+00:00",
                "content": "oh! If this is grok, we shipped a fix!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:03:15.895000+00:00",
                "content": "you should update to the latest version and it'll be addressed"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:04:00.686000+00:00",
                "content": "Nice. Next question, I upgraded the cli client to 56.1, but vs code ext is still 55.3"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:04:05.580000+00:00",
                "content": "how do i do vs code?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:06:50.728000+00:00",
                "content": ""
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:09:03.083000+00:00",
                "content": "Perfect thanks. Did that, also updated version manually in generators.baml since it was yelling at me after updating."
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:09:50.561000+00:00",
                "content": "But, same issue. Perhaps it's my fault?\n\n```\nclient<llm> GroqLlamaThree70B {\n  provider openai-generic\n  options {\n    model \"llama3-70b-8192\"\n    api_key env.GROQ_API_KEY\n    base_url \"https://api.groq.com/openai/v1\"\n  }\n}\n```"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:09:59.749000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:11:27.375000+00:00",
                "content": "ah for groq, you also need:\n```\ndefault_role \"user\"\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:11:41.525000+00:00",
                "content": "cause its going in as a system prompt, but for some reason, groq doesn't like that"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-21 16:14:17.115000+00:00",
                "content": "Yup, working now thanks so much"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-21 16:17:00.812000+00:00",
                "content": "If you're down, would you mind submitting a PR to our docs with this?\n\nhttps://github.com/BoundaryML/baml/blob/canary/docs/docs/snippets/clients/providers/groq.mdx\n\nOtherwise i'll get to it in a bit after i fix another bug!"
            }
        ]
    },
    {
        "thread_id": 1287408733671915541,
        "thread_name": "VSCode dependency bug?",
        "messages": [
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:42:41.005000+00:00",
                "content": "Just updated vscode version to 0.56.1 as well as my npm version. Now I'm seeing this in vscode: I'm still abel to use the extension though..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:43:55.780000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:43:56.081000+00:00",
                "content": "I'll track this!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:44:10.583000+00:00",
                "content": "not sure what this is, but looks like we sneaked in a bad dependency"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:45:32.389000+00:00",
                "content": "ðŸ‘"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 13:45:40.728000+00:00",
                "content": "Looks like I'm also getting this error when trying to save a baml file"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:55:18.343000+00:00",
                "content": "yep! You gotta update generators.baml"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:55:29.652000+00:00",
                "content": "we do strict version matching to prevent any accidental updates"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 13:56:18.726000+00:00",
                "content": "for better reliability, you can point it to match you cli version:\n\nhttps://docs.boundaryml.com/docs/calling-baml/generate-baml-client#vscode-generator-settings"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 14:39:23.578000+00:00",
                "content": "Ah ha, that makes sense."
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 14:39:49.928000+00:00",
                "content": "I didn't fully read that error message ðŸ˜¬ \"Action Required\":"
            },
            {
                "author": "seawatts",
                "timestamp": "2024-09-22 14:40:09.741000+00:00",
                "content": "Maybe highlight that or bring it up to the top? imo"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 14:48:13.431000+00:00",
                "content": "<@201399017161097216>"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 15:06:40.211000+00:00",
                "content": "You canignore that error, it just means you are on the latest version of vscode"
            }
        ]
    },
    {
        "thread_id": 1287454716086845482,
        "thread_name": "raw details",
        "messages": [
            {
                "author": "cp2573",
                "timestamp": "2024-09-22 16:45:24.067000+00:00",
                "content": "How can we access the raw LLM call in python? More specifically, I'm interested in getting back the token metrics. Is this possible?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 17:34:22.780000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-22 17:34:23.275000+00:00",
                "content": "Hi, not at the moment, but we will be releasing an update with this soon."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-22 17:52:09.034000+00:00",
                "content": "<@825315731012583424> just out of curiosty, is this just for analytics purposes? Or do you want to do something with tokens metrics at runtime?"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-22 18:02:42.351000+00:00",
                "content": "Thanks for letting me know."
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-22 18:02:55.181000+00:00",
                "content": "Want to use it for analytics. Using datadog traces and want to send it with the trace."
            },
            {
                "author": "loohly",
                "timestamp": "2024-09-23 07:10:23.322000+00:00",
                "content": "Seconding the need for this, likewise for logging"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-23 09:52:50.883000+00:00",
                "content": "I stumbled on this open issue which suggests its posible to at least see token usage in the playground but that doesnt appear to be the case yet:\n\nhttps://github.com/BoundaryML/baml/issues/882#issuecomment-2291528536"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 13:26:47.121000+00:00",
                "content": "The playground should have it! If you look at the headers above the raw text, youâ€™ll see it!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-23 13:26:57.778000+00:00",
                "content": "Youâ€™ll also see what models was used"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-23 23:34:45.658000+00:00",
                "content": "Hm. Am I missing something?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-23 23:35:32.076000+00:00",
                "content": "oh interesting, it seems like Groq doesnt return the token usage in the format we usually expect from openai-compatible clients -- we'll take a look"
            },
            {
                "author": "cp2573",
                "timestamp": "2024-09-23 23:38:12.466000+00:00",
                "content": "Cool thanks, nor does ollama clients it seems"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-24 03:34:57.010000+00:00",
                "content": "yep sadly not ðŸ˜¦"
            }
        ]
    },
    {
        "thread_id": 1288191632814116988,
        "thread_name": "baml is not picking up the changes in",
        "messages": [
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:33:38.706000+00:00",
                "content": "baml is not picking up the changes in clients.baml. I noticed my API key wasn't being set, so I tried chaning the model option as well to see if it gets pickedup when baml makes the call to OpenAI's APIs. It doesn't"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:35:07.559000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:35:08.012000+00:00",
                "content": "can you make sure to add the:\n`import b from baml_client` after you load your env vars?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:35:16.857000+00:00",
                "content": "How do you load env vars? Do you use load_dotenv() ?"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:35:52.681000+00:00",
                "content": "I actually just hardcode the string in baml_client.py for testing. But it's not picking up the change from \"gpt-4o\" to \"gpt-4o-mini\" either"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:36:00.872000+00:00",
                "content": "yeah I have the import statement"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:36:20.175000+00:00",
                "content": "Also tried:\n- deleting and re-generating the entire baml_client directory\n- uninstalling baml and re-installing it."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:41:15.162000+00:00",
                "content": "can you nuke your python env?"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:43:14.360000+00:00",
                "content": "Yeah I tried that too. here's what I did:\n- deleted my .venv\n- deleted baml_src and baml_client\n- pipx uninstall finicapi (i installed the local directory to pipx to test our own libray)\n- pipx uninstall baml-py (just in case)\n- poetry install (includes baml-py as a dependency)\n- pipx install -e . (to be able to use our library)"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:45:47.625000+00:00",
                "content": "it seems no matter what I do, changing this code does nothing:\n\n```client<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}```\n\nbut if I change from `client \"openai/gpt-4o\"` to `client \"openai/gpt-4o-mini\"`\n\nIt will use the mini client. But changing the client definitions themselves don't seem to work"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:46:05.302000+00:00",
                "content": "ok let me take a look now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:46:33.353000+00:00",
                "content": "what versio of baml do you have installed? what versio is in your generator + cli + vscode?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:46:48.443000+00:00",
                "content": "baml-py i mean (whichc ontains the cli)"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:48:11.367000+00:00",
                "content": "ok so if in my function I change `client \"openai/gpt-4o\"` to `client \"GPT4o\"` to reference the GPT4o client directly it works"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:48:41.769000+00:00",
                "content": "ohh so you were using the shorthand"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:49:03.492000+00:00",
                "content": "basically the shorthand is `provider/model-name`, the `model-name` is not one of your explicit `client`s you've defined"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:49:06.043000+00:00",
                "content": "does that make sense?"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:49:12.051000+00:00",
                "content": "Name: baml-py\nVersion: 0.56.1"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:49:18.089000+00:00",
                "content": "so you can't do `openai/my-openai-client`"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:50:28.988000+00:00",
                "content": "ah I see, makes sense. I just used whatever was included in the boilerplate when running baml-py init so I thought it referenced this client:\n\n```client<llm> Openai {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [GPT4o, GPT4oMini]\n  }\n}```"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-09-24 17:50:43.479000+00:00",
                "content": "thanks aaron!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-24 17:57:30.233000+00:00",
                "content": "ahh we will make it clearer, np"
            }
        ]
    },
    {
        "thread_id": 1288895812901929000,
        "thread_name": "Azure issue",
        "messages": [
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 16:11:48.322000+00:00",
                "content": "has anyone used azure openai client? I keep getting a resource not found error but pretty sure I have all the configs right\n\nRequest failed: {\"error\":{\"code\":\"404\",\"message\": \"Resource not found\"}"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-26 16:18:36.711000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-26 16:18:37.184000+00:00",
                "content": "Mind pasting your client setup? Minus the secrets"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 16:57:13.790000+00:00",
                "content": "have you tried the Raw Request yourself?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 16:57:27.620000+00:00",
                "content": "often its due to a missing or incorrect api_version"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 16:57:41.715000+00:00",
                "content": "azure is dumb and gives bad errors"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 19:37:42.447000+00:00",
                "content": "```\nclient<llm> AzureOpenAI {\n  provider azure-openai\n  options {\n    base_url \"https://resournce-ame.openai.azure.com/openai/deployments/deployment-name\"\n    api_version \"2024-08-06\"\n    api_key env.AZURE_OPEN_AI_API_KEY\n    temperature 0.0\n  }\n}\n\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 19:46:09.379000+00:00",
                "content": "def check what raw curl request is being made. if that one fails, likely a problem with the values there.\n\n(FYI, that looks right to me)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-26 19:46:36.679000+00:00",
                "content": "You can grab it from the playground checkbox that says raw curl"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 22:56:38.686000+00:00",
                "content": "resolved, it was the api_version, I misunderstood that for the model version"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 22:56:46.900000+00:00",
                "content": "curl made it very easy to debug that"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 22:57:21.821000+00:00",
                "content": "nice feature of the playground! ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 22:58:34.013000+00:00",
                "content": "ðŸ™‚"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:35:04.154000+00:00",
                "content": "I spoke too soon haha worked in playground but looks like streaming is broken, quite odd since we use it prod directly with the openai library"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:35:33.809000+00:00",
                "content": "what do you mean its broken btw?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:35:44.306000+00:00",
                "content": "did the curl request for streaming not work?"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:35:44.472000+00:00",
                "content": "```\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/faizansattar/Workspace/tc-api/.venv/lib/python3.12/site-packages/baml_py/stream.py\", line 48, in __drive_to_completion\n    retval = await self.__ffi_stream.done(self.__ctx_manager)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nbaml_py.BamlError: No events in the chain\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:35:54.831000+00:00",
                "content": "oh.. that is broken"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:36:01.702000+00:00",
                "content": "does this happen every time?"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:36:02.142000+00:00",
                "content": "It did but then I noticed there is no content coming back"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:36:09.514000+00:00",
                "content": "yup every time"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:36:23.625000+00:00",
                "content": "wait in the curl there's not any content?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:39:30.061000+00:00",
                "content": "if the curl request doesn't return anything, then its likely that azure is doing some level of content moderation"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:39:42.539000+00:00",
                "content": "we've seen issues for some customers where they have delayed streamed"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:40:05.004000+00:00",
                "content": "due to content moderation"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:40:27.944000+00:00",
                "content": "we need to propagate up stop reason content moderated"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:40:33.703000+00:00",
                "content": "(that's a bug we have today)"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:41:29.286000+00:00",
                "content": "nope, I stand corrected content is there"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:41:49.871000+00:00",
                "content": "just baml streaming is not working tested a bunch with the curl for different test cases from playground"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:42:49.686000+00:00",
                "content": "<@201399017161097216> can you hop on a call with <@740363257814057004> and help him debug here?"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:42:51.173000+00:00",
                "content": "curl is working as expected"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 23:43:13.790000+00:00",
                "content": "(Iâ€™m out on my phone atm :()"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-26 23:44:28.149000+00:00",
                "content": "all good, ill poke around until you or Aaron are available"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:04:54.468000+00:00",
                "content": "im around now if you want to hop on office hours"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-27 00:09:26.713000+00:00",
                "content": "yupp, jumping on now"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:10:19.001000+00:00",
                "content": "hmm can't hear you yet"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:10:48.432000+00:00",
                "content": "lets use google meet"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:11:11.198000+00:00",
                "content": "https://meet.google.com/ybh-pztv-jjm?ijlm=1727395863761&hs=187&adhoc=1"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:12:13.028000+00:00",
                "content": "shorter link https://meet.google.com/ybh-pztv-jjm"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:27:20.062000+00:00",
                "content": "do you use any of our @trace decorators?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:27:31.454000+00:00",
                "content": "actually, do you use fallback clients?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:32:18.518000+00:00",
                "content": "I think it's some of these issues:\n1. You are using a fallback client (it has a bug where it hides underlying errors from the child clients)\n2. \nHere is my definition:\n```\nclient<llm> GPT35Azure {\n  provider azure-openai\n  options {\n    resource_name \"west-us-azure-baml\"\n    deployment_id \"gpt-35-turbo-default\"\n    // base_url \"https://west-us-azure-baml.openai.azure.com/openai/deployments/gpt-35-turbo-default\"\n    api_version \"2024-02-01\"\n    api_key env.AZURE_OPENAI_API_KEY\n  }\n}\n```\nwhich doesn't use base_url.\n\n3. I think the trailing slash may be the issue in the base_url"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 00:32:27.889000+00:00",
                "content": "(I was able to stream myself in my integ test just now)"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-27 02:56:15.741000+00:00",
                "content": "I am using fallback but only 1 client at the moment which is the AzureOpenAI"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-27 02:57:23.024000+00:00",
                "content": "I dont have a trailing slash in the base url but will try without the base url"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 03:04:13.124000+00:00",
                "content": "Can you remove the fallback and use the client directly?"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-27 03:04:52.954000+00:00",
                "content": "yup, just tried that and resolved the issue around the api version not being exposed correctly"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-27 03:05:01.556000+00:00",
                "content": "looks like its all good now, testing some more"
            },
            {
                "author": "faizansattar",
                "timestamp": "2024-09-27 03:07:00.130000+00:00",
                "content": "appreciate the help! looks good now, it was hiding the underlying errors as you said"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-27 03:55:27.986000+00:00",
                "content": "Hmm what do you mean the api version wasnt exposed? The curl request still worked no?\n\nAnd no worries! We will fix our fallback errors issue"
            }
        ]
    },
    {
        "thread_id": 1288972024403857541,
        "thread_name": "version issue!",
        "messages": [
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:14:38.560000+00:00",
                "content": "hi! I am running on a strange error where my vscode is showing up a strange error when it says that the versions of vscode plugin and pip package are not the same. However I have the vscode plugin and pip version up to date (0.56.1), but the error says that I have apparently 0.53.0 for pip package and 0.56.1 for vscode plugin. This is not allowing me run anything on local env. Thank you in advance"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:15:16.905000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:15:17.300000+00:00",
                "content": "that's odd!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:15:40.701000+00:00",
                "content": "can we hop on the office hours channel and screenshare by chance?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:15:58.408000+00:00",
                "content": "i wonder if there's something super silly we can help patch up"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:17:17.266000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:17:32.971000+00:00",
                "content": "ah yes!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:17:39.189000+00:00",
                "content": "for that if you go to the generators.baml file"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:17:42.813000+00:00",
                "content": "you can modify that"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:17:46.323000+00:00",
                "content": "to match your version"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:18:03.957000+00:00",
                "content": "we should update the language in that error message to make the action more clear"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:18:35.921000+00:00",
                "content": "where can I find that file?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:18:47.141000+00:00",
                "content": "it should be in your `baml_src` folder"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:19:57.049000+00:00",
                "content": "I am using this example https://github.com/BoundaryML/baml-examples/tree/main/python-fastapi-starter"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:20:02.631000+00:00",
                "content": "or in general, there should be something that has the `generator` keyword present in your `baml` files. In there, each generator will have a `version` attribute, that will need to be synced to the version you have"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:20:09.947000+00:00",
                "content": "and there is not such a file"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:20:20.533000+00:00",
                "content": "ah got it, for that project, its in `main.baml`"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:20:31.947000+00:00",
                "content": "got it!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:20:36.188000+00:00",
                "content": "we should add something there called \"latest\" to make it easier"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:20:47.836000+00:00",
                "content": "<@201399017161097216> track this thread for how we broke this user ðŸ™‚"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:21:01.651000+00:00",
                "content": "yes hahaha"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:21:13.973000+00:00",
                "content": "currently baml-examples aren't easy to use to do being pinned to specific versions"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:21:20.977000+00:00",
                "content": "for now I gonna be aware and update it manually"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-26 21:21:40.282000+00:00",
                "content": "thank you for quick response!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-26 21:21:50.336000+00:00",
                "content": "yea, thanks for hopping in to ask the question ðŸ™‚"
            }
        ]
    },
    {
        "thread_id": 1289280867352576010,
        "thread_name": "Panic related to logging",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-09-27 17:41:52.457000+00:00",
                "content": "can I get a DM <@99252724855496704> or <@201399017161097216> ? impacting our prod instance"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-27 19:06:10.440000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-27 19:06:11.530000+00:00",
                "content": "This will be released shorlty, but there is another workaround:\n\nyou can modify the following env var to any other value (x+42 for example), as that will prevent the issue in our code of splitting emojis incorrectly.\n\n```\nBOUNDARY_MAX_LOG_CHUNK_CHARS=3000\n```\n\nhttps://github.com/BoundaryML/baml/pull/987"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-27 19:56:09.409000+00:00",
                "content": "<@99252724855496704> so the initial error went away but another one of my baml functions is stalling as well"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-27 19:56:13.110000+00:00",
                "content": "could it be related?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-27 19:56:35.457000+00:00",
                "content": "Iâ€™ll check! <@201399017161097216> can you help after lunch?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-09-27 19:59:15.756000+00:00",
                "content": "Do i need to update my baml ?"
            }
        ]
    },
    {
        "thread_id": 1289673757392240727,
        "thread_name": "Hi! Having some trouble getting BAML",
        "messages": [
            {
                "author": "brandburner",
                "timestamp": "2024-09-28 19:43:04.742000+00:00",
                "content": "Hi! Having some trouble getting BAML working with Ollama. I have ollama running locally on my mac and have it defined in clients.baml as\n\n`client<llm> MyOllama {\n  provider ollama\n  options {\n    model \"llama3.2\"\n    api_key env.OLLAMA_API_KEY\n    base_url \"http://localhost:11434/v1\"\n  }\n}`\nbut when I run this function in the VSCode playground:\n\n`function ExtractResume2(resume: string) -> Resume {\n  client MyOllama\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}`\n\nI get this error:\n`Unspecified error code: 2\nreqwest::Error { kind: Request, source: \"JsValue(TypeError: Failed to fetch\\nTypeError: Failed to fetch\\n    at __wbg_fetch_1e4e8ed1f64c7e28 (https://file+.vscode-resource.vscode-cdn.net/Users/michaela/.vscode/extensions/boundary.baml-extension-0.57.0/web-panel/dist/assets/baml_schema_build.js:2706:17)\\n    at baml_schema_build.wasm.reqwest::wasm::client::js_fetch::h4c26ca3258bac32c (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[7087]:0x651e5a)\\n    at baml_schema_build.wasm.reqwest::wasm::client::fetch::{{closure}}::h626bbda2d13cfa36 (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[365]:0x1b84a7)\\n    at baml_schema_build.wasm.`"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-28 19:43:42.952000+00:00",
                "content": ""
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-28 19:43:43.348000+00:00",
                "content": "error continued:\n\n`<baml_runtime::internal::llm_client::primitive::openai::openai_client::OpenAIClient as baml_runtime::internal::llm_client::traits::chat::WithStreamChat>::stream_chat::{{closure}}::h265bf2634b631863 (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[392]:0x1d0efe)\\n    at baml_schema_build.wasm.baml_runtime::BamlRuntime::run_test::{{closure}}::h896fc93c3ab1b7b3 (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[139]:0x35733)\\n    at baml_schema_build.wasm.wasm_bindgen_futures::future_to_promise::{{closure}}::{{closure}}::h41f6ad1a253f5b4f (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[636]:0x278de1)\\n    at baml_schema_build.wasm.wasm_bindgen_futures::queue::Queue::new::{{closure}}::h2bd4eab1d7cf1be5 (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[3459]:0x52f8fc)\\n    at baml_schema_build.wasm.<dyn core::ops::function::FnMut<(A,)>+Output = R as wasm_bindgen::closure::WasmClosure>::describe::invoke::h4787a4021637fe52 (wasm://wasm/baml_schema_build.wasm-025e6b3a:wasm-function[12929]:0x6e0beb)\\n    at __wbg_adapter_41 (https://file+.vscode-resource.vscode-cdn.net/Users/michaela/.vscode/extensions/boundary.baml-extension-0.57.0/web-panel/dist/assets/baml_schema_build.js:240:12)\\n    at real (https://file+.vscode-resource.vscode-cdn.net/Users/michaela/.vscode/extensions/boundary.baml-extension-0.57.0/web-panel/dist/assets/baml_schema_build.js:222:16))\" }`"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 19:43:55.526000+00:00",
                "content": "https://www.boundaryml.com/blog/ollama-structured-output"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 19:44:05.721000+00:00",
                "content": "You need that extra env var"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 19:44:23.494000+00:00",
                "content": "To run it with the playground."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 19:44:37.755000+00:00",
                "content": "Weâ€™ll add this in our docs"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-28 19:45:32.366000+00:00",
                "content": "Wow - you're on it! Thanks man - I'll give that a try"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-28 19:46:07.790000+00:00",
                "content": "If youâ€™re down to, mind making the PR to update our docs?"
            },
            {
                "author": "brandburner",
                "timestamp": "2024-09-28 20:20:34.268000+00:00",
                "content": "The method in that blog post worked! I think also switching to llama3 (from 3.1 or 3.2) also helped"
            },
            {
                "author": "hashart",
                "timestamp": "2024-10-09 02:44:46.901000+00:00",
                "content": "any plans on fixing llama3.1 compatibility issues?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 06:47:13.602000+00:00",
                "content": "hey <@678728259252387840> could you know what failed in llama 3.1"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 06:47:19.638000+00:00",
                "content": "not sure why it failed!"
            },
            {
                "author": "hashart",
                "timestamp": "2024-10-09 19:58:51.781000+00:00",
                "content": "```\nFailed to coerce value: Error parsing '<root>': Missing required field: action_type_reasoning\n```\nI am getting this error.\n\nmy client config:\n```\n\nclient<llm> Ollama_LLama {\n  provider \"openai-generic\"  \n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.1:8b\"\n  }\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 19:59:12.818000+00:00",
                "content": "can you try the curl request?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 19:59:34.967000+00:00",
                "content": "in BAML playground, you'll see a \"raw curl\" button that you can use to query the model"
            },
            {
                "author": "hashart",
                "timestamp": "2024-10-09 20:00:04.857000+00:00",
                "content": "curl output:\n```\ndata: {\"id\":\"chatcmpl-329\",\"object\":\"chat.completion.chunk\",\"created\":1728503992,\"model\":\"llama3.1:8b\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:00:27.856000+00:00",
                "content": "interesting, the model is just giving 0 data out for your request"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:00:33.217000+00:00",
                "content": "so we are raising an exception"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:01:06.158000+00:00",
                "content": "things to check:\n\n1. try doing this with your client:\n```\nclient<llm> Ollama_LLama {\n  provider \"openai-generic\"  \n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.1:8b\"\n    default_role \"user\"\n  }\n}\n```"
            },
            {
                "author": "hashart",
                "timestamp": "2024-10-09 20:01:32.758000+00:00",
                "content": "that worked"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:01:35.902000+00:00",
                "content": "ðŸ™‚"
            },
            {
                "author": "hashart",
                "timestamp": "2024-10-09 20:01:44.662000+00:00",
                "content": "awesome, thanks!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:01:48.219000+00:00",
                "content": "its cause llama3.1:8b doesn't do system messages"
            },
            {
                "author": "hashart",
                "timestamp": "2024-10-09 20:01:54.106000+00:00",
                "content": "oh, I see"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:02:02.811000+00:00",
                "content": "and by default we set the message to be a \"system\""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:02:16.058000+00:00",
                "content": "glad we fixed this tho!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 20:02:22.707000+00:00",
                "content": "we should put a troubleshooting guide ðŸ¥²"
            }
        ]
    },
    {
        "thread_id": 1290135045826416651,
        "thread_name": "I was playing on a new project with",
        "messages": [
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 02:16:04.475000+00:00",
                "content": "I was playing on a new project with prompfiddle and suddenly i got a white screen"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 02:16:36.301000+00:00",
                "content": ""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 02:16:36.527000+00:00",
                "content": "I'm using Safari Version 17.6"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 02:16:51.230000+00:00",
                "content": ""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 02:17:15.719000+00:00",
                "content": "https://react.dev/errors/185"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 04:32:09.200000+00:00",
                "content": "i switched to a local env so it's not a bottleneck by any means. if u make it low priority, i wont be offended ðŸ˜…"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 05:46:38.030000+00:00",
                "content": "<@201399017161097216> this looks sad and we should see if anything major broke"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 13:20:15.559000+00:00",
                "content": "Ill take a look today"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 19:05:24.589000+00:00",
                "content": "thank you! if i can be of any help to reproduce it or anything just let me know"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:07:15.427000+00:00",
                "content": "did it happen when you ran a test or something or just typing in the editor?"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 19:20:46.007000+00:00",
                "content": "just typing in the editor i was changing some classes"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:21:12.524000+00:00",
                "content": "sounds good -- our codemirro editor could use a bunch of improvement haha"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 19:22:09.389000+00:00",
                "content": "it gets the job done though! so it's not really that bad. the only actual bad downside is when you lose your progress"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:22:52.705000+00:00",
                "content": "yes we currently don't store things in local storage -- it's on the list for sure. Do you find yourself sketching things out in promptfiddle a bunch vs opening up vscode?"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 19:23:51.961000+00:00",
                "content": "i wanted to share some examples with the team so fiddle is way better for that case bc i can just share a link"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:25:28.436000+00:00",
                "content": "do you remember what kind of function signature / tests you had before it crashed?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:25:39.945000+00:00",
                "content": "like arrays, strings, bools, enums etc"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 19:27:57.873000+00:00",
                "content": "i can record a video doing the same thing and trying to make it crash"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:28:09.792000+00:00",
                "content": "that'd be super helpful"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-09-30 19:28:22.979000+00:00",
                "content": "okay so i will do that hopefully that helps saving you some time"
            },
            {
                "author": "nicarq",
                "timestamp": "2024-10-09 22:40:26.625000+00:00",
                "content": ""
            },
            {
                "author": "nicarq",
                "timestamp": "2024-10-09 22:42:03.141000+00:00",
                "content": "so i think the easiest way to reproduce it it's to write a lot of code and then just press delete delelete delete ... x10"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-09 22:55:18.819000+00:00",
                "content": "Nice hahah thanks for reproducing, will take a look this week"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-09 22:55:30.765000+00:00",
                "content": "Love the cat cameo"
            }
        ]
    },
    {
        "thread_id": 1290318429563125854,
        "thread_name": "multimodal",
        "messages": [
            {
                "author": "xyan4330",
                "timestamp": "2024-09-30 14:24:46.565000+00:00",
                "content": "Hi, I am trying to make a call to a local deployed model using more than one image, but I get the following error\nRequest failed: {\"object\":\"error\",\"message\":\"At most 1 image(s) may be provided in one request.\",\"type\":\"BadRequestError\",\"param\":null,\"code\":400}\nI was searching for an solution on docs but unfortunately didn't find any help there. \n\nI am trying to do some few-shot testing, not sure if there is any examples about this from the docs.\n\nThank you in advance!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:51:07.628000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:51:08.037000+00:00",
                "content": "which model are you trying to call?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:52:18.013000+00:00",
                "content": "are you trying to use this open source model? https://github.com/QwenLM/Qwen2-VL/issues/155"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-30 14:53:43.830000+00:00",
                "content": "I am using this one https://huggingface.co/openbmb/MiniCPM-V-2_6"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:54:18.864000+00:00",
                "content": "seems like you can add this flag https://github.com/QwenLM/Qwen2-VL/issues/155#issuecomment-2340117001"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 14:54:23.010000+00:00",
                "content": "if you use vllm"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-30 14:55:06.611000+00:00",
                "content": "yes I am"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-30 14:55:21.550000+00:00",
                "content": "will check this and notify you if it works"
            },
            {
                "author": "xyan4330",
                "timestamp": "2024-09-30 14:55:23.946000+00:00",
                "content": "thank you!"
            }
        ]
    },
    {
        "thread_id": 1290388246760652942,
        "thread_name": "Potential Parsing bug",
        "messages": [
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:02:12.282000+00:00",
                "content": "Seems like when the streaming baml reaches a \\\\\", it fails to complete streaming and it restarts the parsed output"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:03:25.613000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:03:25.926000+00:00",
                "content": "Thanks <@1069562830812094486> ! Do you know what you're parsing out? and what is your function signature?"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:03:47.432000+00:00",
                "content": "```\n\nclass Section {\n  // thought string?\n  text string?\n  code_language string?\n  code string?\n}\n\nclass DoCommandACReturnType {\n  sections Section[] @description(\"The sections of the response. Must choose one of text or code_language+code.\")\n}\n\nfunction DoCommandAC(messages: MessageAC[]) -> DoCommandACReturnType {\n  client Claude\n  prompt #\"\n    You are a```"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:05:11.906000+00:00",
                "content": "This is what happens when it restarts"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:06:59.024000+00:00",
                "content": "I've also tried other more complex structures, they all seem to fail right when the escaped doublequote hits"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:08:03.054000+00:00",
                "content": "oh interesting. That's good and bad at the same time! (let me write some unit tests)"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:08:54.059000+00:00",
                "content": "Thank you!"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:17:14.247000+00:00",
                "content": "Yeah just wanna add that after a while, it suddenly parses correctly. Then after a while, it resets the output again."
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:17:34.170000+00:00",
                "content": "Seems like when the code section is finished, it correctly parses"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:21:04.015000+00:00",
                "content": "got it, i think this is due to how streaming works.\n\nOur parser is just a massive cost function that we try and minimize. We are adding a few more tweaks to it that will let you encode certain expectations (i.e. these fields MUST not be null during streaming). That will likely solve this problem as well.\n\nThat said, i can totally see this being a bug we hadn't handled yet, so i'll double check with a unit test"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:23:29.745000+00:00",
                "content": "Ok thanks"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:24:02.634000+00:00",
                "content": "It would be great if I could have a fix today (I'd like to show this to fellow batch mates at the yc retreat!)"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:24:12.284000+00:00",
                "content": "But if not no worries"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:24:21.874000+00:00",
                "content": "oh fuck me... you know how to get me to change priorities asap ðŸ˜‰"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:24:26.476000+00:00",
                "content": "Hahahah"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:24:34.016000+00:00",
                "content": "I've been telling people to use baml lol"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:24:38.654000+00:00",
                "content": "â¤ï¸"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:24:55.009000+00:00",
                "content": "ok working now, will update when its patched!"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 19:25:02.381000+00:00",
                "content": "Appreciate u brother"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:25:27.543000+00:00",
                "content": "https://tenor.com/view/yugioh-no-u-no-you-reverse-uno-uno-gif-22508660"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:27:23.412000+00:00",
                "content": "do you have a repro on promptfiddle.com by chance?\n\nor if you can DM me the raw LLM response if possible?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:27:59.366000+00:00",
                "content": "i just need the raw string + the data models (which you already provided)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:51:12.262000+00:00",
                "content": "Hmm, I have a few thoughts, but i may not be understanding this correctly <@1069562830812094486> .\n\nSo far:\n\n```\n{\n  \"sections\": [\n    {\n      \"code_language\": \"swift\",\n      \"code\": \"import SwiftUI\\n\\nstruct ContentView: View {\\n    var body: some View {\\n        Text(\\\"Hello, world!\\\")\\n            .padding()\\n    }\\n}\\n\n```\n\nParses as:\n```json\n{\n  \"sections\": [\n    {\n      \"text\": null,\n      \"code_language\": \"swift\",\n      \"code\": \"import SwiftUI\\\\n\\\\nstruct ContentView: View {\\\\n    var body: some View {\\\\n        Text(\\\\\\\"Hello, world!\\\\\\\")\\\\n            .padding()\\\\n    }\\\\n}\\\\n\\n  \"\n    }\n  ]\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:51:58.633000+00:00",
                "content": "and it appears to parse teh `\\\"` correctly. I have a few hunches for where it breaks, i'll keep digging, but if you can get me the raw text the LLM spit out, it would be fantastic."
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 20:02:11.135000+00:00",
                "content": "Yeah lemme send that to you"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 20:02:28.308000+00:00",
                "content": ""
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 20:03:53.741000+00:00",
                "content": "This is the json:"
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 20:03:54.921000+00:00",
                "content": ""
            },
            {
                "author": "gggooo",
                "timestamp": "2024-09-30 20:11:18.173000+00:00",
                "content": "Just sent u in dm"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 21:02:51.032000+00:00",
                "content": "ok issue is tracked, it seems BAML during streaming doesn't handle escaped characters correctly in new lines."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 21:02:57.150000+00:00",
                "content": "This should be an easy fix"
            }
        ]
    },
    {
        "thread_id": 1290390330180501608,
        "thread_name": "changing clients",
        "messages": [
            {
                "author": "cakecrusher",
                "timestamp": "2024-09-30 19:10:29.008000+00:00",
                "content": "I am currently watching this wakthrough\nhttps://www.youtube.com/watch?v=MITj2ukpB-s\n\nand after setting the client parameters the curl request is not updated on the playground is there something im doing wrong?\n\nMy client:\n```\n// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> GPT4oMini {\n  provider openai\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    temperature 0\n  }\n}\n\nclient<llm> Sonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20240620\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> Haiku {\n  provider anthropic\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> Fast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [GPT4oMini, Haiku]\n  }\n}\n\nclient<llm> Openai {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [GPT4o, GPT4oMini]\n  }\n}\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:12:01.665000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:12:01.915000+00:00",
                "content": "sometimes there's strange caching with VSCode -- have you tried reloading vscode via command+shift+p -> reload window?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:12:30.261000+00:00",
                "content": "did you write the same function as in the video? Does the playground update if you add text to the prompt?"
            },
            {
                "author": "cakecrusher",
                "timestamp": "2024-09-30 19:13:01.184000+00:00",
                "content": "this failed"
            },
            {
                "author": "cakecrusher",
                "timestamp": "2024-09-30 19:13:54.043000+00:00",
                "content": "modifying the text does automatically update curl"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:13:56.707000+00:00",
                "content": "do you have time to hop on office hours and screenshare?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-09-30 19:27:52.999000+00:00",
                "content": "you can loop <@99252724855496704> for any workshop stuff"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-09-30 19:29:16.330000+00:00",
                "content": "hey <@383468192535937026> if you need any assistance or just want to have us help out in any way on the workshop, just email me (vbv@boundaryml.com). I'd be happy to join in / support you for such things!"
            },
            {
                "author": "cakecrusher",
                "timestamp": "2024-09-30 19:30:03.748000+00:00",
                "content": "This is resolved thx but will reach out as needed"
            }
        ]
    },
    {
        "thread_id": 1290830530484371558,
        "thread_name": "hey, is there a way to set up a client",
        "messages": [
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 00:19:40.939000+00:00",
                "content": "hey, is there a way to set up a client that would use the following format:\n\n`\"content\": \"Extract from this content:...`\n\ninstead of:\n\n`\"content\": [\n  {\n    \"type\": \"text\",\n    \"text\": \"Extract from this content:...\"\n  }\n]`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 01:30:23.885000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 01:30:29.783000+00:00",
                "content": "if you use openai-generic client it will use that format. Are you trying to call an open source model? We had a regression in version 0.56 that was fixed with 0.57.1"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:04:23.119000+00:00",
                "content": "I'm using an openai-generic client and the provider seemed to have recently updated their format without knowing it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:05:01.349000+00:00",
                "content": "what version is your baml-py package? Or do you use typescript?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:05:14.649000+00:00",
                "content": "I updated my baml version to 0.57.1"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:05:41.493000+00:00",
                "content": "and your vscode extension is in 0.57.1 as well?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:05:58.884000+00:00",
                "content": "when you look at the \"raw curl\" in the playground, do you still see this format?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:05:59.018000+00:00",
                "content": "yes"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:06:11.065000+00:00",
                "content": "when using an openai-generic client?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:07:04.425000+00:00",
                "content": "here is the raw curl:\n\n`curl -X POST 'http://llm-serving-svc-fastchat-api.llm.k8s.prd.int.seekr.com:8001/v1/inference/chat/completions' -H \"authorization: Bearer please-use-your-token\" -H \"content-type: application/json\" -d \"{\n  \\\"temperature\\\": 0.2,\n  \\\"max_tokens\\\": 2000,\n  \\\"model\\\": \\\"meta-llama/Meta-Llama-3-70B-Instruct\\\",\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": [\n        {\n          \\\"type\\\": \\\"text\\\",\n          \\\"text\\\": \\\"Extract from this content:\\nAnswer in JSON using this schema:\\n{\\n  nodes: [\\n    {\\n      id: string,\\n      type: string,\\n      properties: {\\n        description: string or null,\\n      },\\n    }\\n  ],\\n  relationships: [\\n    {\\n      source_node_id: string,\\n      source_node_type: string,\\n      target_node_id: string,\\n      target_node_type: string,\\n      type: string,\\n      properties: {\\n        description: string or null,\\n      },\\n    }\\n  ],\\n}\\n\\nHere is the extracted information in the correct format:\\\\n\\\\n**Nodes:**\\\\n\\\\n1. **Elon Musk** (person) - Entrepreneur and business magnate\\\\n2. **Open AI** (organization) - Artificial intelligence research organization\\\\n\\\\n**Relationships:**\\\\n\\\\n1. **SUED** (Elon Musk, Open AI) - Elon Musk sued Open AI\\\"\n        }\n      ]\n    }\n  ]\n}\"`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:07:51.541000+00:00",
                "content": "once i manually changed to this, it works:\n\n`curl -X POST 'http://llm-serving-svc-fastchat-api.llm.k8s.prd.int.seekr.com:8001/v1/inference/chat/completions' -H \"authorization: Bearer please-use-your-token\" -H \"content-type: application/json\" -d \"{\n  \\\"temperature\\\": 0.2,\n  \\\"max_tokens\\\": 2000,\n  \\\"model\\\": \\\"meta-llama/Meta-Llama-3-70B-Instruct\\\",\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\":  \\\"Extract from this content:\\nAnswer in JSON using this schema:\\n{\\n  nodes: [\\n    {\\n      id: string,\\n      type: string,\\n      properties: {\\n        description: string or null,\\n      },\\n    }\\n  ],\\n  relationships: [\\n    {\\n      source_node_id: string,\\n      source_node_type: string,\\n      target_node_id: string,\\n      target_node_type: string,\\n      type: string,\\n      properties: {\\n        description: string or null,\\n      },\\n    }\\n  ],\\n}\\n\\nHere is the extracted information in the correct format:\\\\n\\\\n**Nodes:**\\\\n\\\\n1. **Elon Musk** (person) - Entrepreneur and business magnate\\\\n2. **Open AI** (organization) - Artificial intelligence research organization\\\\n\\\\n**Relationships:**\\\\n\\\\n1. **SUED** (Elon Musk, Open AI) - Elon Musk sued Open AI\\\"\n        \n   \n    }\n  ]\n}\"`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:07:53.185000+00:00",
                "content": "is it the same if you use a \"user\" role?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:08:12.804000+00:00",
                "content": "and mind copying your client<llm> definition you use for this?"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:08:57.780000+00:00",
                "content": "`client<llm> Llama70b {\n  provider openai\n  options {\n    api_key env.SEEKR_API_KEY\n    model \"meta-llama/Meta-Llama-3-70B-Instruct\"\n    max_tokens 2000\n    base_url \"http://llm-serving-svc-fastchat-api.llm.k8s.prd.int.seekr.com:8001/v1/inference\"\n    temperature 0.2\n  }\n}\n`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:09:23.732000+00:00",
                "content": "i see whats wrong now! mind changing line 2 to `openai-generic`"
            },
            {
                "author": "arindamkhaled4530",
                "timestamp": "2024-10-02 16:14:23.141000+00:00",
                "content": "perfect! that did it! thank you!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 16:14:34.536000+00:00",
                "content": "np!"
            }
        ]
    },
    {
        "thread_id": 1290888799941034055,
        "thread_name": "BAML + rollup",
        "messages": [
            {
                "author": "lafond",
                "timestamp": "2024-10-02 04:11:13.460000+00:00",
                "content": "hey hey, dont think there's a way to do this but wanted to ask anyways - was trying to use BAML in an obsidian plugin with rollup and i'm getting all sorts of errors with rolling up the `.node` file. was wondering if y'all have seen any way to do this ðŸ™‚"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:12:03.067000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:12:03.290000+00:00",
                "content": "Hmm so the basic gist is that it's a native node module. You may benefit from potentially using our wasm version (although it's not well supported)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-02 04:12:48.627000+00:00",
                "content": "You can use our openapi version!"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:13:18.403000+00:00",
                "content": "if obsidian can call out to other APIs that can also be an option"
            },
            {
                "author": "lafond",
                "timestamp": "2024-10-02 04:17:02.303000+00:00",
                "content": "rollup does apparently play nice with wasm, didn't know that was an option"
            },
            {
                "author": "lafond",
                "timestamp": "2024-10-02 04:18:20.225000+00:00",
                "content": "this also looks interesting! not sure however on the actual flow of everything"
            },
            {
                "author": "lafond",
                "timestamp": "2024-10-02 04:19:06.945000+00:00",
                "content": "the plugin loading into obsidian would be starting the BAML server, which then points at the local LM studio URL?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:36:24.524000+00:00",
                "content": "yeah baml server would use a baml LLM client with a base_url that points at the lm studio"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:37:44.593000+00:00",
                "content": "then your app calls it via http using the openapi generated client. The alternative is to make it work with native node modules and rollup: https://www.reddit.com/r/ObsidianMD/comments/1d9ct7r/use_a_native_module_in_a_plugin/"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 04:38:06.377000+00:00",
                "content": "so you may need to mark @boundaryml/baml as \"external\" in rollup"
            },
            {
                "author": "lafond",
                "timestamp": "2024-10-02 04:46:53.678000+00:00",
                "content": "interesting, will take a look tomorrow - appreciate the quick response, boundaryâ€™s one of the sickest DX tools iâ€™ve used recently"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-02 05:00:44.982000+00:00",
                "content": "thanks, that actually means a lot :), any feedback is welcome"
            }
        ]
    },
    {
        "thread_id": 1291554320667377745,
        "thread_name": "log.txt",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 00:15:45.966000+00:00",
                "content": "hey folks i'm having a strange problem where BAML is not parsing my LLM response correctly as far as i can tell, specifically for a nested array of a union. sometimes it works, sometimes it doesn't. here's a gist with the `BAML_LOG=info` output, showing the LLM returning what looks like valid JSON to me but the schema parser returning an empty array for the `dataModel.fields` path (search for `---Parsed Response`): https://gist.github.com/airhorns/b1df75523b17b8741e6b78a83ef71b88  i also included the source `.baml` file. using version 0.58.0"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 00:16:18.730000+00:00",
                "content": ""
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 00:16:25.178000+00:00",
                "content": "the output JSON is valid JSON but i am not sure why BAML is ignoring the three objects"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 00:16:58.022000+00:00",
                "content": "i also tried to get a BAML test going but i got a very strange error from the VSCode extension around \"refusing to work with recursive type\" or something like that and had to delete the test file to get BAML generating at all"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 00:17:54.952000+00:00",
                "content": "Hey harry! Iâ€™ll take a look when Iâ€™m home tonight (around 10/11 pm PST)"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 13:47:35.985000+00:00",
                "content": "sorry to pester <@99252724855496704> but any insight into this? i am not sure what to do to fix it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 13:50:09.948000+00:00",
                "content": "sorry about this <@262014624122011648> I got home late last night, (we're out traveling this week for a conference so a bit of delayed replies here)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 13:50:15.805000+00:00",
                "content": "taking a look asap!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 13:56:08.014000+00:00",
                "content": "Ok I've got a few ideas which will help:\n\n1. for each of your enums which only have 1 value (it looks like you're using them as literals), i would remove the description, and add the description directly to the field named \"type\" itself.\n<@201399017161097216>  we should do this for prompt automatically when you have an enum with a singular value, just inline the description\n\nThat will improve the prompt.\n\n2. for parsing, what version of BAML is this? (Also are you using recursive types? we technically don't support this atm)"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 13:59:03.315000+00:00",
                "content": "BAML v0.58.0. and no i dont think im using any recursion!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 13:59:23.659000+00:00",
                "content": "what seems strange to me is that i can't tell what would actually be tripping up the parser -- the output JSON seems valid to me"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 13:59:29.747000+00:00",
                "content": "i might be missing something silly"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 14:00:09.190000+00:00",
                "content": "i'm gonna turn it into a test case to find out for sure, but for complex schema's it's sometimes hard to see untill i dump out the scores"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 14:00:19.131000+00:00",
                "content": "give me a few!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 14:00:25.260000+00:00",
                "content": "ðŸ™ thank you ever so much!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 14:02:48.975000+00:00",
                "content": "i omitted a LOT of prompt context from that log which guides that generation as well, not sure if that is relevant"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 15:28:17.321000+00:00",
                "content": "foudn it!\n\n\"type\": \"EnumFieldType\",\n\nBut it should have returned:\n\"type\": \"enum\"\n\nWe have two options:\n1. what i suggested in the prior comment, remove all descriptions from enums with a single value  and instead add the descriptions to the field itself\n\n2. we can update the parser for enums with just 1 value to also look for the enum Name (which we should do anyways here)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 15:28:33.945000+00:00",
                "content": "sorry it took so long, we need to add better debugging tools here!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 15:57:46.356000+00:00",
                "content": "ahhh that makes sense"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 15:57:59.908000+00:00",
                "content": "i will definitely do the descriptions change"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 15:58:23.023000+00:00",
                "content": "i take it any invalidity in one element within the array makes the whole array empty?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:05:23.165000+00:00",
                "content": "nope! each element is evaluated independently"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:05:34.143000+00:00",
                "content": "but each of the elemetns suffers from teh same issue actually!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 16:12:33.510000+00:00",
                "content": "ah ðŸ‘"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 16:12:48.697000+00:00",
                "content": "i feel like a literal type would go a long way to making this simpler for both human and LLM too ðŸ’–"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:13:04.133000+00:00",
                "content": "yea... XD <#1290721397131313184> is in progress!"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 16:13:46.513000+00:00",
                "content": "mega ðŸŽ‰"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 16:13:49.651000+00:00",
                "content": "but the parser will also be updated to make sure you don't have to think as hard about this"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-04 22:56:05.938000+00:00",
                "content": "I've seen the vscode extension error \"recursive type\" thing before, but haven't been able to repro by copying your baml file. If you get it again and have a gist you can share with it lmk"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 22:56:40.444000+00:00",
                "content": "i was able to create it by setting up tests for that function in a different file and passing large messages in the inputs"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-04 22:56:57.706000+00:00",
                "content": "that prompt change made a big difference for me, thanks for the tip! why does that work?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 22:57:42.192000+00:00",
                "content": "you can use the live prompt preview in vscode to take a deeper look ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 22:57:50.388000+00:00",
                "content": "basically the prompt changes from:\n\n```\nenum MyEnumType {\n  MyValue @description(\"some description\")\n}\n\nclass Value {\n   val MyEnumType\n}\n```\n\n```\nMyEnumType\n---\n- MyValue: some description\n\n{\n  val: MyEnumType\n}\n```\n\nbut this does this instead:\n```\nenum MyEnumType {\n  MyValue\n}\n\nclass Value {\n   val MyEnumType @description(\"some description\")\n}\n```\n\n```\n{\n  // some description\n  val: 'MyValue'\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 23:00:52.691000+00:00",
                "content": "we can likely patch this in a few ways:\n1. give warnings when you do something like this\n2. update the parser to support parsing 'MyValue' or 'MyEnumType' into 'MyValue' if there is only value under the enum"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 23:01:48.293000+00:00",
                "content": "Basically in the original case, one could technically assume the enum value is actually 'MyEnumType' and 'MyValue' is just some metadata about it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-04 23:03:31.159000+00:00",
                "content": "hope that helps"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-04 23:14:27.965000+00:00",
                "content": "I repro'd the \"recursive use of an unsafe object\" bug (may be related to yours, so ill patch it)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 06:54:03.719000+00:00",
                "content": "a patch is out in 0.59.0 that fixes one of the VSCode extension crashes. LMK if you run into another one like that."
            }
        ]
    },
    {
        "thread_id": 1292346334116057098,
        "thread_name": "Class parsed different from class returned",
        "messages": [
            {
                "author": "simontam0",
                "timestamp": "2024-10-06 04:42:56.687000+00:00",
                "content": "i'm experiencing a situation where it appears BAML's debug output correct parses what I would have expected for a class, however what I'm actually seeing in the debugger is not that class being returned. What might be wrong here?\n\nHere is my declaration for the union of what classes can be returned:\n`function DoFlightConversationWithTools(travel_context: string, messages: string[], current_date: string) -> SearchForOutboundFlights | SelectOutboundFlight | SelectReturnFlight | SelectFareOption | FlightQuestion {\n`"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-06 04:44:26.380000+00:00",
                "content": ""
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-06 04:44:26.617000+00:00",
                "content": "I'm using BAML 0.58.0"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-06 06:38:02.898000+00:00",
                "content": "Thatâ€™s quite odd! We will check and see how the casting is working under the hood in the generated code"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-06 15:08:47.089000+00:00",
                "content": "Note that the classes SelectOutboundFlight and SelectReturnFlight are similar in structure and differ by only 1 field and subtle changes in the description. Not sure if that matters."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-07 01:43:30.725000+00:00",
                "content": "Hey Simon, ill investigate this tomorrow!"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-09 18:43:03.120000+00:00",
                "content": "<@201399017161097216> any chance to look at this yet or any more i can do to help?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 19:23:41.662000+00:00",
                "content": "Hey Simon, this bug is due to bad union disambiguation on our end. I believe we have a solution here, but may take until sunday to get it in.\n\nContext, while we know what type it is, the python code is parsing it as the first available type it matches against, which doesn't match. We likely need to change how the python class is picked and not rely in pydantic to parse it correctly."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-09 19:23:59.581000+00:00",
                "content": "If you are interested in helping, i can walk you through the code you'd need to change to get it working!"
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-09 21:13:03.465000+00:00",
                "content": "i'm not in any major rush as i currently have a workaround (with a different class). i'd love to help but i have so much other stuff to attend to i'm not sure i'd do it justice or be any quicker :). I'd love to see the portion of code in question though. Just not sure I have the bandwidth myself to enlist in your repo, make the change, build it and test it all."
            },
            {
                "author": "simontam0",
                "timestamp": "2024-10-09 21:13:14.690000+00:00",
                "content": "I am glad though that you have found/figured out what it is though"
            }
        ]
    },
    {
        "thread_id": 1292691236053843998,
        "thread_name": "runtime clients",
        "messages": [
            {
                "author": "jfan8684",
                "timestamp": "2024-10-07 03:33:27.717000+00:00",
                "content": "Is there a way to have OPENAI_API_KEY or ANTHROPIC_API_KEY be passed into functions at runtime? use case is I'm using baml in a python library and the user has to enter their own API key. so I can't use a .env file"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 03:34:09.024000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 03:34:09.265000+00:00",
                "content": "https://docs.boundaryml.com/docs/calling-baml/client-registry"
            },
            {
                "author": "jfan8684",
                "timestamp": "2024-10-07 03:34:29.391000+00:00",
                "content": "awesome thanks"
            }
        ]
    },
    {
        "thread_id": 1292860077144805457,
        "thread_name": "FyiIO Deployment",
        "messages": [
            {
                "author": "moonboie",
                "timestamp": "2024-10-07 14:44:22.570000+00:00",
                "content": "Hi I'm deploying to fly.io the first time with baml for nodejs and I'm running into this error: ``` request_options: {\"model\": String(\"gpt-4o-mini\")}, start_time: SystemTime { tv_sec: 1728312107, tv_nsec: 665987370 }, latency: 8.592489ms, message: \"reqwest::Error { kind: Request, url: Url { scheme: \\\"https\\\", cannot_be_a_base: false, username: \\\"\\\", password: None, host: Some(Domain(\\\"api.openai.com\\\")), port: None, path: \\\"/v1/chat/completions\\\", query: None, fragment: None }, source: hyper_util::client::legacy::Error(Connect, Ssl(Error { code: ErrorCode(5), cause: Some(Ssl(ErrorStack([Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 2147483650, library: \\\"system library\\\", function: \\\"(unknown function)\\\", file: \\\"providers/implementations/storemgmt/file_store.c\\\", line: 263, data: \\\"calling stat(/usr/local/ssl/certs)\\\" }, Error { code: 167772294, library: \\\"SSL routines\\\", function: \\\"(unknown function)\\\", reason: \\\"certificate verify failed\\\", file: \\\"ssl/statem/statem_clnt.c\\\", line: 2092 }]))) }, X509VerifyResult { code: 20, error: \\\"unable to get local issuer certificate\\\" })) }\", code: Other(2)```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 14:47:39.456000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 14:47:39.873000+00:00",
                "content": "hey <@944413275658018837> ! That's odd, we've got flyio working locally (we use it for promptfiddle).\n\n<@711679663746842796> can you investigate why this may not be working for him? It looks like its related to SSL certs. We had a patch we had at some point for this i think."
            },
            {
                "author": "moonboie",
                "timestamp": "2024-10-07 14:52:35.332000+00:00",
                "content": "This is the version for the baml client in my package.json: \"@boundaryml/baml\": \"^0.58.0\""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 14:53:56.594000+00:00",
                "content": "can you run a command on your fly io set up?\n\n`sudo apt-get install ca-certificates`?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 14:54:41.645000+00:00",
                "content": "Oh wait, you can also set up this env var:\n\n```\nDANGER_ACCEPT_INVALID_CERTS=1\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 14:56:47.923000+00:00",
                "content": "while not ideal as the SSL cert should exist on your fly io machine, it will get around this issue for now, and help set you deploy!\n\nSee docs here:\nhttps://docs.boundaryml.com/docs/reference/env-vars"
            },
            {
                "author": "moonboie",
                "timestamp": "2024-10-07 14:58:25.961000+00:00",
                "content": "Interesting could it be an issue with the docker fly I'm using then? I added ca-certificates to the apt install: ```# syntax = docker/dockerfile:1\n\n# Adjust NODE_VERSION as desired\nARG NODE_VERSION=20.9.0\nFROM node:${NODE_VERSION}-slim as base\n\nLABEL fly_launch_runtime=\"Node.js\"\n\n# Node.js app lives here\nWORKDIR /app\n\n# Set production environment\nENV NODE_ENV=\"production\"\n\n\n# Throw-away build stage to reduce size of final image\nFROM base as build\n\n# Install packages needed to build node modules\nRUN apt-get update -qq && \\\n    apt-get install --no-install-recommends -y build-essential node-gyp pkg-config python-is-python3 ca-certificates\n\n# Install node modules\nCOPY --link package-lock.json package.json ./\nRUN npm ci\n\n# Copy application code\nCOPY --link . .\n\n\n# Final stage for app image\nFROM base\n\n# Copy built application\nCOPY --from=build /app /app\n\n# Start the server by default, this can be overwritten at runtime\nEXPOSE 3000\nCMD [ \"npm\", \"start\" ]\n```"
            },
            {
                "author": "moonboie",
                "timestamp": "2024-10-07 14:59:05.269000+00:00",
                "content": "Trying a deployment with the file above, will let you know if that works"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-07 15:00:53.025000+00:00",
                "content": "it looks good to me!\n\nOne small thing i recommend is gitignore the `baml_client` folder, then just run `baml-cli generate` in your docker container"
            },
            {
                "author": "moonboie",
                "timestamp": "2024-10-07 15:04:14.126000+00:00",
                "content": "`DANGER_ACCEPT_INVALID_CERTS=1` did the trick for now! Thanks for your help"
            }
        ]
    },
    {
        "thread_id": 1293349419789324328,
        "thread_name": "baml-cli generate",
        "messages": [
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:08:50.948000+00:00",
                "content": "how are you guys writing files to disk when the client regenerates? i am noticing that another tool that uses fsevents to discover when files changed is getting juked out and not noticing when the BAML client regenerates. i wonder if that is why the tsserver restart thing is happening -- whatever is writing the files isnt emitting change notifications through the normal OS mechanisms"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:10:23.832000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:10:24.133000+00:00",
                "content": "is your behavior the same whether you run `baml-cli generate` vs VSCode? they each use different mecahnisms. The VSCode extension itself writes the files for example"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:10:38.171000+00:00",
                "content": "but the CLI uses our rust runtime to do it (and we nuke the whole baml-cli)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:10:46.094000+00:00",
                "content": "sorry we nuke the baml_client dir and recreate it"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:12:03.312000+00:00",
                "content": "i think the folder swap thing would likely do it, my other tool is watching individual files for changes with `chokidar`"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:12:40.498000+00:00",
                "content": "https://github.com/BoundaryML/baml/blob/canary/typescript/vscode-ext/packages/language-server/src/lib/baml_project_manager.ts#L377-L420"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:12:49.385000+00:00",
                "content": "and when i use the vscode extension i get missed changes in this other tool as well"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:12:50.967000+00:00",
                "content": "this is the VSCode generation"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:13:19.202000+00:00",
                "content": "i think https://github.com/BoundaryML/baml/blob/canary/typescript/vscode-ext/packages/language-server/src/lib/baml_project_manager.ts#L419 would cause problems too"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:13:35.882000+00:00",
                "content": "i am not super familiar but i think that is a folder swap which would change file identities again"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:14:00.424000+00:00",
                "content": "you could definitely argue it is the fault of these file watchers that they arent watching for folder renames too"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:14:19.232000+00:00",
                "content": "so we do the rename so that it's an atomic operation"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:15:16.051000+00:00",
                "content": "checks out"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:15:40.523000+00:00",
                "content": "(but i am betting you wouldnt need to restart the tsserver etc if you just wrote the dumb way)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:15:44.510000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:15:53.727000+00:00",
                "content": "it seems there is an `atomic` flag in chokidrar"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:16:19.362000+00:00",
                "content": "i am not certain but i believe that only controls how it emits events when individual files get removed and recreated quickly"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:16:27.199000+00:00",
                "content": "and i dont think it extends to whole dir trees moving around"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:16:41.527000+00:00",
                "content": "i think it watches inodes, and if you rm the one dir and recreate a whole different dir, its a whole new set of inodes"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:17:11.273000+00:00",
                "content": "we use chokidar in one of our products and had to mess around a lot with those settings"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:17:16.413000+00:00",
                "content": "whats weird is that pylance (python extension) doesnt have this problem. Not sure what sort of problems we'd create for ourselves if we don't do it atomically"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:17:50.298000+00:00",
                "content": "but i can experiment it (though not super high priority at the moment), what do you watch the baml_client for?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:18:39.735000+00:00",
                "content": "are you on mac or windows btw?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:23:31.569000+00:00",
                "content": "i wonder if these settings may help:"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:23:53.342000+00:00",
                "content": "ideally users dont have to do this (it's bad DX, but perhaps to remove your blocker)"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:24:04.210000+00:00",
                "content": "mac for me! and i am watching it via the tsserver so i get updated squiggles in vscode (cursor), as well as a dev server that restarts the dev process when any code changes (https://github.com/gadget-inc/wds)"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:24:24.461000+00:00",
                "content": "similar to nodemon / ts-node-dev or webpack for the server"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:25:32.800000+00:00",
                "content": "ah, it isnt using those atomic flags: https://github.com/gadget-inc/wds/blob/9efd3429144ce96af17a0e15779f2be7add34fc9/src/index.ts#L68 , i can try it otu"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:26:27.322000+00:00",
                "content": "and oh shit i lied we switched away from chokidar entirely in the product i mentioned to https://www.npmjs.com/package/watcher"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:26:44.127000+00:00",
                "content": "> Rename detection: This library can optionally detect when files and directories are renamed, which allows you to provide a better experience to your users in some cases."
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:26:58.869000+00:00",
                "content": "haha, ok maybe theres a setting on this one"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-08 23:27:16.572000+00:00",
                "content": "i shall investigate that on my end"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-08 23:30:35.839000+00:00",
                "content": "yeah my feeling is that\n- TSServer may be able to detect the changes if you mess around with those settings i listed\n- your watch stuff should detect renames\n- there's also some funky stuff you could do with our `on_generate` generator config (click on OpenAPI tab here https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#define-a-generator-clause) + setting the baml.cliPath https://docs.boundaryml.com/docs/calling-baml/generate-baml-client#vscode-generator-settings \n   -so that vscode invokes baml-cli in your terminal, and baml-cli runs whatever command you add to on_generate right after (like perhpas modifying the creation time of these files or whatnot). `on_generate` commands are not executed by VSCode, only the CLI"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:29:11.056000+00:00",
                "content": "were you able to solve this?"
            },
            {
                "author": "airhorns",
                "timestamp": "2024-10-10 23:04:48.680000+00:00",
                "content": "in progress, porting to that better watcher thing still but will let you know if that fixes it"
            }
        ]
    },
    {
        "thread_id": 1293962904411832411,
        "thread_name": "BAML client stall",
        "messages": [
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 15:46:37.080000+00:00",
                "content": "Hi team, \nFor some reason my baml client is stalling randomly for one of the functions and I'm not really sure what is going on. Could I get some help on this?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:51:25.201000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 15:51:25.517000+00:00",
                "content": "yes, give us like 10ish mins"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 15:58:17.333000+00:00",
                "content": "Is there a way to simply enforce timeouts on baml calls? That would make my life a lot easier tbh"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:07:47.372000+00:00",
                "content": "so we've seen anthropic take a while to actually respond, sometimes as long as 30s -- how long does it stall for you?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:08:32.779000+00:00",
                "content": "Also yes, we should really add the timeout -- I assume this is affecting you in prod right? I can add that task to the top of my current work stack"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 16:36:19.031000+00:00",
                "content": "This is impacting our prod environment"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 16:36:23.035000+00:00",
                "content": "it's Azure GPT 4o"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 16:36:37.008000+00:00",
                "content": "it just stalls indefinitely. I worry it's an issue with baml"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 16:36:50.563000+00:00",
                "content": "and if it's an issue with the azure client, i would wnat a timeout"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 16:42:23.464000+00:00",
                "content": "I also use the sync client in case that's helpful to know"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 16:43:20.325000+00:00",
                "content": "ok sounds good, I'll prioritize this"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 18:49:15.095000+00:00",
                "content": "any updates on this front? I have tried creating timeouts around my baml functions though I'm worried something is messing with the event loop which prevents the timout from being applied"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 18:49:58.458000+00:00",
                "content": "Or could it be an issue with the Azure client ratelimiting and baml not propagating that error?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 18:55:01.345000+00:00",
                "content": "Have you tried swapping to the async client there? Is it always happening in the same function? Or different ones?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 18:55:52.832000+00:00",
                "content": "It will take till end of day to fix, maybe till tomorrow morning. Can you try adding BAML_LOG=debug potentially?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:12:26.791000+00:00",
                "content": "yes I tried swapping to async client"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:12:32.073000+00:00",
                "content": "different ones"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:12:37.671000+00:00",
                "content": "I have BAML_LOG set to debug"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:22:40.304000+00:00",
                "content": "Is there anything in the debug logs or does it just pause? Whats the line that prints before pausing?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:22:52.107000+00:00",
                "content": "You should see logs for the actual requests"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:23:33.334000+00:00",
                "content": ""
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:23:51.177000+00:00",
                "content": "Not sure how to interpret this, but it stalls after that last log"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:27:55.167000+00:00",
                "content": "it seems it's just getting stuck in the request to azure"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:28:23.042000+00:00",
                "content": "like it's holding to your request until there's capacity perhaps. I'll work on the timeout"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:30:18.531000+00:00",
                "content": "capacity with Azure or with baml?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:30:26.918000+00:00",
                "content": "like is the issue that I need to increase my TPM limit with Azure?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:30:39.543000+00:00",
                "content": "can you try to increase the TPM in azure?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:30:58.146000+00:00",
                "content": "Is that the issue?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:31:01.446000+00:00",
                "content": "I'm already at max quota"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:31:05.347000+00:00",
                "content": "but i'll make a request if that's the problem"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:31:41.535000+00:00",
                "content": "I think that might be the issue -- but I'll def add a timeout so it can just retry if the endpoint doesnt resopnd within some amount of seconds"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 19:34:22.037000+00:00",
                "content": "I would've thought in that case I'd get a 429 or something from Baml?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:36:48.694000+00:00",
                "content": "yeah this could be something on our end -- ill aim to get a patch out in the next couple hours"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 20:55:25.863000+00:00",
                "content": "What should happen when a 429 is returned?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:30:35.582000+00:00",
                "content": "Hey <@1049713528170364968>\n\nSo there are a few ideas we can try here. I think its cause we reuse the HTTP client and maybe something is wrong there (we don't have a repro, so debugging is hard).\n\nideas of what to do, before calling that pipeline can you run:\n`reset_baml_env_vars`? This will reset the HTTP client we have and likely prevent this issue. (you can't call `reset_baml_env_vars` inside of a traced function fyi, so you'll have to do it before you enter any traced context)\n\nhttps://docs.boundaryml.com/docs/calling-baml/set-env-vars#setting-environment-variables-in-pythontypescriptetc"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:31:10.132000+00:00",
                "content": "what will that do?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:31:26.207000+00:00",
                "content": "429 should be raising an exception, but there is also one case we don't yet handle correctly (`finish_reason=content_policy`). so that could be it as well. We stall there."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:31:42.571000+00:00",
                "content": "that will just re-instantiate the existing `b` object"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:32:23.786000+00:00",
                "content": "there's a chance we have the content policy issue"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:32:31.312000+00:00",
                "content": "OH"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:32:40.538000+00:00",
                "content": "ok, yea for that I'm working on that ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:33:24.312000+00:00",
                "content": "i'm hoping to ship something by tmrw for that (its a language feature, so we're getting approval on the design with the team)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:37:44.291000+00:00",
                "content": "Is there anyway I can validate that the content issue may be the problem?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:37:55.324000+00:00",
                "content": "you can try the curl request?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:38:00.473000+00:00",
                "content": "what curl request?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:38:04.783000+00:00",
                "content": "but likely hard without that"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:38:15.780000+00:00",
                "content": "for that sepcific BAML function that is failing"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:38:30.347000+00:00",
                "content": "i'm not following"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:39:05.029000+00:00",
                "content": "basically call the function that has that prompt"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:39:09.943000+00:00",
                "content": "with the parameters one-off"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:40:02.423000+00:00",
                "content": "but i can't really get that without knowing the underlying prompt right?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:40:11.623000+00:00",
                "content": "when baml stalls, nothing gets flushed into the dashboard"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:40:51.415000+00:00",
                "content": "oh damn yes... any way you can save to disk?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:40:58.599000+00:00",
                "content": "the image"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:47:51.745000+00:00",
                "content": "Have you guys seen this before? \n```\nlogger=src.workflows.opportunity_generation.extract_text_from_document_and_divide_into_pages_task module=extract_text_from_document_and_divide_into_pages_task file=extract_text_from_document_and_divide_into_pages_task.py extract_text_from_document_and_divide_into_pages_task() L83   Error processing document document_01j9w62jrqecs9hhx1f0tj6qtw: Span already finished\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 21:48:18.222000+00:00",
                "content": "yea did that happen after after the reset?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 21:51:18.931000+00:00",
                "content": "no"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:06:11.507000+00:00",
                "content": "hmm, this is odd. I'll take another look into this! This looks like a pretty core issue"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:06:29.106000+00:00",
                "content": "any code changes that may have caused this?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:06:37.549000+00:00",
                "content": "(and is this a random thing or happens regularly)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:11:27.660000+00:00",
                "content": "we are dealing with a lot of volume and now we are really noticing this"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:11:44.157000+00:00",
                "content": "i don't think it's a resources issue because i have added a ton of memory and CPU"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:11:49.416000+00:00",
                "content": "got it, let me spin up a load test!"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:12:28.030000+00:00",
                "content": ""
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:12:29.479000+00:00",
                "content": "this will likely have to be fixed by monday fyi.\n\nDo you know your TPS?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:12:31.923000+00:00",
                "content": "stalling here"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:12:46.250000+00:00",
                "content": "I have 1 Million TPM on Azure for all of my azure clients"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:12:54.066000+00:00",
                "content": "no i mean for your useres"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:13:06.592000+00:00",
                "content": "like how many requests are you getting? so i spin up a load test to simulate it"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 22:13:10.230000+00:00",
                "content": "requests per second*"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:13:16.114000+00:00",
                "content": "```python\n            async def run_baml_extraction():\n                \"\"\"\n                Asynchronously run BAML extraction for compliance requirements and evaluation criteria.\n\n                Returns:\n                    Tuple[List, List]: Lists of extracted compliance requirements and evaluation criteria.\n                \"\"\"\n                loop = asyncio.get_event_loop()\n                \n                baml_compliance_requirements = await loop.run_in_executor(\n                    None,\n                    partial(\n                        baml_sync_client.ExtractComplianceMatrixRequirements,\n                        document_text=formatted_excerpt,\n                    ),\n                )\n                baml_evaluation_criteria = await loop.run_in_executor(\n                    None,\n                    partial(\n                        baml_sync_client.ExtractEvaluationCriteria,\n                        document_text=formatted_excerpt,\n                    ),\n                )\n                return baml_compliance_requirements, baml_evaluation_criteria\n```"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:13:36.506000+00:00",
                "content": "I started running my baml functions like this so that I would be able to force a timeout"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:14:24.497000+00:00",
                "content": "that call pattern may be... not as well supported... that likely caused the span issue (whenever you timeout)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:14:38.351000+00:00",
                "content": "we should fix it regardless"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:14:57.039000+00:00",
                "content": "i think we need some exception handler in the python to handle graceful abortion"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:15:07.794000+00:00",
                "content": "(in our python code fyi, not yours)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:15:16.260000+00:00",
                "content": "but this is helpful, as this should be easier to repro"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:15:18.465000+00:00",
                "content": "i'm not sure if I have another way to enforce a timeout"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:15:23.218000+00:00",
                "content": "since I have to use the sync client"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:15:41.956000+00:00",
                "content": "and thats due to celery workers right?"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:15:45.832000+00:00",
                "content": "correct"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:15:46.352000+00:00",
                "content": "yea that makes sense"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:16:14.891000+00:00",
                "content": "ok, let me try and check, but thats almost def the issue. I'll see what we can do here"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:16:54.642000+00:00",
                "content": "it seems like if we added timeouts in baml functions, that may be a twofer for you"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:17:05.152000+00:00",
                "content": "I think it's a requirement"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:17:11.102000+00:00",
                "content": "since otherwise baml randomly is stalling"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:17:32.674000+00:00",
                "content": "(that bug we likely have a patch for, the core rust http library has a weird known issue accross the python boundary. they have some workarounds that we'll be adding in)"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:23:05.402000+00:00",
                "content": "I removed the timeout logic and am trying again"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:27:27.900000+00:00",
                "content": "Is the fallback provider actually using the fallbacks?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:27:49.527000+00:00",
                "content": "it should be! our tests seem to pass for it"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 22:28:11.119000+00:00",
                "content": "but we only trigger the fallback in non-200 HTTP status cases"
            },
            {
                "author": "gabev2037",
                "timestamp": "2024-10-10 22:28:23.640000+00:00",
                "content": "The logs don't tell me whether a fallback client was used right?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-11 18:55:38.328000+00:00",
                "content": "FYI, https://github.com/BoundaryML/baml/commit/eb90e62ffe21109e0da1bd74439d36bb37246ec3"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-11 18:55:42.448000+00:00",
                "content": "this should fix the stall issue!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-11 18:55:50.081000+00:00",
                "content": "i'll do a release tonight!"
            }
        ]
    },
    {
        "thread_id": 1293965556994801767,
        "thread_name": "Hi there, I'm running into a pydantic",
        "messages": [
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 15:57:09.505000+00:00",
                "content": "Hi there, I'm running into a pydantic error when using nested optional classes in my baml, and will include the class and error below. Any ideas?"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 15:59:30.980000+00:00",
                "content": ""
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 15:59:31.454000+00:00",
                "content": "`class BargeDetails {\n  barge_names string[] @description(#\"\n    A barge or list of barge names, only if they are grouped together\n  \"#)\n  number_of_barrels float?\n  number_of_tons float?\n  product string? @description(#\"\n      The type of cargo on the voyage. It could be a code or a product name\n  \"#)\n  current_location string? @description(#\"\n    current location or mile marker\n  \"#) //added this to stop it confusing location as boat name\n  attached_boat_name string? @alias(tug) @description(#\"\n    The tug boat name, if present. This should be a proper name, not a location or code.\n  \"#)\n  is_loaded bool?\n  ETA ETA? // is can be ETA or NONE. need to figure this out.\n  origin_location string?\n  origin_load_date string? @description(#\"\n    a single datetime for origin or loading, either an exact date or the lower bound.\n  \"#)\n  origin_berth string? @description(#\"\n    The berth or terminal the voyage is starting at, usually the first berth\n  \"#)\n  origin_end_window string? @description(#\"\n    a single datetime for origin or loading, either not given or the upper bound.\n  \"#)\n  origin_port string? @description(#\"\n    \"The port or terminal the voyage is starting at, usually the first port\"\n  \"#)\n  destination_location string?\n  destination_discharge_date string? @description(#\"\n    a single datetime for unload or discharge at the destination location\n  \"#)\n  destination_end_window string? @description(#\"\n    a single datetime for origin or loading, either not given or the upper bound.\n  \"#)\n  destination_berth string? @description(#\"\n    The berth or terminal the voyage is starting at, usually the next berth\n  \"#)\n  destination_port string? @description(#\"\n    \"The port or terminal the voyage is ending at, usually the next port\"\n  \"#)\n  ref_no string? @description(#\"\n    The reference number for the voyage. Sometimes it's referred to as Ref No.\n  \"#)\n  carrier string? @description(#\"\n    The carrier the email is from or to.\n  \"#)\n}`"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 16:00:20.718000+00:00",
                "content": "pydantic_core._pydantic_core.ValidationError: 6 validation errors for ExtractBargeDetailsReturnType\ninner.0.ETA\n  Input should be None [type=none_required, input_value={'hasArrived': False, 'et...datetime': '2025-10-06'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/none_required\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/backend/atds/webatds/inbound_email/baml_client/async_client.py\", line 84, in ExtractBargeDetails\n    return coerce(mdl, raw.parsed())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/backend/atds/webatds/inbound_email/baml_client/async_client.py\", line 34, in coerce\n    raise TypeError(\nTypeError: Internal BAML error while casting output to ExtractBargeDetailsReturnType\n[{'ETA': {'eta_datetime': '2025-10-06', 'hasArrived': False},"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:30:24.038000+00:00",
                "content": "<@99252724855496704> can you take this one?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:31:42.957000+00:00",
                "content": "taking a look!"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:32:23.562000+00:00",
                "content": "mike have you seen our observability dashboard? That may help make this easier to debug fyi"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:32:41.984000+00:00",
                "content": "copying code as text block:\n\n```\nclass BargeDetails {\n  barge_names string[] @description(#\"\n    A barge or list of barge names, only if they are grouped together\n  \"#)\n  number_of_barrels float?\n  number_of_tons float?\n  product string? @description(#\"\n      The type of cargo on the voyage. It could be a code or a product name\n  \"#)\n  current_location string? @description(#\"\n    current location or mile marker\n  \"#) //added this to stop it confusing location as boat name\n  attached_boat_name string? @alias(tug) @description(#\"\n    The tug boat name, if present. This should be a proper name, not a location or code.\n  \"#)\n  is_loaded bool?\n  ETA ETA? // is can be ETA or NONE. need to figure this out.\n  origin_location string?\n  origin_load_date string? @description(#\"\n    a single datetime for origin or loading, either an exact date or the lower bound.\n  \"#)\n  origin_berth string? @description(#\"\n    The berth or terminal the voyage is starting at, usually the first berth\n  \"#)\n  origin_end_window string? @description(#\"\n    a single datetime for origin or loading, either not given or the upper bound.\n  \"#)\n  origin_port string? @description(#\"\n    \"The port or terminal the voyage is starting at, usually the first port\"\n  \"#)\n  destination_location string?\n  destination_discharge_date string? @description(#\"\n    a single datetime for unload or discharge at the destination location\n  \"#)\n  destination_end_window string? @description(#\"\n    a single datetime for origin or loading, either not given or the upper bound.\n  \"#)\n  destination_berth string? @description(#\"\n    The berth or terminal the voyage is starting at, usually the next berth\n  \"#)\n  destination_port string? @description(#\"\n    \"The port or terminal the voyage is ending at, usually the next port\"\n  \"#)\n  ref_no string? @description(#\"\n    The reference number for the voyage. Sometimes it's referred to as Ref No.\n  \"#)\n  carrier string? @description(#\"\n    The carrier the email is from or to.\n  \"#)\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:33:06.731000+00:00",
                "content": "whats the type of ETA?"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 19:33:35.663000+00:00",
                "content": "Ya, you showed it to me on a call once! I wasn't able to get it to load yesterday but will try again"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:34:07.532000+00:00",
                "content": "try beta.app.boundaryml.com if it doesn't load (sorry we are fixing some issues there)"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:34:16.893000+00:00",
                "content": "https://beta.app.boundaryml.com"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 19:35:25.076000+00:00",
                "content": "```class ETA {\n  hasArrived bool //if arrived, voyage should be compeleted\n  eta_datetime string? //anytime our ETA differs from carrier is a value point. We should record and highlight to customers\n}```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:37:10.818000+00:00",
                "content": "it looks like. it might be due to some weird naming thing?\nhttps://docs.pydantic.dev/2.9/errors/validation_errors/#none_required"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:37:54.019000+00:00",
                "content": "can you try renaming the field?\n\nOr paste in the generated Pydantic model for BargeDetails + ETA?"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:38:17.554000+00:00",
                "content": "it would be super weird if `ETA` was reserved somehow..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:38:42.764000+00:00",
                "content": "oh yep thats it!\n\n```\nFor example, the following would yield the none_required validation error since the field int is set to a default value of None and has the exact same name as its type, which causes problems with validation\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:38:55.106000+00:00",
                "content": "pydantic doesn't like a field named the exact same as the type"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:39:21.552000+00:00",
                "content": "you can solve this by doing this in BAML:\n\n```\nclass BargeDetails {\n  eta ETA? @alias(\"ETA\")\n}\n```"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:39:46.304000+00:00",
                "content": "so your python code will get `.eta` but your prompt with still be `ETA`"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 20:15:11.401000+00:00",
                "content": "ðŸ˜‚"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 20:15:13.125000+00:00",
                "content": "wow"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 20:15:14.821000+00:00",
                "content": "nice find"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 20:15:42.144000+00:00",
                "content": "thanks Vaibhav"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 20:15:44.772000+00:00",
                "content": "python man..."
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 20:15:47.903000+00:00",
                "content": "its a crapshoot"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 20:16:08.152000+00:00",
                "content": "I guess that makes sense though"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 20:16:14.177000+00:00",
                "content": "how would it know the difference!"
            }
        ]
    },
    {
        "thread_id": 1294018705407086674,
        "thread_name": "LLM Playground error",
        "messages": [
            {
                "author": "guillec",
                "timestamp": "2024-10-10 19:28:21.074000+00:00",
                "content": "Hello hell, I've been having this issue with Playground. I search on discrod and I think I saw something about env variables but I believe I have all the env variables I need. Any help would be appreciated.\n\n```\nSimpleTest\nLLM Failed\n14ms using Sonnet (+ 443ms for BAML)\nRaw LLM Response\nUnspecified error code: 2\nreqwest::Error { kind: Request, source: \"JsValue(TypeError: Failed to fetch\\nTypeError: Failed to fetch\\n    at __wbg_fetch_1e4e8ed1f64c7e28 (https://vscode-remote+dev-002dcontainer-002b7b22686f737450617468223a222f55736572732f6775696c6c656361726c6f732f70726f6a656374732f415444532f6261636b656e642f61746473222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6775696c6c656361726c6f732f70726f6a656374....\n```"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:29:19.397000+00:00",
                "content": ""
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:29:19.775000+00:00",
                "content": "which model are you using? Are you on version 0.60.0 of the vscode extension?\n\nCan you try running the `raw curl` request (see the checkbox in the playground)"
            },
            {
                "author": "guillec",
                "timestamp": "2024-10-10 19:30:15.722000+00:00",
                "content": "I am pretty sure I am on 0.60.0 '"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:31:05.815000+00:00",
                "content": "there's a curl checkbox above the prompt preview that you can toggle to run things"
            },
            {
                "author": "mikeopentug",
                "timestamp": "2024-10-10 19:31:42.200000+00:00",
                "content": "yes, we tried curl and got a response from anthropic"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:32:34.079000+00:00",
                "content": "Try opening the web developer tools panel (command shift + p -> open web developer tools), and check if you see more exact errors there, especially in the `network` tab)"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:33:21.042000+00:00",
                "content": ""
            },
            {
                "author": "guillec",
                "timestamp": "2024-10-10 19:36:18.405000+00:00",
                "content": "{\"status\":\"error\",\"message\":\"No API token present\"}\n\nI have the API KEY set in the ENV VARS"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:38:49.399000+00:00",
                "content": "can you hop into office hours chat?"
            },
            {
                "author": ".aaronv",
                "timestamp": "2024-10-10 19:38:58.025000+00:00",
                "content": "I can help debug faster there as well if you have a chance"
            },
            {
                "author": "guillec",
                "timestamp": "2024-10-10 19:39:15.341000+00:00",
                "content": "yes i can"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 19:46:08.267000+00:00",
                "content": "https://us06web.zoom.us/j/82900262657?pwd=RR6p0r4aiumxGXh2VsM7jvVo7HP7QL.1"
            },
            {
                "author": "guillec",
                "timestamp": "2024-10-10 20:06:38.910000+00:00",
                "content": "yeah running it outside the container worked"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 20:15:20.834000+00:00",
                "content": "ðŸ™‚"
            },
            {
                "author": "hellovai",
                "timestamp": "2024-10-10 20:15:26.035000+00:00",
                "content": "glad we found the bug"
            },
            {
                "author": "guillec",
                "timestamp": "2024-10-10 20:16:22.384000+00:00",
                "content": "yeah curiously though it seems to work for mike when running via the container. so not sure what is going on but i least we have a solution to move forward. thanks for the help"
            }
        ]
    }
]