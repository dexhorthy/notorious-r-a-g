[
    {
        "text": "",
        "metadata": {
            "message_id": 1121566198178529380,
            "username": "hellovai",
            "created_at": "2023-06-22T22:23:39.338000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1149399926946664488,
            "username": "anaygupta2004",
            "created_at": "2023-09-07T17:44:57.402000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I see",
        "metadata": {
            "message_id": 1149454200888098906,
            "username": "hellovai",
            "created_at": "2023-09-07T21:20:37.318000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "thanks for the feedback! We'll take a look and share some docs soon that elaborate more about the syntax",
        "metadata": {
            "message_id": 1149454271893491823,
            "username": "hellovai",
            "created_at": "2023-09-07T21:20:54.247000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@501592189739204618> what's your take on stuff like this? https://rivet.ironcladapp.com/  https://docs.stack-ai.com/stack-ai/welcome-to-stack-ai/quickstart-guide",
        "metadata": {
            "message_id": 1149758532950949958,
            "username": ".aaronv",
            "created_at": "2023-09-08T17:29:55.736000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1177004038835949578,
            "username": ".null1",
            "created_at": "2023-11-22T21:53:50.536000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1177009330495688828,
            "username": "hellovai",
            "created_at": "2023-11-22T22:14:52.166000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1177837985891352596,
            "username": "chirswoffle",
            "created_at": "2023-11-25T05:07:39.008000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1177877538740129822,
            "username": "hellovai",
            "created_at": "2023-11-25T07:44:49.142000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@248592148708982784> when you're ready tmrw, would be happy to set you up. We did a release, so you should be able to get release versions of baml on vscode and via:\n```\nbrew install gloohq/baml/baml\n```",
        "metadata": {
            "message_id": 1177877759452778596,
            "username": "hellovai",
            "created_at": "2023-11-25T07:45:41.764000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "sweet, I’ll try it out tomorrow",
        "metadata": {
            "message_id": 1177959328133693460,
            "username": "chirswoffle",
            "created_at": "2023-11-25T13:09:49.253000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "are you free around 9:30PM your time?",
        "metadata": {
            "message_id": 1178176971214163988,
            "username": "chirswoffle",
            "created_at": "2023-11-26T03:34:39.407000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "any chance you can do a bit after (11 pst?)? or maybe <@201399017161097216> can you get on?",
        "metadata": {
            "message_id": 1178177726906118266,
            "username": "hellovai",
            "created_at": "2023-11-26T03:37:39.578000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yeah ill be on",
        "metadata": {
            "message_id": 1178177834376773673,
            "username": ".aaronv",
            "created_at": "2023-11-26T03:38:05.201000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yeah, 11pst works for me too",
        "metadata": {
            "message_id": 1178184121642647572,
            "username": "chirswoffle",
            "created_at": "2023-11-26T04:03:04.202000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Awesome, I'll join the meeting in the office hours audio channel",
        "metadata": {
            "message_id": 1178185787842494545,
            "username": "hellovai",
            "created_at": "2023-11-26T04:09:41.455000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> lemme know when you're good to go",
        "metadata": {
            "message_id": 1178229010619187250,
            "username": "chirswoffle",
            "created_at": "2023-11-26T07:01:26.568000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’ll be on in 5!",
        "metadata": {
            "message_id": 1178229059730296873,
            "username": "hellovai",
            "created_at": "2023-11-26T07:01:38.277000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sorry for the delay",
        "metadata": {
            "message_id": 1178229080479514644,
            "username": "hellovai",
            "created_at": "2023-11-26T07:01:43.224000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "no worries, take your time",
        "metadata": {
            "message_id": 1178229234028777536,
            "username": "chirswoffle",
            "created_at": "2023-11-26T07:02:19.833000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@248592148708982784> learned a bit, that our json parser (which uses the JSON5 spec), says that new liens are escaped with \\ instead of \\n\n\n```{\n  \"messages\": [\n    {\n      \"user\": \"User\",\n      \"content\": \"Hey! its urgent \\\nthat we speak to john@corp.co\\\nm\"\n    },\n    {\n      \"user\": \"User\",\n      \"content\": \"Its about the call with Acme Org later today\"\n    },\n    {\n      \"user\": \"AI\",\n      \"content\": \"Sure, when would you like to chat with John?\"\n    },\n    {\n      \"user\": \"User\",\n      \"content\": \"3\"\n    }\n  ]\n}\n```\n\nThis test case works for example. We'll work on fixing this so its handled correctly",
        "metadata": {
            "message_id": 1178248151430348800,
            "username": "hellovai",
            "created_at": "2023-11-26T08:17:30.093000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "but for now this should unblock to you visualize what thew new lines look like at the very least",
        "metadata": {
            "message_id": 1178248251326079037,
            "username": "hellovai",
            "created_at": "2023-11-26T08:17:53.910000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Got a patch, we'll do a release in the AM 🙂",
        "metadata": {
            "message_id": 1178251130690613329,
            "username": "hellovai",
            "created_at": "2023-11-26T08:29:20.404000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "sweet, thanks",
        "metadata": {
            "message_id": 1178252401656987719,
            "username": "chirswoffle",
            "created_at": "2023-11-26T08:34:23.426000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "actually fixed 🙂 \n\nif you do `baml update-client` it should update to the python client to v0.5.1 and that prevents the max recursion issue and lets you know there's a different issue.",
        "metadata": {
            "message_id": 1178254467414622288,
            "username": "hellovai",
            "created_at": "2023-11-26T08:42:35.941000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://github.com/GlooHQ/baml/pull/146",
        "metadata": {
            "message_id": 1178254489967415399,
            "username": "hellovai",
            "created_at": "2023-11-26T08:42:41.318000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@248592148708982784> had any issues so far?",
        "metadata": {
            "message_id": 1179125226357928066,
            "username": "hellovai",
            "created_at": "2023-11-28T18:22:41.051000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I ended up going back to the playground to debug the rest of the prompts, but I might try baml again when debugging the whole pipeline\nbaml has a lot more features, but it didn't feel worth the tradeoff with the UX in the playground, specifically with editing messages array with large strings and newlines",
        "metadata": {
            "message_id": 1179301675509481542,
            "username": "chirswoffle",
            "created_at": "2023-11-29T06:03:49.807000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Interesting! Would love to double click on that at some point.  Do you mean for the prompt or for editing the actual inputs in the prompts?",
        "metadata": {
            "message_id": 1179335630279430224,
            "username": "hellovai",
            "created_at": "2023-11-29T08:18:45.255000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I ended up going back to the playground",
        "metadata": {
            "message_id": 1179336647477821466,
            "username": ".aaronv",
            "created_at": "2023-11-29T08:22:47.774000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1180695739446329474,
            "username": "krawfy",
            "created_at": "2023-12-03T02:23:20.550000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1180698241126371329,
            "username": "hellovai",
            "created_at": "2023-12-03T02:33:16.997000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1181453336654913596,
            "username": "paule47",
            "created_at": "2023-12-05T04:33:45.800000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1181453469387849778,
            "username": "paule47",
            "created_at": "2023-12-05T04:34:17.446000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1181464989245128744,
            "username": "hellovai",
            "created_at": "2023-12-05T05:20:03.994000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1182098371670986762,
            "username": "qajam",
            "created_at": "2023-12-06T23:16:54.132000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1182259435725410304,
            "username": "hellovai",
            "created_at": "2023-12-07T09:56:54.795000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1182729201841868831,
            "username": "abh12",
            "created_at": "2023-12-08T17:03:35.765000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1182771169120759868,
            "username": "hellovai",
            "created_at": "2023-12-08T19:50:21.544000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1183097596122779798,
            "username": "anish.pi",
            "created_at": "2023-12-09T17:27:27.806000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1183112934143041576,
            "username": "hellovai",
            "created_at": "2023-12-09T18:28:24.675000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1183168227765928067,
            "username": "anish.pi",
            "created_at": "2023-12-09T22:08:07.702000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1183204083792875611,
            "username": "gabev2037",
            "created_at": "2023-12-10T00:30:36.445000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1183204095226544169,
            "username": "gabev2037",
            "created_at": "2023-12-10T00:30:39.171000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Let me know if you have any more issues with installing the python client!",
        "metadata": {
            "message_id": 1183204892710543360,
            "username": "hellovai",
            "created_at": "2023-12-10T00:33:49.306000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1183215952087748628,
            "username": "ayusha9arwal",
            "created_at": "2023-12-10T01:17:46.067000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "qq: does gloo support conditionally injecting items into the prompt? I took a quick glance at the docs and didn't find anything\n\nfor example, if i use jinja templating for prompts, i can conditionally render certain sections based on provided variables",
        "metadata": {
            "message_id": 1183429197180436531,
            "username": "gabev2037",
            "created_at": "2023-12-10T15:25:07.659000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1184577435946201139,
            "username": "havok09171992",
            "created_at": "2023-12-13T19:27:49.114000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1184940322212560896,
            "username": ".aaronv",
            "created_at": "2023-12-14T19:29:47.942000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1184986268992548914,
            "username": "pianomansam",
            "created_at": "2023-12-14T22:32:22.508000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1184990319406743562,
            "username": "dsinghvi",
            "created_at": "2023-12-14T22:48:28.202000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hellooo 🙂",
        "metadata": {
            "message_id": 1185005574153314384,
            "username": ".aaronv",
            "created_at": "2023-12-14T23:49:05.217000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey team, \nFinally back in Sao Paulo and diving into baml. Is there  a way to use the messages functionality of the ChatCompletions endpoint in baml? I was going through the `baml_src` directory and noticed there is a `Conversation` object, though I'd really like a simple way to call the ChatCompletions endpoint and just provide the previous chat history the way OpenAI requests it",
        "metadata": {
            "message_id": 1186333071654527090,
            "username": "gabev2037",
            "created_at": "2023-12-18T15:44:05.285000+00:00",
            "edited_at": "2023-12-18T15:44:28.155000+00:00"
        }
    },
    {
        "text": "OpenAI ChatCompletion",
        "metadata": {
            "message_id": 1186338235446738999,
            "username": "hellovai",
            "created_at": "2023-12-18T16:04:36.429000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186469019004452985,
            "username": "samtrunk",
            "created_at": "2023-12-19T00:44:17.659000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi guys, \nI use Doppler for secrets management. This means that my secrets are injected at runtime with a command such as `doppler run -- poetry run python test.py`. As a result, I can't run my Baml tests. Any way I can modify the execution script for the tests ?",
        "metadata": {
            "message_id": 1186661036510679113,
            "username": "gabev2037",
            "created_at": "2023-12-19T13:27:18.200000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Request: I have a containerized application using Docker. Since baml doesn't get thrown into my dependencies, I have to manually copy the baml_client over to the container package which is annoying",
        "metadata": {
            "message_id": 1186667829349593118,
            "username": "gabev2037",
            "created_at": "2023-12-19T13:54:17.739000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi guys,",
        "metadata": {
            "message_id": 1186669853969494059,
            "username": "hellovai",
            "created_at": "2023-12-19T14:02:20.446000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Request: I have a containerized",
        "metadata": {
            "message_id": 1186670551922638943,
            "username": "hellovai",
            "created_at": "2023-12-19T14:05:06.851000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186670650551717909,
            "username": "rossir.paulo",
            "created_at": "2023-12-19T14:05:30.366000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186670683271471196,
            "username": "rossir.paulo",
            "created_at": "2023-12-19T14:05:38.167000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://docs.boundaryml.com/v3/syntax/client/client#openai-azure\n\nAre all of these options required for the azure chat completion?",
        "metadata": {
            "message_id": 1186764842313457664,
            "username": "gabev2037",
            "created_at": "2023-12-19T20:19:47.432000+00:00",
            "edited_at": "2023-12-19T20:20:01.522000+00:00"
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186767358740680785,
            "username": "lukaskf",
            "created_at": "2023-12-19T20:29:47.395000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186767463610863666,
            "username": "lukaskf",
            "created_at": "2023-12-19T20:30:12.398000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Question: Suppose all of my providers fail, am I guaranteed the correct return type for my implementation?",
        "metadata": {
            "message_id": 1186767534695915551,
            "username": "gabev2037",
            "created_at": "2023-12-19T20:30:29.346000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We throw an exception if that happens! Basically if we can’t guarantee the return type (due to parsing errors or because LLMs failed), we forward the last exception back to you",
        "metadata": {
            "message_id": 1186768021797216337,
            "username": "hellovai",
            "created_at": "2023-12-19T20:32:25.480000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "`Open live preview` and whatnot only seems to appear if there is a comment above the prompt",
        "metadata": {
            "message_id": 1186811038822305852,
            "username": "gabev2037",
            "created_at": "2023-12-19T23:23:21.538000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186822297374425150,
            "username": "ddematheu",
            "created_at": "2023-12-20T00:08:05.786000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186822402718564442,
            "username": "ddematheu",
            "created_at": "2023-12-20T00:08:30.902000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1186822422033342484,
            "username": ".aaronv",
            "created_at": "2023-12-20T00:08:35.507000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@136561315777871873> lets chat in the office hours voice chat? when youre ready",
        "metadata": {
            "message_id": 1186822481537945620,
            "username": ".aaronv",
            "created_at": "2023-12-20T00:08:49.694000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Would one of you cats be willing to help me convert one of my ChatCompletions endpoints (which contains chat history) to Baml? I'm open to the idea of directly inputting the chat conversation into the prompt, though the baml docs are lacking in best practices for this",
        "metadata": {
            "message_id": 1187008986642382919,
            "username": "gabev2037",
            "created_at": "2023-12-20T12:29:55.976000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Would one of you cats be willing to help",
        "metadata": {
            "message_id": 1187012647841968143,
            "username": "gabev2037",
            "created_at": "2023-12-20T12:44:28.874000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "you guys have support for GPT vision?",
        "metadata": {
            "message_id": 1187057471613837332,
            "username": "gabev2037",
            "created_at": "2023-12-20T15:42:35.694000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Not yet but it is on the roadmap for january",
        "metadata": {
            "message_id": 1187058270926540830,
            "username": ".aaronv",
            "created_at": "2023-12-20T15:45:46.265000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Cool. Not urgent, i was just curious",
        "metadata": {
            "message_id": 1187058308419420190,
            "username": "gabev2037",
            "created_at": "2023-12-20T15:45:55.204000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Did you guys figure out the logging issue?",
        "metadata": {
            "message_id": 1187063848432316456,
            "username": "gabev2037",
            "created_at": "2023-12-20T16:07:56.046000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Getting a bunch of API connection errors",
        "metadata": {
            "message_id": 1187065771344547933,
            "username": "gabev2037",
            "created_at": "2023-12-20T16:15:34.504000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1187065943457804419,
            "username": "gabev2037",
            "created_at": "2023-12-20T16:16:15.539000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "^ for azure",
        "metadata": {
            "message_id": 1187065997316866140,
            "username": "gabev2037",
            "created_at": "2023-12-20T16:16:28.380000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I can't tell which llm client it used since it doesn't seem to contain the client name anywhere and the engine is censored out in the dashboard",
        "metadata": {
            "message_id": 1187066297998123038,
            "username": "gabev2037",
            "created_at": "2023-12-20T16:17:40.068000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ok ill fix it being censored, and also send out the client name. Is there no other traceback to expand there (like view more details)? Just connection issue?",
        "metadata": {
            "message_id": 1187066974098968587,
            "username": ".aaronv",
            "created_at": "2023-12-20T16:20:21.263000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey gang, \nI was able to get the connection error (event loop closed) with claude this time and it was just while running tests. Not even on the K8s workers",
        "metadata": {
            "message_id": 1187397598626201750,
            "username": "gabev2037",
            "created_at": "2023-12-21T14:14:08.293000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also, how can I set the max timeout for a specific client? not seeing it in the docs",
        "metadata": {
            "message_id": 1187398047525773343,
            "username": "gabev2037",
            "created_at": "2023-12-21T14:15:55.319000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is this a feature or a bug? \nI have an object that has Optional fields. Claude returns JSON with those Optional fields missing, though the deserializer fails. IMO, BAML should properly handle this case and just return the correct type",
        "metadata": {
            "message_id": 1187401928443965500,
            "username": "gabev2037",
            "created_at": "2023-12-21T14:31:20.602000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Parsing",
        "metadata": {
            "message_id": 1187417620983595144,
            "username": "hellovai",
            "created_at": "2023-12-21T15:33:41.995000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "How do I call different impls in application code? I can't find it in the docs\n\nJK: was hidden in the second tab (https://docs.boundaryml.com/v3/syntax/impl)",
        "metadata": {
            "message_id": 1187434146830757939,
            "username": "gabev2037",
            "created_at": "2023-12-21T16:39:22.064000+00:00",
            "edited_at": "2023-12-21T16:40:05.430000+00:00"
        }
    },
    {
        "text": "While running pytest on the terminal?",
        "metadata": {
            "message_id": 1187445073915424870,
            "username": ".aaronv",
            "created_at": "2023-12-21T17:22:47.284000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "P sure I used the playground but ya basically.\nWas 2 tests for 2 impls so 4 tests total",
        "metadata": {
            "message_id": 1187448168263188500,
            "username": "gabev2037",
            "created_at": "2023-12-21T17:35:05.034000+00:00",
            "edited_at": "2023-12-21T17:35:35.474000+00:00"
        }
    },
    {
        "text": "Is there a clean way to set a timeout in the baml_client call instead of the llm client?",
        "metadata": {
            "message_id": 1187458833661034546,
            "username": "gabev2037",
            "created_at": "2023-12-21T18:17:27.863000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "You want all the baml functions to have a timeout right?",
        "metadata": {
            "message_id": 1187464425607798855,
            "username": ".aaronv",
            "created_at": "2023-12-21T18:39:41.087000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hmmm i'm not sure. might need to think on that one",
        "metadata": {
            "message_id": 1187464677299589230,
            "username": "gabev2037",
            "created_at": "2023-12-21T18:40:41.095000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We will be pushing s fix to our deserializer tomorrow evening to fix issues running tests",
        "metadata": {
            "message_id": 1187601163936346266,
            "username": ".aaronv",
            "created_at": "2023-12-22T03:43:02.045000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Not urgent: do you guys support streaming?",
        "metadata": {
            "message_id": 1187790230854569984,
            "username": "gabev2037",
            "created_at": "2023-12-22T16:14:19.111000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Not yet",
        "metadata": {
            "message_id": 1187790886248132659,
            "username": ".aaronv",
            "created_at": "2023-12-22T16:16:55.369000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "just FYI that on the README there's a typo: \"A programming language for LLM Guardrails, **Type-safey,** and Observability\"\n\nType-safety?",
        "metadata": {
            "message_id": 1189539626713620482,
            "username": "gabev2037",
            "created_at": "2023-12-27T12:05:47.586000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "just FYI that on the README there's a",
        "metadata": {
            "message_id": 1189635681333555415,
            "username": "hellovai",
            "created_at": "2023-12-27T18:27:28.792000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1189695283311939788,
            "username": "joatmon.pockets",
            "created_at": "2023-12-27T22:24:19.011000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Letting you guys know that the tracing detection did not automatically work since I do not call the subflow methods directly. I use a special prefect run deployment command",
        "metadata": {
            "message_id": 1190035377491288115,
            "username": "gabev2037",
            "created_at": "2023-12-28T20:55:43.782000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Bigger issue: I'm still not seeing my retry policy work as intended. I have a max 3 retries and i'm seeing like 9 requests went through",
        "metadata": {
            "message_id": 1190036660528873543,
            "username": "gabev2037",
            "created_at": "2023-12-28T21:00:49.682000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Still running into strange issues with prefect 🧵",
        "metadata": {
            "message_id": 1190314471378403378,
            "username": "gabev2037",
            "created_at": "2023-12-29T15:24:44.950000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Wanted to drop a note\n\nNow that all the initial onboarding problems are *mostly* taken care of, I have to say I **really really** like what Baml offers. I know it's still early and plenty of features to offer, but I do think it's a step-wise improvement over Marvin. Having complete control over the prompt WITH strong type guarantees is fantastic\n\nI also think the dedicated testing playground is awesome. I had a dedicated pytest environment for marvin functions, but even that required setting up a proper conftest and other dependencies, baml just works ! \n\nI'm sure I'll I have a lot more feedback as I continue iterating, but for now, I think it's a huge productivity improvement 😄",
        "metadata": {
            "message_id": 1190368069621657742,
            "username": "gabev2037",
            "created_at": "2023-12-29T18:57:43.767000+00:00",
            "edited_at": "2023-12-29T18:57:58.473000+00:00"
        }
    },
    {
        "text": "i would like a nicer way to see my flows lol",
        "metadata": {
            "message_id": 1190399901587222681,
            "username": "gabev2037",
            "created_at": "2023-12-29T21:04:13.099000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I think the problem is the retries duplicate things. If we removed the duplicates would it be better?",
        "metadata": {
            "message_id": 1190400067169951876,
            "username": ".aaronv",
            "created_at": "2023-12-29T21:04:52.577000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "probably, ya lot of this was before the most recent update i think haha",
        "metadata": {
            "message_id": 1190402058466111509,
            "username": "gabev2037",
            "created_at": "2023-12-29T21:12:47.339000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "noted, we will first take care of the duplicate problem (ECD next week) and then we can see if that's enough or change the UX here a bit more.",
        "metadata": {
            "message_id": 1190405063533940736,
            "username": ".aaronv",
            "created_at": "2023-12-29T21:24:43.803000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "attempts aren't rendered in order in this case",
        "metadata": {
            "message_id": 1190408388845260984,
            "username": "gabev2037",
            "created_at": "2023-12-29T21:37:56.619000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Why is it that when I go to my request history and set the filter to the past 7 days, I only see traces for 2 functions? Is this a bug?",
        "metadata": {
            "message_id": 1191028201154162828,
            "username": "gabev2037",
            "created_at": "2023-12-31T14:40:51.389000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "The dropdown shows the top-level root function. If your top-level function with a @trace calls other nested functions the other ones will show up there",
        "metadata": {
            "message_id": 1191028703149424732,
            "username": ".aaronv",
            "created_at": "2023-12-31T14:42:51.074000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "In this case, the functions which are missing are unrelated to those shown. No nesting. In fact, they were present in the dashboard earlier this week",
        "metadata": {
            "message_id": 1191029021308354582,
            "username": "gabev2037",
            "created_at": "2023-12-31T14:44:06.929000+00:00",
            "edited_at": "2023-12-31T14:44:27.283000+00:00"
        }
    },
    {
        "text": "can you refresh potentially? The default analytics query is 7 days when you refresh and i see tons of functions show up for your project. Seems like a bug",
        "metadata": {
            "message_id": 1191029516458541066,
            "username": ".aaronv",
            "created_at": "2023-12-31T14:46:04.982000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I refreshed and still only see the two... let me try resetting the 7 day filter",
        "metadata": {
            "message_id": 1191029689997869126,
            "username": "gabev2037",
            "created_at": "2023-12-31T14:46:46.357000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm trying to use Optional inputs to a baml function though the client is telling me I have to supply them. Do you guys not actually support optional parameters?",
        "metadata": {
            "message_id": 1191041988859023360,
            "username": "gabev2037",
            "created_at": "2023-12-31T15:35:38.634000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Can you show me your function signature in baml?\n\nSeems like a bug",
        "metadata": {
            "message_id": 1191042833214357574,
            "username": ".aaronv",
            "created_at": "2023-12-31T15:38:59.944000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm trying to use Optional inputs to a",
        "metadata": {
            "message_id": 1191052512095117417,
            "username": "hellovai",
            "created_at": "2023-12-31T16:17:27.569000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "The more I use boundary, the more I would really like to easily convert my production function results into test cases easily",
        "metadata": {
            "message_id": 1191088996277833878,
            "username": "gabev2037",
            "created_at": "2023-12-31T18:42:26.076000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I really like how easy it is for me to score and provide feedback on each result",
        "metadata": {
            "message_id": 1191089061587320852,
            "username": "gabev2037",
            "created_at": "2023-12-31T18:42:41.647000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "What is the default sort for requests? Doesn't appear to be timestamp (unless the timestamp rendered is diff from the one being sorted on)",
        "metadata": {
            "message_id": 1191105245493854308,
            "username": "gabev2037",
            "created_at": "2023-12-31T19:47:00.191000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Wanted to drop in here that the Porter team got back to me and confirmed that the azure OpenAI connection was a known issue due to DNS resolution. They have fixed it for web apps but not for workers. Sounds like they updated it now https://moonape1226.medium.com/domain-resolve-error-name-or-service-not-known-forazure-openai-service-domain-c32607357e57",
        "metadata": {
            "message_id": 1191359658879881287,
            "username": "gabev2037",
            "created_at": "2024-01-01T12:37:57.069000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Domain resolve error (“Name or service n...",
        "metadata": {
            "message_id": 1191368692542488652,
            "username": "hellovai",
            "created_at": "2024-01-01T13:13:50.862000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I found that the top right section of this screen does not update as I go through the various calls of the same function when my retry policy is invoked. Would be nice if I could see on a per-retry basis what the latency was and when it was called",
        "metadata": {
            "message_id": 1191371033018638346,
            "username": "gabev2037",
            "created_at": "2024-01-01T13:23:08.875000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1191836342066352219,
            "username": "estaudere",
            "created_at": "2024-01-02T20:12:07.197000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "How long until I can automatically convert a request's input into a test case with a click?",
        "metadata": {
            "message_id": 1191891237364576407,
            "username": "gabev2037",
            "created_at": "2024-01-02T23:50:15.256000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is it possible to programmatically rate an Baml function result? I want to use the user’s actions to programmatically assign a rating to each result",
        "metadata": {
            "message_id": 1192165284493131826,
            "username": "gabev2037",
            "created_at": "2024-01-03T17:59:13.183000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1192178061014204417,
            "username": "akash_17396",
            "created_at": "2024-01-03T18:49:59.343000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1192178132191547504,
            "username": "hellovai",
            "created_at": "2024-01-03T18:50:16.313000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1192178404674515116,
            "username": "gabev2037",
            "created_at": "2024-01-03T18:51:21.278000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1192178482231382108,
            "username": ".aaronv",
            "created_at": "2024-01-03T18:51:39.769000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Did we lose the shortcut to open the .baml file from a function invocation? Not seeing the option 😦",
        "metadata": {
            "message_id": 1192518862311280712,
            "username": "gabev2037",
            "created_at": "2024-01-04T17:24:12.704000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Claude works a lot better with xml tags than markdown formatting. Is there a way I could configure maybe an input adapter to check the LLM being invoked and then update the prompt accordingly?",
        "metadata": {
            "message_id": 1192542918267908096,
            "username": "gabev2037",
            "created_at": "2024-01-04T18:59:48.091000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Why is the default for rendering a class not to render a JSON object?",
        "metadata": {
            "message_id": 1192559002324828190,
            "username": "gabev2037",
            "created_at": "2024-01-04T20:03:42.829000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "no easy way to delete tests from the vscode playground huh? not urgent, just a nice to have",
        "metadata": {
            "message_id": 1192597386745348176,
            "username": "gabev2037",
            "created_at": "2024-01-04T22:36:14.388000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Unable to filter by specific tags for some reason in the requests history?",
        "metadata": {
            "message_id": 1192818637644505138,
            "username": "gabev2037",
            "created_at": "2024-01-05T13:15:24.713000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If a python function that has the @trace decorator fails, does it not get captured in the gloo dashboard?",
        "metadata": {
            "message_id": 1192820206884634654,
            "username": "gabev2037",
            "created_at": "2024-01-05T13:21:38.849000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Got \n```\nERROR: Failed to create file: No such file or directory (os error 2)\n```\n\nWhen trying to import a baml request as a test case",
        "metadata": {
            "message_id": 1192824688271962133,
            "username": "gabev2037",
            "created_at": "2024-01-05T13:39:27.295000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "btw, can I actually trust the `Total Cost (all dependencies)` ? Doesn't pass the sniff test... I have 510 invocations with a 1% error rate and the total cost is $0.191...",
        "metadata": {
            "message_id": 1192829009269117061,
            "username": "gabev2037",
            "created_at": "2024-01-05T13:56:37.501000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Analytics - cost",
        "metadata": {
            "message_id": 1192856339899220054,
            "username": ".aaronv",
            "created_at": "2024-01-05T15:45:13.631000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If a python function that has the @trace",
        "metadata": {
            "message_id": 1192863647215661173,
            "username": "hellovai",
            "created_at": "2024-01-05T16:14:15.831000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1193670509611729016,
            "username": "akash345.",
            "created_at": "2024-01-07T21:40:26.817000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1193670538585964619,
            "username": "gabev2037",
            "created_at": "2024-01-07T21:40:33.725000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Apologies for the likely repeat question: There still isn't support for input adapters right? In that i can check for the presence of an optional field, then decide whether to include it in the prompt?",
        "metadata": {
            "message_id": 1193984359162712094,
            "username": "gabev2037",
            "created_at": "2024-01-08T18:27:34.381000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Seems as though Optional types are still requires when invoking a function",
        "metadata": {
            "message_id": 1193985562261069854,
            "username": "gabev2037",
            "created_at": "2024-01-08T18:32:21.222000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yep we'll pick the optionals-are-required issue up soon https://github.com/BoundaryML/baml/issues/235\n\n<@99252724855496704>  CC input adapters",
        "metadata": {
            "message_id": 1193988368523067472,
            "username": ".aaronv",
            "created_at": "2024-01-08T18:43:30.287000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Apologies for the likely repeat question",
        "metadata": {
            "message_id": 1193992749226131468,
            "username": "hellovai",
            "created_at": "2024-01-08T19:00:54.728000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "While running my test suite which uses Claude 2.1, I got an Event loop is closed error",
        "metadata": {
            "message_id": 1194724758785364150,
            "username": "gabev2037",
            "created_at": "2024-01-10T19:29:39.407000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey guys I find that sometimes when the LLM just spits out a bunch of raw text which doesn't match my expected JSON, BAML will still serialize that to an empty list. For context, i'm tryin get a list of objects",
        "metadata": {
            "message_id": 1194742169127374860,
            "username": "gabev2037",
            "created_at": "2024-01-10T20:38:50.356000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "In these cases, I'd rather it just retry the function call",
        "metadata": {
            "message_id": 1194742215042412594,
            "username": "gabev2037",
            "created_at": "2024-01-10T20:39:01.303000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also is the caching configurable? I find that sending mutliple responses with the same input leads to the same result. Is that because my temperature setting or something with Baml?",
        "metadata": {
            "message_id": 1194742378678976633,
            "username": "gabev2037",
            "created_at": "2024-01-10T20:39:40.317000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Every morning I look forward to a `baml update && baml update-client` just in case there are some exciting new updates/improvements",
        "metadata": {
            "message_id": 1195042718280331354,
            "username": "gabev2037",
            "created_at": "2024-01-11T16:33:06.858000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "🤣 they are coming next week 😛",
        "metadata": {
            "message_id": 1195042852305121341,
            "username": ".aaronv",
            "created_at": "2024-01-11T16:33:38.812000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "You guys seen this error before?\nanthropic.InternalServerError: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}",
        "metadata": {
            "message_id": 1195049944592171099,
            "username": "gabev2037",
            "created_at": "2024-01-11T17:01:49.745000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "So I tried rendering XML tags and the gloo dashboard is not showing them.... they should still be in the prompt though right?",
        "metadata": {
            "message_id": 1195050167016116234,
            "username": "gabev2037",
            "created_at": "2024-01-11T17:02:42.775000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "btw https://www.linkedin.com/feed/update/urn:li:activity:7151263284866736129/",
        "metadata": {
            "message_id": 1195058794464563231,
            "username": "gabev2037",
            "created_at": "2024-01-11T17:36:59.719000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "thanks! I just sent him a msg!",
        "metadata": {
            "message_id": 1195059820500365402,
            "username": ".aaronv",
            "created_at": "2024-01-11T17:41:04.345000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "nit: can't have test cases with the same name even though they are across different functions",
        "metadata": {
            "message_id": 1195109119074521198,
            "username": "gabev2037",
            "created_at": "2024-01-11T20:56:58.041000+00:00",
            "edited_at": "2024-01-11T20:57:07.007000+00:00"
        }
    },
    {
        "text": "Tracking in https://github.com/BoundaryML/baml/issues/329",
        "metadata": {
            "message_id": 1195113187645857833,
            "username": ".aaronv",
            "created_at": "2024-01-11T21:13:08.064000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1195121780663275630,
            "username": "shivanishirolkar",
            "created_at": "2024-01-11T21:47:16.799000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1195121948934557806,
            "username": ".aaronv",
            "created_at": "2024-01-11T21:47:56.918000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1195121980383428741,
            "username": "hellovai",
            "created_at": "2024-01-11T21:48:04.416000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1195122009814868098,
            "username": "shivanishirolkar",
            "created_at": "2024-01-11T21:48:11.433000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "sorry I was muted!",
        "metadata": {
            "message_id": 1195122255689166949,
            "username": "shivanishirolkar",
            "created_at": "2024-01-11T21:49:10.054000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "giving discord permissions to microphone 🙂 sorry to have bothered you so out of the blue lol",
        "metadata": {
            "message_id": 1195122473742643283,
            "username": "shivanishirolkar",
            "created_at": "2024-01-11T21:50:02.042000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "No worries! if you have any questions (including about the design principals or more discussion oriented about tradeoffs), definetly feel free to chime in!\n\nAnd not a bother at all. I'm online so you can bother me! 🙂",
        "metadata": {
            "message_id": 1195122561944653914,
            "username": "hellovai",
            "created_at": "2024-01-11T21:50:23.071000+00:00",
            "edited_at": "2024-01-11T21:50:31.587000+00:00"
        }
    },
    {
        "text": "For future reference, how often do you hold office hours? Just to give you some background, my name is Shivani and I came across a comment you made on Linkedin and thought your profile looked interesting. I was looking through your website and ended up on the Discord! Glad to be here and I plan to go through the docs to learn more!",
        "metadata": {
            "message_id": 1195123252033495141,
            "username": "shivanishirolkar",
            "created_at": "2024-01-11T21:53:07.601000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Awesome, I'm usually online most days PST about 9 am - 5 pm PST (barring any meetings, we'll be posting a more official schedule by the end of this month as we prepare to launch mroe publicly, but if you have any questions, feel free to grab a hold of me at any point).",
        "metadata": {
            "message_id": 1195123603688128602,
            "username": "hellovai",
            "created_at": "2024-01-11T21:54:31.442000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sounds great, thank you Vaibhav! Also out of curiosity, are you guys hiring? 😄",
        "metadata": {
            "message_id": 1195123888292634656,
            "username": "shivanishirolkar",
            "created_at": "2024-01-11T21:55:39.297000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "we are! Shoot me a DM and we can chat 🙂",
        "metadata": {
            "message_id": 1195124045776162986,
            "username": "hellovai",
            "created_at": "2024-01-11T21:56:16.844000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey gang, \nIs there a way in a `@trace` call I could potentially log application code things? I have a workflow which has \nbaml function --> application code --> baml function --> application code --> baml function\n\nAnd i find the baml dashboard is great for observability so I would like to add some of those intermediary application code steps (even if i can just add specific logs that would be perfect)\n\nAny chance this exists or is in the roadmap?",
        "metadata": {
            "message_id": 1195370271528456284,
            "username": "gabev2037",
            "created_at": "2024-01-12T14:14:41.638000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Tracing app code",
        "metadata": {
            "message_id": 1195401333054902328,
            "username": ".aaronv",
            "created_at": "2024-01-12T16:18:07.283000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1195415343569584290,
            "username": "panziewanz",
            "created_at": "2024-01-12T17:13:47.650000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is it possible to add comments in baml files outside of strings? Not seeing anything in docs",
        "metadata": {
            "message_id": 1195429408253677709,
            "username": "gabev2037",
            "created_at": "2024-01-12T18:09:40.932000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "use //",
        "metadata": {
            "message_id": 1195429445553631273,
            "username": ".aaronv",
            "created_at": "2024-01-12T18:09:49.825000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i lied",
        "metadata": {
            "message_id": 1195429481595285645,
            "username": "gabev2037",
            "created_at": "2024-01-12T18:09:58.418000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "its ///",
        "metadata": {
            "message_id": 1195429491196043325,
            "username": "gabev2037",
            "created_at": "2024-01-12T18:10:00.707000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "those are docstrings, but comments are just //, did it not work? Sometimes it wont highlight (known issue)",
        "metadata": {
            "message_id": 1195429664852807741,
            "username": ".aaronv",
            "created_at": "2024-01-12T18:10:42.110000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "but it will still compile",
        "metadata": {
            "message_id": 1195429682284343389,
            "username": ".aaronv",
            "created_at": "2024-01-12T18:10:46.266000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "it didn't compile",
        "metadata": {
            "message_id": 1195429721987629126,
            "username": "gabev2037",
            "created_at": "2024-01-12T18:10:55.732000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Do you guys have support for generics? Use case:\n\nI want my baml class to have a generic list field which is going to be skipped since I don't actually want the LLM to output an answer for it. I simply want to fill that field myself later on in application code logic",
        "metadata": {
            "message_id": 1195435208711884980,
            "username": "gabev2037",
            "created_at": "2024-01-12T18:32:43.869000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just wanna plus 1 this: https://github.com/BoundaryML/baml/issues/318\n\nWould be massively useful",
        "metadata": {
            "message_id": 1195469638314758235,
            "username": "gabev2037",
            "created_at": "2024-01-12T20:49:32.526000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also retry policy seems to be off? I'm firing a baml function N times using asyncio.gather and it appears that some of the invocations will have up to 10 retries... My policy requires max 3",
        "metadata": {
            "message_id": 1195471056350228580,
            "username": "gabev2037",
            "created_at": "2024-01-12T20:55:10.612000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://github.com/anysphere/priompt\nHave you guys taken a look at how the anysphere team builds prompts in case it could serve as inspiration for input adapters?",
        "metadata": {
            "message_id": 1195492596101947422,
            "username": "gabev2037",
            "created_at": "2024-01-12T22:20:46.089000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "The overview pricing seems wildly off 🧵",
        "metadata": {
            "message_id": 1195734213165592690,
            "username": "gabev2037",
            "created_at": "2024-01-13T14:20:52.086000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey gang, \nwe've been running into memory leaks following some of the baml deployment changes (not saying it's due to baml, just that it coincides with the deployment and my asyncio calls on top of the baml calls) and I'm wondering if there is any chance some of the baml calls may be hanging? Like, do you guarantee that if the timeout is not met, the ai function will terminate? \n\nHappy to hop in OH if that makes sense, still trying to determine root cause on our end",
        "metadata": {
            "message_id": 1196444305024372816,
            "username": "gabev2037",
            "created_at": "2024-01-15T13:22:31.178000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey gang,",
        "metadata": {
            "message_id": 1196486907652210769,
            "username": "gabev2037",
            "created_at": "2024-01-15T16:11:48.436000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey guys, one thing I would really LOVE is if i could in the \"Filter by Tags\" have a \"is not equal to\" option",
        "metadata": {
            "message_id": 1196930974043361420,
            "username": "gabev2037",
            "created_at": "2024-01-16T21:36:22.114000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "what's going on here?",
        "metadata": {
            "message_id": 1196932050318528612,
            "username": "gabev2037",
            "created_at": "2024-01-16T21:40:38.718000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "what's going on here?",
        "metadata": {
            "message_id": 1196935061665218628,
            "username": ".aaronv",
            "created_at": "2024-01-16T21:52:36.679000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1197196429123407983,
            "username": "walterqian",
            "created_at": "2024-01-17T15:11:11.537000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1197220142422184027,
            "username": ".aaronv",
            "created_at": "2024-01-17T16:45:25.228000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "How hard would it be to make it so that I can expand/shrink the output/input JSONS in the boundary dashboard?",
        "metadata": {
            "message_id": 1197657225607516240,
            "username": "gabev2037",
            "created_at": "2024-01-18T21:42:13.980000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is it possible to define a prompt in Baml with certain inputs which I can then use in my application code? Figure this would get me 90% of the way there with the streaming stuff",
        "metadata": {
            "message_id": 1198282015351505057,
            "username": "gabev2037",
            "created_at": "2024-01-20T15:04:55.463000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Like a function where when I call it, instead of calling the LLM, it just returns the prompt which I can then use for the LLM (or other reasons… feel like there’s something interesting here)",
        "metadata": {
            "message_id": 1198282140836696144,
            "username": "gabev2037",
            "created_at": "2024-01-20T15:05:25.381000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Streaming",
        "metadata": {
            "message_id": 1198309231439401126,
            "username": ".aaronv",
            "created_at": "2024-01-20T16:53:04.284000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is it possible to define a prompt in",
        "metadata": {
            "message_id": 1198320759337128031,
            "username": "hellovai",
            "created_at": "2024-01-20T17:38:52.749000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1198989817124503594,
            "username": "emeka19",
            "created_at": "2024-01-22T13:57:28.556000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1199009505887928390,
            "username": "hellovai",
            "created_at": "2024-01-22T15:15:42.723000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I was toying around with this idea of using output adapters to enforce field-level uniqueness in one of my LLM functions. I want to maintain the same output type, and just do some extra python logic to make sure the results of the LLM are unique and fit some other criteria.\n\nRight now I do this in application code once I get the results from the LLM. \n\nIs this something you think is worth porting to BAML or does that go against the recommended use of the DSL?",
        "metadata": {
            "message_id": 1199029838460235816,
            "username": "gabev2037",
            "created_at": "2024-01-22T16:36:30.386000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I was toying around with this idea of",
        "metadata": {
            "message_id": 1199035991311269893,
            "username": "hellovai",
            "created_at": "2024-01-22T17:00:57.340000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1199049943466393712,
            "username": "Deleted User",
            "created_at": "2024-01-22T17:56:23.793000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1199050208454131743,
            "username": ".aaronv",
            "created_at": "2024-01-22T17:57:26.971000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "how'd you hear about us Jack?",
        "metadata": {
            "message_id": 1199050359054807151,
            "username": ".aaronv",
            "created_at": "2024-01-22T17:58:02.877000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1199423700953464903,
            "username": "nickinkeep",
            "created_at": "2024-01-23T18:41:34.522000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1199423753424212068,
            "username": "hellovai",
            "created_at": "2024-01-23T18:41:47.032000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1199424221907009688,
            "username": "gabev2037",
            "created_at": "2024-01-23T18:43:38.727000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey guys any chance there was some sort of breaking change with enums?",
        "metadata": {
            "message_id": 1199755511445323837,
            "username": "gabev2037",
            "created_at": "2024-01-24T16:40:04.308000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "There's no `skip` equivalent for classes is there?",
        "metadata": {
            "message_id": 1199843261234294985,
            "username": "gabev2037",
            "created_at": "2024-01-24T22:28:45.488000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "There's no `skip` equivalent for classes",
        "metadata": {
            "message_id": 1199848045492310191,
            "username": ".aaronv",
            "created_at": "2024-01-24T22:47:46.144000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any tips for getting LLMs to understand whether a user is asking about time-related events? I know we workshopped it before but the quality has regressed a lot",
        "metadata": {
            "message_id": 1199885586849333348,
            "username": "gabev2037",
            "created_at": "2024-01-25T01:16:56.701000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any tips for getting LLMs to understand",
        "metadata": {
            "message_id": 1199890785403932746,
            "username": ".aaronv",
            "created_at": "2024-01-25T01:37:36.133000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there a way to work with dynamic enums in boundary? I have user-defined categories which I need to use in a classification task, though this would require defining the enum in application code. \n\nIs that feasible?",
        "metadata": {
            "message_id": 1200071446202953778,
            "username": "gabev2037",
            "created_at": "2024-01-25T13:35:29.024000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Dynamic enums",
        "metadata": {
            "message_id": 1200105148966510627,
            "username": "hellovai",
            "created_at": "2024-01-25T15:49:24.389000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any good tips for how I would be able to determine from a user query whether they are seeking specific information ? \n\nE.g. I have a lot of users who will say \"What did I learn recently?\" and I want to recognize that I can pull anything from recent saves. When they say \"What have I learned about agile development?\" I want to run against my search cluster",
        "metadata": {
            "message_id": 1200542440524890223,
            "username": "gabev2037",
            "created_at": "2024-01-26T20:47:02.821000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If I have \nfunction A `calls` Function B `calls` Function C\nand function A has @trace \nfunction C has @trace \n\nLooks like I'm not seeing function C in the dashboard",
        "metadata": {
            "message_id": 1200561439501455430,
            "username": "gabev2037",
            "created_at": "2024-01-26T22:02:32.530000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Determining specific info from query",
        "metadata": {
            "message_id": 1200580423131074615,
            "username": ".aaronv",
            "created_at": "2024-01-26T23:17:58.580000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1202013790452064357,
            "username": "anoopreddi",
            "created_at": "2024-01-30T22:13:39.980000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1202020219720044584,
            "username": "hellovai",
            "created_at": "2024-01-30T22:39:12.837000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1202748846934069248,
            "username": ".rathesungod",
            "created_at": "2024-02-01T22:54:31.102000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1202762113857232906,
            "username": ".aaronv",
            "created_at": "2024-02-01T23:47:14.183000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Have you guys changed anything related to where the generated baml_client goes?",
        "metadata": {
            "message_id": 1203096284076965888,
            "username": "gabev2037",
            "created_at": "2024-02-02T21:55:06.572000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1203137382476677140,
            "username": "mukesh0486",
            "created_at": "2024-02-03T00:38:25.194000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1203137482821079121,
            "username": ".aaronv",
            "created_at": "2024-02-03T00:38:49.118000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbotocore 1.31.83 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.0 which is incompatible.\nllama-index 0.9.3 requires openai>=1.1.0, but you have openai 0.28.1 which is incompatible.\nllama-index 0.9.3 requires urllib3<2, but you have urllib3 2.2.0 which is incompatible.\nmarqo 2.1.0 requires pydantic<2.0.0, but you have pydantic 2.6.1 which is incompatible.\nneumai 0.0.40 requires openai==1.2.4, but you have openai 0.28.1 which is incompatible.\nneumai 0.0.40 requires pydantic==1.10.13, but you have pydantic 2.6.1 which is incompatible.\nneumai-tools 0.0.19 requires openai==1.2.4, but you have openai 0.28.1 which is incompatible.\nqdrant-client 1.6.9 requires urllib3<2.0.0,>=1.26.14, but you have urllib3 2.2.0 which is incompatible.\nsupabase-py 0.0.2 requires gotrue==0.2.0, but you have gotrue 1.3.0 which is incompatible.\nsupabase-py 0.0.2 requires pytest<7,>=6, but you have pytest 7.4.4 which is incompatible.\nsupabase-py 0.0.2 requires requests==2.25.1, but you have requests 2.31.0 which is incompatible.",
        "metadata": {
            "message_id": 1204204142478762014,
            "username": "ddematheu",
            "created_at": "2024-02-05T23:17:20.598000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ERROR: pip's dependency resolver does",
        "metadata": {
            "message_id": 1204887406587674689,
            "username": "hellovai",
            "created_at": "2024-02-07T20:32:23.456000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1205955790729510922,
            "username": "dean_42925",
            "created_at": "2024-02-10T19:17:46.085000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1205956301042098216,
            "username": "dean.coffee",
            "created_at": "2024-02-10T19:19:47.753000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Day 1: \nSuper simple setup, some handholding required by vaibhav to get it setup since onboarding instructions slightly out of date\n\nbrew install boundaryml/baml/baml\nsetup VS code extension\nbaml init (think we can add a suggestion vscode file to suggest the right python extension when running baml init, since suggested vs code extension doesn't work with it)\nbaml update-client (would be nice if it hot refreshed similar to how SCSS does / you can track files bottom right of vs code)\n\nInitial impressions\n- Set up basic parser for HTML -> our calendar app's schema\n- Schema mapping is perfect 10/10\n- Test cases are super helpful, would love to be able to pop out to the main file from the \"Edit test\" window, it was hard to find the little \"Open test file\" button on previous screen\n- Time conversion to UTC time zone not accurate, would be cool if there was a way to ensure datetime returns in defined format and timezone\n- Might be cool to define hints and/or not rely on the schema name entirely, right now i have a separate field mapping instruction in the prompt\n- Biggest issue, the output is truncating  (raw output shows more than what is parsed into schema)\n\nNext steps for me\n- HTML parse by URL in Python (or in our base repo we'll see) \n- setup basic python container + single endpoint to pass in URLs and get JSON back\n- prob deploy within same container setup I have \n- call from our backend w private data\n- explore other APIs that might be cheaper than OpenAI to use for some use cases\n\nany suggestions / feedback super welcome! i'm a total n00b",
        "metadata": {
            "message_id": 1205965916374966352,
            "username": "dean.coffee",
            "created_at": "2024-02-10T19:58:00.227000+00:00",
            "edited_at": "2024-02-10T19:58:55.235000+00:00"
        }
    },
    {
        "text": "```\ngabevillasana@Gabes-MacBook-Pro ambient-ai-backend % baml update && baml update-client\n[baml] Updating 0.10.0 -> 0.11.1\n[baml] Running: brew [\"tap\", \"gloohq/baml\"]\n[baml] Running: brew [\"update\"]\n[baml] Running: brew [\"upgrade\", \"baml\"]\nInstalling/Upgrading client for python:\n  project_root: /Users/gabevillasana/zenfetch-code/ambient-ai-backend/baml_src/../\n  install_command: \"poetry\" \"install\" \"baml@latest\"\n  Failed!\n  No arguments expected for \"install\" command, got \"baml@latest\"\ngabevillasana@Gabes-MacBook-Pro ambient-ai-backend %\n```",
        "metadata": {
            "message_id": 1206624577002799145,
            "username": "gabev2037",
            "created_at": "2024-02-12T15:35:17.158000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Bam update",
        "metadata": {
            "message_id": 1206628960054542407,
            "username": "hellovai",
            "created_at": "2024-02-12T15:52:42.159000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://docs.boundaryml.com/v3/how-to/streaming/streaming\n\nI believe this code snippet has incorrect indentation:\n```python\nasync def main():\n    async with baml.MyFunction.stream(MyInput(...)) as stream:\n        async for output in stream.parsed_stream:\n           \n            if output.is_parseable:\n              assert output.parsed.my_property is not None\n              print(\"my property is present\", output.parsed.my_property)\n              print(f\"streaming: {output.parsed.model_dump_json()}\")\n            \n            # You can also get the current delta. This will always be present.\n            print(f\"streaming: {output.delta}\")\n\n        final_output = await stream.get_final_response()\n        if final_output.has_value:\n            print(f\"final response: {final_output.value}\")\n        else:\n            # A deserialization error likely occurred.\n            print(f\"final resopnse didnt have a value\")\n```\n\nshouldn't the `final_output = ...` be unindented?",
        "metadata": {
            "message_id": 1206656078453014610,
            "username": "gabev2037",
            "created_at": "2024-02-12T17:40:27.689000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "backend-1  | TypeError: AsyncCompletions.create() got an unexpected keyword argument 'engine'",
        "metadata": {
            "message_id": 1206657771848732682,
            "username": "gabev2037",
            "created_at": "2024-02-12T17:47:11.426000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "^ doesn't seem like my AI functions are working",
        "metadata": {
            "message_id": 1206657823539204176,
            "username": "gabev2037",
            "created_at": "2024-02-12T17:47:23.750000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1206747855663145000,
            "username": "joseph_42285",
            "created_at": "2024-02-12T23:45:09.081000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "in the spirit of fitting the \"always dissatisfied customer\" mold, i was wondering whether you cats are still working on function chaining? Also curious if there's any progress on function client strategies (specifically, making let's say 3 parallel calls at once and using the first returned response)",
        "metadata": {
            "message_id": 1207678771721797703,
            "username": "gabev2037",
            "created_at": "2024-02-15T13:24:16.765000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1208055622973792268,
            "username": "netbot2410",
            "created_at": "2024-02-16T14:21:45.105000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Playground status is stuck in compiling. When I run don't see any output (thread)",
        "metadata": {
            "message_id": 1210034605470646352,
            "username": "ddematheu",
            "created_at": "2024-02-22T01:25:31.300000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Haven't used boundary in a bit since been focused on other priorities, so was nice to login today and see some delightful options (e.g., copying specific pieces of inputs from the dashboard & streaming in VSCode extension) 😄",
        "metadata": {
            "message_id": 1210318819764080720,
            "username": "gabev2037",
            "created_at": "2024-02-22T20:14:53.268000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there an easy way in the boundary dashboard to just see requests that errors or failed",
        "metadata": {
            "message_id": 1211655155234246676,
            "username": "gabev2037",
            "created_at": "2024-02-26T12:45:00.472000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I wanna get an idea for how often I’m getting rate limited",
        "metadata": {
            "message_id": 1211655207495143484,
            "username": "gabev2037",
            "created_at": "2024-02-26T12:45:12.932000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there an easy way in the boundary",
        "metadata": {
            "message_id": 1211703783684309057,
            "username": "hellovai",
            "created_at": "2024-02-26T15:58:14.398000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there any reason why my `baml_client/baml_types/partial.py` changes everytime I run `baml build`?",
        "metadata": {
            "message_id": 1211781246485532782,
            "username": "gabev2037",
            "created_at": "2024-02-26T21:06:02.969000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there any reason why my `baml_client/",
        "metadata": {
            "message_id": 1211786202668343390,
            "username": "hellovai",
            "created_at": "2024-02-26T21:25:44.615000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1213171450815578124,
            "username": "ashayas",
            "created_at": "2024-03-01T17:10:13.510000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1213360819904184340,
            "username": "joyyyyyyyyyy",
            "created_at": "2024-03-02T05:42:42.619000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1213360865986879581,
            "username": "hellovai",
            "created_at": "2024-03-02T05:42:53.606000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I came from the article about Seattle-based companies. This is a pretty cool idea. I'll try to give it a go this weekend 😄",
        "metadata": {
            "message_id": 1213362039679098910,
            "username": "joyyyyyyyyyy",
            "created_at": "2024-03-02T05:47:33.436000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "sweet, let us know if you run into issues 🙂 Typescript is early access but Python is good to go",
        "metadata": {
            "message_id": 1213362329786523678,
            "username": ".aaronv",
            "created_at": "2024-03-02T05:48:42.603000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1213845641755623424,
            "username": "space_hosting",
            "created_at": "2024-03-03T13:49:13.156000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is Azure gpt 4 turbo still choppy even with content moderation turned off? (wondering whether you guys have any insights)",
        "metadata": {
            "message_id": 1214660212187865180,
            "username": "gabev2037",
            "created_at": "2024-03-05T19:46:01.881000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We want to support cancelling a stream (similar to cursor experience). Does baml do anything to support that? Anything I should know other than just killing the process?",
        "metadata": {
            "message_id": 1214952620037246976,
            "username": "gabev2037",
            "created_at": "2024-03-06T15:07:57.345000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "That should do it! We’ll log a work item to expose a proper cancel endpoint",
        "metadata": {
            "message_id": 1214957117631365120,
            "username": "hellovai",
            "created_at": "2024-03-06T15:25:49.655000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "is there a `request timeout` equivalent for claude?",
        "metadata": {
            "message_id": 1215079191515631647,
            "username": "gabev2037",
            "created_at": "2024-03-06T23:30:54.337000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "can't remember how to find the anthropic provider",
        "metadata": {
            "message_id": 1215079211765727292,
            "username": "gabev2037",
            "created_at": "2024-03-06T23:30:59.165000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "found it",
        "metadata": {
            "message_id": 1215079393030971472,
            "username": "gabev2037",
            "created_at": "2024-03-06T23:31:42.382000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "FWIW, compiler did not catch that",
        "metadata": {
            "message_id": 1215079452594278480,
            "username": "gabev2037",
            "created_at": "2024-03-06T23:31:56.583000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "only when I tried invoking the client did it fail",
        "metadata": {
            "message_id": 1215079473444028436,
            "username": "gabev2037",
            "created_at": "2024-03-06T23:32:01.554000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "FWIW, compiler did not catch that",
        "metadata": {
            "message_id": 1215332911016968246,
            "username": "hellovai",
            "created_at": "2024-03-07T16:19:05.780000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Does the anthropic provider leverage their messages API?\n\nContext: I want to switch our streams to Claude 3 and current BAML setup is only compatible with gpt. Getting the following error :\n```\nBadRequestError: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages: first message must use the \"user\" role'}}\n```",
        "metadata": {
            "message_id": 1215333452916989982,
            "username": "gabev2037",
            "created_at": "2024-03-07T16:21:14.979000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1216472183849877504,
            "username": "ketillg",
            "created_at": "2024-03-10T19:46:09.590000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Running into the issue again where on boundary dashboard, the xml tags do not render",
        "metadata": {
            "message_id": 1216766017683066992,
            "username": "gabev2037",
            "created_at": "2024-03-11T15:13:45.035000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Xml tags",
        "metadata": {
            "message_id": 1216782007061971009,
            "username": ".aaronv",
            "created_at": "2024-03-11T16:17:17.200000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there a way to configure a specific Baml function to have a timeout policy, rather than at client level?",
        "metadata": {
            "message_id": 1217103270716244129,
            "username": "gabev2037",
            "created_at": "2024-03-12T13:33:52.424000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Function timeout",
        "metadata": {
            "message_id": 1217127223341875397,
            "username": "hellovai",
            "created_at": "2024-03-12T15:09:03.175000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Haiku dropped!",
        "metadata": {
            "message_id": 1217585727160713287,
            "username": "gabev2037",
            "created_at": "2024-03-13T21:30:59.004000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "any updates on the messaging API for non-stream?? 👀",
        "metadata": {
            "message_id": 1217585780092960831,
            "username": "gabev2037",
            "created_at": "2024-03-13T21:31:11.624000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "use baml-anthropic-chat and it will use `messages` api for non-stream calls",
        "metadata": {
            "message_id": 1217585944446894100,
            "username": ".aaronv",
            "created_at": "2024-03-13T21:31:50.809000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://tenor.com/bQrzp.gif",
        "metadata": {
            "message_id": 1217586019659153488,
            "username": "gabev2037",
            "created_at": "2024-03-13T21:32:08.741000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Follow up: My prompts are not currently separated into system vs user vs assistant for the non-streaming ones. Can i still add my haiku client to the fallback strategy without modifying the prompt?",
        "metadata": {
            "message_id": 1217586774004596737,
            "username": "gabev2037",
            "created_at": "2024-03-13T21:35:08.591000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1217849926067949679,
            "username": "eborgnia",
            "created_at": "2024-03-14T15:00:48.932000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1217850909758062683,
            "username": "hellovai",
            "created_at": "2024-03-14T15:04:43.462000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1217850954125414460,
            "username": "eborgnia",
            "created_at": "2024-03-14T15:04:54.040000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey <@201399017161097216> <@99252724855496704>, just saw a diserialization error.  First one we have seen tbh. Looking at the generated json I am not sure why it happened. Info on the thread",
        "metadata": {
            "message_id": 1218980698325782548,
            "username": "ddematheu",
            "created_at": "2024-03-17T17:54:06.047000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1219326286204371034,
            "username": "paulswell",
            "created_at": "2024-03-18T16:47:20.618000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1219326334367563816,
            "username": "hellovai",
            "created_at": "2024-03-18T16:47:32.101000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey! Any way to try the typescript client? I know it's in beta",
        "metadata": {
            "message_id": 1219328142859178024,
            "username": "paulswell",
            "created_at": "2024-03-18T16:54:43.279000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Everytime I see a `new baml update` i get pumped",
        "metadata": {
            "message_id": 1220023788536856668,
            "username": "gabev2037",
            "created_at": "2024-03-20T14:58:58.133000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Haha we had a big update with typescript but we have some other goodies coming up on the roadmap. One of them is being able to dynamically alter a type at runtime, like dynamic enums with user-defined categories",
        "metadata": {
            "message_id": 1220024890653020241,
            "username": ".aaronv",
            "created_at": "2024-03-20T15:03:20.898000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220060658138353734,
            "username": "hankelbao.",
            "created_at": "2024-03-20T17:25:28.531000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220060688593059900,
            "username": "hellovai",
            "created_at": "2024-03-20T17:25:35.792000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220147081004584980,
            "username": "atupem",
            "created_at": "2024-03-20T23:08:53.348000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220147563550740520,
            "username": "hellovai",
            "created_at": "2024-03-20T23:10:48.396000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey gusy i'm running into the problem where claude continues to continue the conversation and it's creating problems.\n\nLike I prompt inject \"Zenfetch Assistant:\" but then after it answers it will start to say\n\"User: blah blah\nZenfetch Assistant: blah blah\"",
        "metadata": {
            "message_id": 1220441415138545745,
            "username": "gabev2037",
            "created_at": "2024-03-21T18:38:28.074000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "less than ideal. One potential fix is the ability to use multiple human vs assistant messages in the baml stream instead of thorwing everytying into a Human message.",
        "metadata": {
            "message_id": 1220441526883188939,
            "username": "gabev2037",
            "created_at": "2024-03-21T18:38:54.716000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220446479936782418,
            "username": "saintvinasse",
            "created_at": "2024-03-21T18:58:35.616000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220446517790375957,
            "username": "hellovai",
            "created_at": "2024-03-21T18:58:44.641000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220446582688845924,
            "username": "saintvinasse",
            "created_at": "2024-03-21T18:59:00.114000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’m in the middle of compilation errors, hence me being on twitter, but would love to chat more soon",
        "metadata": {
            "message_id": 1220446791426904174,
            "username": "saintvinasse",
            "created_at": "2024-03-21T18:59:49.881000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "😉",
        "metadata": {
            "message_id": 1220446808862490705,
            "username": "saintvinasse",
            "created_at": "2024-03-21T18:59:54.038000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Looking forward to it 🙂",
        "metadata": {
            "message_id": 1220446835286610011,
            "username": "hellovai",
            "created_at": "2024-03-21T19:00:00.338000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://imgs.xkcd.com/comics/compiling.png",
        "metadata": {
            "message_id": 1220446919948501063,
            "username": "hellovai",
            "created_at": "2024-03-21T19:00:20.523000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ahah, yes, exactly",
        "metadata": {
            "message_id": 1220447055164608654,
            "username": "saintvinasse",
            "created_at": "2024-03-21T19:00:52.761000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "saw your msg on twitter about synthetic data, but BAML helps you build prompts that return structured json output. The tooling makes it possible to declare a prompt in a .baml file, and get a generated typescript function that is fully type-safe with a defined input/output type, not just a string. Jump in the office hours when you're around and we can give ya a quick 5min demo",
        "metadata": {
            "message_id": 1220451605044924508,
            "username": ".aaronv",
            "created_at": "2024-03-21T19:18:57.537000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there any special BAML logic that takes place when streaming which would delay TTFT? Trying to figure out if delay is a result of provider or Baml",
        "metadata": {
            "message_id": 1220511989344768093,
            "username": "gabev2037",
            "created_at": "2024-03-21T23:18:54.276000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Streaming just strings",
        "metadata": {
            "message_id": 1220512016637100043,
            "username": "gabev2037",
            "created_at": "2024-03-21T23:19:00.783000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hello mates - I see that the node package `@boundaryml/baml-core` comes with platform specific `.node` binary - I'm wondering is it useful in the runtime? Is it possible to exclude these platform specific dependencies in runtime?",
        "metadata": {
            "message_id": 1220768400440033350,
            "username": "hankelbao.",
            "created_at": "2024-03-22T16:17:47.445000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hello guys - I run into this issue when testing azure openai; baml version is 0.15.0 and baml-core version is 0.0.2 (both latest); did I used a wrong name for the provider?",
        "metadata": {
            "message_id": 1220838860586745976,
            "username": "hankelbao.",
            "created_at": "2024-03-22T20:57:46.453000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "qq folks - does secret apply to all projects? I received the following error (p1) when after adding the secrets to env as p2",
        "metadata": {
            "message_id": 1220876330791600159,
            "username": "hankelbao.",
            "created_at": "2024-03-22T23:26:40.046000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220956511644418158,
            "username": "lloydchang",
            "created_at": "2024-03-23T04:45:16.651000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1220956542057058315,
            "username": "hellovai",
            "created_at": "2024-03-23T04:45:23.902000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Having users complain about the continued conversation :/",
        "metadata": {
            "message_id": 1221899530287452160,
            "username": "gabev2037",
            "created_at": "2024-03-25T19:12:29.816000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "btw had a quick prompting question, \nSo we have configured Zenfetch to be able to ask things like \"what have I learned yesterday\" and we are really good at actually retrieving context that is based on datetime references. However, the model sometimes will still say \"Unfortunately, I don't know what you learned yesterday blah blah blah\". \n\nI'm using Claude 3, and I am adding the current date, along with the date of each retrieved context piece. yet, it still fails here. Any tips? At this point I'm just thinking specifying something like \"if the user asks about time reference, use the provided context regardless\" or something",
        "metadata": {
            "message_id": 1221934891672010763,
            "username": "gabev2037",
            "created_at": "2024-03-25T21:33:00.627000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Getting deserializer failed issues with the chat stream",
        "metadata": {
            "message_id": 1222167637824507955,
            "username": "gabev2037",
            "created_at": "2024-03-26T12:57:51.634000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1222460547065319438,
            "username": "smravec.",
            "created_at": "2024-03-27T08:21:46.639000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hmm are there any plans to do ollama client?",
        "metadata": {
            "message_id": 1222461697315442740,
            "username": "smravec.",
            "created_at": "2024-03-27T08:26:20.880000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ollama",
        "metadata": {
            "message_id": 1222556546022772791,
            "username": "hellovai",
            "created_at": "2024-03-27T14:43:14.574000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Have you guys entertained the idea of dynamic class outputs? IN other words, at runtime I provide the schema and you guys handle deserialization then? Not even sure that's feasible",
        "metadata": {
            "message_id": 1222623936055869590,
            "username": "gabev2037",
            "created_at": "2024-03-27T19:11:01.610000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@766305750125379594>  try\nBAML_LOG_LEVEL=DEBUG\nas your env var",
        "metadata": {
            "message_id": 1222694957949784115,
            "username": ".aaronv",
            "created_at": "2024-03-27T23:53:14.549000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1223074311779389492,
            "username": "a463e8",
            "created_at": "2024-03-29T01:00:39.550000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1223074383128428595,
            "username": "hellovai",
            "created_at": "2024-03-29T01:00:56.561000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1224002503264436315,
            "username": "falconicx",
            "created_at": "2024-03-31T14:28:57.645000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1224018614886207529,
            "username": "hellovai",
            "created_at": "2024-03-31T15:32:58.955000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "feature suggestion which would have been a life saver: BAML Linter warns me when the client I am using does not have any redunandancy measures",
        "metadata": {
            "message_id": 1224084344797134969,
            "username": "gabev2037",
            "created_at": "2024-03-31T19:54:10.187000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Realized the f up after running the cron JOB 🤦‍♂️",
        "metadata": {
            "message_id": 1224084411717517414,
            "username": "gabev2037",
            "created_at": "2024-03-31T19:54:26.142000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Seems the costing shown in Boundary Studio/Dashboard hardcoded for GPT-4 (even when model being used is different say gpt-4-0125-preview, costing calculations are done using GPT-4.",
        "metadata": {
            "message_id": 1224335739576385599,
            "username": "falconicx",
            "created_at": "2024-04-01T12:33:07.373000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1224336384106561587,
            "username": "falconicx",
            "created_at": "2024-04-01T12:35:41.041000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Bug on pricing",
        "metadata": {
            "message_id": 1224343272495775749,
            "username": "hellovai",
            "created_at": "2024-04-01T13:03:03.361000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1224553986468745236,
            "username": "danielbichuetti",
            "created_at": "2024-04-02T03:00:21.487000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1224554255994851388,
            "username": "hellovai",
            "created_at": "2024-04-02T03:01:25.747000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "any updates on the Json fix?",
        "metadata": {
            "message_id": 1224714559684612170,
            "username": "gabev2037",
            "created_at": "2024-04-02T13:38:25.125000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also have a bunch fo deserializer failures that I'm not sure I totally understand. 🧵",
        "metadata": {
            "message_id": 1224715166462115883,
            "username": "gabev2037",
            "created_at": "2024-04-02T13:40:49.792000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Jsonish fix",
        "metadata": {
            "message_id": 1224721214019211379,
            "username": "hellovai",
            "created_at": "2024-04-02T14:04:51.642000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://app.boundaryml.com/dashboard/projects/proj_de1f1417-d1d4-4d69-8444-6e685547b81b/drilldown?start_time=2024-03-20T16%3A21%3A33.087Z&end_time=2024-04-03T16%3A27%3A30.675Z&eid=4d8c18a5-7de5-4311-8cb9-45ad6fad993b&s_eid=5e3c1fcf-86e0-48b4-9245-cb17c880fda7&test=false&onlyRootEvents=true",
        "metadata": {
            "message_id": 1225119614460235816,
            "username": "gabev2037",
            "created_at": "2024-04-03T16:27:57.709000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Guys the deserializer issue is causing a lot of churn. going to move off of it",
        "metadata": {
            "message_id": 1225466848842088509,
            "username": "gabev2037",
            "created_at": "2024-04-04T15:27:44.837000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Have been seeing the model spit out a lot of this type of output where it includes the `\\n` in the message https://app.boundaryml.com/dashboard/projects/proj_de1f1417-d1d4-4d69-8444-6e685547b81b/drilldown?start_time=2024-03-21T15%3A24%3A25.546Z&end_time=2024-04-04T18%3A31%3A48.914Z&tags=user_id%3Ais%3Auser_01hqzn2ypretevscv4kq19n7v3&eid=46b98ca7-f6f7-427d-95c0-fb0ee476e60a&s_eid=fa78a550-7427-4b54-b838-65d698f15dc2&test=false&onlyRootEvents=true",
        "metadata": {
            "message_id": 1225514014411587666,
            "username": "gabev2037",
            "created_at": "2024-04-04T18:35:09.985000+00:00",
            "edited_at": "2024-04-04T18:35:26.097000+00:00"
        }
    },
    {
        "text": "Have been seeing the model spit out a",
        "metadata": {
            "message_id": 1225530015182098548,
            "username": "hellovai",
            "created_at": "2024-04-04T19:38:44.866000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1225532936401715220,
            "username": "matt_arbury_labs",
            "created_at": "2024-04-04T19:50:21.339000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1226727486331551784,
            "username": "bassemyacoube",
            "created_at": "2024-04-08T02:57:04.236000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1226951827241828412,
            "username": "gabev2037",
            "created_at": "2024-04-08T17:48:31.277000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1226951832413274135,
            "username": "gabev2037",
            "created_at": "2024-04-08T17:48:32.510000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1227355656064077987,
            "username": "breadchris",
            "created_at": "2024-04-09T20:33:11.571000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1227355693766672415,
            "username": "hellovai",
            "created_at": "2024-04-09T20:33:20.560000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hello baml peeps",
        "metadata": {
            "message_id": 1227355720228274206,
            "username": "breadchris",
            "created_at": "2024-04-09T20:33:26.869000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1227355754487615609,
            "username": ".aaronv",
            "created_at": "2024-04-09T20:33:35.037000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "do you guys have a discord bot I can use to get structured data from 4chan",
        "metadata": {
            "message_id": 1227355874041790474,
            "username": "breadchris",
            "created_at": "2024-04-09T20:34:03.541000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i need to integrate it into my psyop pipeline",
        "metadata": {
            "message_id": 1227355965657976882,
            "username": "breadchris",
            "created_at": "2024-04-09T20:34:25.384000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "lol im joking, this is chris from seattle",
        "metadata": {
            "message_id": 1227356361390690335,
            "username": "breadchris",
            "created_at": "2024-04-09T20:35:59.734000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "idk if y'all picked up on that from my username",
        "metadata": {
            "message_id": 1227356396534628412,
            "username": "breadchris",
            "created_at": "2024-04-09T20:36:08.113000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I did haha",
        "metadata": {
            "message_id": 1227356412443889714,
            "username": "hellovai",
            "created_at": "2024-04-09T20:36:11.906000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "welcome back to seattle 🙂",
        "metadata": {
            "message_id": 1227356437550989454,
            "username": "hellovai",
            "created_at": "2024-04-09T20:36:17.892000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i don't think I went away from the last time we talked",
        "metadata": {
            "message_id": 1227356531771576360,
            "username": "breadchris",
            "created_at": "2024-04-09T20:36:40.356000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i don't see the playground on your site, where is my playground",
        "metadata": {
            "message_id": 1227356580257730621,
            "username": "breadchris",
            "created_at": "2024-04-09T20:36:51.916000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "we got a new playground! If you're down to wait for a day or two, you should be able to play around with BAML on the WEB soon! (no download / setup)",
        "metadata": {
            "message_id": 1227356725389299732,
            "username": "hellovai",
            "created_at": "2024-04-09T20:37:26.518000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "all i see is a broken image on the docs",
        "metadata": {
            "message_id": 1227356734725558395,
            "username": "breadchris",
            "created_at": "2024-04-09T20:37:28.744000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "let's go",
        "metadata": {
            "message_id": 1227356808960540752,
            "username": "breadchris",
            "created_at": "2024-04-09T20:37:46.443000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "im sure I will see it on the front page of hn",
        "metadata": {
            "message_id": 1227357361673470145,
            "username": "breadchris",
            "created_at": "2024-04-09T20:39:58.220000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "lmk when it is up so i can get people to vote on it",
        "metadata": {
            "message_id": 1227357463062253619,
            "username": "breadchris",
            "created_at": "2024-04-09T20:40:22.393000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1228037481132331058,
            "username": "timelycomics",
            "created_at": "2024-04-11T17:42:31.335000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1228047011844980797,
            "username": ".aaronv",
            "created_at": "2024-04-11T18:20:23.634000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey gang, \nEvery now and then we'll get LLM responses that prints out the `\\n\\n` newline characters. Any tips for how we can prompt the LLM to stop doing this?",
        "metadata": {
            "message_id": 1228436135399657562,
            "username": "gabev2037",
            "created_at": "2024-04-12T20:06:37.919000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "where is my playground",
        "metadata": {
            "message_id": 1228938326632038490,
            "username": "breadchris",
            "created_at": "2024-04-14T05:22:09.639000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1229855095202648124,
            "username": "shehrum_45469",
            "created_at": "2024-04-16T18:05:04.299000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1229863399090028586,
            "username": "hellovai",
            "created_at": "2024-04-16T18:38:04.100000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@971897968967950336> https://www.promptfiddle.com",
        "metadata": {
            "message_id": 1231128385330352158,
            "username": "hellovai",
            "created_at": "2024-04-20T06:24:40.322000+00:00",
            "edited_at": "2024-04-20T06:24:53.431000+00:00"
        }
    },
    {
        "text": "new baml requires ollama?",
        "metadata": {
            "message_id": 1231710655698960384,
            "username": "gabev2037",
            "created_at": "2024-04-21T20:58:24.396000+00:00",
            "edited_at": "2024-04-21T20:58:28.563000+00:00"
        }
    },
    {
        "text": "Does the newest update correct the JSON-ish issues?",
        "metadata": {
            "message_id": 1231774431186124820,
            "username": "gabev2037",
            "created_at": "2024-04-22T01:11:49.657000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Not yet. But instead we now support for loops so you can do dynamic number of chat roles. I’ll send a snippet by tonight for you to try!",
        "metadata": {
            "message_id": 1231780973348130966,
            "username": "hellovai",
            "created_at": "2024-04-22T01:37:49.430000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1232828273482924124,
            "username": "shen01624",
            "created_at": "2024-04-24T22:59:25.240000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1232861001733640213,
            "username": "adebayooriyomi3984",
            "created_at": "2024-04-25T01:09:28.263000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1232861075482083410,
            "username": ".aaronv",
            "created_at": "2024-04-25T01:09:45.846000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1233634484608434237,
            "username": "tekumara",
            "created_at": "2024-04-27T04:23:00.951000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1233634682554552380,
            "username": ".aaronv",
            "created_at": "2024-04-27T04:23:48.145000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1233703842265301042,
            "username": "larsen1088",
            "created_at": "2024-04-27T08:58:37.105000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Really impressed by the tooling/DX you have created 🙌 \nDo you have some docs for how to run/setup development environment?\n\nWould love to submit a PR or maybe someone has a suggestion of how i can submit headers in runtime such that i can have different properties to different on cost etc?\ncurrent only working solution\n```\nclient<llm> GPT4Turbo {\n  provider baml-openai-chat\n  options {\n    model gpt-4-1106-preview\n    api_key env.OPENAI_API_KEY\n    // base_url env.OPENAI_BASE_URL seems to not work \n    extra_headers { \n     Helicone-Auth \"Bearer xxx\"\n     Helicone-Property-Environment development\n     Helicone-Property-Tenant test\n    }\n  }\n}\n```\n\nTried something like this\n```\nb.GPT4Turbo._set_args(\n    extra_headers={\n        \"Helicone-Property-Tenant\": \"test\",\n    },\n)\n```\n\nBut it had no effect.",
        "metadata": {
            "message_id": 1233704781902774414,
            "username": "larsen1088",
            "created_at": "2024-04-27T09:02:21.132000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1233764995859218433,
            "username": "hellovai",
            "created_at": "2024-04-27T13:01:37.258000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "headers",
        "metadata": {
            "message_id": 1233766611018715210,
            "username": "hellovai",
            "created_at": "2024-04-27T13:08:02.342000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1233921614060589116,
            "username": "jaco4054",
            "created_at": "2024-04-27T23:23:57.947000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey,\n\nI'm just wondering does baml have support for just generating the prompt rather than calling the client aswell?",
        "metadata": {
            "message_id": 1233924022077620304,
            "username": "jaco4054",
            "created_at": "2024-04-27T23:33:32.063000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1233955481328943129,
            "username": "kalel_1",
            "created_at": "2024-04-28T01:38:32.533000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi, just wondering, will we be able to upload images so that GPT can read the image? I looked into the documentation and I cant find anything regarding non-string inputs for prompts",
        "metadata": {
            "message_id": 1233956153277419620,
            "username": "kalel_1",
            "created_at": "2024-04-28T01:41:12.738000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We are working on this :). We will release it in a few days / a week",
        "metadata": {
            "message_id": 1233960751115407442,
            "username": ".aaronv",
            "created_at": "2024-04-28T01:59:28.948000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm not sure whether its an issue at my end or not, but today started getting these errors (I'm using 0.19.0): python baml_example_app.py\nTraceback (most recent call last):\n  File \"C:\\baml_example_app.py\", line 9, in <module>\n    from baml_client import baml as b\n  File \"\\baml_client\\__init__.py\", line 12, in <module>     \n    from baml_lib import baml_init\n  File \"C:\\Python310\\lib\\site-packages\\baml_lib\\__init__.py\", line 41, in <module>\n    bar.model_fields.keys()\nAttributeError: 'Bar' object has no attribute 'model_fields'",
        "metadata": {
            "message_id": 1234482954076553329,
            "username": "falconicx",
            "created_at": "2024-04-29T12:34:31.836000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "one suggestion: please create a seperate channel for issues/problems being faced etc.",
        "metadata": {
            "message_id": 1234484171527815210,
            "username": "falconicx",
            "created_at": "2024-04-29T12:39:22.099000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Baml init issue",
        "metadata": {
            "message_id": 1234486375714263045,
            "username": "hellovai",
            "created_at": "2024-04-29T12:48:07.618000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1234647464263417927,
            "username": "matt_47278",
            "created_at": "2024-04-29T23:28:14.121000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1234647544924209192,
            "username": "hellovai",
            "created_at": "2024-04-29T23:28:33.352000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is it possible for GPT to parse through a PDF?",
        "metadata": {
            "message_id": 1234724370262396978,
            "username": "kalel_1",
            "created_at": "2024-04-30T04:33:49.940000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Pdf parsing",
        "metadata": {
            "message_id": 1234733809518514180,
            "username": ".aaronv",
            "created_at": "2024-04-30T05:11:20.434000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any reason I'm getting errors for azure api_version? It's the same syntax with the documentation",
        "metadata": {
            "message_id": 1235075448971722883,
            "username": "kalel_1",
            "created_at": "2024-05-01T03:48:53.627000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1235098228077105213,
            "username": "rishabhpanwar05",
            "created_at": "2024-05-01T05:19:24.589000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1235098357165461545,
            "username": ".aaronv",
            "created_at": "2024-05-01T05:19:55.366000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1236928213091876864,
            "username": "parni9283",
            "created_at": "2024-05-06T06:31:07.016000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi, I'm using FastAPI and BAML. I called the api through fetch in my javascript frontend and keeps getting 500.\n\nI think there's an error in the baml_client\n\n> TypeError: ExtractContract[impl:default_config]() takes 0 positional arguments but 1 was given\n\nAnybody knows why this is happening?",
        "metadata": {
            "message_id": 1236936085481721939,
            "username": "kalel_1",
            "created_at": "2024-05-06T07:02:23.940000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Try running ‘baml update’",
        "metadata": {
            "message_id": 1237016906725068820,
            "username": ".aaronv",
            "created_at": "2024-05-06T12:23:33.226000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "And regenerating the code by saving your baml file",
        "metadata": {
            "message_id": 1237016985032724541,
            "username": ".aaronv",
            "created_at": "2024-05-06T12:23:51.896000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also run ‘npm uninstall @boundaryml/baml-core’ and install it again to update the js lib",
        "metadata": {
            "message_id": 1237017263689961482,
            "username": ".aaronv",
            "created_at": "2024-05-06T12:24:58.333000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "With npm add",
        "metadata": {
            "message_id": 1237017310200467456,
            "username": ".aaronv",
            "created_at": "2024-05-06T12:25:09.422000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Baml client error",
        "metadata": {
            "message_id": 1237051958309818393,
            "username": "hellovai",
            "created_at": "2024-05-06T14:42:50.175000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "tried it and got \n\n> [baml] Updating 0.18.0 -> 0.19.0\n> [baml] Running: scoop [\"update\"]\n> ERROR: Shell command failed: program not found",
        "metadata": {
            "message_id": 1237052454772936814,
            "username": "kalel_1",
            "created_at": "2024-05-06T14:44:48.541000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> mind assisting?",
        "metadata": {
            "message_id": 1237096576430903306,
            "username": ".aaronv",
            "created_at": "2024-05-06T17:40:07.964000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "@hellovai mind assisting?",
        "metadata": {
            "message_id": 1237102924363206812,
            "username": "hellovai",
            "created_at": "2024-05-06T18:05:21.429000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1237172082069213284,
            "username": "lukehartman",
            "created_at": "2024-05-06T22:40:09.911000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1237204551552073770,
            "username": "hellovai",
            "created_at": "2024-05-07T00:49:11.239000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1237354324397391953,
            "username": "rinshadkasharaf",
            "created_at": "2024-05-07T10:44:19.868000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1237388854689005709,
            "username": "hellovai",
            "created_at": "2024-05-07T13:01:32.531000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Did you guys change your docs? https://docs.boundaryml.com/docs/syntax/function\nTrying to figure out if I can return a list in a function given the new syntax",
        "metadata": {
            "message_id": 1237522740534317067,
            "username": "gabev2037",
            "created_at": "2024-05-07T21:53:33.404000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm unable to connect to my ollama instance with baml. Any chance i could debug what the network request is?",
        "metadata": {
            "message_id": 1237529052458520708,
            "username": "gabev2037",
            "created_at": "2024-05-07T22:18:38.284000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "You guys have office hours today? Would love to hop on for a couple of questions related to streaming a new type of object and handling it depending on the values of the fields",
        "metadata": {
            "message_id": 1238151258225770538,
            "username": "gabev2037",
            "created_at": "2024-05-09T15:31:03.699000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Remind me: Since there is no `is_complete` method on the stream,  how can I check whether a field has been fully passed? can i just check for \"None\"?",
        "metadata": {
            "message_id": 1238247139411497092,
            "username": "gabev2037",
            "created_at": "2024-05-09T21:52:03.555000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1239293239954243685,
            "username": ".samuli.",
            "created_at": "2024-05-12T19:08:53.360000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1239381633652490291,
            "username": "hellovai",
            "created_at": "2024-05-13T01:00:08.060000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1240523899138080820,
            "username": "bavik6413",
            "created_at": "2024-05-16T04:39:05.374000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1240524307982188608,
            "username": "bavik6413",
            "created_at": "2024-05-16T04:40:42.850000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1240529766583631892,
            "username": "hellovai",
            "created_at": "2024-05-16T05:02:24.282000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi all, does BAML work with deno typescript?",
        "metadata": {
            "message_id": 1240645585384636436,
            "username": "bavik6413",
            "created_at": "2024-05-16T12:42:37.636000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Deno support",
        "metadata": {
            "message_id": 1240648405642907722,
            "username": ".aaronv",
            "created_at": "2024-05-16T12:53:50.038000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi, Im pretty new to using this tool, can BAML create embeddings for say a text, or preferably a document. using the provided clients.",
        "metadata": {
            "message_id": 1240967964530966600,
            "username": "bavik6413",
            "created_at": "2024-05-17T10:03:38.814000+00:00",
            "edited_at": "2024-05-17T10:04:20.354000+00:00"
        }
    },
    {
        "text": "Hi, Im pretty new to using this tool,",
        "metadata": {
            "message_id": 1241031432172994606,
            "username": "hellovai",
            "created_at": "2024-05-17T14:15:50.679000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1243445073056895046,
            "username": "mikef206",
            "created_at": "2024-05-24T06:06:47.509000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1243449858610827274,
            "username": "hellovai",
            "created_at": "2024-05-24T06:25:48.474000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey, qq, is there a way to declare a variable that I can use across prompts?\n\nEx. we have glossary that we leverage across prompts and rather than having to update it in every prompt, would like to have it declared as a variable I can use.",
        "metadata": {
            "message_id": 1244309563902001163,
            "username": "ddematheu",
            "created_at": "2024-05-26T15:21:58.188000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1246207585497907261,
            "username": "mariena0892",
            "created_at": "2024-05-31T21:04:01.800000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1246208317970452541,
            "username": "hellovai",
            "created_at": "2024-05-31T21:06:56.435000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1246996697058512906,
            "username": "beauhilton",
            "created_at": "2024-06-03T01:19:40.657000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1247000492064768071,
            "username": "hellovai",
            "created_at": "2024-06-03T01:34:45.457000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1247035200794525716,
            "username": "beauhilton",
            "created_at": "2024-06-03T03:52:40.663000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1247241233923969044,
            "username": "larsen8506",
            "created_at": "2024-06-03T17:31:22.789000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any chance the future executor issue was fixed?",
        "metadata": {
            "message_id": 1248047230577803305,
            "username": "gabev2037",
            "created_at": "2024-06-05T22:54:07.366000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Btw do u guys support audio?",
        "metadata": {
            "message_id": 1248049519665483776,
            "username": "gabev2037",
            "created_at": "2024-06-05T23:03:13.127000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Audio support",
        "metadata": {
            "message_id": 1248058971781206113,
            "username": "hellovai",
            "created_at": "2024-06-05T23:40:46.687000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "1. Are you guys seeing issues with Anthropic not leveraging the system prompt? Specifically using Sonnet\n2. In the dashboard, when I expand the LLM prompt / LLM Raw String Output, I can't scroll",
        "metadata": {
            "message_id": 1248618099830161502,
            "username": "gabev2037",
            "created_at": "2024-06-07T12:42:33.202000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1249637263969746999,
            "username": "feres_65239",
            "created_at": "2024-06-10T08:12:20.868000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1249741151024709784,
            "username": "hellovai",
            "created_at": "2024-06-10T15:05:09.472000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "1. Are you guys seeing issues with",
        "metadata": {
            "message_id": 1249744905480306774,
            "username": "hellovai",
            "created_at": "2024-06-10T15:20:04.604000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1249978880312217611,
            "username": "gabriel4685",
            "created_at": "2024-06-11T06:49:48.551000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1249983072175259689,
            "username": ".aaronv",
            "created_at": "2024-06-11T07:06:27.969000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1250294048888197261,
            "username": "zwf0",
            "created_at": "2024-06-12T03:42:10.595000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "welcome!",
        "metadata": {
            "message_id": 1250296255935152200,
            "username": ".aaronv",
            "created_at": "2024-06-12T03:50:56.796000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1250469951182209034,
            "username": "j.l9883",
            "created_at": "2024-06-12T15:21:08.970000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hello!!",
        "metadata": {
            "message_id": 1250470058933616711,
            "username": ".aaronv",
            "created_at": "2024-06-12T15:21:34.660000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1250542342512050176,
            "username": "guid_druid",
            "created_at": "2024-06-12T20:08:48.408000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "#!/usr/bin/env node",
        "metadata": {
            "message_id": 1250865007999193233,
            "username": "gabriel4685",
            "created_at": "2024-06-13T17:30:57.855000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "nx.dev",
        "metadata": {
            "message_id": 1250865106540167220,
            "username": "gabriel4685",
            "created_at": "2024-06-13T17:31:21.349000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1251184686722580551,
            "username": "shof",
            "created_at": "2024-06-14T14:41:15.202000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1251184749393739888,
            "username": "hellovai",
            "created_at": "2024-06-14T14:41:30.144000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252318373727965184,
            "username": "bgoss7761",
            "created_at": "2024-06-17T17:46:07.247000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252318505336832120,
            "username": "hellovai",
            "created_at": "2024-06-17T17:46:38.625000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252371410953830400,
            "username": "swirl0001",
            "created_at": "2024-06-17T21:16:52.307000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252371608773857320,
            "username": "hellovai",
            "created_at": "2024-06-17T21:17:39.471000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252421433636683866,
            "username": "dantheman252",
            "created_at": "2024-06-18T00:35:38.644000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "welcome! 👋🏾",
        "metadata": {
            "message_id": 1252421570220003388,
            "username": "hellovai",
            "created_at": "2024-06-18T00:36:11.208000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252520482096545843,
            "username": "handfuloflight",
            "created_at": "2024-06-18T07:09:13.637000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252531362515128351,
            "username": "hellovai",
            "created_at": "2024-06-18T07:52:27.731000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252533576755314780,
            "username": "dangerousyams",
            "created_at": "2024-06-18T08:01:15.647000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://tenor.com/view/hi-gif-5545306308765291118",
        "metadata": {
            "message_id": 1252535738055266346,
            "username": "handfuloflight",
            "created_at": "2024-06-18T08:09:50.941000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252539035176538225,
            "username": "Deleted User",
            "created_at": "2024-06-18T08:22:57.036000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252546518943273050,
            "username": "jonas_93113",
            "created_at": "2024-06-18T08:52:41.305000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252583229039312926,
            "username": "deoxykev",
            "created_at": "2024-06-18T11:18:33.674000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi, from HN. Cool project. I see that BAML is converted to typescript before being sent to the LLM. Would it be possible to convert json schema into baml?",
        "metadata": {
            "message_id": 1252583742980100189,
            "username": "deoxykev",
            "created_at": "2024-06-18T11:20:36.207000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also, are there methods to expose the logit_bias parameter when calling openai api?",
        "metadata": {
            "message_id": 1252584072887144479,
            "username": "deoxykev",
            "created_at": "2024-06-18T11:21:54.863000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252631588832018482,
            "username": "cat_ethos",
            "created_at": "2024-06-18T14:30:43.547000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252642806238023741,
            "username": "josephsirosh_22062",
            "created_at": "2024-06-18T15:15:17.985000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "JSON Schema -> BAML",
        "metadata": {
            "message_id": 1252650606603145312,
            "username": "hellovai",
            "created_at": "2024-06-18T15:46:17.737000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Logit Bias",
        "metadata": {
            "message_id": 1252650999211098260,
            "username": "hellovai",
            "created_at": "2024-06-18T15:47:51.342000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252686077584605256,
            "username": "tholeg",
            "created_at": "2024-06-18T18:07:14.678000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252690617402789938,
            "username": "xcd1947.",
            "created_at": "2024-06-18T18:25:17.055000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252695203173896265,
            "username": "sudhanshug",
            "created_at": "2024-06-18T18:43:30.388000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252695641059098705,
            "username": ".nihao",
            "created_at": "2024-06-18T18:45:14.788000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I am not building in the AI space anymore, but this is exactly what I have been dreaming of for a year! Godspeed.",
        "metadata": {
            "message_id": 1252696020702203945,
            "username": "sudhanshug",
            "created_at": "2024-06-18T18:46:45.302000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "thanks for the kind words! If at some point you have ideas for the roadmap we're happy to hear them!",
        "metadata": {
            "message_id": 1252696352442552360,
            "username": ".aaronv",
            "created_at": "2024-06-18T18:48:04.395000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "thanks for the kind words! If at some",
        "metadata": {
            "message_id": 1252699417924862094,
            "username": "sudhanshug",
            "created_at": "2024-06-18T19:00:15.263000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252716978217554051,
            "username": "elijas_ai",
            "created_at": "2024-06-18T20:10:01.963000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Let me know what you think;\nFor catching traction, it could be worth finding relevant stack overflow answers (like [this](https://genai.stackexchange.com/questions/202/how-to-generate-structured-data-like-json-with-llm-models/684#684), [this](https://genai.stackexchange.com/questions/234/file-format-for-generating-error-free-structured-data-with-llms/685#685), [this](https://genai.stackexchange.com/questions/641/match-llm-output-to-fixed-ontology/683#683), [this](https://genai.stackexchange.com/questions/686/how-to-force-llm-to-output-a-variable-list-of-items-such-as-strings), etc.) and adding example of how it could be done with BAML",
        "metadata": {
            "message_id": 1252717605438095450,
            "username": "elijas_ai",
            "created_at": "2024-06-18T20:12:31.504000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252718506818932867,
            "username": "rawwerks",
            "created_at": "2024-06-18T20:16:06.410000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hi! i am very excited to find out about BAML / Boundary via Cerebral Valley. i have one question, as someone coming from instructor/magentic/DSPy: \n\ndoes BAML strictly enforce a Pydantic type for the output? \n\nthis is something that is possible to do in any of those three frameworks, but not particularly fun to set up. (in other words --- can i be 100% sure that the output will have the correct type? (or fail elegantly)).\n\n(this wasn't obvious to me after reading the \"vs instructor\" blog post.)",
        "metadata": {
            "message_id": 1252719283784388701,
            "username": "rawwerks",
            "created_at": "2024-06-18T20:19:11.653000+00:00",
            "edited_at": "2024-06-18T20:21:12.637000+00:00"
        }
    },
    {
        "text": "How to force LLM to output a variable li...",
        "metadata": {
            "message_id": 1252727502581923865,
            "username": "hellovai",
            "created_at": "2024-06-18T20:51:51.167000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hi! i am very excited to find out about",
        "metadata": {
            "message_id": 1252727739501514767,
            "username": "hellovai",
            "created_at": "2024-06-18T20:52:47.653000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252746720920600586,
            "username": "enoonoone",
            "created_at": "2024-06-18T22:08:13.176000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "How to make VSCode Playground `clients.baml` use `.env` file in the root of the project?",
        "metadata": {
            "message_id": 1252758471640879156,
            "username": "elijas_ai",
            "created_at": "2024-06-18T22:54:54.766000+00:00",
            "edited_at": "2024-06-18T22:58:00.897000+00:00"
        }
    },
    {
        "text": "Can we pass images as input through BAML?",
        "metadata": {
            "message_id": 1252765788692152442,
            "username": "elijas_ai",
            "created_at": "2024-06-18T23:23:59.287000+00:00",
            "edited_at": "2024-06-18T23:25:06.133000+00:00"
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252808331832197161,
            "username": "aniraga",
            "created_at": "2024-06-19T02:13:02.362000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252834714021138452,
            "username": "kargnas",
            "created_at": "2024-06-19T03:57:52.366000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I wish that baml be available in PHP!",
        "metadata": {
            "message_id": 1252834878404296766,
            "username": "kargnas",
            "created_at": "2024-06-19T03:58:31.558000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Php support",
        "metadata": {
            "message_id": 1252844413655191588,
            "username": "hellovai",
            "created_at": "2024-06-19T04:36:24.939000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252864152171446343,
            "username": "arunbahl",
            "created_at": "2024-06-19T05:54:50.968000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252933835453108314,
            "username": "ludwigkr",
            "created_at": "2024-06-19T10:31:44.757000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252938365683372063,
            "username": "grzjur",
            "created_at": "2024-06-19T10:49:44.848000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1252991366078660691,
            "username": "caiolang",
            "created_at": "2024-06-19T14:20:21.127000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253001340498477057,
            "username": "caiolang",
            "created_at": "2024-06-19T14:59:59.214000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253004279363539048,
            "username": "_rgunn",
            "created_at": "2024-06-19T15:11:39.894000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253004589440176209,
            "username": ".aaronv",
            "created_at": "2024-06-19T15:12:53.822000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253011965626810488,
            "username": "winterday5739",
            "created_at": "2024-06-19T15:42:12.442000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "For all new folks who joined -- let me know if you'd prefer have some specific channels. Most of the time I've seen channels die on discord so that's why we only have General and use Threads as a way to organize convos",
        "metadata": {
            "message_id": 1253025421897891871,
            "username": ".aaronv",
            "created_at": "2024-06-19T16:35:40.667000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253028779731976323,
            "username": "hellovai",
            "created_at": "2024-06-19T16:49:01.237000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253035903589290025,
            "username": "etomoynik",
            "created_at": "2024-06-19T17:17:19.697000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is it expected to need to reset the env keys in the Baml playground each time VSCode loads?",
        "metadata": {
            "message_id": 1253061969368584213,
            "username": "gabev2037",
            "created_at": "2024-06-19T19:00:54.263000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Request: When running a test in playground, I don't see an option to see the result in the boundary dashboard. Mostly want it to be able to see how many tokens were used in the output",
        "metadata": {
            "message_id": 1253064023604330688,
            "username": "gabev2037",
            "created_at": "2024-06-19T19:09:04.031000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "btw BIG FAN of writing tests directly in the baml files",
        "metadata": {
            "message_id": 1253065725942304859,
            "username": "gabev2037",
            "created_at": "2024-06-19T19:15:49.900000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Request: When running a test in",
        "metadata": {
            "message_id": 1253068297667346587,
            "username": ".aaronv",
            "created_at": "2024-06-19T19:26:03.047000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253068413765685348,
            "username": "mariustrovik",
            "created_at": "2024-06-19T19:26:30.727000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is OH happening?",
        "metadata": {
            "message_id": 1253087489384517642,
            "username": "gabev2037",
            "created_at": "2024-06-19T20:42:18.709000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253090477054627962,
            "username": "hadou_cannot",
            "created_at": "2024-06-19T20:54:11.025000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253094511031357500,
            "username": "berry06127",
            "created_at": "2024-06-19T21:10:12.800000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Can enums in baml have @description or similar?",
        "metadata": {
            "message_id": 1253098908868153476,
            "username": "gabev2037",
            "created_at": "2024-06-19T21:27:41.326000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@1164458147604353085>  Anything you think we can improve on onboarding after you used BAML now for the first time? We are working on better docs, but all feedback is appreciated.",
        "metadata": {
            "message_id": 1253118514135306280,
            "username": ".aaronv",
            "created_at": "2024-06-19T22:45:35.586000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253161921092386879,
            "username": "predatedtomcat",
            "created_at": "2024-06-20T01:38:04.611000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I do have a few suggestions:\nMove the welcome channel into #welcome\n\nAnd a few channels: <#1119375594984050779> #troubleshooting #suggestions #contributing #questions",
        "metadata": {
            "message_id": 1253167669998518293,
            "username": "deoxykev",
            "created_at": "2024-06-20T02:00:55.257000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253168688408952915,
            "username": "bradp6496",
            "created_at": "2024-06-20T02:04:58.065000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey guys. I am using a managed identity on Azure with bearer token which I pass to OpenAI “ad token provider” param.\nHow can I instantiate this client to use it with BAML?",
        "metadata": {
            "message_id": 1253264577433567242,
            "username": "paulicapopcorn",
            "created_at": "2024-06-20T08:25:59.790000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey guys. I am using a managed identity",
        "metadata": {
            "message_id": 1253340839904673893,
            "username": "hellovai",
            "created_at": "2024-06-20T13:29:02.180000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi everyone, instead of using this channel, consider using <#1253172277449855029> , <#1253172325205934181>  or <#1253172394345107466> ! \n\nIf you're interested in contributing, come over to <#1253172368671772732> and share your idea.",
        "metadata": {
            "message_id": 1253369270126116977,
            "username": "hellovai",
            "created_at": "2024-06-20T15:22:00.473000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1253369295136751617,
            "username": "hellovai",
            "created_at": "2024-06-20T15:22:06.436000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "just set up baml for my project, 10/10 experience and much faster than langchain",
        "metadata": {
            "message_id": 1253409441844301866,
            "username": "jfan8684",
            "created_at": "2024-06-20T18:01:38.157000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Developer experience working with test images could be better\n\nCould we maybe point BAML to an image, and then let BAML generate the base64 during the test run?\nI suppose I could script this myself quite easily (run a script to overwrite all the image tests with freshly generated base64 byte tests, based on `test_name` found in BAML and `test_name->image_path` python dict)",
        "metadata": {
            "message_id": 1253430461720694835,
            "username": "elijas_ai",
            "created_at": "2024-06-20T19:25:09.686000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ah should've used <#1253172277449855029> -- will keep in mind for the future ✅",
        "metadata": {
            "message_id": 1253430676687032320,
            "username": "elijas_ai",
            "created_at": "2024-06-20T19:26:00.938000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "funny thing that we saw -- if you name your string[] fields with a singular name like this:\n`row string[]`\n\nan LLM may be likely to try to \"fix\" the schema variable to `rows` (plural). Names are important, especially for dumber models.",
        "metadata": {
            "message_id": 1255315039670243519,
            "username": ".aaronv",
            "created_at": "2024-06-26T00:13:48.083000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```\n  errors string[] @description(#\"List any processing errors with details about the problem and its impact\"#)\n\n  warnings string[] @description(#\"List potential issues or inconsistencies that need attention\"#)\n```\nI tend to keep fields like these around when having more complex processing steps, and later pipe the output to `logger` 🤔, allows a way for an LLM to surface issues and decontaminate other fields from LLM comments and disclaimers",
        "metadata": {
            "message_id": 1255475575796138085,
            "username": "elijas_ai",
            "created_at": "2024-06-26T10:51:42.878000+00:00",
            "edited_at": "2024-06-26T10:53:47.864000+00:00"
        }
    },
    {
        "text": "```",
        "metadata": {
            "message_id": 1255496501703086117,
            "username": "gabev2037",
            "created_at": "2024-06-26T12:14:52.003000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi folks! New to this exciting project! Was wondering if there's a publicly available roadmap for BAML?",
        "metadata": {
            "message_id": 1258841102132445316,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T17:45:06.885000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Welcome! We currently dont have a public one yet, we’ll be publishing soon, possibly next week. Any features specifically youd like to see? Feel free to send your feedback or thoughts on BAML!",
        "metadata": {
            "message_id": 1258842481647292498,
            "username": ".aaronv",
            "created_at": "2024-07-05T17:50:35.787000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Awesome! Just scrolling through the docs rn, will get back once I know what is and isn't possible yet 🙂",
        "metadata": {
            "message_id": 1258843220016894132,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T17:53:31.828000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Impressive stuff! I guess my most important question is this: We're working in an environment where only local LLM deployments are possible. Therefore, we're currently using vLLM + Outlines to locally generate  structured output (with Llama 3, fwiw).  Outlines is nice because it constrains vLLM generations in a way that __guarantees__ correctly structured outputs, 100% of the time. Can BAML also __guarantee__ correctly structured outputs when using it with a local LLM engine like vLLM?",
        "metadata": {
            "message_id": 1258847617111162990,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T18:11:00.177000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Constrained generation vs BAML",
        "metadata": {
            "message_id": 1258850991684452464,
            "username": ".aaronv",
            "created_at": "2024-07-05T18:24:24.738000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Two more questions:\n\n* We wouldn't be able to use the observability platform, since we're required to run everything locally. Would you, in principle, support local deployments of the observability platform?\n\n* We're currently caching all our requests to the LLM with `requests-cache`. Does BAML support caching?",
        "metadata": {
            "message_id": 1258863315401310308,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T19:13:22.941000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "- we do want to support local deployments 100%. We just need to prioritize it. For now we do offer a hook that contains each request log input/output that you could use to export elsewhere.\n\n- we dont currently cache requests. Do you use the cache for tests? Or do you also cache in  production?",
        "metadata": {
            "message_id": 1258864668391510066,
            "username": ".aaronv",
            "created_at": "2024-07-05T19:18:45.519000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also wondering — do you use python?",
        "metadata": {
            "message_id": 1258864923526692936,
            "username": ".aaronv",
            "created_at": "2024-07-05T19:19:46.348000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> we dont currently cache requests. Do you use the cache for tests? Or do you also cache in  production?\nWe use it for both. Essentially, we're periodically running the same 20-30 prompts on a quite large set of documents. Every now and then, a document / prompt gets added / modified, and we re-run everything. Caching makes the re-run efficient without additional engineering.",
        "metadata": {
            "message_id": 1258865367485513879,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T19:21:32.196000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> Also wondering — do you use python?\nYep.",
        "metadata": {
            "message_id": 1258865404785459322,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T19:21:41.089000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Do you cache requests locally in memory, in a file, or other storage like redis?",
        "metadata": {
            "message_id": 1258865734503764068,
            "username": ".aaronv",
            "created_at": "2024-07-05T19:22:59.700000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Right now, in a file.",
        "metadata": {
            "message_id": 1258866019380887704,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T19:24:07.620000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oks, we do have caching in our roadmap so we could move that up for sure if it s a blocker to use BAML in production",
        "metadata": {
            "message_id": 1258866508118098041,
            "username": ".aaronv",
            "created_at": "2024-07-05T19:26:04.144000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Cool! No need to push it right now, but will let you know if we're moving closer to deploying BAML.",
        "metadata": {
            "message_id": 1258867557243555902,
            "username": "robert_hoenig",
            "created_at": "2024-07-05T19:30:14.275000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sounds good",
        "metadata": {
            "message_id": 1258867699124277279,
            "username": ".aaronv",
            "created_at": "2024-07-05T19:30:48.102000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Probably due to our use case, but my team saw our openai bill cut in half when we started caching prompts with redis",
        "metadata": {
            "message_id": 1259140174391017512,
            "username": "deoxykev",
            "created_at": "2024-07-06T13:33:31.268000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I assume you also get lots of the same queries?? Interesting",
        "metadata": {
            "message_id": 1259184370095231097,
            "username": ".aaronv",
            "created_at": "2024-07-06T16:29:08.345000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "oh yeah, tons. A lot of form fields that need to be normalized in some non-trivial way. There’s a lot of WTF moments when you hit production with real data. Again, maybe my particular use case.",
        "metadata": {
            "message_id": 1259495563318268017,
            "username": "deoxykev",
            "created_at": "2024-07-07T13:05:42.591000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> For now we do offer a hook that contains each request log input/output that you could use to export elsewhere.\n\nFollow-up question: Is `on_log_event` the hook that you offer? When I use `on_log_event`, the resulting `BamlLogEvent` includes some, but not all data that I'm interested in. Most importantly, I'd like to log the token usage information\n```\n \"usage\": {\n    \"completion_tokens\": 123,\n    \"prompt_tokens\": 456,\n    \"total_tokens\": 789\n  }\n```.\nIs that possible?\n\nMore generally, I'd like to log all incoming and outgoing traffic.",
        "metadata": {
            "message_id": 1263042839781179444,
            "username": "robert_hoenig",
            "created_at": "2024-07-17T08:01:19.205000+00:00",
            "edited_at": "2024-07-17T08:17:08.034000+00:00"
        }
    },
    {
        "text": "on log event",
        "metadata": {
            "message_id": 1263054832017870942,
            "username": "hellovai",
            "created_at": "2024-07-17T08:48:58.377000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Clarification question: I'm comparing Outlines, Instructor, and BAML, and trying to understand the technical nuances that differentiate BAML. Especially over time, as both Outlines and Instructor are also continuously improving their codebases. What is enduringly different about the BAML approach?",
        "metadata": {
            "message_id": 1263508776581992581,
            "username": "josephsirosh_22062",
            "created_at": "2024-07-18T14:52:47.195000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "BAML vs other frameworks",
        "metadata": {
            "message_id": 1263529649976115287,
            "username": "hellovai",
            "created_at": "2024-07-18T16:15:43.800000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "[official] New Feature Proposal: Field Validations",
        "metadata": {
            "message_id": 1265356689796890820,
            "username": "hellovai",
            "created_at": "2024-07-23T17:15:44.038000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "@everyone, we're proposing a huge feature into BAML: Type Validations (like Zod and Pydantic, but more powerful!).\n\nBefore launching it, we'd love anyone interested to provide feedback on the docs to see if this is a valuable feature.\n\nWith `@assert` you'll be able to do something like:\n\n```rust\n// Ensure that the invoice is only valid if the \n// total is the sum of all the line items\nclass Invoice {\n  line_item Item[]\n  total float @assert(this == block.line_item|map('cost')|sum)\n}\n```\n\nIf we get that the `total !== line_item.map(x => x.cost).sum()`, we would raise an exception, almost as if `total` wasn't found or was the wrong type.\n\nWe also support other things like:\n* Regex matching (along with ==, !=, >, etc)\n* Getting a list of failing tests, but not as an exception\n* Relationships between fields and assertions (e.g. asking the LLM for a citation and ensuring the citation is in your original next)\n\nPlease provide feedback if you think this would help you.\n\nSee full docs: \nhttps://boundary-preview-0c038fad-a0a4-4706-8cdc-c48c097b232d.docs.buildwithfern.com/docs/calling-baml/assertions\n\nSpecial thanks to <@1062441178022289479> who inspired this work!",
        "metadata": {
            "message_id": 1265356697120276494,
            "username": "hellovai",
            "created_at": "2024-07-23T17:15:45.784000+00:00",
            "edited_at": "2024-07-23T20:55:55.841000+00:00"
        }
    },
    {
        "text": "[official] [Proposal]: Field Validations",
        "metadata": {
            "message_id": 1265356844222644254,
            "username": "hellovai",
            "created_at": "2024-07-23T17:16:20.856000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1265712210273894503,
            "username": "hellovai",
            "created_at": "2024-07-24T16:48:26.725000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@550656390679494657>",
        "metadata": {
            "message_id": 1280667003979305013,
            "username": "hellovai",
            "created_at": "2024-09-03T23:13:27.391000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@503733199130722314>",
        "metadata": {
            "message_id": 1286372883526062092,
            "username": "hellovai",
            "created_at": "2024-09-19T17:06:35.085000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "This looks awesome and really useful. Could potentially use similar in the test cases to assert that for example, classifying an image should return Receipt",
        "metadata": {
            "message_id": 1287816482704593057,
            "username": "davidyoung",
            "created_at": "2024-09-23T16:42:55.950000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yep! That is gonna land shortly after this lands 🙂 <@503733199130722314>",
        "metadata": {
            "message_id": 1287816685276893184,
            "username": "hellovai",
            "created_at": "2024-09-23T16:43:44.247000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Incredible, very excited. Must say, I'm very impressed with BAML so far and I've tried a lot of libraries and solutions",
        "metadata": {
            "message_id": 1287817392327753790,
            "username": "davidyoung",
            "created_at": "2024-09-23T16:46:32.821000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Especially the playground and tests, works very well",
        "metadata": {
            "message_id": 1287817419003396218,
            "username": "davidyoung",
            "created_at": "2024-09-23T16:46:39.181000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "glad you're liking it david 🙂 Keep the suggestions and feedback coming! 😉",
        "metadata": {
            "message_id": 1287817525173817404,
            "username": "hellovai",
            "created_at": "2024-09-23T16:47:04.494000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "willdo!",
        "metadata": {
            "message_id": 1287817566198300712,
            "username": "davidyoung",
            "created_at": "2024-09-23T16:47:14.275000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Do you have an ETA on the porposal above?",
        "metadata": {
            "message_id": 1287817592962289704,
            "username": "davidyoung",
            "created_at": "2024-09-23T16:47:20.656000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@503733199130722314> can share that once he's on 🙂",
        "metadata": {
            "message_id": 1287817727548985404,
            "username": "hellovai",
            "created_at": "2024-09-23T16:47:52.744000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@969548252900913193> We're currently implementing it, and hoping to have the first version out in a couple of weeks.",
        "metadata": {
            "message_id": 1287845865637281884,
            "username": "imalsogreg",
            "created_at": "2024-09-23T18:39:41.387000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Awesome, super helpful 🙂",
        "metadata": {
            "message_id": 1287857478477811743,
            "username": "davidyoung",
            "created_at": "2024-09-23T19:25:50.104000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just launched my first BAML project to a client, and holy shit, it's awesome. BAML took the hardest parts of working with LLMs and made them ridiculously easy. Out of 10 points, I give it 1000.",
        "metadata": {
            "message_id": 1268665589988069387,
            "username": "bsachs10",
            "created_at": "2024-08-01T20:24:07.320000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "My umich friend texting me at 11pm last night while working on his side project: “Baml is beautiful” 🤣",
        "metadata": {
            "message_id": 1268928687424933990,
            "username": "ashwin.a.kumar",
            "created_at": "2024-08-02T13:49:34.637000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1268929109807992852,
            "username": "ashwin.a.kumar",
            "created_at": "2024-08-02T13:51:15.341000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thats amazing. Thanks for sharing this and spreading the word",
        "metadata": {
            "message_id": 1268940984125755494,
            "username": ".aaronv",
            "created_at": "2024-08-02T14:38:26.399000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any hot takes on the OpenAI structured output announcement?",
        "metadata": {
            "message_id": 1270480120930500711,
            "username": "unsignedint.",
            "created_at": "2024-08-06T20:34:25.209000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://openai.com/index/introducing-structured-outputs-in-the-api/",
        "metadata": {
            "message_id": 1270508424525778987,
            "username": "deoxykev",
            "created_at": "2024-08-06T22:26:53.312000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "see <#1270480120930500711>",
        "metadata": {
            "message_id": 1270510376085360763,
            "username": "hellovai",
            "created_at": "2024-08-06T22:34:38.600000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey, \n\nI currently use Instructor and have been looking into alternatives to JSON schema as I am realizing that so much of my output tokens (80%) are just extra tokens to fit a json schema structure vs the actual content and that's making latency a little too high for our use cases.\n\nI just came across BAML after a quick google search. From a super quick glance on the website, I see how BAML definitely reduces input tokens. But does it reduce output tokens considerably as well? Is there an analysis I can read about it.",
        "metadata": {
            "message_id": 1270617492703674379,
            "username": "anmolsood",
            "created_at": "2024-08-07T05:40:17.192000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I think we all had an intuition that this was true but it’s neat to see some quantitative rigor behind it \n\nhttps://arxiv.org/abs/2408.02442",
        "metadata": {
            "message_id": 1271846638281031680,
            "username": "deoxykev",
            "created_at": "2024-08-10T15:04:28.334000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "can I have control over length or number of items?",
        "metadata": {
            "message_id": 1271915938928791615,
            "username": "underdog6143",
            "created_at": "2024-08-10T19:39:50.896000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey guys, \nDoes baml support Python runtime in AWS lambda?",
        "metadata": {
            "message_id": 1272029027766370395,
            "username": "saransh7966",
            "created_at": "2024-08-11T03:09:13.376000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "guys hosting in vercel \ngetting this\ncause]: [\n    Error: Cannot find module './baml.linux-x64-gnu.node'",
        "metadata": {
            "message_id": 1272247041564868649,
            "username": "underdog6143",
            "created_at": "2024-08-11T17:35:31.916000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "how can i resolve this",
        "metadata": {
            "message_id": 1272247065312755813,
            "username": "underdog6143",
            "created_at": "2024-08-11T17:35:37.578000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Typeerror async for requires an object with aiyer method, got bamlsyncstream",
        "metadata": {
            "message_id": 1273537155481993290,
            "username": "underdog6143",
            "created_at": "2024-08-15T07:01:59.043000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Python with fastapi",
        "metadata": {
            "message_id": 1273537202969903205,
            "username": "underdog6143",
            "created_at": "2024-08-15T07:02:10.365000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I seem to get error when trying to make an array optional. Is this because the rust model will automatically make it an empty array if there is no data? I can fix it by making the type (Obj[] | null)",
        "metadata": {
            "message_id": 1273949033685057536,
            "username": ".alex4o",
            "created_at": "2024-08-16T10:18:38.459000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Null arrays",
        "metadata": {
            "message_id": 1274030974694461534,
            "username": ".aaronv",
            "created_at": "2024-08-16T15:44:14.718000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey for function calling; currently i cant make third party api calls right?",
        "metadata": {
            "message_id": 1274778713711706215,
            "username": "underdog6143",
            "created_at": "2024-08-18T17:15:29.592000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Function calling 3rd party api calls",
        "metadata": {
            "message_id": 1274785094820954159,
            "username": "hellovai",
            "created_at": "2024-08-18T17:40:50.967000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi guys, I'm very new to BAML. I've just been looking at it today for the first time, and I have a few questions:\n\nCurrently, do I *need* to use the Python, TS or Ruby API's? Or do these just make it *easier* to use BAML? Essentially what I'm asking is, I see that most of the code that powers BAML is written in rust, but where does this underlying engine live? Is it behind these language-specific APIs?\n\nIf not, is there a way I can use BAML and integrate it into an application no matter the language?",
        "metadata": {
            "message_id": 1275121038815920191,
            "username": "dob.son",
            "created_at": "2024-08-19T15:55:46.257000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1275131205448106146,
            "username": ".aaronv",
            "created_at": "2024-08-19T16:36:10.171000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We are working on a way to setup a BAML server that can serve all your functions, and we can give you an OpenAPI client in the language of your choice to interact with all your functions. This server oculd be deployed by you using docker or anywhere really.\n\nAre you looking for a specific language support?",
        "metadata": {
            "message_id": 1275131208551895163,
            "username": ".aaronv",
            "created_at": "2024-08-19T16:36:10.911000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "BAML has a runtime written in rust for which we have bindings for in JS, TS and Ruby. For other languages we would need to do the \"baml server\" approach until we can create those native bindings",
        "metadata": {
            "message_id": 1275131422260072600,
            "username": ".aaronv",
            "created_at": "2024-08-19T16:37:01.863000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "The Rust runtime is where the parser / API calls to LLMs live",
        "metadata": {
            "message_id": 1275131508256018513,
            "username": ".aaronv",
            "created_at": "2024-08-19T16:37:22.366000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’m working with Kotlin, so it would be some Java bindings. That BAML server approach sounds great though. Guessing there’d be a Java API to interact with said server",
        "metadata": {
            "message_id": 1275164603659194439,
            "username": "dob.son",
            "created_at": "2024-08-19T18:48:52.925000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "exactly! we are actively working on this now!",
        "metadata": {
            "message_id": 1275164682352595061,
            "username": ".aaronv",
            "created_at": "2024-08-19T18:49:11.687000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey there! We're getting closer to shipping this, and wanted to ask: how do you deploy your backend services?\n\nWe're currently going to provide a docker-compose example in our docs, but if you have a container stack that you're going to use, we can also try to add an example for that",
        "metadata": {
            "message_id": 1278104245014626415,
            "username": "joatmon.pockets",
            "created_at": "2024-08-27T21:29:58.040000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just following up- we're getting closer and closer to the release! Expect it some time this week; we'll update this thread when we do.\n\nTo give you a sense of the current status, we're doing final passes now (e.g. reviewing generated code and making sure that we can preserve backwards compatibility for future releases).\n\nThings you can expect:\n\n- we've implemented OpenAPI codegen that can also run `openapi-generator-cli` for you automatically\n- we've implemented a server with hot reload that will re-generate everything when you edit BAML files\n- we've added not only quickstart docs, but also a `docker-compose` example ([preview](https://boundary-preview-1eabadb4-99c1-45a6-8131-2277b2373fe9.docs.buildwithfern.com/docs/get-started/deploying/openapi))\n- we've published example code for Go, Java, PHP, Rust at https://github.com/BoundaryML/baml-examples",
        "metadata": {
            "message_id": 1280605729190838324,
            "username": "joatmon.pockets",
            "created_at": "2024-09-03T19:09:58.343000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@114076226133557253>  BAML 0.55.0 is out, which means you can now try out BAML from Java/Kotlin!\n\n- [announcement](https://discord.com/channels/1119368998161752075/1119375433666920530/1282818005084016721)\n- [quickstart docs](https://docs.boundaryml.com/docs/get-started/quickstart/openapi)\n- [baml-examples (java with gradle)](https://github.com/BoundaryML/baml-examples/tree/main/java-gradle-openapi-starter)",
        "metadata": {
            "message_id": 1282820065544110244,
            "username": "joatmon.pockets",
            "created_at": "2024-09-09T21:48:57.270000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Amazing! Can’t wait to try this out, thanks guys 😁",
        "metadata": {
            "message_id": 1282820474954580039,
            "username": "dob.son",
            "created_at": "2024-09-09T21:50:34.881000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sorry for not responding to anything else in this thread, I literally didn’t receive any notifications",
        "metadata": {
            "message_id": 1282820915956285533,
            "username": "dob.son",
            "created_at": "2024-09-09T21:52:20.024000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "That’s my bad",
        "metadata": {
            "message_id": 1282820925997322335,
            "username": "dob.son",
            "created_at": "2024-09-09T21:52:22.418000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "no worries! glad this one got your attention 🙂",
        "metadata": {
            "message_id": 1282820983413149798,
            "username": "joatmon.pockets",
            "created_at": "2024-09-09T21:52:36.107000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Think it was the “@“ haha",
        "metadata": {
            "message_id": 1282821072558882927,
            "username": "dob.son",
            "created_at": "2024-09-09T21:52:57.361000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "But yeah the docker approach you mentioned sounds perfect",
        "metadata": {
            "message_id": 1282821138145083475,
            "username": "dob.son",
            "created_at": "2024-09-09T21:53:12.998000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "FYI- I tried out the Java code generator but didn't try out the Kotlin one. I don't *expect* there to be any major surprises, given what I know about Kotlin's type system (some of the client languages had some quirks if, say, a given language didn't have a first-class notion of \"enums\"), but hopefully the Java + Gradle example is enough for you to get going",
        "metadata": {
            "message_id": 1282821508825350209,
            "username": "joatmon.pockets",
            "created_at": "2024-09-09T21:54:41.375000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah we tend to use a lot of Java-native APIs if you like in our Kotlin code and get on just fine. So if it works with Java, I’m sure it’ll do fine with Kotlin 😁\n\nIf I encounter any issues or anything I’ll make sure to post them here",
        "metadata": {
            "message_id": 1282821841446240266,
            "username": "dob.son",
            "created_at": "2024-09-09T21:56:00.678000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks again guys",
        "metadata": {
            "message_id": 1282821852376465429,
            "username": "dob.son",
            "created_at": "2024-09-09T21:56:03.284000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey, just wanted to check in - were you able to get the OpenAPI client working?\n\nI just spent most of today digging in with my Windows machine, and I was able to get the example working, but it did take some wrestling",
        "metadata": {
            "message_id": 1285440876264554547,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T03:23:07.238000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "In particular, I had to do two things:",
        "metadata": {
            "message_id": 1285441030346248266,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T03:23:43.974000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "`npm install -g npm@latest` to bump npm to 10.8.3 (it turns out there's a pretty bad bug in 10.8.2)",
        "metadata": {
            "message_id": 1285441149208760321,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T03:24:12.313000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "and",
        "metadata": {
            "message_id": 1285441169580621885,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T03:24:17.170000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "`on_generate \"npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g java -o . --additional-properties invokerPackage=com.boundaryml.baml_client,modelPackage=com.boundaryml.baml_client.model,apiPackage=com.boundaryml.baml_client.api,java8=true && '/c/Program Files/JetBrains/IntelliJ IDEA Community Edition 2024.2.1/plugins/maven/lib/maven3/bin/mvn' clean install\"`",
        "metadata": {
            "message_id": 1285441197170757715,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T03:24:23.748000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I know that `mvn` command isn't exactly portable, but I haven't yet decided what the best way to handle that is, and also found openapi-generator-gradle-plugin and maven-plugin today, so wanted to check in with you before I spent more time on this.",
        "metadata": {
            "message_id": 1285441498430701568,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T03:25:35.574000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'll give that a try later today, thanks Sam!",
        "metadata": {
            "message_id": 1285538228354420748,
            "username": "dob.son",
            "created_at": "2024-09-17T09:49:57.785000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "feel free to tag me to hop on a call today too",
        "metadata": {
            "message_id": 1285646514185703444,
            "username": "joatmon.pockets",
            "created_at": "2024-09-17T17:00:15.139000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sorry, been away this past week with work, but I’m back tomorrow\n\nI’ll sit down and try and get it all working 🙂",
        "metadata": {
            "message_id": 1287717898508308581,
            "username": "dob.son",
            "created_at": "2024-09-23T10:11:11.646000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Intro to baml",
        "metadata": {
            "message_id": 1275131206073319560,
            "username": ".aaronv",
            "created_at": "2024-08-19T16:36:10.320000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "guys having a problem\nI am trying to achieve streaming but I think its generating whole response and sending everything at once\n```\nasync def stream_response(request: ChatCompletionRequest):\n            stream = async_b.stream.GenerateAnswer(request)\n            state = \"\"\n            print(stream)\n            async for chunk in stream:\n                delta = chunk[len(state):]\n                state = chunk\n                yield delta\n\n        return StreamingResponse(stream_response(request), media_type=\"text/event-stream\")\n```",
        "metadata": {
            "message_id": 1275356271251427400,
            "username": "underdog6143",
            "created_at": "2024-08-20T07:30:30.040000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1275356458225111071,
            "username": "underdog6143",
            "created_at": "2024-08-20T07:31:14.618000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "but with openai this works\n```\ndef get_response_openai(prompt):\n    try:\n        response = openai.chat.completions.create(\n            model=\"gpt-4o\",\n            temperature=0.5,\n            max_tokens=1000,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an expert creative marketer. Create a campaign for the brand the user enters.\",\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            stream=True,\n        )\n    except Exception as e:\n        print(\"Error in creating campaigns from openAI:\", str(e))\n        raise HTTPException(503, \"Error in creating campaigns from openAI\")\n    try:\n        for chunk in response:\n            print(chunk)\n            print(chunk.choices[0].delta.content)\n            print(\"****************\")\n            yield chunk.choices[0].delta.content + \"\\n\"\n    except Exception as e:\n        print(\"OpenAI Response (Streaming) Error: \" + str(e))\n        raise HTTPException(503, \"OpenAI Response (Streaming) Error\")\n```",
        "metadata": {
            "message_id": 1275356460955471934,
            "username": "underdog6143",
            "created_at": "2024-08-20T07:31:15.269000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Try to add a new line to each delta",
        "metadata": {
            "message_id": 1275357594931822633,
            "username": ".aaronv",
            "created_at": "2024-08-20T07:35:45.630000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Otherwise it wont stream if you dont add the new line",
        "metadata": {
            "message_id": 1275357635318648854,
            "username": ".aaronv",
            "created_at": "2024-08-20T07:35:55.259000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ya but it still feels like everything is coming at once;",
        "metadata": {
            "message_id": 1275358246357434370,
            "username": "underdog6143",
            "created_at": "2024-08-20T07:38:20.942000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "and it takes a while to start",
        "metadata": {
            "message_id": 1275358354373611542,
            "username": "underdog6143",
            "created_at": "2024-08-20T07:38:46.695000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Interesting, we ll take a look in around 8 hours (tomorrow)",
        "metadata": {
            "message_id": 1275358378549444710,
            "username": ".aaronv",
            "created_at": "2024-08-20T07:38:52.459000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any update <@201399017161097216>",
        "metadata": {
            "message_id": 1277215740507979937,
            "username": "underdog6143",
            "created_at": "2024-08-25T10:39:22.058000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Still same issue, unless I do timer await in backend, in frontend it seems like I got all data at once",
        "metadata": {
            "message_id": 1277215844413210624,
            "username": "underdog6143",
            "created_at": "2024-08-25T10:39:46.831000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> do you know what this issue is? Otherwise ill try and run the example later today",
        "metadata": {
            "message_id": 1277286771117391903,
            "username": ".aaronv",
            "created_at": "2024-08-25T15:21:37.075000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "are you using azure?",
        "metadata": {
            "message_id": 1277290699154391081,
            "username": "hellovai",
            "created_at": "2024-08-25T15:37:13.592000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i've noticed that some LLMs return large chunks at once.\n\nalso how are you recieving the data?\n\none quick way to determine if the issue is on the frontend or backend, is to directly curl the backend server you're running.\n\nIf you curl that and send it out you should see it streaming in.\n\nAdditionally you can change your return type to include a timestamp for that test, so you can see:\n\n```python\nasync def stream_response(request: ChatCompletionRequest):\n            stream = async_b.stream.GenerateAnswer(request)\n            state = \"\"\n            start = time.time()\n            async for chunk in stream:\n                delta = chunk[len(state):]\n                state = chunk\n                yield [time.time() - start, delta]\n\nreturn StreamingResponse(stream_response(request), media_type=\"text/event-stream\")\n```",
        "metadata": {
            "message_id": 1277291332590768211,
            "username": "hellovai",
            "created_at": "2024-08-25T15:39:44.615000+00:00",
            "edited_at": "2024-08-25T15:40:19.226000+00:00"
        }
    },
    {
        "text": "and one thing to check is your return statement for `StreamingResponse` is not inside the `stream_response` function, bur rather the top level post request.\n(i suspect its not, but your code snippet you copied in had it tabbed in)",
        "metadata": {
            "message_id": 1277291729267069019,
            "username": "hellovai",
            "created_at": "2024-08-25T15:41:19.190000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@510947001936904206>  I fixed the issue in our baml-examples python-fastapi-starter repo, with it not streaming, sorry it took weeks to get to this",
        "metadata": {
            "message_id": 1293245008065069059,
            "username": ".aaronv",
            "created_at": "2024-10-08T16:13:57.253000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey are there any plans for namespacing because I am in the need to provide a different field description for some of the fields of one of my classes. It would have been easier to just namespace it (maybe namespace it to the file) so I can have the same class name with different alias/descriptions",
        "metadata": {
            "message_id": 1275378881653637120,
            "username": ".alex4o",
            "created_at": "2024-08-20T09:00:20.780000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Namespaces",
        "metadata": {
            "message_id": 1275441977651105911,
            "username": "hellovai",
            "created_at": "2024-08-20T13:11:04.038000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "FYI folks: sonnet 3.5 is now up to `max_tokens=8192`\n\nhttps://x.com/alexalbert__/status/1825920737326281184",
        "metadata": {
            "message_id": 1275499411379716207,
            "username": "joatmon.pockets",
            "created_at": "2024-08-20T16:59:17.306000+00:00",
            "edited_at": "2024-08-20T16:59:34.274000+00:00"
        }
    },
    {
        "text": "how can I do guardrails; like I dont want it to spill sysstem guidelines?",
        "metadata": {
            "message_id": 1275520554866180107,
            "username": "underdog6143",
            "created_at": "2024-08-20T18:23:18.306000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Whoever made the book list from the analyze_books.baml file from python-fastapi-starter has fantastic taste LOL",
        "metadata": {
            "message_id": 1276182688654426133,
            "username": "jawnathonjones",
            "created_at": "2024-08-22T14:14:23.303000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "That would be your best friend <@201399017161097216>",
        "metadata": {
            "message_id": 1276183005416788052,
            "username": "hellovai",
            "created_at": "2024-08-22T14:15:38.825000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Lmao what a legend",
        "metadata": {
            "message_id": 1276183612391424120,
            "username": "jawnathonjones",
            "created_at": "2024-08-22T14:18:03.539000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@342454629021253632> I think youre afk in office hours chat, lmk if you want to chat",
        "metadata": {
            "message_id": 1276265376996397127,
            "username": ".aaronv",
            "created_at": "2024-08-22T19:42:57.740000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sorry, was on call with <@99252724855496704> and forgot to leave",
        "metadata": {
            "message_id": 1276269004276895744,
            "username": "jawnathonjones",
            "created_at": "2024-08-22T19:57:22.551000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "What are you guys using for observability, benchmarking prompts, user feedback",
        "metadata": {
            "message_id": 1276334288928899233,
            "username": "yungweedle",
            "created_at": "2024-08-23T00:16:47.625000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If your interested, we offer observability for BAML and non-baml functions! https://docs.boundaryml.com/docs/observability/tracing-tagging\n\nFor benchmarking, what i personally recommend, is building a test suite that you run regularly.\n\nFor user feedback, we'll be giving that soon (Q4 with BAML as well).",
        "metadata": {
            "message_id": 1276578174024814649,
            "username": "hellovai",
            "created_at": "2024-08-23T16:25:54.363000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey baml team, any timeline on when recursive definitions are supported? Or support for a generic json/object type? haven't been able to figure out how to represent json data of unknown depth without either of these features\n\ne.g.\n```class ObjectProperty {\n  name string\n  type DataType\n  required string[]\n  properties ObjectProperty[] | null\n}```",
        "metadata": {
            "message_id": 1276593619431587870,
            "username": "jfan8684",
            "created_at": "2024-08-23T17:27:16.835000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Recursive types",
        "metadata": {
            "message_id": 1276604360268189739,
            "username": "hellovai",
            "created_at": "2024-08-23T18:09:57.650000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "BAML and  AWS Secrets Manager.",
        "metadata": {
            "message_id": 1277663775813668885,
            "username": "noble_fawn_80154_44873",
            "created_at": "2024-08-26T16:19:41.999000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "heya <@99252724855496704>",
        "metadata": {
            "message_id": 1277666233289801920,
            "username": "aryan733",
            "created_at": "2024-08-26T16:29:27.907000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "how's it going! Welcome! Feel free to share any questions about BAML / prompting here.",
        "metadata": {
            "message_id": 1277684412732936192,
            "username": "hellovai",
            "created_at": "2024-08-26T17:41:42.224000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@1083848314694410321> did you figure out the issue with Azure in the playground? Try and see if the \"raw curl\" request works (see the checkbox on the playground)",
        "metadata": {
            "message_id": 1277744229564813413,
            "username": ".aaronv",
            "created_at": "2024-08-26T21:39:23.668000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@323873214336073730> https://github.com/BoundaryML/baml/blob/canary/docs/docs/snippets/clients/providers/anthropic.mdx",
        "metadata": {
            "message_id": 1278459447014068244,
            "username": "joatmon.pockets",
            "created_at": "2024-08-28T21:01:24.796000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Unspecified error code: 400\nRequest failed: {\"error\":{\"message\":\"'messages.0' : for 'role:system' the following must be satisfied[('messages.0.content' : value must be a string)]\",\"type\":\"invalid_request_error\"}}\n\nnew issue; was not getting ti before",
        "metadata": {
            "message_id": 1278661760152506469,
            "username": "underdog6143",
            "created_at": "2024-08-29T10:25:20.007000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "oh okay; extension 0.54.0 gives this",
        "metadata": {
            "message_id": 1278662097219354665,
            "username": "underdog6143",
            "created_at": "2024-08-29T10:26:40.370000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "reverted back, sovled",
        "metadata": {
            "message_id": 1278662112641552424,
            "username": "underdog6143",
            "created_at": "2024-08-29T10:26:44.047000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "possible bug",
        "metadata": {
            "message_id": 1278710060809453675,
            "username": "hellovai",
            "created_at": "2024-08-29T13:37:15.781000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey guys. Working on an application which is using multiple \"agents\" right now. I'm writing the agents on my own right now and is a bit hard to prompt engineer is the response. Looking forward to using BAML. <@201399017161097216> told me on twitter that support for Go is coming soon. Happy to test it out for you guys!",
        "metadata": {
            "message_id": 1278775526563123200,
            "username": "charizard_98",
            "created_at": "2024-08-29T17:57:24.033000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1278778609024241724,
            "username": "hellovai",
            "created_at": "2024-08-29T18:09:38.949000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hey <@410093421420871680> awesome 🙂 Excited to see your thoughts. We've got it landing hopefully today / tmrw, so starting next week it should be live.",
        "metadata": {
            "message_id": 1278778610614014045,
            "username": "hellovai",
            "created_at": "2024-08-29T18:09:39.328000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "very keen to see your experience with it",
        "metadata": {
            "message_id": 1278778650044665927,
            "username": "hellovai",
            "created_at": "2024-08-29T18:09:48.729000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Looking forward to using BAML! I didn't realize there was a channel for introductions.",
        "metadata": {
            "message_id": 1278779099225129082,
            "username": "charizard_98",
            "created_at": "2024-08-29T18:11:35.822000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "no worries, we're still figuring out discord 🥲",
        "metadata": {
            "message_id": 1278779189058867376,
            "username": "hellovai",
            "created_at": "2024-08-29T18:11:57.240000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> any update on the go package? Just want to make sure I didn’t miss the release over the weekend.",
        "metadata": {
            "message_id": 1280351525864214629,
            "username": "charizard_98",
            "created_at": "2024-09-03T02:19:51.547000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@711679663746842796> said I think the timeline was pushed back by a few days due to some final testing! We should announce in the next 1-2 days!",
        "metadata": {
            "message_id": 1280354485276905577,
            "username": "hellovai",
            "created_at": "2024-09-03T02:31:37.126000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just following up- we're getting closer and closer to the release! Expect it some time this week; we'll update this thread when we do.\n\nTo give you a sense of the current status, we're doing final passes now (e.g. reviewing generated code and making sure that we can preserve backwards compatibility for future releases).\n\nThings you can expect:\n\n- we've implemented OpenAPI codegen that can also run `openapi-generator-cli` for you automatically (for go users like yourself, `ogen` or `oapi-codegen` will also work)\n- we've implemented a server with hot reload that will re-generate everything when you edit BAML files\n- we've added not only quickstart docs, but also a `docker-compose` example ([preview](https://boundary-preview-1eabadb4-99c1-45a6-8131-2277b2373fe9.docs.buildwithfern.com/docs/get-started/deploying/openapi))\n- we've published example code for Go, Java, PHP, Rust at https://github.com/BoundaryML/baml-examples",
        "metadata": {
            "message_id": 1280605463091875963,
            "username": "joatmon.pockets",
            "created_at": "2024-09-03T19:08:54.900000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks <@711679663746842796> . I'll take a look at the examples and doc in the meanwhile",
        "metadata": {
            "message_id": 1281444465742974977,
            "username": "charizard_98",
            "created_at": "2024-09-06T02:42:48.720000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@410093421420871680> BAML 0.55.0 is out, which means you can now try out BAML from Go!\n\n- [announcement](https://discord.com/channels/1119368998161752075/1119375433666920530/1282818005084016721)\n- [quickstart docs](https://docs.boundaryml.com/docs/get-started/quickstart/openapi)\n- [baml-examples for golang-openapi](https://github.com/BoundaryML/baml-examples/tree/main/golang-openapi-starter)",
        "metadata": {
            "message_id": 1282820844011257947,
            "username": "joatmon.pockets",
            "created_at": "2024-09-09T21:52:02.871000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@711679663746842796> thanks for letting me know. I’ll take a look at it today!",
        "metadata": {
            "message_id": 1282821061523800196,
            "username": "charizard_98",
            "created_at": "2024-09-09T21:52:54.730000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@711679663746842796> I was trying to setup the folders. Here's what I did: \n\n1. ran `bunx @boundaryml/baml init   --client-type rest/openapi --openapi-client-type go`. This created the baml_src folder. \n2. Then I ran `bunx @boundaryml/baml dev --preview`. But it threw this error: \n\n```\nerror: /bin/sh: 1: java: not found\n\n      at /tmp/bunx-1000-@openapitools/openapi-generator-cli@latest/node_modules/@openapitools/openapi-generator-cli/main.js:2:27569\n      at exitHandler (node:child_process:69:23)\n      at emit (node:events:180:48)\n      at #maybeClose (node:child_process:805:14)\n      at #handleOnExit (node:child_process:587:16)\n\nBun v1.1.24 (Linux x64)\n[2024-09-10T00:33:28Z ERROR baml_runtime::cli::generate] Error generating clients: Client generation failed\n\n    Caused by:\n        0: Error while running generator defined at ./baml_src/generators.baml:3:0\n        1: on_generate command finished with exit code 1: \"bunx @openapitools/openapi-generator-cli generate -i openapi.yaml -g go -o . --additional-properties enumClassPrefix=true,isGoSubmodule=true,packageName=baml_client,withGoMod=false\"\n\n```\n\nDo I need to  install Java? Also, I changed `npx` to `bunx` in generators.baml in this line `on_generate \"bunx @openapitools/openapi-generator-cli generate -i openapi.yaml -g go -o . --additional-properties enumClassPrefix=true,isGoSubmodule=true,packageName=baml_client,withGoMod=false\"`",
        "metadata": {
            "message_id": 1282867277317800023,
            "username": "charizard_98",
            "created_at": "2024-09-10T00:56:33.434000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Agghhhh- I totally forgot about that part. You might be able to use `brew install openapi-generator` and then `openapi-generator-cli generate` instead",
        "metadata": {
            "message_id": 1282867917175783496,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T00:59:05.988000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Got it. I'll try that now.",
        "metadata": {
            "message_id": 1282868241609396235,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:00:23.339000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Any more issues? Did it work?",
        "metadata": {
            "message_id": 1282874630130110495,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T01:25:46.481000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm on Linux. So, I installed it with npm/bun https://openapi-generator.tech/docs/installation/",
        "metadata": {
            "message_id": 1282875148449480727,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:27:50.058000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Gotcha- I'm adding Java install instructions to the docs right now 😅",
        "metadata": {
            "message_id": 1282875306977394771,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T01:28:27.854000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "After installing I ran `bunx @boundaryml/baml dev --preview`\n\nstill got this error: \n\n```\nerror: /bin/sh: 1: java: not found\n\n      at /home/hari/.bun/install/global/node_modules/@openapitools/openapi-generator-cli/main.js:2:27569\n      at exitHandler (node:child_process:69:23)\n      at emit (node:events:180:48)\n      at #maybeClose (node:child_process:805:14)\n      at #handleOnExit (node:child_process:587:16)\n\nBun v1.1.24 (Linux x64)\n[2024-09-10T01:30:07Z ERROR baml_runtime::cli::generate] Error generating clients: Client generation failed\n\n    Caused by:\n        0: Error while running generator defined at ./baml_src/generators.baml:3:0\n        1: on_generate command finished with exit code 1: \"bunx @openapitools/openapi-generator-cli generate -i openapi.yaml -g go -o . --additional-properties enumClassPrefix=true,isGoSubmodule=true,packageName=baml_client,withGoMod=false\"\n\n```",
        "metadata": {
            "message_id": 1282876052997148774,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:31:25.719000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah, you need to install Java 😦",
        "metadata": {
            "message_id": 1282876122857734166,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T01:31:42.375000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "okay!",
        "metadata": {
            "message_id": 1282876155447218258,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:31:50.145000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If it's a debian-based distro, `apt install default-jdk` should do the trick",
        "metadata": {
            "message_id": 1282876206370394202,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T01:32:02.286000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Would Java been installed in macOS when I would have ran `brew install openapi-generator`?",
        "metadata": {
            "message_id": 1282876297340649526,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:32:23.975000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah, the brew package lists a dependency on java",
        "metadata": {
            "message_id": 1282876536516775957,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T01:33:20.999000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just curious",
        "metadata": {
            "message_id": 1282876551385317427,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:33:24.544000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Gotcha! Now it makes sense.",
        "metadata": {
            "message_id": 1282876598617636926,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:33:35.805000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "To run the BAML dev server, I just need to run `npx @boundaryml/baml dev --preview` everytime?",
        "metadata": {
            "message_id": 1282877857072414864,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:38:35.844000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yep!",
        "metadata": {
            "message_id": 1282877933941162087,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T01:38:54.171000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Small typo in the docs:",
        "metadata": {
            "message_id": 1282880273863803072,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:48:12.052000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We've tested **a** the below",
        "metadata": {
            "message_id": 1282880336992403528,
            "username": "charizard_98",
            "created_at": "2024-09-10T01:48:27.103000+00:00",
            "edited_at": "2024-09-10T01:48:48.275000+00:00"
        }
    },
    {
        "text": "The resume extractor example is working!",
        "metadata": {
            "message_id": 1282913939939921945,
            "username": "charizard_98",
            "created_at": "2024-09-10T04:01:58.670000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Glad to hear it! Any notes on what did / didn’t work, or other pain points / surprises?\n\nI’ve noted the java and bun issues you ran into earlier",
        "metadata": {
            "message_id": 1282914449753112719,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T04:04:00.219000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I think just those 2. I'll let you know if I run into anything.",
        "metadata": {
            "message_id": 1282931205741875232,
            "username": "charizard_98",
            "created_at": "2024-09-10T05:10:35.158000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@711679663746842796> quick question. I'm going through the docs. Is BAML like an alternative to Structured output in OpenAI?",
        "metadata": {
            "message_id": 1283063331841769592,
            "username": "charizard_98",
            "created_at": "2024-09-10T13:55:36.475000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "A little bit, yes!\n\nWe wrote about it a bit in these blog posts:\n\n- https://www.boundaryml.com/blog/sota-function-calling?q=0\n- https://www.boundaryml.com/blog/schema-aligned-parsing",
        "metadata": {
            "message_id": 1283063805303193611,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T13:57:29.357000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks! I'm trying to build a mental model on how to build using BAML.",
        "metadata": {
            "message_id": 1283090567026311251,
            "username": "charizard_98",
            "created_at": "2024-09-10T15:43:49.849000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@711679663746842796> I was the one who asked about Zed support. I also saw this issue: https://github.com/BoundaryML/baml/issues/889. How do I run the `baml-cli playground --port 3000`? Should I install baml-cli python package?",
        "metadata": {
            "message_id": 1283144390189125774,
            "username": "charizard_98",
            "created_at": "2024-09-10T19:17:42.291000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ah, sorry, to be clear this isn't something we yet offer 😅  but it's come up a few times in planning as something that we want to do",
        "metadata": {
            "message_id": 1283144624629612566,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T19:18:38.186000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ah! Figured so.",
        "metadata": {
            "message_id": 1283145653211631677,
            "username": "charizard_98",
            "created_at": "2024-09-10T19:22:43.419000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just got confused with the wording",
        "metadata": {
            "message_id": 1283145778214338601,
            "username": "charizard_98",
            "created_at": "2024-09-10T19:23:13.222000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Updated the comment on the other issue to be more clear 🙂",
        "metadata": {
            "message_id": 1283146117869207552,
            "username": "joatmon.pockets",
            "created_at": "2024-09-10T19:24:34.202000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ah this would definitely help until we can get zed support. We’ll see if we can get something  working in a couple weeks",
        "metadata": {
            "message_id": 1283146380097097820,
            "username": ".aaronv",
            "created_at": "2024-09-10T19:25:36.722000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```go\nfunc Classify(ctx context.Context, messages []Message) (string, error) {\n\n    var conversationHistory strings.Builder\n    for _, msg := range messages {\n        conversationHistory.WriteString(fmt.Sprintf(\"%s: %s\\n\", msg.Role, msg.Content))\n    }\n\n    cfg := baml.NewConfiguration()\n    b := baml.NewAPIClient(cfg).DefaultAPI\n\n    classifyUserPrompt := baml.GetClassifiedRequest{\n        UserPrompt: conversationHistory.String(),\n    }\n\n    resp, r, err := b.GetClassified(context.Background()).GetClassifiedRequest(classifyUserPrompt).Execute()\n    if err != nil {\n        fmt.Printf(\"Error when calling b.ExtractResume: %v\\n\", err)\n        fmt.Printf(\"Full HTTP response: %v\\n\", r)\n        return fmt.Sprintf(\"Error when calling b.ExtractResume: %v\\n\", err), err\n    }\n\n    return string(*resp), err\n\n}\n```\n\nMy first BAML powered Go func is working!!",
        "metadata": {
            "message_id": 1283220127826841620,
            "username": "charizard_98",
            "created_at": "2024-09-11T00:18:39.551000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Daaamn you are officially the first to call a custom Go function (that we know of :P).",
        "metadata": {
            "message_id": 1283231063031808072,
            "username": ".aaronv",
            "created_at": "2024-09-11T01:02:06.707000+00:00",
            "edited_at": "2024-09-11T01:02:23.364000+00:00"
        }
    },
    {
        "text": "I replaced two classifying calls. It's working seamlessly!",
        "metadata": {
            "message_id": 1283294264687857716,
            "username": "charizard_98",
            "created_at": "2024-09-11T05:13:15.156000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Niice",
        "metadata": {
            "message_id": 1283298149271601194,
            "username": ".aaronv",
            "created_at": "2024-09-11T05:28:41.313000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@410093421420871680> do you have any feedback on the Go client? Is the interface ok to use? Is the Developer Experience easy, or would you prefer a native Go client?",
        "metadata": {
            "message_id": 1289114158645903392,
            "username": ".aaronv",
            "created_at": "2024-09-27T06:39:26.004000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I think the developer experience is good! How would a native Go client be different? Just thought about this. I have around 9 .baml files now. I’ll have a lot more. Do you think the boiler plate code that we have now because of OpenAI be an issue later as I have more baml files in my project?",
        "metadata": {
            "message_id": 1289201731787296798,
            "username": "charizard_98",
            "created_at": "2024-09-27T12:27:25.068000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "For openapi? No since you only need to jnstantiate the baml openapi client once.\n\nLet us know if you run into more issues!",
        "metadata": {
            "message_id": 1289263910251270154,
            "username": ".aaronv",
            "created_at": "2024-09-27T16:34:29.569000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sure thing!",
        "metadata": {
            "message_id": 1289642159196082261,
            "username": "charizard_98",
            "created_at": "2024-09-28T17:37:31.145000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Welcome!",
        "metadata": {
            "message_id": 1278778609024241729,
            "username": "hellovai",
            "created_at": "2024-08-29T18:09:38.949000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Does BAML work well with prompt caching ?",
        "metadata": {
            "message_id": 1278987133306142741,
            "username": "abeeshake456",
            "created_at": "2024-08-30T07:58:15.012000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey I figured out there was a problem with my Azure OpenAI setup, right when I was testing my coworker reset the API creds 😐",
        "metadata": {
            "message_id": 1279055777390264482,
            "username": "nazimgirach",
            "created_at": "2024-08-30T12:31:01.037000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "BTW guys do you support Pydentic EmailStr in BAML?",
        "metadata": {
            "message_id": 1279055880054243339,
            "username": "nazimgirach",
            "created_at": "2024-08-30T12:31:25.514000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also HttpUrl",
        "metadata": {
            "message_id": 1279056312528797860,
            "username": "nazimgirach",
            "created_at": "2024-08-30T12:33:08.624000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "You can't make a list optional like 'SampleVariable string[]?'",
        "metadata": {
            "message_id": 1279065721393119275,
            "username": "nazimgirach",
            "created_at": "2024-08-30T13:10:31.872000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Or am I doing something wrong? Could only see in the doc that a string, int or bool can be made optimal with a '?'.",
        "metadata": {
            "message_id": 1279065959088259102,
            "username": "nazimgirach",
            "created_at": "2024-08-30T13:11:28.543000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "BTW guys do you support Pydentic",
        "metadata": {
            "message_id": 1279067998401069248,
            "username": "hellovai",
            "created_at": "2024-08-30T13:19:34.753000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "optional types",
        "metadata": {
            "message_id": 1279068291897360450,
            "username": "hellovai",
            "created_at": "2024-08-30T13:20:44.728000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "@everyone this is a bit late notice, but if you’d like to learn about Anthripics new caching feature, what it is, how to use it in BAML, and WHEN to use it it in your app, I’ll be demoing today! https://www.linkedin.com/events/aihackerspacelive-august307235280355518201857",
        "metadata": {
            "message_id": 1279093656527175792,
            "username": "hellovai",
            "created_at": "2024-08-30T15:01:32.127000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Looks like that link is expired?",
        "metadata": {
            "message_id": 1279117691592114269,
            "username": "seawatts",
            "created_at": "2024-08-30T16:37:02.533000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Vaibhav the hackerspace demo is super eye opening. Dope work yall",
        "metadata": {
            "message_id": 1279124957300920463,
            "username": "elshep",
            "created_at": "2024-08-30T17:05:54.813000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is there a recording?",
        "metadata": {
            "message_id": 1279134796693635245,
            "username": "rossir.paulo",
            "created_at": "2024-08-30T17:45:00.707000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Recording will be shared once I've got it! And thanks <@756577848629788672>",
        "metadata": {
            "message_id": 1279140764479127583,
            "username": "hellovai",
            "created_at": "2024-08-30T18:08:43.538000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1279615166282993716,
            "username": "qiying_85943",
            "created_at": "2024-09-01T01:33:49.742000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hello! Current BAML only supports verification through OPENAI_API_KEY, but I have two other ways to initialize my OpenAI client and use it, could you please provide some support?\n\n1. Through `openai.AzureOpenAI`, we need to pass three arguments `azure_endpoint`, `api_version`, and `api_key` to it.\nclient = openai.AzureOpenAI(\n    azure_endpoint = base_url,\n    api_version = api_version,\n    api_key = api_key,\n)\n2. Through `openai.OpenAI` with a new `base_url`, we need to pass three arguments `base_url`, `api_key`, and `model` in client.chat.completions.create:\nclient = openai.OpenAI(\n    base_url = base_url,\n    api_key = api_key\n)\nresponse = client.chat.completions.create(\n      model=customized_model,\n)\n\nThere is a large portion of usage through the AzureOpenAI or with a new base_url. I believe this would help a lot~ Thank you",
        "metadata": {
            "message_id": 1279618716811661456,
            "username": "qiying_85943",
            "created_at": "2024-09-01T01:47:56.254000+00:00",
            "edited_at": "2024-09-01T01:48:46.502000+00:00"
        }
    },
    {
        "text": "Quick question, any writing on BAML's roadmap moving forward?",
        "metadata": {
            "message_id": 1279693077170225164,
            "username": "gabriel_syme",
            "created_at": "2024-09-01T06:43:25.145000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Also, I would appreciate some examples towards deploying baml functions in production (if those are coming soon, awesome) 🙂",
        "metadata": {
            "message_id": 1279693163463704576,
            "username": "gabriel_syme",
            "created_at": "2024-09-01T06:43:45.719000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "BTW, excellent work! Was really easy to get going, made a complex EPD extractor in 1h or so",
        "metadata": {
            "message_id": 1279693267423985704,
            "username": "gabriel_syme",
            "created_at": "2024-09-01T06:44:10.505000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "roadmap",
        "metadata": {
            "message_id": 1279695742856069192,
            "username": "joatmon.pockets",
            "created_at": "2024-09-01T06:54:00.694000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "👋 hey <@201399017161097216> and <@99252724855496704>  (core team) , quick question about channel etiquette: I am working on a project that can do agents orchestration. We've actually played around with BAML a little bit when building demo apps for this project. Is there a good way to share about this project with your community? We also have a talk about it online next week. Thanks ! 🙏",
        "metadata": {
            "message_id": 1280993173522354267,
            "username": "maxdml",
            "created_at": "2024-09-04T20:49:32.270000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "(its open source, python and TS)",
        "metadata": {
            "message_id": 1280993246100852756,
            "username": "maxdml",
            "created_at": "2024-09-04T20:49:49.574000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "lets use <#1280993403168886896>   🙂 Feel free to post BAML-enabled stuff there",
        "metadata": {
            "message_id": 1280993638431723653,
            "username": ".aaronv",
            "created_at": "2024-09-04T20:51:23.113000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks !",
        "metadata": {
            "message_id": 1280993719318872146,
            "username": "maxdml",
            "created_at": "2024-09-04T20:51:42.398000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1280994449429631017,
            "username": "hellovai",
            "created_at": "2024-09-04T20:54:36.470000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1280994503368380531,
            "username": "hellovai",
            "created_at": "2024-09-04T20:54:49.330000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I learned about BAML thanks to one of our users who deployed a BAML app. They were doing a workflow with pupeteer + BAML + s3 + postgres + some other 3rd party service)",
        "metadata": {
            "message_id": 1280994920789971036,
            "username": "maxdml",
            "created_at": "2024-09-04T20:56:28.851000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "(and I went ahead to play around myself)",
        "metadata": {
            "message_id": 1280995029858521231,
            "username": "maxdml",
            "created_at": "2024-09-04T20:56:54.855000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1281356854517305425,
            "username": "hellovai",
            "created_at": "2024-09-05T20:54:40.576000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Do you have a recording of the session? I'd love to form a better mental model of baml... At the moment it feels a bit like dspy..?",
        "metadata": {
            "message_id": 1281542642068492369,
            "username": "_screwy",
            "created_at": "2024-09-06T09:12:55.780000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Can I use Gemini's context caching with BAML? https://ai.google.dev/api/caching\nHas anyone got this working?",
        "metadata": {
            "message_id": 1281560715769024523,
            "username": "brandburner",
            "created_at": "2024-09-06T10:24:44.886000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1283891901287890986,
            "username": "joatmon.pockets",
            "created_at": "2024-09-12T20:48:02.825000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sorry that none of us responded to this! We haven't implemented this, and I don't think we provide any hooks that a user could use to plumb this in.",
        "metadata": {
            "message_id": 1283891903221207091,
            "username": "joatmon.pockets",
            "created_at": "2024-09-12T20:48:03.286000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Are you using Gemini for most of your app right now?",
        "metadata": {
            "message_id": 1283891950222573690,
            "username": "joatmon.pockets",
            "created_at": "2024-09-12T20:48:14.492000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks <@711679663746842796> - right now I'm using Claude with context caching though its still super-expensive (each run of my script costs around $2-5 with Sonnet). Using Gemini in a similar way might(?) be better as it has a number of free requests per day and also a 2M token context window.",
        "metadata": {
            "message_id": 1284087987633651732,
            "username": "brandburner",
            "created_at": "2024-09-13T09:47:13.453000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oof, that's an expensive run! We'll definitely look into this",
        "metadata": {
            "message_id": 1284192400163606630,
            "username": "joatmon.pockets",
            "created_at": "2024-09-13T16:42:07.340000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "did the recording of this ever surface?",
        "metadata": {
            "message_id": 1281586926696005654,
            "username": "brandburner",
            "created_at": "2024-09-06T12:08:54.058000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Session recording",
        "metadata": {
            "message_id": 1282067040135544951,
            "username": ".aaronv",
            "created_at": "2024-09-07T19:56:42.022000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Recording",
        "metadata": {
            "message_id": 1282067430016815243,
            "username": ".aaronv",
            "created_at": "2024-09-07T19:58:14.977000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Guys baml studio doesn't show pricing anywhere, in docs it's marked with paid tag",
        "metadata": {
            "message_id": 1282382327997140992,
            "username": "underdog6143",
            "created_at": "2024-09-08T16:49:32.506000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Studio pricing",
        "metadata": {
            "message_id": 1282713928903692426,
            "username": "hellovai",
            "created_at": "2024-09-09T14:47:12.323000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Baml + Gemini context caching",
        "metadata": {
            "message_id": 1283891901929623587,
            "username": "joatmon.pockets",
            "created_at": "2024-09-12T20:48:02.978000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi guys! Anyone got any examples of using BAML for sophisticated intent classification? I'm creating a chatbot for use in the retail/eCommerce space and I think BAML will really help me out. I would like to use an LLM due to it's power of understanding conversation in comparison to a legacy NLU approach which doesn't take a whole conversation thus far into account. For example, I would like the chatbot to handle a multitude of tasks, all the way from product recommendations, to questions about past orders, to even purchasing items. However, if the user types something that isn't within the \"scope\" of what I want the chatbot to be able to handle, I don't want the LLM to entertain the request, which is what would happen if I was to use an LLM blindly.\n\nTo get around this my current idea is to use the LLM for intent classification, where it reads in an entire conversation and outputs whatever tasks the user wishes to complete based on the conversation as well as their latest message, as well as any piece of information relevant to these tasks. Anyone had any experiences working on something similar?",
        "metadata": {
            "message_id": 1284153344172232734,
            "username": "dob.son",
            "created_at": "2024-09-13T14:06:55.666000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1284155084170723441,
            "username": "hellovai",
            "created_at": "2024-09-13T14:13:50.514000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hi <@114076226133557253> ! There's a few examples in prompt fiddle you can check out for this, but overall you should use enums here.\n\nthe file check to check out is 03-classify-intent.baml on https://www.promptfiddle.com",
        "metadata": {
            "message_id": 1284155085735198792,
            "username": "hellovai",
            "created_at": "2024-09-13T14:13:50.887000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Let me know if that gets you started?",
        "metadata": {
            "message_id": 1284155112734064730,
            "username": "hellovai",
            "created_at": "2024-09-13T14:13:57.324000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Otherwise glad to show off more examples!",
        "metadata": {
            "message_id": 1284155143738228756,
            "username": "hellovai",
            "created_at": "2024-09-13T14:14:04.716000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks <@99252724855496704> , I'll take a look soon!",
        "metadata": {
            "message_id": 1284155776788594829,
            "username": "dob.son",
            "created_at": "2024-09-13T14:16:35.647000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "My end goal is to make it seem like the user is talking to something akin to GPT, Claude, etc, but little do they know they are heavily constrained by what they can actually do. For example, it would be possible for a user to want to do multiple things, like take this for instance:\n\n\"Hi, I want to send some money to John. Before I do this, how much do I have in my account?\"\n\nTo which the ideal output would be something like:\n\n`{\n  \"intents\": [\n    {\n      \"intent\": \"CheckAccountBalance\",\n      \"entities\": {}\n    },\n    {\n      \"intent\": \"TransferMoney\",\n      \"entities\": {\n        \"recipient\": \"John\"\n      }\n    }\n  ]\n}`\n\nWhere the order of the intents also matters. In the example above, we need to figure out how much money the user has before they send any to John. Do you have any examples similar to this?",
        "metadata": {
            "message_id": 1284158789611688008,
            "username": "dob.son",
            "created_at": "2024-09-13T14:28:33.960000+00:00",
            "edited_at": "2024-09-13T14:45:03.254000+00:00"
        }
    },
    {
        "text": "Worth noting as well the entire conversation thus far will also be passed into the prompt. I'm unsure yet if I also need to pass any \"active intents\" as well as their slots/enitities so the LLM has more context",
        "metadata": {
            "message_id": 1284159114234171434,
            "username": "dob.son",
            "created_at": "2024-09-13T14:29:51.356000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "All in all the LLM needs to know each intent possible as well as the required entities/slots for each of them so it knows the options it can pick between",
        "metadata": {
            "message_id": 1284159445617872896,
            "username": "dob.son",
            "created_at": "2024-09-13T14:31:10.364000+00:00",
            "edited_at": "2024-09-13T14:39:43.146000+00:00"
        }
    },
    {
        "text": "Another thing to consider is I need a way for the LLM to notify me when whatever the user has typed in their latest message is inappropriate/out of scope when it comes to the context of the chat bot. This isn't just anything not relevant to retail/eCommerce, but also includes things that might be in the realm of retail/eCommerce, but is something that the chatbot just doesn't handle",
        "metadata": {
            "message_id": 1284160883731337267,
            "username": "dob.son",
            "created_at": "2024-09-13T14:36:53.237000+00:00",
            "edited_at": "2024-09-13T14:40:35.400000+00:00"
        }
    },
    {
        "text": "As the scope of the chatbot grows, we might have to handle hundreds/thousands of intents, all with their own number and types of entities/slots that each one requires to be filled",
        "metadata": {
            "message_id": 1284163249541087292,
            "username": "dob.son",
            "created_at": "2024-09-13T14:46:17.290000+00:00",
            "edited_at": "2024-09-13T14:47:12.734000+00:00"
        }
    },
    {
        "text": "oh yea, all you need to do is just return an array!",
        "metadata": {
            "message_id": 1284167623080280094,
            "username": "hellovai",
            "created_at": "2024-09-13T15:03:40.023000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If you'd like perhaps this would be more efficient over a quick call in the office hours channel. Care to hop on?",
        "metadata": {
            "message_id": 1284167719565922315,
            "username": "hellovai",
            "created_at": "2024-09-13T15:04:03.027000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "sure thing!",
        "metadata": {
            "message_id": 1284167754395684974,
            "username": "dob.son",
            "created_at": "2024-09-13T15:04:11.331000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "thanks 🙂",
        "metadata": {
            "message_id": 1284167763157585982,
            "username": "dob.son",
            "created_at": "2024-09-13T15:04:13.420000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@711679663746842796>",
        "metadata": {
            "message_id": 1284178486277439571,
            "username": "hellovai",
            "created_at": "2024-09-13T15:46:50.011000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I know this is subjective, but maybe this is one of those instances where actually setting things up manually, rather than using generators and such, might actually be easier for the user, as they understand clearly what's going on and how things connect to one another",
        "metadata": {
            "message_id": 1284179724301635584,
            "username": "dob.son",
            "created_at": "2024-09-13T15:51:45.179000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I guess more control = better understanding approach",
        "metadata": {
            "message_id": 1284179811383906367,
            "username": "dob.son",
            "created_at": "2024-09-13T15:52:05.941000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "But I understand for some users, they don't really care about this and they just want to get up and running without wondering about whats going on behind the scenes",
        "metadata": {
            "message_id": 1284179922209996934,
            "username": "dob.son",
            "created_at": "2024-09-13T15:52:32.364000+00:00",
            "edited_at": "2024-09-13T15:52:55.270000+00:00"
        }
    },
    {
        "text": "In this code example taken from the documentation, you're importing some packages from dependencies:\n\n`import com.boundaryml.baml_client.ApiClient;\nimport com.boundaryml.baml_client.ApiException;\nimport com.boundaryml.baml_client.Configuration;\nimport com.boundaryml.baml_client.model.*;\nimport com.boundaryml.baml_client.api.DefaultApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    DefaultApi apiInstance = new DefaultApi(defaultClient);\n    ExtractResumeRequest extractResumeRequest = new ExtractResumeRequest(); // ExtractResumeRequest | \n    try {\n      Resume result = apiInstance.extractResume(extractResumeRequest);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling DefaultApi#extractResume\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}`\n\nDo you have a reference/documentation on these dependencies and the other Classes, Methods, Symbols etc available? What do I need to add to my build.gradle or pom.xml to access these?",
        "metadata": {
            "message_id": 1284181967146979493,
            "username": "dob.son",
            "created_at": "2024-09-13T16:00:39.915000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> I know this is subjective, but maybe this is one of those instances where actually setting things up manually, rather than using generators and such, might actually be easier for the user, as they understand clearly what's going on and how things connect to one another\nheard!",
        "metadata": {
            "message_id": 1284191390296838268,
            "username": "joatmon.pockets",
            "created_at": "2024-09-13T16:38:06.569000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> Do you have a reference/documentation on these dependencies and the other Classes, Methods, Symbols etc available? What do I need to add to my build.gradle or pom.xml to access these?\nUnfortunately, the OpenAPI documentation is pretty abysmal in this regard\n\nThe generated `README` will have the gradle/maven instructions, but what you need is:\n- add `mavenLocal()` to your repositories (unfortunately you have to `mvn install` the generated java client to make it accessible)\n- add `implementation(\"org.openapitools:openapi-java-client:0.1.0\")`\n\nhttps://github.com/BoundaryML/baml-examples/blob/main/java-gradle-openapi-starter/app/build.gradle.kts#L16 has an example",
        "metadata": {
            "message_id": 1284191939641479230,
            "username": "joatmon.pockets",
            "created_at": "2024-09-13T16:40:17.543000+00:00",
            "edited_at": "2024-09-13T16:41:12.571000+00:00"
        }
    },
    {
        "text": "<@114076226133557253> what timezone are you in, out of curiosity?",
        "metadata": {
            "message_id": 1284250661881512046,
            "username": "joatmon.pockets",
            "created_at": "2024-09-13T20:33:38.016000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’m from the UK, so BST atm",
        "metadata": {
            "message_id": 1284268808269205575,
            "username": "dob.son",
            "created_at": "2024-09-13T21:45:44.452000+00:00",
            "edited_at": "2024-09-14T15:22:59.271000+00:00"
        }
    },
    {
        "text": "Was wondering if you guys could assist me with something. So as mentioned above, my use case for BAML is for Intent Classification and Entity Extraction, however I'm unsure how to do this efficiently. \n\nIn comparison to a traditional NLU system where a single message will be inputted, and a single Intent and any present Entities will be returned, I would like an entire conversation between User and Agent to be part of the input, as well as supporting the ability for multiple Intents and their filled Entities to be outputted. For example, the user would be able to say something like:\n\n\"I want to order a large pepperoni pizza please. What kind of crusts to you guys have?\"\n\n....to which an output might be something like:\n\n`{\n  \"intents\": [\n    {\n      \"intent\": \"OrderPizza\",\n      \"entities\": {\n        \"size\": \"large\",\n        \"type\": \"pepperoni\",\n        \"category\": \"pizza\"\n      }\n    },\n    {\n      \"intent\": \"AskAboutCrustOptions\",\n      \"entities\": {}\n    }\n  ]\n}`\n\nThen I can act on behalf of the agent and respond with something like:\n\n\"We have classic, thin and stuffed crust options! Which one would you like to choose and we can continue ordering you pizza!\"\n\nThe LLM needs to know about each Intent it can choose from, as well as the required Entitities within each Intent that need filling, along with their types (bool, int, float, string). Any ideas how I could approach this?",
        "metadata": {
            "message_id": 1284565781136281725,
            "username": "dob.son",
            "created_at": "2024-09-14T17:25:48.301000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ah yes. I think for this you really have more of an extraction + classification problem.",
        "metadata": {
            "message_id": 1284566062293061735,
            "username": "hellovai",
            "created_at": "2024-09-14T17:26:55.334000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Want to hop on office hours and write some BAML code together?",
        "metadata": {
            "message_id": 1284566132417630260,
            "username": "hellovai",
            "created_at": "2024-09-14T17:27:12.053000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "tl'dr of how to do this is:\n\n```\nclass OrderPizza {\n  size string?\n  type string?\n  category string?\n}\n\nenum IntentOrderPizza {\n   OrderPizza\n}\n\nclass Order {\n  type OrderPizzaIntent\n  entities OrderPizza\n}\n\nenum IntentCrustOptions {\n  AskAboutCrustOptions\n}\n\nclass CrustOptions {\n  type AskAboutCrustOptions\n}\n\nfunction Foo(..) -> (CrustOptions | Order)[] {\n   ...\n}\n```",
        "metadata": {
            "message_id": 1284566859336781895,
            "username": "hellovai",
            "created_at": "2024-09-14T17:30:05.364000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "note that we are working on literals so this code would be much simpler. Soon you'll be able to do this:\n\n```\nclass OrderPizza {\n  type \"OrderPizza\"\n  size string?\n  type string?\n  category string?\n}\n\nclass CrustOptions {\n  type \"AskAboutCrustOptions\"\n}\n\nfunction Foo(..) -> (CrustOptions | OrderPizza)[] {\n   ...\n}\n```",
        "metadata": {
            "message_id": 1284567117450182748,
            "username": "hellovai",
            "created_at": "2024-09-14T17:31:06.903000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’m out right now looking at some paint for my new house 😅 So I can’t join at the moment haha, apologies",
        "metadata": {
            "message_id": 1284567118712672360,
            "username": "dob.son",
            "created_at": "2024-09-14T17:31:07.204000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ahh I see",
        "metadata": {
            "message_id": 1284567176304656396,
            "username": "dob.son",
            "created_at": "2024-09-14T17:31:20.935000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "No worries, added some examples, main restrictions today is lack of literals which makes it a bit more verbose, but that will be lifted soon",
        "metadata": {
            "message_id": 1284567260895379571,
            "username": "hellovai",
            "created_at": "2024-09-14T17:31:41.103000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah I saw that you guys are composition over inheritance, and a lot of my coding experience comes from Java/Kotlin where inheritance is encouraged, so I was just wondering how you’d approach this problem of outputting various intents, where each intent will have different names, as well as different entities and entity types",
        "metadata": {
            "message_id": 1284567550432252009,
            "username": "dob.son",
            "created_at": "2024-09-14T17:32:50.134000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "we'll likely support anonymous types which will make creating types much easier",
        "metadata": {
            "message_id": 1284567882147168458,
            "username": "hellovai",
            "created_at": "2024-09-14T17:34:09.221000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Main take away I really took is that we need more video tutorials and docs",
        "metadata": {
            "message_id": 1284568566502260839,
            "username": "hellovai",
            "created_at": "2024-09-14T17:36:52.384000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Definitely this was a really useful discussion although still curious what the `(CrustOptions | OrderPizza) []` does 😆",
        "metadata": {
            "message_id": 1286212693300350987,
            "username": "gabriel_syme",
            "created_at": "2024-09-19T06:30:02.759000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I too am really curious about the actual application of this XD Maybe <@114076226133557253> works for Dominos Pizza 🤯",
        "metadata": {
            "message_id": 1286213013992640513,
            "username": "hellovai",
            "created_at": "2024-09-19T06:31:19.218000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Just an example intent haha",
        "metadata": {
            "message_id": 1286213166220574765,
            "username": "dob.son",
            "created_at": "2024-09-19T06:31:55.512000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’m working on a retail/ecommerce chatbot which would be my actual use case",
        "metadata": {
            "message_id": 1286213327697215592,
            "username": "dob.son",
            "created_at": "2024-09-19T06:32:34.011000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://tenor.com/qy4a.gif",
        "metadata": {
            "message_id": 1286213353383002164,
            "username": "hellovai",
            "created_at": "2024-09-19T06:32:40.135000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I want it to feel to the user like they are directly talking to GPT/Claude etc, but actually they are heavily constrained by what the chatbot actually can/can’t do.\n\nI tried experimenting with the LLM “agents” for this with RAG, but it introduced more problems than actually solving my issue",
        "metadata": {
            "message_id": 1286213928078151741,
            "username": "dob.son",
            "created_at": "2024-09-19T06:34:57.153000+00:00",
            "edited_at": "2024-09-19T06:35:47.152000+00:00"
        }
    },
    {
        "text": "I guess using BAML kind of like guardrails for the LLM",
        "metadata": {
            "message_id": 1286214028628070423,
            "username": "dob.son",
            "created_at": "2024-09-19T06:35:21.126000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yea RAG will basically fail in many many scenarios, i'm talking with someone that actually did a great agentic workflow with BAML tmrw. I'll be able to share some sample code after that!",
        "metadata": {
            "message_id": 1286214452173082686,
            "username": "hellovai",
            "created_at": "2024-09-19T06:37:02.107000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "he got dynamic UIs working",
        "metadata": {
            "message_id": 1286214484607635488,
            "username": "hellovai",
            "created_at": "2024-09-19T06:37:09.840000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oo yeah please!",
        "metadata": {
            "message_id": 1286214537372237825,
            "username": "dob.son",
            "created_at": "2024-09-19T06:37:22.420000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "See <#1285388973761626122> for context. But sample code will be out tmrw!",
        "metadata": {
            "message_id": 1286214653851996170,
            "username": "hellovai",
            "created_at": "2024-09-19T06:37:50.191000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah I think there’s definitely something to the agentic workflow part, it was just RAG that seemed like it was putting a band-aid over my actually problem haha",
        "metadata": {
            "message_id": 1286214789885988864,
            "username": "dob.son",
            "created_at": "2024-09-19T06:38:22.624000+00:00",
            "edited_at": "2024-09-19T06:40:56.124000+00:00"
        }
    },
    {
        "text": "Didn’t like it at all",
        "metadata": {
            "message_id": 1286214801663721475,
            "username": "dob.son",
            "created_at": "2024-09-19T06:38:25.432000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yea rag alone w/o strucutred outputs leads mostly to hallucinations and drift of the conversations quite often",
        "metadata": {
            "message_id": 1286215603912314902,
            "username": "hellovai",
            "created_at": "2024-09-19T06:41:36.703000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Never heard of langgraph or ReAct agents, I’ll look into those prior to when you share the code",
        "metadata": {
            "message_id": 1286215799157166121,
            "username": "dob.son",
            "created_at": "2024-09-19T06:42:23.253000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ty btw <@99252724855496704> 🙂",
        "metadata": {
            "message_id": 1286215866014498837,
            "username": "dob.son",
            "created_at": "2024-09-19T06:42:39.193000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "in this case thank <@740363257814057004> who did the work 😉",
        "metadata": {
            "message_id": 1286215941780275293,
            "username": "hellovai",
            "created_at": "2024-09-19T06:42:57.257000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Joining after The AI Conference -- thanks for bringing BAML to the world <@99252724855496704>! Looking forward to getting your team in front of mine at some point in the near future.\n\nAt the conference, you had a fantastic demo showing an image data extraction use case. Is there documentation on how you and your team recommend leveraging BAML in that specific context?",
        "metadata": {
            "message_id": 1284196350866751539,
            "username": "miles.erickson.slalom",
            "created_at": "2024-09-13T16:57:49.261000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1284203648725024768,
            "username": "hellovai",
            "created_at": "2024-09-13T17:26:49.206000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hi Miles! I was just doing all my emails this morning 🙂 You should see one shortly (1-2 hours) in your inbox.\n\nWe are working on setting up docs for the image extraction use cases actually. We ended up whipping up that demo just a day or two before the conference. We didn't expect that sort of response tbh 🥲",
        "metadata": {
            "message_id": 1284203650323054654,
            "username": "hellovai",
            "created_at": "2024-09-13T17:26:49.587000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "What language does your team use <@702668260713300100>  ? Python / TS or something else?",
        "metadata": {
            "message_id": 1284254886330105867,
            "username": ".aaronv",
            "created_at": "2024-09-13T20:50:25.203000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "My team uses Python <@201399017161097216>",
        "metadata": {
            "message_id": 1284259643992047770,
            "username": "miles.erickson.slalom",
            "created_at": "2024-09-13T21:09:19.518000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Welcome!",
        "metadata": {
            "message_id": 1284203649182072928,
            "username": "hellovai",
            "created_at": "2024-09-13T17:26:49.315000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We see quite a lot of \"safety\" errors from Gemini 1.5 flash. But seems BAML doesn't have a way to tell it to allow this? Context: https://stackoverflow.com/questions/77947637/how-to-disable-safety-settings-in-gemini-vision-pro-model-using-api",
        "metadata": {
            "message_id": 1284532274377003101,
            "username": "samos123",
            "created_at": "2024-09-14T15:12:39.667000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1284532580724510771,
            "username": "hellovai",
            "created_at": "2024-09-14T15:13:52.706000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "It should be similar to this:\n\nhttps://docs.boundaryml.com/docs/snippets/clients/providers/vertex#forwarded-options",
        "metadata": {
            "message_id": 1284532581789995219,
            "username": "hellovai",
            "created_at": "2024-09-14T15:13:52.960000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ah so it will just forward anything by default?",
        "metadata": {
            "message_id": 1284532662761033768,
            "username": "samos123",
            "created_at": "2024-09-14T15:14:12.265000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yep!",
        "metadata": {
            "message_id": 1284532697737334795,
            "username": "hellovai",
            "created_at": "2024-09-14T15:14:20.604000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "except for non-forwarded options",
        "metadata": {
            "message_id": 1284532725608349787,
            "username": "hellovai",
            "created_at": "2024-09-14T15:14:27.249000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "thanks a bunch! will give that a try",
        "metadata": {
            "message_id": 1284532740099932334,
            "username": "samos123",
            "created_at": "2024-09-14T15:14:30.704000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i wonder if we should rename this to make it more clear",
        "metadata": {
            "message_id": 1284532767362908272,
            "username": "hellovai",
            "created_at": "2024-09-14T15:14:37.204000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I think having more examples would be helpful like you do on vertex ai page",
        "metadata": {
            "message_id": 1284532874388705310,
            "username": "samos123",
            "created_at": "2024-09-14T15:15:02.721000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I was looking at the google-ai provider docs in BAML",
        "metadata": {
            "message_id": 1284532935566954576,
            "username": "samos123",
            "created_at": "2024-09-14T15:15:17.307000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ah got it! I'll copy and paste them over!",
        "metadata": {
            "message_id": 1284532989568614602,
            "username": "hellovai",
            "created_at": "2024-09-14T15:15:30.182000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I would also rename this: \"For all other options, see the official Vertex AI documentation.\"\nto\n\"All other options documented in the Vertex AI documentation are forwarded automatically\"",
        "metadata": {
            "message_id": 1284533281798230058,
            "username": "samos123",
            "created_at": "2024-09-14T15:16:39.855000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "make it more explicit what BAML's behavior is",
        "metadata": {
            "message_id": 1284533428464914472,
            "username": "samos123",
            "created_at": "2024-09-14T15:17:14.823000+00:00",
            "edited_at": "2024-09-14T15:17:21.870000+00:00"
        }
    },
    {
        "text": "there seems to be no \"Method\" field?",
        "metadata": {
            "message_id": 1284534598558027857,
            "username": "samos123",
            "created_at": "2024-09-14T15:21:53.795000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://ai.google.dev/api/generate-content#v1beta.SafetySetting",
        "metadata": {
            "message_id": 1284534646608236565,
            "username": "samos123",
            "created_at": "2024-09-14T15:22:05.251000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "wdym?",
        "metadata": {
            "message_id": 1284534650953273455,
            "username": "hellovai",
            "created_at": "2024-09-14T15:22:06.287000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "is that a BAML specific thing?",
        "metadata": {
            "message_id": 1284534665570549761,
            "username": "samos123",
            "created_at": "2024-09-14T15:22:09.772000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "this is the baml example:\n```\n    safetySettings {\n      category HARM_CATEGORY_HATE_SPEECH\n      threshold BLOCK_LOW_AND_ABOVE\n      method SEVERITY\n    }\n```",
        "metadata": {
            "message_id": 1284534722625802392,
            "username": "samos123",
            "created_at": "2024-09-14T15:22:23.375000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "maybe that's a vertex ai specific thing?",
        "metadata": {
            "message_id": 1284534776237133847,
            "username": "samos123",
            "created_at": "2024-09-14T15:22:36.157000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yea that may be! We just turn it into JSON directly. if you take a look at the \"Raw CURL\" button, that should explain the request we are constructing",
        "metadata": {
            "message_id": 1284534896232104147,
            "username": "hellovai",
            "created_at": "2024-09-14T15:23:04.766000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "nope, vertex ai shows this:\n```\n  \"safetySettings\": [\n    {\n      \"category\": enum (HarmCategory),\n      \"threshold\": enum (HarmBlockThreshold)\n    }\n  ],\n```",
        "metadata": {
            "message_id": 1284534899017257050,
            "username": "samos123",
            "created_at": "2024-09-14T15:23:05.430000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "seems method should be removed from that example for both vertex ai and google-ai provider",
        "metadata": {
            "message_id": 1284535016679805029,
            "username": "samos123",
            "created_at": "2024-09-14T15:23:33.483000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "oh odd... I think they updated their docs! Yes. I'll do so!",
        "metadata": {
            "message_id": 1284535189015363594,
            "username": "hellovai",
            "created_at": "2024-09-14T15:24:14.571000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "it would make my life easy if there was a copy pasteable example to set BLOCK_NONE for all categories",
        "metadata": {
            "message_id": 1284535428124508263,
            "username": "samos123",
            "created_at": "2024-09-14T15:25:11.579000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "not sure if that's a common use case or not",
        "metadata": {
            "message_id": 1284535478434922651,
            "username": "samos123",
            "created_at": "2024-09-14T15:25:23.574000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "got it! yea i think thats super valid. We should likely put a bunch of examples for client paramteres to explain what is possible in general like vertex-ai",
        "metadata": {
            "message_id": 1284535633544740988,
            "username": "hellovai",
            "created_at": "2024-09-14T15:26:00.555000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> / <@201399017161097216> do either of you hvae time to pop into office hours and talk through some architecture considerations for our product?",
        "metadata": {
            "message_id": 1285677855849709721,
            "username": "dean.coffee",
            "created_at": "2024-09-17T19:04:47.574000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1285681601208127529,
            "username": "hellovai",
            "created_at": "2024-09-17T19:19:40.537000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'll be on in 5!",
        "metadata": {
            "message_id": 1285681602843906150,
            "username": "hellovai",
            "created_at": "2024-09-17T19:19:40.927000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "we have another at 12:30 -- would same time 12 on Friday work?",
        "metadata": {
            "message_id": 1285682219725361216,
            "username": "dean.coffee",
            "created_at": "2024-09-17T19:22:08.003000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "like next friday?",
        "metadata": {
            "message_id": 1285682670629552158,
            "username": "hellovai",
            "created_at": "2024-09-17T19:23:55.507000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "oh you mean this friday",
        "metadata": {
            "message_id": 1285682686467375155,
            "username": "hellovai",
            "created_at": "2024-09-17T19:23:59.283000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "we can do later today! or https://calendly.com/boundary-founders/30min this works best",
        "metadata": {
            "message_id": 1285682740368244850,
            "username": "hellovai",
            "created_at": "2024-09-17T19:24:12.134000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@919047616392757299> did you still wanna follow up",
        "metadata": {
            "message_id": 1288187423209033740,
            "username": "hellovai",
            "created_at": "2024-09-24T17:16:55.058000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Financial data",
        "metadata": {
            "message_id": 1285680037114282015,
            "username": "hellovai",
            "created_at": "2024-09-17T19:13:27.628000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@459190677054554112> Here's an example of how I would build a pipeline in python for finding financial data:\n\n```python\nfrom typing import List\nfrom baml_client.async_client import b\nfrom baml_client.types import FinancialData\n\n\nasync def handle_pages(page: str) -> FinancialData | None:\n    number_of_digits = 0\n    for char in page:\n        if char.isdigit():\n            number_of_digits += 1\n    if number_of_digits < 100:\n        return None\n    \n    if await b.HasFinancialData(page):\n        return await b.ExtractFinancialData(page)\n    return None\n\nimport asyncio\n\n\nasync def parse_pages(pages: List[str]) -> List[FinancialData]:\n    financial_data = await asyncio.gather(\n        *[handle_pages(page) for page in pages]\n    )\n\n    return [data for data in financial_data if data is not None]\n```",
        "metadata": {
            "message_id": 1285680059998535722,
            "username": "hellovai",
            "created_at": "2024-09-17T19:13:33.084000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```\nfunction HasFinancialData(text: string) -> bool {\n  client Llama\n  prompt #\"\n    {{ ctx.output_format }}\n\n    {{ _.role('user') }}\n    {{ text }}\n  \"#\n}\n\nclass FinancialData {\n  amount int\n  currency string\n  date string\n}\n\nfunction ExtractFinancialData(text: string) -> FinancialData {\n  client Llama\n  prompt #\"\n    {{ ctx.output_format }}\n\n    {{ _.role('user') }}\n    {{ text }}\n  \"#\n}\n```",
        "metadata": {
            "message_id": 1285680148997476443,
            "username": "hellovai",
            "created_at": "2024-09-17T19:13:54.303000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "chat about design",
        "metadata": {
            "message_id": 1285681601208127534,
            "username": "hellovai",
            "created_at": "2024-09-17T19:19:40.537000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "👋",
        "metadata": {
            "message_id": 1288186506778902588,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:13:16.564000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "👋🏾",
        "metadata": {
            "message_id": 1288187373720436779,
            "username": "hellovai",
            "created_at": "2024-09-24T17:16:43.259000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "IDK how to read the baml errors",
        "metadata": {
            "message_id": 1288190981455347796,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:31:03.410000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1288191279687139471,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:32:14.514000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "you're using it with typescript right?",
        "metadata": {
            "message_id": 1288191282166104084,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:32:15.105000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yep",
        "metadata": {
            "message_id": 1288191305159147591,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:32:20.587000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```baml\nclass Address {\n  locality string\n  region string\n  postal string\n  street string\n}\n\nclass Business {\n  address Address\n  name string\n  openingHours string[] @description(\"Format: 'Day(s) OpenTime-CloseTime', e.g., 'Mo-Fr 09:00-17:00'\")\n  phone string\n  fax string\n  email string\n  website string\n}\n\n\nfunction ValidateBusinessInfo(raw_text: string) -> Business {\n  client Llama3\n  prompt #\"\n    Validate the following business information:\n    {{ raw_text }}\n\n    Ensure the following:\n    1. The 'region' in the address is exactly 2 characters long.\n    2. The 'postal' code in the address is exactly 5 characters long.\n    3. The 'email' is a valid email address.\n    4. The 'website' is a valid URL.\n    5. Each 'openingHour' follows the format: Day(s) OpenTime-CloseTime\n       Where Day is Mo, Tu, We, Th, Fr, Sa, or Su (can use ranges like Mo-Fr),\n       and Time is in HH:MM format (24-hour).\n       Example: \"Mo-Fr 09:00-17:00\"\n    6. For each 'openingHour', ensure the opening time is before the closing time.\n\n    Return the validated Business object, correcting any issues found.\n    If any field is invalid and cannot be corrected, omit it from the result.\n\n    Output JSON format (only include these fields, and no others):\n  \"#\n}\n```\n```baml\nclient<llm> Llama3 {\n  provider \"openai\"\n  options {\n    api_key \"whatever\" \n    base_url \"http://127.0.0.1:1337/v1\"\n    model \"llama3.1-8b-instruct\"\n    temperature 0.3\n  }\n}\n```",
        "metadata": {
            "message_id": 1288191431701303448,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:32:50.757000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "it looks like we're trying to serialize a json object that has a circular reference, let me see if I can reproduce this",
        "metadata": {
            "message_id": 1288191724765581416,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:34:00.629000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Weird because:\na.) here's the data:\n```json\n{\n            \"business_name\": \"1st Dental Care\",\n            \"first_name\": \"Vinh\",\n            \"last_name\": \"Bui\",\n            \"address_1\": \"1150 S. Hwy 92  Suite A\",\n            \"address_2\": \"\",\n            \"address_3\": \"\",\n            \"city\": \"Sierra Vista\",\n            \"state_province\": \"AZ\",\n            \"zip_postal\": \"85635\",\n            \"phone\": \"(520) 226-8685\",\n            \"secondary_phone\": \"(520) 459-5166\",\n            \"email\": \"firstdentalcare@yahoo.com\",\n            \"url\": \"https://www.1stdentalcareaz.net/\",\n            \"co_catalog_id\": \"\",\n            \"tcs\": \"\",\n            \"member_type\": \"\",\n            \"map_url\": \"https://mms.skyislandsrp.com/members/googlemaps/google_maps_ind.php?orgcode=SVAC&mid=938922157\",\n            \"categories\": {\n                \"[NO-SUPERCAT]\": [\n                    \"Health &amp; Wellness\"\n                ]\n            },\n            \"lat\": \"31.544523\",\n            \"lng\": \"-110.258325\",\n            \"geo_acc\": \"0\",\n            \"photo_no\": \"\",\n            \"photo_path\": \"mms.sierravistaareachamber.org/sierravista/photos/MP7683452101190659P.PNG\",\n            \"photo_w\": \"150\",\n            \"photo_h\": \"150\",\n            \"about_us\": \"\",\n            \"keywords\": \"\",\n            \"url_store\": \"\"\n        },\n```",
        "metadata": {
            "message_id": 1288192501466796104,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:37:05.809000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "you need to add {{ ctx.output_format }} to your prompt to serialize the format instruction as well",
        "metadata": {
            "message_id": 1288192563488100403,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:37:20.596000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Done still same problem",
        "metadata": {
            "message_id": 1288192824252301404,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:38:22.767000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ok one sec",
        "metadata": {
            "message_id": 1288192885178630214,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:38:37.293000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "can you copy the `curl` request that's in the playground and run it in your terminal?",
        "metadata": {
            "message_id": 1288193116989292665,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:39:32.561000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1288193226418946149,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:39:58.651000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "and also copy the whole curl command here. Do you use ollama to host llama 3.1?",
        "metadata": {
            "message_id": 1288193327971438602,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:40:22.863000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Using Jan which is OpenAI compatable",
        "metadata": {
            "message_id": 1288193807086518317,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:42:17.093000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```\ncurl -X POST 'http://127.0.0.1:1337/v1/chat/completions' -H \"authorization: Bearer whatever\" -H \"content-type: application/json\" -d \"{\n  \\\"model\\\": \\\"llama3.1-8b-instruct\\\",\n  \\\"temperature\\\": 0.3,\n  \\\"messages\\\": [\n    {\n      \\\"role\\\": \\\"system\\\",\n      \\\"content\\\": [\n        {\n          \\\"type\\\": \\\"text\\\",\n          \\\"text\\\": \\\"Validate the following business information:\\nhello world\\n\\nEnsure the following:\\n1. The 'region' in the address is exactly 2 characters long.\\n2. The 'postal' code in the address is exactly 5 characters long.\\n3. The 'email' is a valid email address.\\n4. The 'website' is a valid URL.\\n5. Each 'openingHour' follows the format: Day(s) OpenTime-CloseTime\\n   Where Day is Mo, Tu, We, Th, Fr, Sa, or Su (can use ranges like Mo-Fr),\\n   and Time is in HH:MM format (24-hour).\\n   Example: \\\\\\\"Mo-Fr 09:00-17:00\\\\\\\"\\n6. For each 'openingHour', ensure the opening time is before the closing time.\\n\\nReturn the validated Business object, correcting any issues found.\\nIf any field is invalid and cannot be corrected, omit it from the result.\\n\\nOutput JSON format (only include these fields, and no others):\\nAnswer in JSON using this schema:\\n{\\n  address: {\\n    locality: string,\\n    region: string,\\n    postal: string,\\n    street: string,\\n  },\\n  name: string,\\n  // Format: 'Day(s) OpenTime-CloseTime', e.g., 'Mo-Fr 09:00-17:00'\\n  openingHours: string[],\\n  phone: string,\\n  fax: string,\\n  email: string,\\n  website: string,\\n}\\\"\n        }\n      ]\n    }\n  ],\n  \\\"stream\\\": true,\n  \\\"stream_options\\\": {\n    \\\"include_usage\\\": true\n  }\n}\"\n```",
        "metadata": {
            "message_id": 1288194189560909885,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:43:48.282000+00:00",
            "edited_at": "2024-09-24T17:44:09.943000+00:00"
        }
    },
    {
        "text": "this ran correctly in your terminal, right?",
        "metadata": {
            "message_id": 1288194328233119765,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:44:21.344000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> {\"statusCode\":500,\"error\":\"Internal Server Error\",\"message\":\"Converting circular structure to JSON\\n    --> starting at object with constructor 'Socket'\\n    |     property 'parser' -> object with constructor 'HTTPParser'\\n    --- property 'socket' closes the circle\"}%              indifferentghost@Thomass-MacBook-Pro busdir %",
        "metadata": {
            "message_id": 1288194431736090635,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:44:46.021000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ok so this seems like an issue with Jan itself",
        "metadata": {
            "message_id": 1288194484278001665,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:44:58.548000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "since this curl request simulates a simple http call to the LLM provider",
        "metadata": {
            "message_id": 1288194542188625982,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:45:12.355000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ah yes.",
        "metadata": {
            "message_id": 1288194637525159946,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:45:35.085000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "see if you can give this http request to the Jan devs, they should be able to repro",
        "metadata": {
            "message_id": 1288194665895432355,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:45:41.849000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "for postarity:\n> 2024-09-24T17:44:35.373Z [SERVER]::{\"reqId\":\"req-12g\",\"res\":{\"statusCode\":500},\"req\":{},\"msg\":\"request completed\",\"responseTime\":17.762374997138977}\n> 2024-09-24T17:46:29.545Z [SERVER]::{\"reqId\":\"req-12h\",\"res\":{},\"req\":{\"method\":\"POST\",\"url\":\"/v1/chat/completions\",\"hostname\":\"127.0.0.1:1337\"},\"msg\":\"incoming request\"}\n> 2024-09-24T17:46:29.563Z [CORTEX]::Debug: 20240924 17:46:29.563359 UTC 397235 INFO  Request 2481, model llama3.1-8b-instruct: Generating response for inference request - llama_engine.cc:469\n> 20240924 17:46:29.563377 UTC 397235 INFO  Request 2481: Stop words:null\n>  - llama_engine.cc:486\n> 20240924 17:46:29.563493 UTC 397235 ERROR Unhandled exception in /inferences/server/chat_completion, what(): Type is not convertible to string - HttpAppFrameworkImpl.cc:124\n> \n> 2024-09-24T17:46:29.564Z [SERVER]::{\"reqId\":\"req-12h\",\"res\":{\"statusCode\":500},\"req\":{},\"msg\":\"request completed\",\"responseTime\":18.63374999910593}",
        "metadata": {
            "message_id": 1288195044548935691,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:47:12.127000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yep basically there's something off in llama.cpp that serializes a json with a ciruclar ref",
        "metadata": {
            "message_id": 1288195837695889500,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:50:21.228000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oh wait I'm fairly sure this is because Jan doesn't accept JSON refs\n\n`-H \"content-type: application/json\"`",
        "metadata": {
            "message_id": 1288195878556930058,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:50:30.970000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I was using langchain to do structured data after the fact.",
        "metadata": {
            "message_id": 1288196018713788459,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:51:04.386000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "But it's messy and I was looking for alternatives.",
        "metadata": {
            "message_id": 1288196045049958473,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:51:10.665000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i think it's because this llama model is the `instruct` model, which likely doesn't accept this `chat openai` format",
        "metadata": {
            "message_id": 1288196179296911413,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:51:42.672000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ahh...",
        "metadata": {
            "message_id": 1288196332158320773,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:52:19.117000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Would there be a way to customize that in BAML",
        "metadata": {
            "message_id": 1288196439318597663,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:52:44.666000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "or should i be on the lookout for a small model that can do similar",
        "metadata": {
            "message_id": 1288196496935616566,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:52:58.403000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "(or could you recommend one)",
        "metadata": {
            "message_id": 1288196526342148217,
            "username": "indifferentghost",
            "created_at": "2024-09-24T17:53:05.414000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "i would recommend Ollama, since it works just fine with BAML if you're looking for open-source models",
        "metadata": {
            "message_id": 1288196659246923827,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:53:37.101000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://github.com/BoundaryML/baml-examples/pull/30 here's an example with ollama + typescript",
        "metadata": {
            "message_id": 1288196780747391017,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:54:06.069000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "this is one way to simplify your prompt as well: https://www.promptfiddle.com/New-Project-xYuhf",
        "metadata": {
            "message_id": 1288197340121006153,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:56:19.434000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I also recommend doing validation of actual schema in your TS/Python code. LLMs can be bad at counting letters for example.\n\nWe will be adding programmatic validations to BAML like this: https://boundary-preview-0c038fad-a0a4-4706-8cdc-c48c097b232d.docs.buildwithfern.com/docs/calling-baml/assertions in the next 2 weeks",
        "metadata": {
            "message_id": 1288197833429614642,
            "username": ".aaronv",
            "created_at": "2024-09-24T17:58:17.048000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Downloading new model, will update when finished.",
        "metadata": {
            "message_id": 1288198894387204117,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:02:30+00:00",
            "edited_at": null
        }
    },
    {
        "text": "This is dope",
        "metadata": {
            "message_id": 1288199061450657915,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:03:09.831000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Is this in a beta branch anywhere?",
        "metadata": {
            "message_id": 1288199228807712840,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:03:49.732000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@503733199130722314>  will let you know once this is live! Feel free to give us any feedback on the syntax / ideas.",
        "metadata": {
            "message_id": 1288199431207780444,
            "username": ".aaronv",
            "created_at": "2024-09-24T18:04:37.988000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Regex?",
        "metadata": {
            "message_id": 1288200068289269850,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:07:09.880000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oh nevermind",
        "metadata": {
            "message_id": 1288200145720184904,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:07:28.341000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "(this|matches(\"^[A-Z]{1,2}[0-9][A-Z0-9]? [0-9][ABD-HJLNP-UW-Z]{2}$\") and block.country == \"UK\"),",
        "metadata": {
            "message_id": 1288200162073903184,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:07:32.240000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yep exactly",
        "metadata": {
            "message_id": 1288200182286258317,
            "username": ".aaronv",
            "created_at": "2024-09-24T18:07:37.059000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "This is dope I'm excited",
        "metadata": {
            "message_id": 1288200229656596481,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:07:48.353000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "sweeet, we are too",
        "metadata": {
            "message_id": 1288200270215385180,
            "username": ".aaronv",
            "created_at": "2024-09-24T18:07:58.023000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "some easy built in functions mgiht be nice:\n```ts\n  email: z.string().email(),\n  website: z.string().url(),\n```\nare two I'm currently using in zod",
        "metadata": {
            "message_id": 1288200695119613972,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:09:39.328000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Maybe some way to do comparisons extracted from strings:\n```ts\nconst OpeningHourSchema = z\n  .string()\n  .regex(\n    new RegExp(\n      `^${dayPattern.source} ${timePattern.source}-${timePattern.source}$`,\n    ),\n  )\n  .refine((value) => {\n    const [days, times] = value.split(\" \");\n    const [openTime, closeTime] = times.split(\"-\");\n    return openTime < closeTime;\n  }, {\n    message: \"Opening time must be before closing time\",\n  });\n```\n\n(lol looking at this I realize it's not right, but w/e genereated this code with an LLM)",
        "metadata": {
            "message_id": 1288201171118325851,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:11:32.815000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "yes you will be able to do stuff like this. And we will add some built-ins one so you can just do:\n```\n   myEmail string @isEmail\n```",
        "metadata": {
            "message_id": 1288202883258515476,
            "username": ".aaronv",
            "created_at": "2024-09-24T18:18:21.021000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "`Meta-Llama-3.1-8B.Q5_1.gguf` failed.",
        "metadata": {
            "message_id": 1288212263995572235,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:55:37.563000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I don't want to sign the stuff to get llama 3.1",
        "metadata": {
            "message_id": 1288212355477536801,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:55:59.374000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "basically because I don't want to read it.",
        "metadata": {
            "message_id": 1288212385047511146,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:56:06.424000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "same error",
        "metadata": {
            "message_id": 1288212393192591474,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:56:08.366000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "waiting for response from Jan",
        "metadata": {
            "message_id": 1288212406694182962,
            "username": "indifferentghost",
            "created_at": "2024-09-24T18:56:11.585000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hmm there seems to be something going on with Jan itself, maybe the API that's being sent in particular\ncodeninja also didn't work.",
        "metadata": {
            "message_id": 1288359042938441829,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:38:52.390000+00:00",
            "edited_at": "2024-09-25T04:39:02.270000+00:00"
        }
    },
    {
        "text": "More logs:\n```log\n2024-09-25T04:39:32.048Z [SERVER]::{\"reqId\":\"req-1\",\"res\":{},\"req\":{\"method\":\"POST\",\"url\":\"/v1/chat/completions\",\"hostname\":\"127.0.0.1:1337\"},\"msg\":\"incoming request\"}\n2024-09-25T04:39:32.064Z [CORTEX]::Debug: 20240925 04:39:32.063881 UTC 1045343 INFO  Request 2, model codeninja-1.0-7b: Generating response for inference request - llama_engine.cc:469\n20240925 04:39:32.063950 UTC 1045343 INFO  Request 2: Stop words:null\n - llama_engine.cc:486\n20240925 04:39:32.064055 UTC 1045343 ERROR Unhandled exception in /inferences/server/chat_completion, what(): Type is not convertible to string - HttpAppFrameworkImpl.cc:124\n\n2024-09-25T04:39:32.065Z [SERVER]::{\"reqId\":\"req-1\",\"res\":{\"statusCode\":500},\"req\":{},\"msg\":\"request completed\",\"responseTime\":16.647541001439095}\n```",
        "metadata": {
            "message_id": 1288359363190325260,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:40:08.744000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah this seems like a Jan issue",
        "metadata": {
            "message_id": 1288359633450438728,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:41:13.179000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oops meant to post that in the Jan server.",
        "metadata": {
            "message_id": 1288359772088963164,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:41:46.233000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Actually it may be this? https://github.com/BoundaryML/baml/issues/983",
        "metadata": {
            "message_id": 1288360413859418175,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:44:19.243000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "It may be us…. We will patch this tomorrow if so",
        "metadata": {
            "message_id": 1288360624690171924,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:45:09.509000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@290334365018357760> ill double check the openai spec tomorrrow",
        "metadata": {
            "message_id": 1288361334538375220,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:47:58.750000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm trying to revert to 0.53.1",
        "metadata": {
            "message_id": 1288361394445615125,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:48:13.033000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "just to see if it works.",
        "metadata": {
            "message_id": 1288361408588808193,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:48:16.405000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oh wow, can confirm",
        "metadata": {
            "message_id": 1288361608330088549,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:49:04.027000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "works on 0.53.1",
        "metadata": {
            "message_id": 1288361616852779040,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:49:06.059000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "You will need to run the generate command manually from the terminal using that version (or install 0.53.1 version of vscode as well)",
        "metadata": {
            "message_id": 1288361617792176139,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:49:06.283000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ok sorry for not catcjing earlier!",
        "metadata": {
            "message_id": 1288361667767439455,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:49:18.198000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "It's all good, I'm glad we could find it.",
        "metadata": {
            "message_id": 1288361869563662397,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:50:06.310000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Switching from langchain to baml saves 10 seconds per transform btw",
        "metadata": {
            "message_id": 1288362109779836948,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:51:03.582000+00:00",
            "edited_at": "2024-09-25T04:51:14.280000+00:00"
        }
    },
    {
        "text": "We knew we were good but not thaaat good. Awesome to hear",
        "metadata": {
            "message_id": 1288362552333565973,
            "username": ".aaronv",
            "created_at": "2024-09-25T04:52:49.095000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I also went from 180 lines to 85 and removed 4 dependencies.",
        "metadata": {
            "message_id": 1288364311026208823,
            "username": "indifferentghost",
            "created_at": "2024-09-25T04:59:48.400000+00:00",
            "edited_at": "2024-09-25T05:00:21.121000+00:00"
        }
    },
    {
        "text": "> removed 74 packages, and audited 329 packages in 1s",
        "metadata": {
            "message_id": 1288364630254817291,
            "username": "indifferentghost",
            "created_at": "2024-09-25T05:01:04.510000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Daaamn",
        "metadata": {
            "message_id": 1288364679143362591,
            "username": ".aaronv",
            "created_at": "2024-09-25T05:01:16.166000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Now we just have to get more people switched haha",
        "metadata": {
            "message_id": 1288364784412000277,
            "username": ".aaronv",
            "created_at": "2024-09-25T05:01:41.264000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm definitely an evangelist now.",
        "metadata": {
            "message_id": 1288365152776749118,
            "username": "indifferentghost",
            "created_at": "2024-09-25T05:03:09.089000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Jan also confirmed that that is indeed the problem.",
        "metadata": {
            "message_id": 1288365598291660842,
            "username": "indifferentghost",
            "created_at": "2024-09-25T05:04:55.308000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Cool, we’ll have a solution tomorrow",
        "metadata": {
            "message_id": 1288366358916239461,
            "username": ".aaronv",
            "created_at": "2024-09-25T05:07:56.655000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "The issue with open-source models is now fixed in version 0.57.0",
        "metadata": {
            "message_id": 1289347050084896818,
            "username": ".aaronv",
            "created_at": "2024-09-27T22:04:51.650000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If you guys haven't been introduced to crawlee, the two separate technologies together beat anything out of the water that's trying to \"have AI scrape the web\"",
        "metadata": {
            "message_id": 1289035744412696697,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:27:50.594000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1289041327014809706,
            "username": "hellovai",
            "created_at": "2024-09-27T01:50:01.590000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "What do they do? Just scrape?",
        "metadata": {
            "message_id": 1289041332265811970,
            "username": "hellovai",
            "created_at": "2024-09-27T01:50:02.842000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We should make some tools that do this stuff for people 😂",
        "metadata": {
            "message_id": 1289041391162359808,
            "username": "hellovai",
            "created_at": "2024-09-27T01:50:16.884000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Or demos",
        "metadata": {
            "message_id": 1289041404571549706,
            "username": "hellovai",
            "created_at": "2024-09-27T01:50:20.081000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We have a few customers that use scrapingbee",
        "metadata": {
            "message_id": 1289041534620139595,
            "username": "hellovai",
            "created_at": "2024-09-27T01:50:51.087000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yeah, they're a layer built ontop of playwright, puppeteer and cheerio that make it easy to scrape.",
        "metadata": {
            "message_id": 1289041729248297004,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:51:37.490000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "The framework they build is similar to the state machines I used at MX finance for similar things.",
        "metadata": {
            "message_id": 1289041824266194966,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:52:00.144000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oh that’s cool. I’ll take a look this weekend and see if I can hack a starter repo in baml-examples or something",
        "metadata": {
            "message_id": 1289041861830377606,
            "username": "hellovai",
            "created_at": "2024-09-27T01:52:09.100000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Oh super cake I can throw some stuff up. The reason I brink it up is because I've seen like three HTML -> LLM products this week.",
        "metadata": {
            "message_id": 1289042110565318789,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:53:08.403000+00:00",
            "edited_at": "2024-09-27T01:53:17.996000+00:00"
        }
    },
    {
        "text": "🤦‍♂️",
        "metadata": {
            "message_id": 1289042175883087924,
            "username": "hellovai",
            "created_at": "2024-09-27T01:53:23.976000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "That’d be amazing",
        "metadata": {
            "message_id": 1289042198813347982,
            "username": "hellovai",
            "created_at": "2024-09-27T01:53:29.443000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```ts\n// main.tsx\nimport { PlaywrightCrawler, ProxyConfiguration } from 'crawlee';\nimport { router } from './nogalas-router';\n\nconst startUrls = ['https://tombstonechamber.com/directory/']\n\nconst crawler = new PlaywrightCrawler({\n    // proxyConfiguration: new ProxyConfiguration({ proxyUrls: ['...'] }),\n    requestHandler: router,\n    // Comment this option to scrape the full website.\n    maxRequestsPerMinute: 100,\n    maxConcurrency: 10,\n});\n\nawait crawler.run(startUrls);\n```\n```ts\n// nogalas-router\nimport { createPlaywrightRouter } from 'crawlee';\nimport { Page } from 'playwright';\nimport Turndown from 'turndown';\n\nimport { b } from \"../baml_client/index.js\";\nimport type { Business } from \"../baml_client/types.js\";\n\nexport const router = createPlaywrightRouter();\n\nlet first = true;\n\nconst handlePopup = async (page: Page) => {\n    const popup = page.locator('#popup-widget316646-close-icon');\n    try {\n        await popup.waitFor({ timeout: 500 })\n    } finally {\n      if (await popup.isVisible()) {\n        await popup.click();\n      }\n    }\n}\n\nrouter.addDefaultHandler(async ({ enqueueLinks, log, page, session }) => {\n    await handlePopup(page);\n    if (first) {\n        session?.setCookie('wam_widgets_popup_closed_65231df9-8e76-4997-93d8-20fc764b2d56_1726460451249=true', 'https://thenogaleschamber.org/')\n        log.info(`enqueueing new URLs`);\n        await page.getByText('More', { exact: true }).click();\n\n        await enqueueLinks({\n            selector: '[id=\"more-316417\"] [data-ux=\"NavListNested\"] [role=\"link\"]',\n            strategy: 'same-origin',\n            label: 'html'\n        });\n    }\n});\n\nconst turndownService = new Turndown()\n\nrouter.addHandler('html', async ({ request, page, log, pushData }) => {\n    const title = await page.title();\n    log.info(`${title}`, { url: request.loadedUrl });\n    await handlePopup(page);\n\n    const values = await page.locator('[data-ux=\"ContentCard\"]').all()\n    const resolved = await Promise.allSettled(values.map(v => v.innerHTML()));\n    const clean = resolved\n        .filter((value): value is PromiseFulfilledResult<string> => value.status === 'fulfilled')\n        .map(({ value }) => turndownService.turndown(value));\n\n    await pushData({ title, url: request.loadedUrl, md: clean; });\n});\n```\n\nI end up post processing the `value` with `b.ValidateBusinessInfo` because my PC is slowwww",
        "metadata": {
            "message_id": 1289043395465052261,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:58:14.747000+00:00",
            "edited_at": "2024-09-27T01:59:19.426000+00:00"
        }
    },
    {
        "text": "I've noticed that markdown works better than HTML for most things.",
        "metadata": {
            "message_id": 1289043450221559870,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:58:27.802000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "turndown is just html -> md as a proper parser.",
        "metadata": {
            "message_id": 1289043486825250989,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:58:36.529000+00:00",
            "edited_at": "2024-09-27T01:58:41.894000+00:00"
        }
    },
    {
        "text": "Got it. We’re thinking about adding more string types into BAML officially. I wonder if HTML and MD make good first class citizens",
        "metadata": {
            "message_id": 1289044710551457925,
            "username": "hellovai",
            "created_at": "2024-09-27T02:03:28.288000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’ll pick your brain on that soon with a doc 😅",
        "metadata": {
            "message_id": 1289044751030554647,
            "username": "hellovai",
            "created_at": "2024-09-27T02:03:37.939000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "MD would be hard because of all the flavors, is there a weak-type available in BAML?",
        "metadata": {
            "message_id": 1289045337335398412,
            "username": "indifferentghost",
            "created_at": "2024-09-27T02:05:57.725000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Not yet. But we can do whatever seems right.",
        "metadata": {
            "message_id": 1289047909144465519,
            "username": "hellovai",
            "created_at": "2024-09-27T02:16:10.892000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "We should spend time talking about this in more details",
        "metadata": {
            "message_id": 1289047909823676548,
            "username": "hellovai",
            "created_at": "2024-09-27T02:16:11.054000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Honestly has been a pleasurable experience.",
        "metadata": {
            "message_id": 1289035835706179668,
            "username": "indifferentghost",
            "created_at": "2024-09-27T01:28:12.360000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Crawlee",
        "metadata": {
            "message_id": 1289041328176369810,
            "username": "hellovai",
            "created_at": "2024-09-27T01:50:01.867000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@1289884980603916319> did you figure out yoru azure issue?",
        "metadata": {
            "message_id": 1289932643118350359,
            "username": "hellovai",
            "created_at": "2024-09-29T12:51:47.909000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hello,\nI found out about Baml earlier and I've been trying to use it in a SvelteKit (TypeScript) application.\n\nBut encountering and issue.\n\nWhenever I do `import { b } from '$lib/baml_client';` in a file, I get this error\n\n```\n[vite] Named export 'BamlLogEvent' not found. The requested module '@boundaryml/baml' is a CommonJS module, which may not support all module.exports as named exports.\nCommonJS modules can always be imported via the default export, for example using:\n\nimport pkg from '@boundaryml/baml';\nconst {BamlLogEvent} = pkg;\n```",
        "metadata": {
            "message_id": 1289962302224990248,
            "username": "segbedji",
            "created_at": "2024-09-29T14:49:39.191000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1289962441962426441,
            "username": "segbedji",
            "created_at": "2024-09-29T14:50:12.507000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "It seems to be coming from `baml_client/tracing.ts`",
        "metadata": {
            "message_id": 1289962444244123679,
            "username": "segbedji",
            "created_at": "2024-09-29T14:50:13.051000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Named export 'BamlLogEvent' not found",
        "metadata": {
            "message_id": 1289962539593240597,
            "username": "segbedji",
            "created_at": "2024-09-29T14:50:35.784000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Here's my tracing.ts file\n\n```\n/*************************************************************************************************\n\nWelcome to Baml! To use this generated code, please run one of the following:\n\n$ npm install @boundaryml/baml\n$ yarn add @boundaryml/baml\n$ pnpm add @boundaryml/baml\n\n*************************************************************************************************/\n\n// This file was generated by BAML: do not edit it. Instead, edit the BAML\n// files and re-generate this code.\n//\n/* eslint-disable */\n// tslint:disable\n// @ts-nocheck\n// biome-ignore format: autogenerated code\nimport { BamlLogEvent } from '@boundaryml/baml';\nimport { DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX } from './globals';\n\nconst traceAsync =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnAsync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst traceSync =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.traceFnSync.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst setTags =\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.upsertTags.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)\nconst flush = () => {\n  DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.flush.bind(DO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX)()\n}\nconst onLogEvent = (callback: undefined | ((event: BamlLogEvent) => void)) =>\nDO_NOT_USE_DIRECTLY_UNLESS_YOU_KNOW_WHAT_YOURE_DOING_CTX.onLogEvent(callback)\n\nexport { traceAsync, traceSync, setTags, flush, onLogEvent }\n```",
        "metadata": {
            "message_id": 1289962872914575442,
            "username": "segbedji",
            "created_at": "2024-09-29T14:51:55.254000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "cc <@99252724855496704>",
        "metadata": {
            "message_id": 1289962890304163931,
            "username": "segbedji",
            "created_at": "2024-09-29T14:51:59.400000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I'm on `0.57.0`\n\nI downgrade up to 0.54.0, but still getting the same issue",
        "metadata": {
            "message_id": 1289963682809384980,
            "username": "segbedji",
            "created_at": "2024-09-29T14:55:08.348000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hmm this is quite odd! <@201399017161097216> can you take a look?\n\nCan you try 0.53.*?\n\nI wonder if our imports don't work well with SveltKit for some reason. It looks like we need different ways of importing it?\n\nIs SveltKit client side only?",
        "metadata": {
            "message_id": 1289971392460357695,
            "username": "hellovai",
            "created_at": "2024-09-29T15:25:46.472000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "or is it server side?",
        "metadata": {
            "message_id": 1289971412546883756,
            "username": "hellovai",
            "created_at": "2024-09-29T15:25:51.261000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ah i get it, yes we should document this better.\n\nAs of now, BAML can only be used in backend code. This is because of API keys used in BAML.\n\nWe can work on a way of making it functional in frontend code if that would help out there? Are you not worried about embedding API keys into your brwoser? (That would allow users to accidentally gain access to your API keys when they go to your site)",
        "metadata": {
            "message_id": 1289972896638439445,
            "username": "hellovai",
            "created_at": "2024-09-29T15:31:45.096000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> Can you try 0.53.*?\n\nSame error on that version as well",
        "metadata": {
            "message_id": 1289975965186920458,
            "username": "segbedji",
            "created_at": "2024-09-29T15:43:56.695000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> As of now, BAML can only be used in backend code. This is because of API keys used in BAML.\n\nI'm actually doing the import in backend Nodejs code, in a `+page.server.ts`\n\nSo we it's likely not the cause",
        "metadata": {
            "message_id": 1289976283970670724,
            "username": "segbedji",
            "created_at": "2024-09-29T15:45:12.699000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "hmm, yea i think <@201399017161097216> can take it on when he's awake. (I'm out traveling this weekend)",
        "metadata": {
            "message_id": 1289976593170567251,
            "username": "hellovai",
            "created_at": "2024-09-29T15:46:26.418000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Could it be an import that's actually missing in the npm package?\n\nWhen I look at `nodes_modules/@boundaryml/balm`, there's no `BamlLogEvent` being exported.",
        "metadata": {
            "message_id": 1289976811349872762,
            "username": "segbedji",
            "created_at": "2024-09-29T15:47:18.436000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "> hmm, yea i think @aaronv can take it on when he's awake. (I'm out traveling this weekend)\n\nThank you 🙂",
        "metadata": {
            "message_id": 1289976859299287040,
            "username": "segbedji",
            "created_at": "2024-09-29T15:47:29.868000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Ill take a look in 30min!",
        "metadata": {
            "message_id": 1290002919097237535,
            "username": ".aaronv",
            "created_at": "2024-09-29T17:31:03.008000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "So baml can only be used in your svelte server code, it cant run on client components / browser",
        "metadata": {
            "message_id": 1290003138920714332,
            "username": ".aaronv",
            "created_at": "2024-09-29T17:31:55.418000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "But ill try to reproduce in a svelte starter app",
        "metadata": {
            "message_id": 1290003193090150420,
            "username": ".aaronv",
            "created_at": "2024-09-29T17:32:08.333000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I see the issue, will patch it",
        "metadata": {
            "message_id": 1290025838875639959,
            "username": ".aaronv",
            "created_at": "2024-09-29T19:02:07.509000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "mind posting your vite configuration? Do you use JS or TS?",
        "metadata": {
            "message_id": 1290028608076648590,
            "username": ".aaronv",
            "created_at": "2024-09-29T19:13:07.738000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "```\n// vite.config.ts\n\nimport { sveltekit } from '@sveltejs/kit/vite';\nimport { defineConfig } from 'vite';\n\nexport default defineConfig({\n    plugins: [sveltekit()],\n    optimizeDeps: {\n        exclude: ['oslo']\n    }\n});\n```",
        "metadata": {
            "message_id": 1290029231937556554,
            "username": "segbedji",
            "created_at": "2024-09-29T19:15:36.478000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Using TypeScript",
        "metadata": {
            "message_id": 1290029267916292249,
            "username": "segbedji",
            "created_at": "2024-09-29T19:15:45.056000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "so i fixed the type issue, which should be on the next release but sveltekit should just be ignoring those files anyway, so that's what I'm looking into (almost done). \n\nAre you on svelte 4 or 5?",
        "metadata": {
            "message_id": 1290042040591061034,
            "username": ".aaronv",
            "created_at": "2024-09-29T20:06:30.299000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I’m on Svelte 4",
        "metadata": {
            "message_id": 1290043602994593845,
            "username": "segbedji",
            "created_at": "2024-09-29T20:12:42.805000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1290043648825753701,
            "username": "segbedji",
            "created_at": "2024-09-29T20:12:53.732000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "ok this is fixed in 0.57.1!",
        "metadata": {
            "message_id": 1290064582953533451,
            "username": ".aaronv",
            "created_at": "2024-09-29T21:36:04.817000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yes, thanks for your concern. But I'm still exploring the usage of BAML.",
        "metadata": {
            "message_id": 1290133809777606678,
            "username": "jcanha1",
            "created_at": "2024-09-30T02:11:09.778000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1290188389991190578,
            "username": "hellovai",
            "created_at": "2024-09-30T05:48:02.715000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "If you ever have questions, please let us know and we’ll be glad to help!",
        "metadata": {
            "message_id": 1290188400871346216,
            "username": "hellovai",
            "created_at": "2024-09-30T05:48:05.309000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Baml usage",
        "metadata": {
            "message_id": 1290188389991190583,
            "username": "hellovai",
            "created_at": "2024-09-30T05:48:02.715000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hello, last night I was on a flight from Detroit sitting next to someone from your Company.  I failed to get his name, but we chatted a bit and I said I would follow up.  I work with a branded merch sourcing company in Seattle and he mentioned your company was looking to make some orders in that space",
        "metadata": {
            "message_id": 1290341955263401984,
            "username": "jasonstob",
            "created_at": "2024-09-30T15:58:15.529000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1290342894586302575,
            "username": "hellovai",
            "created_at": "2024-09-30T16:01:59.481000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey Calvin! That was me! Feel free to email me: vbv@boundaryml.com",
        "metadata": {
            "message_id": 1290342897027252376,
            "username": "hellovai",
            "created_at": "2024-09-30T16:02:00.063000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks. Will touch base with you shortly",
        "metadata": {
            "message_id": 1290345027628830783,
            "username": "jasonstob",
            "created_at": "2024-09-30T16:10:28.038000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Do you know who I could speak with to follow up on this?",
        "metadata": {
            "message_id": 1290342050683945073,
            "username": "jasonstob",
            "created_at": "2024-09-30T15:58:38.279000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Literals",
        "metadata": {
            "message_id": 1290721397131313184,
            "username": "hellovai",
            "created_at": "2024-10-01T17:06:01.520000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@622036251863679006> is working on literals! He'll keep thiso thread updated on timelines.",
        "metadata": {
            "message_id": 1290721421672058904,
            "username": "hellovai",
            "created_at": "2024-10-01T17:06:07.371000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@740363257814057004> for ctx",
        "metadata": {
            "message_id": 1290721451120529519,
            "username": "hellovai",
            "created_at": "2024-10-01T17:06:14.392000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Hey what's up 👋, <@740363257814057004>  timelines are a little hard to estimate since I'm still new to the codebase but I think it should take roughly 3 weeks to get a basic working implementation of literals (strings, ints and bools). Rest of the data types remains to be seen.",
        "metadata": {
            "message_id": 1290729057520193576,
            "username": "antoniosarosi",
            "created_at": "2024-10-01T17:36:27.899000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "understood, i'll follow the thread here",
        "metadata": {
            "message_id": 1290736713706242068,
            "username": "faizansattar",
            "created_at": "2024-10-01T18:06:53.276000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> maybe it would be helpful for you to share the Sherlock use case with <@622036251863679006> so I can better understand what release will unlock strucutured UI outputs for us",
        "metadata": {
            "message_id": 1290736894203662387,
            "username": "faizansattar",
            "created_at": "2024-10-01T18:07:36.310000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "FYI as an update, we may be able to get an initial version of this out next week thanks to some great work by <@622036251863679006>",
        "metadata": {
            "message_id": 1291795341598785606,
            "username": "hellovai",
            "created_at": "2024-10-04T16:13:29.834000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Emphasis on **we may** 🤣",
        "metadata": {
            "message_id": 1291795468078153749,
            "username": "antoniosarosi",
            "created_at": "2024-10-04T16:13:59.989000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "@everyone We'll be hosting a quick talk + live coding session this Friday about how to do some good old prompt engineering mostly focused on agentic workflows (think multiple LLM calls performing a larger action).\n\nThe first half will be a discussion and the second half will be code!\n\nSwing by if you think it'll be helpful and please share on linked in (Since we're hosting this, we'll also record it and post that here - last event wasn't recorded by the host)\nhttps://lu.ma/sjn31d8l\n\nI'll be joined by community member <@740363257814057004> as well!",
        "metadata": {
            "message_id": 1291007910096273469,
            "username": "hellovai",
            "created_at": "2024-10-02T12:04:31.534000+00:00",
            "edited_at": "2024-10-02T16:30:35.728000+00:00"
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1291012363226578964,
            "username": "hellovai",
            "created_at": "2024-10-02T12:22:13.243000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "\\\\",
        "metadata": {
            "message_id": 1291012368368668726,
            "username": "hellovai",
            "created_at": "2024-10-02T12:22:14.469000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Amazing!! I’ll be there. \n\nI’d love to see some technique on anthropic caching",
        "metadata": {
            "message_id": 1291013550252494880,
            "username": "seawatts",
            "created_at": "2024-10-02T12:26:56.252000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I was planning on talking more about pipelines a bit more, but i'll cover caching a bit closer to the end!",
        "metadata": {
            "message_id": 1291015027977093184,
            "username": "hellovai",
            "created_at": "2024-10-02T12:32:48.569000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Great topic!  Shared link with some team members as well.",
        "metadata": {
            "message_id": 1291015755638374520,
            "username": "mikef206",
            "created_at": "2024-10-02T12:35:42.057000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "I didn't realize you too are an early  riser as well <@764573625629933569> 😅",
        "metadata": {
            "message_id": 1291016144806740060,
            "username": "hellovai",
            "created_at": "2024-10-02T12:37:14.842000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Yep!  6-10 in the morning busiest for activities spanning multiple teams.",
        "metadata": {
            "message_id": 1291017402955337770,
            "username": "mikef206",
            "created_at": "2024-10-02T12:42:14.808000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Awesome! Will be there 🙂",
        "metadata": {
            "message_id": 1291098459612512266,
            "username": "davidyoung",
            "created_at": "2024-10-02T18:04:20.221000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@99252724855496704> and <@201399017161097216> that was great!! Is this a one-off thing or will be periodic?",
        "metadata": {
            "message_id": 1291878792591184012,
            "username": "demontrius",
            "created_at": "2024-10-04T21:45:06.101000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "we're going to do more. Aiming to do it weekly",
        "metadata": {
            "message_id": 1291890839924117654,
            "username": ".aaronv",
            "created_at": "2024-10-04T22:32:58.409000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Do you guys have a link for the recording? I wasn't able to make it today :/",
        "metadata": {
            "message_id": 1291939772750893076,
            "username": "charizard_98",
            "created_at": "2024-10-05T01:47:24.904000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "it'll be posted soon!",
        "metadata": {
            "message_id": 1292543733824491571,
            "username": ".aaronv",
            "created_at": "2024-10-06T17:47:20.445000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@410093421420871680> here's a replay https://youtu.be/RnyicgiSkis?t=552",
        "metadata": {
            "message_id": 1293002194164908107,
            "username": ".aaronv",
            "created_at": "2024-10-08T00:09:05.908000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thank you so much!",
        "metadata": {
            "message_id": 1293002261802516622,
            "username": "charizard_98",
            "created_at": "2024-10-08T00:09:22.034000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "you can skip to like the last hour if you want more than just intro stuff",
        "metadata": {
            "message_id": 1293002419566809130,
            "username": ".aaronv",
            "created_at": "2024-10-08T00:09:59.648000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Prompt Shepard: AI Agents with Determins...",
        "metadata": {
            "message_id": 1291012364149194847,
            "username": "hellovai",
            "created_at": "2024-10-02T12:22:13.463000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "@everyone If anyone is interested, around 8 am (25 mins from now) PST, we're going to be live coding in the Office Hours channel to build a pipeline that can take the 90 minute transcript from <#1291007910096273469> and make a great email out of it.",
        "metadata": {
            "message_id": 1293220665943658496,
            "username": "hellovai",
            "created_at": "2024-10-08T14:37:13.639000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Link please",
        "metadata": {
            "message_id": 1293220962828943421,
            "username": "demontrius",
            "created_at": "2024-10-08T14:38:24.422000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Sorry, discord was wonky on quality: https://us06web.zoom.us/j/85655636334?pwd=K9efSMbuvGqAWGIz950kRn4Za6aRrX.1",
        "metadata": {
            "message_id": 1293227210408464385,
            "username": "hellovai",
            "created_at": "2024-10-08T15:03:13.961000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "https://github.com/ytang07/baml-email-writing - the audio file is a bit large",
        "metadata": {
            "message_id": 1293242898556518443,
            "username": ".rathesungod",
            "created_at": "2024-10-08T16:05:34.307000+00:00",
            "edited_at": "2024-10-08T16:05:49.974000+00:00"
        }
    },
    {
        "text": "we have recording?",
        "metadata": {
            "message_id": 1293332678959042654,
            "username": "gabriel_syme",
            "created_at": "2024-10-08T22:02:19.623000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "<@147185014117892096> figured the notebook setup on Google Colab... <@99252724855496704> the walkthrough was helpful\nHere is the repo using Google Colab https://github.com/kinghendrix10/baml-notebook.",
        "metadata": {
            "message_id": 1293765424143667230,
            "username": "demontrius",
            "created_at": "2024-10-10T02:41:54.115000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "",
        "metadata": {
            "message_id": 1293783760151777341,
            "username": ".aaronv",
            "created_at": "2024-10-10T03:54:45.760000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Thanks for setting this up! We’ll link to this in one of our guides",
        "metadata": {
            "message_id": 1293783784264699924,
            "username": ".aaronv",
            "created_at": "2024-10-10T03:54:51.509000+00:00",
            "edited_at": null
        }
    },
    {
        "text": "Notebook",
        "metadata": {
            "message_id": 1293783760151777346,
            "username": ".aaronv",
            "created_at": "2024-10-10T03:54:45.760000+00:00",
            "edited_at": null
        }
    }
]